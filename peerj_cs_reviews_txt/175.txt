Review History for Software engineering principles to improve quality and performance of R software [PeerJ]
PeerJ Computer Science PeerJ – the Journal of Life & Environmental Sciences PeerJ Computer Science PeerJ Physical Chemistry PeerJ Organic Chemistry PeerJ Inorganic Chemistry PeerJ Analytical Chemistry PeerJ Materials Science Visit PeerJ.org and get involved About PeerJ Journals Overview PeerJ Journals FAQ What we publish 5 Years publishing Solutions for authors Reputation High quality peer review Fast publishing Indexing and Impact Factor Global readership Feature comparison Reduced cost publishing Author feedback Early career researcher benefits Senior researcher benefits Open review (optional) Rebuttal letters Sections About the journal Sections Aquatic Biology Biochemistry, Biophysics and Molecular Biology Biodiversity and Conservation Bioinformatics and Genomics Brain and Cognition Ecology Environmental Science Microbiology Paleontology and Evolutionary Science Plant Biology Zoological Science About PeerJ Journals Overview PeerJ Journals FAQ What we publish 5 Years publishing Solutions for authors Reputation High quality peer review Fast publishing Indexing and Impact Factor Global readership Feature comparison Reduced cost publishing Author feedback Early career researcher benefits Senior researcher benefits Open review (optional) Rebuttal letters More Subjects Search articles Peer-reviewed Journals PeerJ (Life, Biological, Environmental and Health Sciences) PeerJ Computer Science PeerJ Physical Chemistry PeerJ Organic Chemistry PeerJ Inorganic Chemistry PeerJ Analytical Chemistry PeerJ Materials Science Preprints PeerJ Preprints Table of contents Table of Contents - current and archives PeerJ - Medicine articles PeerJ - Biology & Life science articles PeerJ - Environmental Science articles PeerJ - General bio (stats, legal, policy, edu) PeerJ Computer Science PeerJ Preprints Academic advisors Volunteer to review Collections Job listings Discussions Blog Institutional plans Reviews and awards Spread the word Who are we? Contact Login AUTHORS Peer Journals Overview Submission Guidelines Subject Areas Editorial Board Editorial Criteria Pricing General FAQ Computer Science FAQ Aims and Scope Author Interviews Policies and Procedures SUBMIT ARTICLE
Review History Software engineering principles to improve quality and performance of R software To increase transparency, PeerJ operates a system of 'optional signed reviews and history'. This takes two forms: (1) peer reviewers are encouraged, but not required, to provide their names (if they do so, then their profile page records the articles they have reviewed), and (2) authors are given the option of reproducing their entire peer review history alongside their published article (in which case the complete peer review process is provided, including revisions, rebuttal letters and editor decision letters). New to public reviews? Learn more about optional signed reviews and how to write a better rebuttal letter .
Summary
The initial submission of this article was received on October 12th, 2018 and was peer-reviewed by 2 reviewers and the Academic Editor. The Academic Editor made their initial decision on October 30th, 2018. The first revision was submitted on December 26th, 2018 and was reviewed by 1 reviewer and the Academic Editor. The article was Accepted by the Academic Editor on January 11th, 2019.
label_version_1
Version 0.2 (accepted)
Sebastian Ventura
·
Jan 11, 2019
label_recommendation_1
·
Academic Editor
Accept
This new version improves considerably the previous one. Now the paper is ready to be accepted. Congratulations.
label_author_1
Reviewer 3 ·
Jan 10, 2019
Basic reporting
label_br_1
See the general comments.
Experimental design
label_ed_1
See the general comments.
Validity of the findings
label_votf_1
The discussion and conclusions are well stated, they are limited to the results obtained.
Comments for the author
label_cfta_1
The paper is interesting and motivating. The main goal and motivation of the paper are clear. Nowadays, the development of scientific software needs a guide to optimize and test the code and, therefore, to improve software maintenance. Generally speaking, I consider the work has been significantly improved. The authors have effectively addressed all the comments raised in the previous version. I think that this work can be accepted for publication. Cite this review as
Anonymous Reviewer ( 2019 ) Peer Review #3 of "Software engineering principles to improve quality and performance of R software (v0.2)" . PeerJ Computer Science https://doi.org/10.7287/peerj-cs.175v0.2/reviews/3 Download Version 0.2 (PDF)
Download author's rebuttal letter
- submitted Dec 26, 2018
label_version_2
Version 0.1 (original submission)
Sebastian Ventura
·
Oct 30, 2018
label_recommendation_2
·
Academic Editor
Major Revisions
As you can check, both reviewers have several interesting comments and corrections to the paper. Please address both of them.
label_author_2
Rory Nolan ·
Oct 14, 2018
Basic reporting
label_br_2
I think for the sake of saving people the need for a dictionary, "autodictacts" should be replaced by "self-taught individuals" and "codified through" should be replaced by "constructed using" or something like that. Line 69 is missing a full stop. Line 129 is missing its last word (years) Line 140 has a misformatted citation (jpresse) Line 166 has "overtime" where it should say "over time" Line 167 should say "percentage" and not "percent" Line 483 has a misformed citation Other than this, the English used is clear and professional and the citations provide nice background/context. The raw data is shared. The article is self-contained with relevant results to hypotheses.
Experimental design
label_ed_2
The research question is well defined, relevant & meaningful. It is stated how this research fills an identified knowledge gap. Rigorous investigation is performed to a high technical and ethical standard. Methods are described with sufficient detail and info te replicate.
Validity of the findings
label_votf_2
The findings are valid and the authors have gone to considerable and sufficient effort to ensure that the data is robust. The conclusions are well stated.
Comments for the author
label_cfta_2
I find the article interesting and extremely relevant. In particular I welcome the improved analysis of unit testing in CRAN packages relative to what I did myself a few years ago. I find the examples of unit testing using the pccc package a little difficult to understand. Sure, they can be understood with some effort, but I think there must be an easier example out there which would permit a more concise code listing. Perhaps the glue package? I know pccc is a niche example and you're probably trying to encourage niche developers to test their packages too, but I think for the sake of example, simpler is better. The study of packages employing optimization is very interesting. However, with testing I think it's easy to say if a package isn't tested, it should be, but a package without obvious optimization attempts could just be very well written (using only vectorized code from other packages) and hence not need more explicit optimizations of its own. I think this should be stated: a package without obvious optimization isn't necessarily in need of optimization. Having said this, figure 3 is interesting, giving the change in explicit optimization efforts over the years. In figure 1, I think the "No" colour should be on the bottom and in particular in fig 4, "None" should be on the bottom, not in the middle. "None" should maybe also be given a striking colour to show that it's different to the other results. It should be made clear in the caption of fig 4 what the n>14 is about. Consider using bench::mark instead of microbenchmark (I read recently that bench is better and the tidyverse people are pushing it now). Thanks for a very nice article. Cite this review as
Nolan R ( 2019 ) Peer Review #1 of "Software engineering principles to improve quality and performance of R software (v0.1)" . PeerJ Computer Science https://doi.org/10.7287/peerj-cs.175v0.1/reviews/1
label_author_3
Greg Wilson ·
Oct 24, 2018
Basic reporting
label_br_3
- Professional structure (but I have asked for some reorganization). - References provided are good, but more are needed to substantiate specific claims (see general discussion below).
Experimental design
label_ed_3
- Data collection and analysis is good.
Validity of the findings
label_votf_3
- Empirical results on testing and performance optimization seem trustworthy. - High-level discussion of rules to follow only partly backed up by empirical evidence (see general discussion).
Comments for the author
General: I feel the authors are trying to: 1. Introduce readers to key ideas and tools in software testing. 2. Survey the current state of software testing for R packages. 3. Do both of the above all over again for performance optimization. The empirical study of how many packages on CRAN do testing and optimization was the most interesting part of the paper for me, but I acknowledge that I'm not representative of likely readers of this paper. I think that the overview of testing the authors give in lines 180-262 is not detailed enough to serve as a tutorial for people who aren't already familiar with the topic, while being too long for people who _are_ already familiar with it. (I feel the same way about the discussion of big-O notation starting at L440.) I therefore recommend that the authors: 1. Put the empirical material on testing and optimization in one section near the front of the paper to show that there is significant room for improvement. 2. Replace the paragraph-length high-level advice on how to test and tune with bulleted lists of rules, each having pointers to longer-form discussions. This will help experienced readers (who will nod at the lists of rules), while also helping newcomers operationalize those rules (which I think they would struggle to do based on the current brief explanations). 3. Expand the examples to show specific applications of the general rules. For example, I would like to see the performance results for the PCCC code on line 413 and following, and then see what changes the authors made to the code to speed it up, and a second set of performance figures. Similarly, in the testing example starting on line 301, I do not know what the bug was that the test found, or how the bug was detected before the test was written and its detection then translated into a test. --- Specific: 28: "we show that reproducible and replicable software tests are not available" -> as written, this is a very strong claim. 39: Introduction of software engineering seems disconnected with preceding material. 69: missing "." between "maintenance" and "Software maintenance" 71: "chief factor" -> isn't people's time the chief factor in _all_ maintenance, not just that for statistical software? 76: "As research scientists tend to think..." -> Are they wrong? I.e., if I assert that the majority of software written by researchers exists to solve one-off problems, rather than to be used repeatedly, is there data to show that I'm wrong? 81: Here and elsewhere, I worry that "best practices" is not validated. There is, for example, no published research showing that the use of version control makes people more productive. (Believe me, I've looked.) I think the authors need to present evidence that various practices actually improve productivity and/or reliability before calling them "best". 102: Do the authors have data showing that commercial software and/or open source software are tested any more frequently? (I've seen a _lot_ of projects on GitHub that don't have any tests...) 140: What is "jpreese"? 149: I'm unclear what is meant by "Grep for...directories" - are the authors working from textual manifests of projects, or are they using "grep" as a synonym for "search for"? 160: should "updated" be "most recently updated"? 166: "over time" rather than "overtime" 172: a table might be a better way to display this summary of "have X but not Y". 184: There is also growing evidence that TDD doesn't actually make developers more productive (see e.g. http://people.brunel.ac.uk/~csstmms/FucciEtAl_ESEM2016.pdf ). 185: "A better approach..." Better by what criteria, and what data can the authors cite to support this contention? 194: "In an ideal world...100% test coverage" Do the authors mean line coverage, statement coverage, branch coverage, combinatorial coverage, ...? 208: "Once answers to these questions are known..." I have found that giving high-level advice like this only frustrates most scientists, because they don't know how to operationalize it. Can the authors point at concrete examples of how to translate these general rules into specific decisions, priorities, and/or tests for specific software packages? 226: "In addition to unit tests, users should perform..." I believe the authors mean "developers", not "users" (but could be wrong). I also think that this statement trips over an important distinction between testing what-if contingencies for software tools (which developers of packages should do), and testing specific users of those tools and their inputs for particular analyses (which analysts doing particular analyses should do). I think that repeatable sanity checks make sense for the latter, but that doesn't necessarily mean use of unit testing frameworks. 233: A notebook demonstrating the use of the software is _not_ the same thing as an acceptance test, though the automatic validation of the expected end result may be. 238: Radcliffe et al have developed a nice framework for thinking about the ways in which data analysis can go wrong, which is summarized in the figure in http://stochasticsolutions.com/pdf/TDDA-One-Pager.pdf . It seems that most of the discussion in this paper is focusing on Step 2 of that model's 5-step process - if so, the authors may wish to cite that model and make this explicit, and if not, expand their recommendations for other phases. 301-302: some odd indentation. 430: "run 10 timess and find the mean" No - this can easily give a misleadingly high performance result because of caching effects. 437: "There are many optimizations that can be considered..." This is the kind of advice that my students used to find frustrating, because it tells readers that something exists without telling them what it is or where to find it. 560: the summary of possible optimizations is very condensed - can the authors point at resources that have lengthier and more detailed coverage? (There are several guides to high performance R...) Cite this review as
Wilson G ( 2019 ) Peer Review #2 of "Software engineering principles to improve quality and performance of R software (v0.1)" . PeerJ Computer Science https://doi.org/10.7287/peerj-cs.175v0.1/reviews/2 Download Original Submission (PDF)
- submitted Oct 12, 2018 All text and materials provided via this peer-review history page are made available under a Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.
About us - PeerJ team | Our publications | Benefits | Partnerships | Endorsements Awards Resources - FAQ | Careers | Pressroom | Terms of use | Privacy | Contact Academic boards - Advisors | Editors | Subject areas Follow us - PeerJ blog | Twitter | Facebook | LinkedIn | Pinterest Submission guides - PeerJ – Life and Environment | PeerJ Computer Science | PeerJ Chemistry Spread the word - Activities | Resources PeerJ feeds - Atom | RSS 1.0 | RSS 2.0 | JSON PeerJ Computer Science feeds - Atom | RSS 1.0 | RSS 2.0 | JSON Archives - PeerJ – Life and Environment | PeerJ Computer Science
©2012-2019 PeerJ, Inc | Public user content licensed CC BY 4.0 unless otherwise specified. PeerJ ISSN: 2167-8359 PeerJ Comput. Sci. ISSN: 2376-5992 PeerJ Preprints ISSN: 2167-9843
computer science
