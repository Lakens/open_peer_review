Review History for The appropriation of GitHub for curation [PeerJ]
PeerJ Computer Science PeerJ – the Journal of Life & Environmental Sciences PeerJ Computer Science PeerJ Physical Chemistry PeerJ Organic Chemistry PeerJ Inorganic Chemistry PeerJ Analytical Chemistry PeerJ Materials Science Visit PeerJ.org and get involved About PeerJ Journals Overview PeerJ Journals FAQ What we publish 5 Years publishing Solutions for authors Reputation High quality peer review Fast publishing Indexing and Impact Factor Global readership Feature comparison Reduced cost publishing Author feedback Early career researcher benefits Senior researcher benefits Open review (optional) Rebuttal letters Sections About the journal Sections Aquatic Biology Biochemistry, Biophysics and Molecular Biology Biodiversity and Conservation Bioinformatics and Genomics Brain and Cognition Ecology Environmental Science Microbiology Paleontology and Evolutionary Science Plant Biology Zoological Science About PeerJ Journals Overview PeerJ Journals FAQ What we publish 5 Years publishing Solutions for authors Reputation High quality peer review Fast publishing Indexing and Impact Factor Global readership Feature comparison Reduced cost publishing Author feedback Early career researcher benefits Senior researcher benefits Open review (optional) Rebuttal letters More Subjects Search articles Peer-reviewed Journals PeerJ (Life, Biological, Environmental and Health Sciences) PeerJ Computer Science PeerJ Physical Chemistry PeerJ Organic Chemistry PeerJ Inorganic Chemistry PeerJ Analytical Chemistry PeerJ Materials Science Preprints PeerJ Preprints Table of contents Table of Contents - current and archives PeerJ - Medicine articles PeerJ - Biology & Life science articles PeerJ - Environmental Science articles PeerJ - General bio (stats, legal, policy, edu) PeerJ Computer Science PeerJ Preprints Academic advisors Volunteer to review Collections Job listings Discussions Blog Institutional plans Reviews and awards Spread the word Who are we? Contact Login AUTHORS Peer Journals Overview Submission Guidelines Subject Areas Editorial Board Editorial Criteria Pricing General FAQ Computer Science FAQ Aims and Scope Author Interviews Policies and Procedures SUBMIT ARTICLE
Review History The appropriation of GitHub for curation To increase transparency, PeerJ operates a system of 'optional signed reviews and history'. This takes two forms: (1) peer reviewers are encouraged, but not required, to provide their names (if they do so, then their profile page records the articles they have reviewed), and (2) authors are given the option of reproducing their entire peer review history alongside their published article (in which case the complete peer review process is provided, including revisions, rebuttal letters and editor decision letters). New to public reviews? Learn more about optional signed reviews and how to write a better rebuttal letter .
Summary
The initial submission of this article was received on April 28th, 2017 and was peer-reviewed by 2 reviewers and the Academic Editor. The Academic Editor made their initial decision on May 26th, 2017. The first revision was submitted on August 10th, 2017 and was reviewed by 1 reviewer and the Academic Editor. A further revision was submitted on September 12th, 2017 and was reviewed by the Academic Editor. The article was Accepted by the Academic Editor on September 13th, 2017.
label_version_1
Version 0.3 (accepted)
Philipp Leitner
·
Sep 13, 2017
label_recommendation_1
·
Academic Editor
Accept
After editorial review, I have come to the decision that the paper is now ready for publication. Well done!
Download Version 0.3 (PDF)
Download author's rebuttal letter
- submitted Sep 12, 2017
label_version_2
Version 0.2
Philipp Leitner
·
Aug 21, 2017
label_recommendation_2
·
Academic Editor
Minor Revisions
I understand and agree with your reluctance to add more quantitative results due to limited sample size. It is my opinion that the experimental design and analysis as presented in this paper is rigorous and adequate, and does not require further revision. Please address the minor reporting issues that have been raised. Specifically, I agree with the reviewer that there still is a bit of unnecessary redundancy between results and discussion, which could be cleaned up further. After these steps, I am sure the paper will be good to go.
label_author_1
Reviewer 2 ·
Aug 21, 2017
Basic reporting
label_br_1
The authors have made substantial changes in this revised version, some of the most important review comments have been addressed. The readability and structure of the paper has improved from the initial version. Thanks also for preparing a "diff" version which made it easier to track the changes from the previous version. A few minor comments: * line 101: "Software Developers’ Motivations in Participating Online Communities" -> "Software Developers’ Motivations for Participating in Online Communities" * line 468: "to keep them up-to-date" -> "to keep themselves up-to-date" * line 502: "a only starting point" -> "only a starting point" * line 508: "was to used ”curated list”" -> "was to use ”curated list”" Please give the paper another thorough pass and check for any orthographic issues, typos, etc.
Experimental design
label_ed_1
Although the design and methodology have been improved (e.g., by adding the coding scheme in Table 1), I still believe that the paper could have been improved by following a more rigorous methodology for analyzing the data and by presenting the results in a more structured and "condensed" way (i.e., aggregating the results). For example, in addition to quoting exemplary statements by individual respondents, the authors could have put more emphasis on extracting some numbers on differences and commonalities in the respondents' answers. (See also the comments related to "Validity of the findings" below.) There still seems to be a bit of overlap between the Results and Discussion sections. For example, both sections discuss motivations for curators. That said, the Discussions section has improved from the last version as it now discusses limitations and future directions.
Validity of the findings
label_votf_1
I still find it a bit unfortunate that the authors were unable to extract a few more quantitative results in their analysis. Performing a basic numerical analysis and cross-correlation in terms of answers by different respondents could substantially add to the value of the contribution. It has been mentioned in the rebuttal that the sample size is too small to draw such conclusions. Arguably, this should be mentioned in the paper (Limitations section) as well. Note that, even if the data does not allow for conclusions with a very high level of confidence (e.g., 95+%), the confidence interval could be adjusted (e.g., ~90%) to at least show a general trend which can then be further substantiated with more data in future work. I am hesitant to flag this as a strong requirement for acceptance, but at least I would urge the authors to consider it for the final revision, and for future work.
Cite this review as
Anonymous Reviewer ( 2017 ) Peer Review #2 of "The appropriation of GitHub for curation (v0.2)" . PeerJ Computer Science https://doi.org/10.7287/peerj-cs.134v0.2/reviews/2 Download Version 0.2 (PDF)
Download author's rebuttal letter
- submitted Aug 10, 2017
label_version_3
Version 0.1 (original submission)
Philipp Leitner
·
May 26, 2017
label_recommendation_3
·
Academic Editor
Major Revisions
As you see, the reviewers and myself were largely positive about your manuscript. Well done! However, there are a few text-level changes that I would like to see addressed. The most important ones are summarized below, but please also take the other comments of the reviewers into account: * Both reviewers commented about your methodology, or at least the description thereof. Please carefully revise and extend how you actually conducted your study, and include rationales. * Relatedly, I would prefer if you made the threats to the validity of your study a bit more explicit. I understand that the "Limitations" section attempts this, but a more standard way is to have a dedicated section on Threats, discuss them more detailedly, and group them using standard treat classes (e.g., internal threats, external threats, construct threats, ...). * The discussion and outcome sections are somewhat 'thin' and speculative as it is. Please make sure that what you discuss in the 'Discussion' actually follows or directly relates to the outcomes of your study, as opposed to just be a general reflection of the area and your opinions about it.
label_author_2
Stefan Wagner ·
May 21, 2017
Basic reporting
label_br_2
* I'd find it helpful if added some examples of resources that are curated (somewhere around page 1 line 40). * P2,l53: why are personal needs extrinsic? * There are several smaller errors in the English, e.g. line 211: "participant" -> participate * Is it possible that you show an overview of your coding schema at the beginning of the results section?
Experimental design
label_ed_2
* why do you only do open coding? Why not the further steps of Grounded Theory? * The relative counting of the occurrences of codes that you use in the results section should be explained here as well. * I'm also confused by the claim that you use open coding and then your codes are the categories found be other studies on motivation on OS projects. Wasn't it more a preexisting categorisation?
Validity of the findings
label_votf_2
* the high kappa indicates a good agreement and hence generalisability of the codes
Comments for the author
label_cfta_2
- While I found the discussion interesting and well-informed, I think it could be better integrated with the results of the interview. Some parts of it feel like they could have written without the interviews. - The conclusions are very short. Could you please summarise what we should actually conclude? What do we learn and what effect should/could that have on software engineering practice? Cite this review as
Wagner S ( 2017 ) Peer Review #1 of "The appropriation of GitHub for curation (v0.1)" . PeerJ Computer Science https://doi.org/10.7287/peerj-cs.134v0.1/reviews/1
label_author_3
Reviewer 2 ·
May 25, 2017
Basic reporting
label_br_3
PROs: * The paper is generally well written, and discusses an interesting and timely topic which has not been extensively studied in the past. CONs: * At various points in the paper, the authors seem to use language with a certain amount of personal bias or over-emphasis, for example in the statement "[...] provides them with a perfect starting point" on page 8 the word "perfect" might be considered as over-emphasizing. This is not a critical comment, but it does make a subtle difference, and it would be advisable to use cautious and hedging language wherever possible. Also, the authors should carefully review whether all of the presented conclusions are actually drawn from and backed by the survey data collected. Style/language (minor): * abstract: "motivates software developers to engaged" -> "motivates software developers to engage"
Experimental design
label_ed_3
* The methodology of the paper is primarily based on quoting exemplary study responses for different dimensions/categories of the survey. It is not entirely clear how the authors came up with the dimensions that are discussed in the study. Which of the two came first - 1) were the dimensions defined beforehand and the survey results are then used to discuss/confirm these dimensions, or 2) are the dimensions derived directly from the survey results? Generally, it may be advisable to be a bit more precise and specific about the methodology and how the survey has been constructed. Some details are provided in the supplemental material (e.g., chat messages exchanged with the respondents), but including the actually survey questions in the text (or appendix) would help making the paper more self-contained. * In addition to the qualitative analysis of the free-text study answers, it would be interesting to see a basic quantitative analysis to provide an overview of the numeric results of the study. Due to the way the results are currently summarized in the paper, the presentation seems fairly lengthy and it is hard for the reader to extract the key insights. Some of the discussed results appear straight-forward or at least not very surprising, but if we had some numbers for these individual results, they could become more meaningful (e.g., X% of respondents gave a similar answer, etc).
Validity of the findings
* I generally wonder if any kind of cross-correlation has been performed. For each of the quoted statements the paper lists the survey respondents (P1-P16) who provided the answer, which in itself does not provide a great added benefit (the paper would be equally valid and valuable if the reference to the respondents were entirely left out). An interesting aspect, though, would be to see how the individual respondents provided answers to different combinations of survey questions. Maybe there is a common theme among the answers? Are any of the survey questions in any way related to one another? (e.g., if a respondent provides answer X to question A, they are likely to provide answer Y to question B) It would be interesting to analyze these aspects more systematically. * In my view, the Discussion section should be more focused on discussing the results and limitations particularly related to the presented contribution (survey). In its current form, the section seems a bit like an extension of the Background section, rather than a specific discussion and critical reflection of the presented results. This is evidenced by the fact that most of the paragraphs/statements in the Discussion are backed by external resources. Using external references is not a problem in itself, but the paper should draw a clear line between discussion of related work / background, and discussion of the results and limitations of the approach itself.
Cite this review as
Anonymous Reviewer ( 2017 ) Peer Review #2 of "The appropriation of GitHub for curation (v0.1)" . PeerJ Computer Science https://doi.org/10.7287/peerj-cs.134v0.1/reviews/2 Download Original Submission (PDF)
- submitted Apr 28, 2017 All text and materials provided via this peer-review history page are made available under a Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.
About us - PeerJ team | Our publications | Benefits | Partnerships | Endorsements Awards Resources - FAQ | Careers | Pressroom | Terms of use | Privacy | Contact Academic boards - Advisors | Editors | Subject areas Follow us - PeerJ blog | Twitter | Facebook | LinkedIn | Pinterest Submission guides - PeerJ – Life and Environment | PeerJ Computer Science | PeerJ Chemistry Spread the word - Activities | Resources PeerJ feeds - Atom | RSS 1.0 | RSS 2.0 | JSON PeerJ Computer Science feeds - Atom | RSS 1.0 | RSS 2.0 | JSON Archives - PeerJ – Life and Environment | PeerJ Computer Science
©2012-2019 PeerJ, Inc | Public user content licensed CC BY 4.0 unless otherwise specified. PeerJ ISSN: 2167-8359 PeerJ Comput. Sci. ISSN: 2376-5992 PeerJ Preprints ISSN: 2167-9843
computer science
