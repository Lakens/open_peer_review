Review History for Pay attention and you won’t lose it: a deep learning approach to sequence imputation [PeerJ]
PeerJ Computer Science PeerJ – the Journal of Life & Environmental Sciences PeerJ Computer Science PeerJ Physical Chemistry PeerJ Organic Chemistry PeerJ Inorganic Chemistry PeerJ Analytical Chemistry PeerJ Materials Science Visit PeerJ.org and get involved About PeerJ Journals Overview PeerJ Journals FAQ What we publish 5 Years publishing Solutions for authors Reputation High quality peer review Fast publishing Indexing and Impact Factor Global readership Feature comparison Reduced cost publishing Author feedback Early career researcher benefits Senior researcher benefits Open review (optional) Rebuttal letters Sections About the journal Sections Aquatic Biology Biochemistry, Biophysics and Molecular Biology Biodiversity and Conservation Bioinformatics and Genomics Brain and Cognition Ecology Environmental Science Microbiology Paleontology and Evolutionary Science Plant Biology Zoological Science About PeerJ Journals Overview PeerJ Journals FAQ What we publish 5 Years publishing Solutions for authors Reputation High quality peer review Fast publishing Indexing and Impact Factor Global readership Feature comparison Reduced cost publishing Author feedback Early career researcher benefits Senior researcher benefits Open review (optional) Rebuttal letters More Subjects Search articles Peer-reviewed Journals PeerJ (Life, Biological, Environmental and Health Sciences) PeerJ Computer Science PeerJ Physical Chemistry PeerJ Organic Chemistry PeerJ Inorganic Chemistry PeerJ Analytical Chemistry PeerJ Materials Science Preprints PeerJ Preprints Table of contents Table of Contents - current and archives PeerJ - Medicine articles PeerJ - Biology & Life science articles PeerJ - Environmental Science articles PeerJ - General bio (stats, legal, policy, edu) PeerJ Computer Science PeerJ Preprints Academic advisors Volunteer to review Collections Job listings Discussions Blog Institutional plans Reviews and awards Spread the word Who are we? Contact Login AUTHORS Peer Journals Overview Submission Guidelines Subject Areas Editorial Board Editorial Criteria Pricing General FAQ Computer Science FAQ Aims and Scope Author Interviews Policies and Procedures SUBMIT ARTICLE
Review History Pay attention and you won’t lose it: a deep learning approach to sequence imputation To increase transparency, PeerJ operates a system of 'optional signed reviews and history'. This takes two forms: (1) peer reviewers are encouraged, but not required, to provide their names (if they do so, then their profile page records the articles they have reviewed), and (2) authors are given the option of reproducing their entire peer review history alongside their published article (in which case the complete peer review process is provided, including revisions, rebuttal letters and editor decision letters). New to public reviews? Learn more about optional signed reviews and how to write a better rebuttal letter .
Summary
The initial submission of this article was received on April 19th, 2019 and was peer-reviewed by 3 reviewers and the Academic Editor. The Academic Editor made their initial decision on May 30th, 2019. The first revision was submitted on June 18th, 2019 and was reviewed by 1 reviewer and the Academic Editor. The article was Accepted by the Academic Editor on July 8th, 2019.
label_version_1
Version 0.2 (accepted)
Rong Qu
·
Jul 8, 2019
label_recommendation_1
·
Academic Editor
Accept
Two of the reviewers are satisfied with the revision. I am happy to accept the revised manuscript.
label_author_1
Reviewer 1 ·
Jul 1, 2019
Basic reporting
label_br_1
The authors have satisfactorily addressed all my comments and now the paper can be accepted for publication.
Experimental design
label_ed_1
The authors have satisfactorily addressed all my comments and now the paper can be accepted for publication.
Validity of the findings
label_votf_1
The authors have satisfactorily addressed all my comments and now the paper can be accepted for publication.
Comments for the author
label_cfta_1
The authors have satisfactorily addressed all my comments and now the paper can be accepted for publication. Cite this review as
Anonymous Reviewer ( 2019 ) Peer Review #1 of "Pay attention and you won’t lose it: a deep learning approach to sequence imputation (v0.2)" . PeerJ Computer Science https://doi.org/10.7287/peerj-cs.210v0.2/reviews/1 Download Version 0.2 (PDF)
Download author's rebuttal letter
- submitted Jun 18, 2019
label_version_2
Version 0.1 (original submission)
Rong Qu
·
May 30, 2019
label_recommendation_2
·
Academic Editor
Major Revisions
One of the reviewers commented on the additional contributions this research presents compared to your previous publication at IJCNN. The other reviewer, who provided very detailed comments, raised the question of how the presented algorithm apply to other datasets.
label_author_2
Reviewer 1 ·
May 16, 2019
Basic reporting
label_br_2
no comment
Experimental design
label_ed_2
no comment
Validity of the findings
label_votf_2
no comment
Comments for the author
label_cfta_2
In this paper, the authors proposed the use of attention mechanisms for the imputation of values in discrete sequences using LSTMs. According to the authors, this is the first time that this type of imputation mechanisms are used in discrete sequences and they argue this fact. They use the architecture of Vaswani's transformer model and test the results in the CAN data set. The paper is well written and well understood. The results compared to an LSTM baseline are surprising in terms of accuracy and time reduction. However, I am not clear about the novelty of the idea with respect to an accepted paper written by the same authors in IJCNN this year. I'm sure there are differences, beyond the implementation of the model in Keras instead of TensorFlow, but I think that the authors should record each and every one of the novelties that this new study represents with respect to the previous publication. I am referring to the differences in the mechanism of attention and architecture, as well as the extension of experimental results and tests that are carried out in this paper. A new section (or subsection) should be included for this purpose. From my point of view, with a clear justification of the novelty, the paper could be accepted, as it represents an interesting advance in the pre-processing of discrete sequence data. Cite this review as
Anonymous Reviewer ( 2019 ) Peer Review #1 of "Pay attention and you won’t lose it: a deep learning approach to sequence imputation (v0.1)" . PeerJ Computer Science https://doi.org/10.7287/peerj-cs.210v0.1/reviews/1
label_author_3
Andrew Gray ·
May 20, 2019
Basic reporting
label_br_3
Line 40: I’d still delete “only” from here so it reads “very few attempts” rather than “only very few attempts”. I’d prefer a legend on Figure 2 that didn’t rely on underscores and clarified the names a little more, particularly since these names are not defined in the manuscript (I appreciate that the configuration names are clear enough in intent, but they could definitely be improved). Line 181: I don’t think this comma is needed (or another matching comma is missing from earlier in the sentence). Lines 206–207: Perhaps “In the context of…” (adding “the”) here and a comma after “systems”. Line 248: I don’t think the first comma (after “model”) is needed (or another matching comma is missing from earlier in the sentence).
Experimental design
label_ed_3
No comment.
Validity of the findings
label_votf_3
No comment.
Comments for the author
label_cfta_3
The revised version of the manuscript addresses my primary concern from last time (the reliance on an anonymous unpublished manuscript) and addresses most of the other comments I made also. I have only a few minor comments above, one which is a repeat from the previous version of the manuscript. Cite this review as
Gray AR ( 2019 ) Peer Review #2 of "Pay attention and you won’t lose it: a deep learning approach to sequence imputation (v0.1)" . PeerJ Computer Science https://doi.org/10.7287/peerj-cs.210v0.1/reviews/2
label_author_4
Reviewer 3 ·
May 29, 2019
Basic reporting
label_br_4
This paper studies the use of LSTMs for data restoration. The current state-of-the-art performs a large number of sequential operations, slowing the training down. The aim of this paper is to alleviate such issues by means of attention mechanisms. This is inspired in the paper of Vaswani 2017, and the authors use a version of ‘Transformer with modified hyper-parameters. As such, the novelty is not extremely high, but I believe that it is definitely of interest for the deep learning community. Overall, the paper is well-written and is easy to follow. I believe, however, it requires to extend a few sections to contextualise better their proposal.
Experimental design
label_ed_4
- The authors are using data collected from a CAN of an SUV. Following the description in section 3.1, it is unclear what kind of machine learning problem the authors are tackling. I don’t understand the meaning of 43 unique messages Ids. The authors need to clarify what is going to be input into the proposed model. - The training/test split seems to be a bit too simplistic. Why should you apply a fold-cross validation? - Why table 4 doesn’t report the time for 3000 epochs?
Validity of the findings
label_votf_4
The algorithm seems to be quite competitive in comparison with the previous authors’ proposals. However, this is only applied on a single dataset; the authors should apply this on more datasets to check if the proposed method actually restores well in other scenarios.
Comments for the author
The introduction is missing general references on data preprocessing, data cleaning, obtaining ‘smart data’ from big data. I would also extend the introduction to motivate better the problem of restoration and when it is applied. I very much like the discussion about Deep learning not being used for data-preprocessing or cleaning. I wonder if the authors could elaborate a bit more on that regard. For example, data augmentation, GANs could be considered preprocessing as well (as oversampling technique). But it is definitely true that it is frequently not being used for data filtering or noise detection. I think that would also be a good opportunity to contextualise better the proposed method. Is restoration a missing values imputation mechanism? If yes, it is unclear in the introduction if the proposed method is devised to tackling images/videos or ‘any kind’ of data. In the case of deep learning, the authors should also mention in-painting techniques as related work, and for standard classification/regression/time series problems, there are many missing values imputation mechanisms which are related. I slightly disagree that the classical techniques for missing values imputation revolve around local and global means (they are of course the basic), but classical techniques also include techniques such as the k-nearest neighbours. The description of the proposed ‘restorer’ method is a bit shallow. The authors have assumed lots of background information which could be solved by adding some additional information in the background. Figure 1 should belong to section 2.2. Cite this review as
Anonymous Reviewer ( 2019 ) Peer Review #3 of "Pay attention and you won’t lose it: a deep learning approach to sequence imputation (v0.1)" . PeerJ Computer Science https://doi.org/10.7287/peerj-cs.210v0.1/reviews/3 Download Original Submission (PDF)
- submitted Apr 19, 2019 All text and materials provided via this peer-review history page are made available under a Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.
About us - PeerJ team | Our publications | Benefits | Partnerships | Endorsements Awards Resources - FAQ | Careers | Pressroom | Terms of use | Privacy | Contact Academic boards - Advisors | Editors | Subject areas Follow us - PeerJ blog | Twitter | Facebook | LinkedIn | Pinterest Submission guides - PeerJ – Life and Environment | PeerJ Computer Science | PeerJ Chemistry Spread the word - Activities | Resources PeerJ feeds - Atom | RSS 1.0 | RSS 2.0 | JSON PeerJ Computer Science feeds - Atom | RSS 1.0 | RSS 2.0 | JSON Archives - PeerJ – Life and Environment | PeerJ Computer Science
©2012-2019 PeerJ, Inc | Public user content licensed CC BY 4.0 unless otherwise specified. PeerJ ISSN: 2167-8359 PeerJ Comput. Sci. ISSN: 2376-5992 PeerJ Preprints ISSN: 2167-9843
computer science
