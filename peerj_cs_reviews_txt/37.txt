Review History for Semantic representation of scientific literature: bringing claims, contributions and named entities onto the Linked Open Data cloud [PeerJ]
PeerJ Computer Science PeerJ – the Journal of Life & Environmental Sciences PeerJ Computer Science PeerJ Physical Chemistry PeerJ Organic Chemistry PeerJ Inorganic Chemistry PeerJ Analytical Chemistry PeerJ Materials Science Visit PeerJ.org and get involved About PeerJ Journals Overview PeerJ Journals FAQ What we publish 5 Years publishing Solutions for authors Reputation High quality peer review Fast publishing Indexing and Impact Factor Global readership Feature comparison Reduced cost publishing Author feedback Early career researcher benefits Senior researcher benefits Open review (optional) Rebuttal letters Sections About the journal Sections Aquatic Biology Biochemistry, Biophysics and Molecular Biology Biodiversity and Conservation Bioinformatics and Genomics Brain and Cognition Ecology Environmental Science Microbiology Paleontology and Evolutionary Science Plant Biology Zoological Science About PeerJ Journals Overview PeerJ Journals FAQ What we publish 5 Years publishing Solutions for authors Reputation High quality peer review Fast publishing Indexing and Impact Factor Global readership Feature comparison Reduced cost publishing Author feedback Early career researcher benefits Senior researcher benefits Open review (optional) Rebuttal letters More Subjects Search articles Peer-reviewed Journals PeerJ (Life, Biological, Environmental and Health Sciences) PeerJ Computer Science PeerJ Physical Chemistry PeerJ Organic Chemistry PeerJ Inorganic Chemistry PeerJ Analytical Chemistry PeerJ Materials Science Preprints PeerJ Preprints Table of contents Table of Contents - current and archives PeerJ - Medicine articles PeerJ - Biology & Life science articles PeerJ - Environmental Science articles PeerJ - General bio (stats, legal, policy, edu) PeerJ Computer Science PeerJ Preprints Academic advisors Volunteer to review Collections Job listings Discussions Blog Institutional plans Reviews and awards Spread the word Who are we? Contact Login AUTHORS Peer Journals Overview Submission Guidelines Subject Areas Editorial Board Editorial Criteria Pricing General FAQ Computer Science FAQ Aims and Scope Author Interviews Policies and Procedures SUBMIT ARTICLE
Review History Semantic representation of scientific literature: bringing claims, contributions and named entities onto the Linked Open Data cloud To increase transparency, PeerJ operates a system of 'optional signed reviews and history'. This takes two forms: (1) peer reviewers are encouraged, but not required, to provide their names (if they do so, then their profile page records the articles they have reviewed), and (2) authors are given the option of reproducing their entire peer review history alongside their published article (in which case the complete peer review process is provided, including revisions, rebuttal letters and editor decision letters). New to public reviews? Learn more about optional signed reviews and how to write a better rebuttal letter .
Summary
The initial submission of this article was received on August 4th, 2015 and was peer-reviewed by 2 reviewers and the Academic Editor. The Academic Editor made their initial decision on August 24th, 2015. The first revision was submitted on November 10th, 2015 and was reviewed by the Academic Editor. The article was Accepted by the Academic Editor on November 13th, 2015.
label_version_1
Version 0.2 (accepted)
Tamara Sumner
·
Nov 13, 2015
label_recommendation_1
·
Academic Editor
Accept
Thank you for addressing the reviewer and editor concerns so thoroughly in this revision. Your document describing in detail how you responded to each concern was extremely helpful for enabling a timely decision. Congratulations on getting this interesting and important work into publication.
Download Version 0.2 (PDF)
Download author's rebuttal letter
- submitted Nov 10, 2015
label_version_2
Version 0.1 (original submission)
Tamara Sumner
·
Aug 24, 2015
label_recommendation_2
·
Academic Editor
Major Revisions
This work is interesting, important, and with one exception, well-described and well-executed. This submission needs to have a very targeted rewrite of section 5.2 Text Mining Pipeline Intrinsic Evaluation. As currently described, this evaluation does not meet the PeerJ guidelines on experimental design (conducted rigorously and to a high technical standard) or validity of findings (data should be robust, statistically sound, and controlled). The authors do not need to prove that their algorithm outperforms others; however, the evaluation should make it clear to what degree their algorithm is able to reliably detect Claims and Contributions in their three corpora. The current evaluation only compares the algorithm results with a human annotated set for one corpus, which has different characteristics then the others (looking at data presented in Table 2). There are (at least) three options: (1) Reviewer number 2 offers advice for comparing against known baselines, which is a great idea. (2) Make it clear why choosing to only sample documents from one corpus is a reasonable choice. It is not obvious to a reader that it is. (3) Create an evaluation sample that includes a random mix of documents from all three corpora. The authors should also more clearly describe their processes for creating the gold standard corpus. (Were documents double annotated? What was the agreement between human experts?) This gold standard data set should also be released to support replications of their experiments. They should also provide precision and recall values, in addition to the F-measure. There are also a few typos to be fixed: 10 for a wide-spread adoption (for wide-spread adoption) 40 provide for use cases such as summarization (provide support for) 42 (NEs) present in a document (e.g., algorithms, methods, technologies) can help locating (can help locate) 83 extraction task by determining zones of text where further analysis is needed. (extraction tasks) 117 mining, controlled vocabularies are used in form of markup languages, which are added (not grammatical) 220 scientific documents LOD entities combined with rhetorical entities. (documents’) 229 Detection of Named Entities). Finally, the extracted information are stored in a seman- (is stored) 624 the concepts of REs and NEs, enhanced retrieval of document becomes possible, e.g., (documents)
label_author_1
Md Arafat Sultan ·
Aug 21, 2015
Basic reporting
label_br_1
No Comments.
Experimental design
label_ed_1
The overall methodology is sound; however, some system components are simplistic and naive in nature, especially, the rule-based approach for rhetorical entity detection. It is not clear how expressive and accurate the set of 190 gazetteer entries and the subsequent rules are. The authors present results on a small data set with only a modest 68% F1 score, making the assessment even harder. A more detailed description of these entries and rules along with a qualitative analysis is required (e.g., what are the major categories of claim and contribution statements? what portion of these are targeted by the system? how much linguistic variation do claim and contribution statements exhibit? how much of it can the designed rules accommodate? etc.).
Validity of the findings
label_votf_1
Without any baseline/benchmark, it is not clear how to properly judge the reported results. I suggest that the authors compare their RE and NE detection subsystems with at least one other system (some of which are mentioned by reviewer 1 in the earlier set of reviews). In addition, the authors should also provide experimental evidence that such a system is helpful for users in some quantifiable way. Something along the line of what reviewer 2 suggested in the earlier set of reviews would be helpful.
Comments for the author
label_cfta_1
Overall, a well-written paper describing a sound methodology. However, I do strongly feel that the authors should additionally carry out the above experiments and report the results. The 'major revisions' recommendation that follows is really meant to stress the importance of these experiments rather than to suggest extensive rework. Cite this review as
Sultan MA ( 2015 ) Peer Review #1 of "Semantic representation of scientific literature: bringing claims, contributions and named entities onto the Linked Open Data cloud (v0.1)" . PeerJ Computer Science https://doi.org/10.7287/peerj-cs.37v0.1/reviews/1
label_author_2
Reviewer 2 ·
Aug 24, 2015
Basic reporting
label_br_2
No comments.
Experimental design
label_ed_2
No comments.
Validity of the findings
label_votf_2
No comments.
Comments for the author
label_cfta_2
Typo : In the results section of the abstract "semantic queries than show" -> "semantic queries then show". This paper lays out a framework for a valuable contribution to bridging the gaps in semantic annotation for scientific literature. It will be interesting to see the results of your future experiments honing DBPedia Spotlight onto scientific domains so that the over-generalizations addressed in this paper do not dim the importance of this line of research. Cite this review as
Anonymous Reviewer ( 2015 ) Peer Review #2 of "Semantic representation of scientific literature: bringing claims, contributions and named entities onto the Linked Open Data cloud (v0.1)" . PeerJ Computer Science https://doi.org/10.7287/peerj-cs.37v0.1/reviews/2 Download Original Submission (PDF)
- submitted Aug 4, 2015 All text and materials provided via this peer-review history page are made available under a Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.
About us - PeerJ team | Our publications | Benefits | Partnerships | Endorsements Awards Resources - FAQ | Careers | Pressroom | Terms of use | Privacy | Contact Academic boards - Advisors | Editors | Subject areas Follow us - PeerJ blog | Twitter | Facebook | LinkedIn | Pinterest Submission guides - PeerJ – Life and Environment | PeerJ Computer Science | PeerJ Chemistry Spread the word - Activities | Resources PeerJ feeds - Atom | RSS 1.0 | RSS 2.0 | JSON PeerJ Computer Science feeds - Atom | RSS 1.0 | RSS 2.0 | JSON Archives - PeerJ – Life and Environment | PeerJ Computer Science
©2012-2019 PeerJ, Inc | Public user content licensed CC BY 4.0 unless otherwise specified. PeerJ ISSN: 2167-8359 PeerJ Comput. Sci. ISSN: 2376-5992 PeerJ Preprints ISSN: 2167-9843
computer science
