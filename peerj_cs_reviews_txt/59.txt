Review History for A technology prototype system for rating therapist empathy from audio recordings in addiction counseling [PeerJ]
PeerJ Computer Science PeerJ – the Journal of Life & Environmental Sciences PeerJ Computer Science PeerJ Physical Chemistry PeerJ Organic Chemistry PeerJ Inorganic Chemistry PeerJ Analytical Chemistry PeerJ Materials Science Visit PeerJ.org and get involved About PeerJ Journals Overview PeerJ Journals FAQ What we publish 5 Years publishing Solutions for authors Reputation High quality peer review Fast publishing Indexing and Impact Factor Global readership Feature comparison Reduced cost publishing Author feedback Early career researcher benefits Senior researcher benefits Open review (optional) Rebuttal letters Sections About the journal Sections Aquatic Biology Biochemistry, Biophysics and Molecular Biology Biodiversity and Conservation Bioinformatics and Genomics Brain and Cognition Ecology Environmental Science Microbiology Paleontology and Evolutionary Science Plant Biology Zoological Science About PeerJ Journals Overview PeerJ Journals FAQ What we publish 5 Years publishing Solutions for authors Reputation High quality peer review Fast publishing Indexing and Impact Factor Global readership Feature comparison Reduced cost publishing Author feedback Early career researcher benefits Senior researcher benefits Open review (optional) Rebuttal letters More Subjects Search articles Peer-reviewed Journals PeerJ (Life, Biological, Environmental and Health Sciences) PeerJ Computer Science PeerJ Physical Chemistry PeerJ Organic Chemistry PeerJ Inorganic Chemistry PeerJ Analytical Chemistry PeerJ Materials Science Preprints PeerJ Preprints Table of contents Table of Contents - current and archives PeerJ - Medicine articles PeerJ - Biology & Life science articles PeerJ - Environmental Science articles PeerJ - General bio (stats, legal, policy, edu) PeerJ Computer Science PeerJ Preprints Academic advisors Volunteer to review Collections Job listings Discussions Blog Institutional plans Reviews and awards Spread the word Who are we? Contact Login AUTHORS Peer Journals Overview Submission Guidelines Subject Areas Editorial Board Editorial Criteria Pricing General FAQ Computer Science FAQ Aims and Scope Author Interviews Policies and Procedures SUBMIT ARTICLE
Review History A technology prototype system for rating therapist empathy from audio recordings in addiction counseling To increase transparency, PeerJ operates a system of 'optional signed reviews and history'. This takes two forms: (1) peer reviewers are encouraged, but not required, to provide their names (if they do so, then their profile page records the articles they have reviewed), and (2) authors are given the option of reproducing their entire peer review history alongside their published article (in which case the complete peer review process is provided, including revisions, rebuttal letters and editor decision letters). New to public reviews? Learn more about optional signed reviews and how to write a better rebuttal letter .
Summary
The initial submission of this article was received on November 19th, 2015 and was peer-reviewed by 3 reviewers and the Academic Editor. The Academic Editor made their initial decision on January 25th, 2016. The first revision was submitted on March 7th, 2016 and was reviewed by 2 reviewers and the Academic Editor. The article was Accepted by the Academic Editor on March 29th, 2016.
label_version_1
Version 0.2 (accepted)
Julita Vassileva
·
Mar 29, 2016
label_recommendation_1
·
Academic Editor
Accept
Both reviewers and I are happy with the revisions made and recommend the paper to be accepted as it is.
label_author_1
Reviewer 1 ·
Mar 20, 2016
Basic reporting
label_br_1
The revised article conforms with PeerJ standards. The authors' revisions clarify the presentation, tie the figures better to the narrative, and improve the style overall.
Experimental design
label_ed_1
The revisions clarify the experimental design and technical approach. The revised version is fully satisfactory.
Validity of the findings
label_votf_1
The revised manuscript presents the findings and conclusions clearly.
Comments for the author
label_cfta_1
It is clear from the revised manuscript that the authors have made a concerted effort to respond to the reviewers' comments on the earlier version of the manuscript. Cite this review as
Anonymous Reviewer ( 2016 ) Peer Review #1 of "A technology prototype system for rating therapist empathy from audio recordings in addiction counseling (v0.2)" . PeerJ Computer Science https://doi.org/10.7287/peerj-cs.59v0.2/reviews/1
label_author_2
Reviewer 3 ·
Mar 24, 2016
Basic reporting
label_br_2
N/A
Experimental design
label_ed_2
N/A
Validity of the findings
label_votf_2
N/A
Comments for the author
label_cfta_2
Thanks for the authors' efforts on addressing the comments from the three reviewers. Now, my major concerns to this article have been resolved. I feel that the paper could be published in PeerJ. Cite this review as
Anonymous Reviewer ( 2016 ) Peer Review #3 of "A technology prototype system for rating therapist empathy from audio recordings in addiction counseling (v0.2)" . PeerJ Computer Science https://doi.org/10.7287/peerj-cs.59v0.2/reviews/3 Download Version 0.2 (PDF)
Download author's rebuttal letter
- submitted Mar 7, 2016
label_version_2
Version 0.1 (original submission)
Julita Vassileva
·
Jan 25, 2016
label_recommendation_2
·
Academic Editor
Minor Revisions
The article presents an interesting research problem and is well motivated and presented. The technique proposed for training role-specific language models is novel. Please, note the recommendations of the three reviewers and make sure you address their questions and suggestions when preparing the final version of the paper.
label_author_3
Reviewer 1 ·
Nov 25, 2015
Basic reporting
label_br_3
The abstract and introduction of the article are quite well written. From the abstract it is clear what problem the authors addressed and what results they obtained. From the introduction how their work relates to broader research, and what approach they took. Many manuscripts that I have reviewed fail to meet this standard. I do have one request though. The authors mention that they propose three quantification models, and then propose to fuse the results of these models. It would be helpful to briefly summarize why the authors chose this approach. Do these models have characteristics that lead one to suggest a priori that this is the best approach to take? Have others employed them for similar tasks? The authors explain this a bit later, but only in the context of a detailed description of the methods.. Also the correspondence between the narrative and the diagram in Figure 1 could be made clearer, since the diagram says “N-gram language model” and the narrative says “Maximum likelihood model training with human-generated transcripts”. I gather these refer to the same thing. Overall I felt that the body of the manuscript clearly describes the methods used. I found the technique of training role-specific language models to be clever and novel. I would like to see some clarification of the following points: - How does the diarization algorithm perform in the case where speakers overlap or interrupt each other? The authors note that this is an infrequent occurrence. Was it necessary to train an acoustic model specially for this purpose? How much performance improvement does this offer over off-the-shelf acoustic models?
Experimental design
label_ed_3
The experimental design is clear, and evaluates the contributions of each component of the method.
Validity of the findings
label_votf_3
The results appear to be valid, however I did not examine the modeling algorithms in detail.
Comments for the author
label_cfta_3
In the final manuscript, make sure that all acronyms (e.g., MFCCs, OOV, SRILM) are explained or are obvious in context. Typos: p. 3: “clinical trails” p. 5: “experiment results” -> “experimental results” p. 9: “highly biased result” -> “highly biased results” p. 11: “while drops” -> “while it drops” p. 13: “pathes” -> “paths” p. 14: “a SP” -> “an SP” Cite this review as
Anonymous Reviewer ( 2016 ) Peer Review #1 of "A technology prototype system for rating therapist empathy from audio recordings in addiction counseling (v0.1)" . PeerJ Computer Science https://doi.org/10.7287/peerj-cs.59v0.1/reviews/1
label_author_4
Reviewer 2 ·
Dec 16, 2015
Basic reporting
label_br_4
The paper is readable but the English has to be improved. In particular using the first person style “We......” is should be removed. There are also frequent grammatical errors, to many to list; the whole style in which the paper is written should be revised. From the engineering point of view, the paper does not introduce any novel technologies. Existing methods were put together for a specific application. However, this particular application (automatic rating) could be of interest to many readers.
Experimental design
label_ed_4
Good.
Validity of the findings
label_votf_4
Good
Comments for the author
label_cfta_4
Please improve English. Cite this review as
Anonymous Reviewer ( 2016 ) Peer Review #2 of "A technology prototype system for rating therapist empathy from audio recordings in addiction counseling (v0.1)" . PeerJ Computer Science https://doi.org/10.7287/peerj-cs.59v0.1/reviews/2
label_author_5
Reviewer 3 ·
Jan 25, 2016
Basic reporting
label_br_5
Generally speaking, this article is well written. Most parts of the article reach professional English writing standards. To further improve clarity of the article, I feel that the authors need enrich most of captions in both figures and tables. For example, Figure 2 is quite complicate and its existing caption is too brief to properly guide the readers to fully understand this figure.
Experimental design
label_ed_5
Mostly, the experimental design presented in this article meets research community’s standard. However, I feel that the following issues need the author's attentions. around the line 60, the authors mentioned that they already “quantified prosodic features of the therapist and patient, and … “. I am wondering why the authors only limited their experiments on analyzing ASR outputs rather than considering prosodic cues (could be in a very simple format) given their previous research findings. regarding 2.4 speaker role matching, I feel that many possible useful cues could be used beyond the current LM only approach. For example, it is possible that a therapist always initializes the dialog and he or she tends to use shorter time compared to a patient during the entire dialog. regarding section 3, the motivations of using both n-gram LM and MaxEnt model were not clearly introduced. Reader need know why these methods were considered to be useful. regarding Table 4, on which levels, the VAD and diarization, were evaluated. This seems not very clear from reading the paper. around the line 415, session level ASR WER has a large variation range (from about 20% to 90%). Since the article focused on predicting empathy based on lexical cues, the accuracy of ASR impacts the prediction accuracy very much. In this sense, I am wondering whether the authors should focus their study on the sessions with good enough ASR WER. It is hard to convince the readers that ASR result with a WER about 0.4 or 0.5 still could be processed by the proposed empathy prediction method.
Validity of the findings
Overall, based on the research questions made and the experiments being conducted, the findings presented in this article are valid. One issue I spotted is from Table 9. For RP, the Acc on ORA-T case was worse than ORA-D case (79.0% < 86.8%). This seems be against the intuition. Why the result from transcriptions (WER in such case is close to 0.0) could be worse than the result from an ASR output with an averaged WER about 0.4.
Cite this review as
Anonymous Reviewer ( 2016 ) Peer Review #3 of "A technology prototype system for rating therapist empathy from audio recordings in addiction counseling (v0.1)" . PeerJ Computer Science https://doi.org/10.7287/peerj-cs.59v0.1/reviews/3 Download Original Submission (PDF)
- submitted Nov 19, 2015 All text and materials provided via this peer-review history page are made available under a Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.
About us - PeerJ team | Our publications | Benefits | Partnerships | Endorsements Awards Resources - FAQ | Careers | Pressroom | Terms of use | Privacy | Contact Academic boards - Advisors | Editors | Subject areas Follow us - PeerJ blog | Twitter | Facebook | LinkedIn | Pinterest Submission guides - PeerJ – Life and Environment | PeerJ Computer Science | PeerJ Chemistry Spread the word - Activities | Resources PeerJ feeds - Atom | RSS 1.0 | RSS 2.0 | JSON PeerJ Computer Science feeds - Atom | RSS 1.0 | RSS 2.0 | JSON Archives - PeerJ – Life and Environment | PeerJ Computer Science
©2012-2019 PeerJ, Inc | Public user content licensed CC BY 4.0 unless otherwise specified. PeerJ ISSN: 2167-8359 PeerJ Comput. Sci. ISSN: 2376-5992 PeerJ Preprints ISSN: 2167-9843
computer science
