Review History for Verifiability in computer-aided research: the role of digital scientific notations at the human-computer interface [PeerJ]
PeerJ Computer Science PeerJ – the Journal of Life & Environmental Sciences PeerJ Computer Science PeerJ Physical Chemistry PeerJ Organic Chemistry PeerJ Inorganic Chemistry PeerJ Analytical Chemistry PeerJ Materials Science Visit PeerJ.org and get involved About PeerJ Journals Overview PeerJ Journals FAQ What we publish 5 Years publishing Solutions for authors Reputation High quality peer review Fast publishing Indexing and Impact Factor Global readership Feature comparison Reduced cost publishing Author feedback Early career researcher benefits Senior researcher benefits Open review (optional) Rebuttal letters Sections About the journal Sections Aquatic Biology Biochemistry, Biophysics and Molecular Biology Biodiversity and Conservation Bioinformatics and Genomics Brain and Cognition Ecology Environmental Science Microbiology Paleontology and Evolutionary Science Plant Biology Zoological Science About PeerJ Journals Overview PeerJ Journals FAQ What we publish 5 Years publishing Solutions for authors Reputation High quality peer review Fast publishing Indexing and Impact Factor Global readership Feature comparison Reduced cost publishing Author feedback Early career researcher benefits Senior researcher benefits Open review (optional) Rebuttal letters More Subjects Search articles Peer-reviewed Journals PeerJ (Life, Biological, Environmental and Health Sciences) PeerJ Computer Science PeerJ Physical Chemistry PeerJ Organic Chemistry PeerJ Inorganic Chemistry PeerJ Analytical Chemistry PeerJ Materials Science Preprints PeerJ Preprints Table of contents Table of Contents - current and archives PeerJ - Medicine articles PeerJ - Biology & Life science articles PeerJ - Environmental Science articles PeerJ - General bio (stats, legal, policy, edu) PeerJ Computer Science PeerJ Preprints Academic advisors Volunteer to review Collections Job listings Discussions Blog Institutional plans Reviews and awards Spread the word Who are we? Contact Login AUTHORS Peer Journals Overview Submission Guidelines Subject Areas Editorial Board Editorial Criteria Pricing General FAQ Computer Science FAQ Aims and Scope Author Interviews Policies and Procedures SUBMIT ARTICLE
Review History Verifiability in computer-aided research: the role of digital scientific notations at the human-computer interface To increase transparency, PeerJ operates a system of 'optional signed reviews and history'. This takes two forms: (1) peer reviewers are encouraged, but not required, to provide their names (if they do so, then their profile page records the articles they have reviewed), and (2) authors are given the option of reproducing their entire peer review history alongside their published article (in which case the complete peer review process is provided, including revisions, rebuttal letters and editor decision letters). New to public reviews? Learn more about optional signed reviews and how to write a better rebuttal letter .
Summary
The initial submission of this article was received on March 5th, 2018 and was peer-reviewed by 2 reviewers and the Academic Editor. The Academic Editor made their initial decision on April 23rd, 2018. The first revision was submitted on May 24th, 2018 and was reviewed by 2 reviewers and the Academic Editor. A further revision was submitted on June 21st, 2018 and was reviewed by the Academic Editor. The article was Accepted by the Academic Editor on June 22nd, 2018.
label_version_1
Version 0.3 (accepted)
Daniel Katz
·
Jun 22, 2018
label_recommendation_1
·
Academic Editor
Accept
Thanks for making these few changes quickly.
Download Version 0.3 (PDF)
Download author's rebuttal letter
- submitted Jun 21, 2018
label_version_2
Version 0.2
Daniel Katz
·
Jun 15, 2018
label_recommendation_2
·
Academic Editor
Minor Revisions
If you can quickly address the minor comments from the two reviewers, we can move this to acceptance - it's quite close now.
label_author_1
Greg Wilson ·
Jun 10, 2018
Basic reporting
label_br_1
See "general comments".
Experimental design
label_ed_1
See "general comments".
Validity of the findings
label_votf_1
See "general comments".
Comments for the author
label_cfta_1
Thank you for giving me the opportunity to review the changes to this paper - I am grateful to the author for addressing most of my original comments. I still find the transition from the general discussion of verifiability to the specific solution in Leibniz awkward, but I recognize that this is a case of "I wouldn't have written it that way", which isn't a legitimate criticism. The use of the mass-and-spring example throughout is welcome, as is placing the technical discussion of Leibniz in one section; I feel the AMBER anecdote (Section 2.1) could be shortened, but again, that is stylistic rather than substantive. Recommendation: the paper should be accepted without further changes (other than replacing "literal programming" with "literate programming" on line 373). Cite this review as
Wilson G ( 2018 ) Peer Review #1 of "Verifiability in computer-aided research: the role of digital scientific notations at the human-computer interface (v0.2)" . PeerJ Computer Science https://doi.org/10.7287/peerj-cs.158v0.2/reviews/1
label_author_2
Reviewer 2 ·
Jun 14, 2018
Basic reporting
label_br_2
No comment.
Experimental design
label_ed_2
The experimental design is appropriate, although the introduction, motivation and background are rather lengthy.
Validity of the findings
label_votf_2
The findings appear appropriate.
Comments for the author
label_cfta_2
The revision strongly considered and attempted to address concerns raised in previous review. the revised manuscript is improved, although the intro/motivation/background are a little lengthy. Line 121 - "and the decision might well have been taken by an inexperienced graduate student" is pure speculation and unnecessary to mention. I would suggest omitting this. Line 123 - "the feature wasn't documented anywhere else than in the source code of a piece of software, which was never peer reviewed at all." Again, speculation without evidence. Many different people have effectively "peer reviewed" the AMBER software. I would suggest removing the phrase "which was never peer reviewed at all". Line 126-127 "To the best of my knowledge, no paper and no software documentation mentions this behavior" -- omit. It is true that it is non-trivial to figure this out without reading the AMBER code, but there is documentation out there which can be found on Google. https://books.google.com/books?id=mPgpMig3tx0C&pg=PA77&lpg=PA77&dq=amber+atom+type+ordering+improper&source=bl&ots=h0EegD4GBZ&sig=KQ2jDNV0e8I6-GpOSAvKOUE9Kaw&hl=en&sa=X&ved=0ahUKEwiK89PYitTbAhUSiIMKHY4aCJ4Q6AEITTAG#v=onepage&q=amber%20atom%20type%20ordering%20improper&f=false From: A Practical Introduction to the Simulation of Molecular Systems By Martin J. Field (1999). Also blog posts on OpenMM: https://github.com/pandegroup/openmm/issues/220 Line 129-131 "In fact, I am not even sure that all software implementing AMBER handles this the same way..." See: "Lessons learned from comparing molecular dynamics engines on the SAMPL5 dataset." Shirts MR, Klein C, Swails JM, Yin J, Gilson MK, Mobley DL, Case DA, Zhong ED. J Comput Aided Mol Des. 2017 Jan;31(1):147-161. doi: 10.1007/s10822-016-9977-1. Like 138 - "It would clearly be less effort for everybody to simply fix the force field definition". The definition is fixed / specific, just poorly documented. Cite this review as
Anonymous Reviewer ( 2018 ) Peer Review #2 of "Verifiability in computer-aided research: the role of digital scientific notations at the human-computer interface (v0.2)" . PeerJ Computer Science https://doi.org/10.7287/peerj-cs.158v0.2/reviews/2 Download Version 0.2 (PDF)
Download author's rebuttal letter
- submitted May 24, 2018
label_version_3
Version 0.1 (original submission)
Daniel Katz
·
Apr 23, 2018
label_recommendation_3
·
Academic Editor
Major Revisions
I am unsure if major or minor revisions are needed. It depends on how you respond to the comments from the first reviewer. If you do not split the paper, the revisions might be considered minor. If you do spilt, this would certainly be major. But in either case, I generally agree with the points of the reviewers, and would like to see this paper revised so that it can be published.
label_author_3
Greg Wilson ·
Apr 7, 2018
Basic reporting
label_br_3
The paper is well written, but I have concerns about overall structure and about the lack of evidence to back up some of its statements - please see the general notes below.
Experimental design
label_ed_3
Not applicable.
Validity of the findings
label_votf_3
Not applicable.
Comments for the author
label_cfta_3
Thank you for giving me the opportunity to review this paper - I enjoyed reading it, and believe that the work is both interesting and valuable, but have three reservations about it in its present form: 1. Even upon second reading, I felt that two papers had been combined to make one: a general discussion of reproducibility (or its lack) in science, and a description of the design of a particular literate programming tool. I recognize that the first motivates the second, but I think the audience for the two halves of the paper is likely different, and recommend splitting. 2. Claims or implications are made throughout the paper about the scale and severity of the reproducibility crisis in scientific computing and the usability (or lack thereof) of various software tools, but these are only backed up by specific instances or anecdotes. For example, the author mentions several cases of papers being found to be in error because of computational mistakes, but we simply do not have any solid (quantitative) understanding of how widespread this is - as I note in comments, fewer than 50 of the 5 million papers published 1990-2000 have, to my knowledge, subsequently been withdrawn because of computational errors, which puts our *proven* error rate at 1 in 100,000. Like the author, I believe that the actual rate is much higher than this, but I have no proof, and am increasingly wary of repeating dire warnings that I cannot substantiate while simultaneously telling people that *their* work should be more reproducible. 3. The second part of the paper (describing the design of Leibniz) uses concepts from programming language analysis and design that will be unfamiliar to most people with backgrounds in other sciences. This is not to say that those ideas should be removed - they are as necessary to understanding the work as a knowledge of partial differential equations is to understanding a paper about fluid mechanics - but in places I felt that having worked on this tool for so long, the author may have lost sight of how much background knowledge the description assumes or requires. I hope these comments and the others embedded in the PDF are helpful, and I would be happy to discuss them directly with the author if so desired. Thanks, Greg Wilson Download annotated manuscript Cite this review as
Wilson G ( 2018 ) Peer Review #1 of "Verifiability in computer-aided research: the role of digital scientific notations at the human-computer interface (v0.1)" . PeerJ Computer Science https://doi.org/10.7287/peerj-cs.158v0.1/reviews/1
label_author_4
Reviewer 2 ·
Apr 22, 2018
Basic reporting
label_br_4
The basic reporting meets requirements in terms of language, citation, figures and scientific notation.
Experimental design
label_ed_4
The experimental design is appropriate, although the introduction and background is fairly lengthy. Methods are described with sufficient detail.
Validity of the findings
See main review.
Comments for the author
The article by Hinsen focuses on a needs assessment for understanding how users can make mistakes and have difficulty with reproducibility due to the complexity and black-box nature of scientific software. After a fairly lengthy discussion of issues encountered over the authors career, the paper identifies the key role of digital scientific notations at the human-computer interface. A proof of concept implementation of Leibniz as a language for specifying digital scientific notation from models as math equations is one part of a solution. It, however does not entirely solve problems due to the nature of non-determinism in most implementations, needs for a means to specify and understand constants, accuracy and means of implementing the math equations digitally. However, the overall discussion and issues raised are important and the prototype solution via a purposed domain specific language and documentation/code could be a worthwhile path forward. General adoption of such an approach, given the significantly complexity of long term and evolving community scientific/research codes that are growing, highly optimized, and complicated in terms of workflow is unlikely. The scientific validity (with some clarifications noted) appears sufficient and the article appears suitable to join the scholarly literature. Page 1, lines 38-39: It is stated that computations are fully deterministic. This is typically not true, especially in parallel, due to differing order of operations. Codes can be made fully deterministic, for example with fixed point precision on GPUs, however most often they are not implemented (when optimized) to be fully deterministic. Moreover, reproducibility does not necessarily require determinism. Page 2, lines 107-108: To this reviewer’s knowledge, even back in 1997, AMBER energies were not dependent in the order of atoms in the coordinate file since they were mapped back to the residue definitions. It may be possible that different order of atoms in the residue definitions could have led to different energies, but this needs to be clarified, i.e. exactly what input file is the author referring to. The issues with implementing force fields is significantly more complicated than simply atom-ordering and there are clear issues about choice of algorithms to implement that may be tricky to figure out without reading the code. For example, the choice of improper angles assigned in AMBER does depend on the alphabetic ordering of the atom types which means if you change a type from CA to ZA that may change the impropers assigned. Moreover, there are issues today with accuracy and speed where many shortcuts are made in the name of performance that may lead to different forces and ultimately results. Even implementation choices such as the value of pi, or the conversion factor from charge to kcal/mol; implementations of cutoffs with or without buffers and automated pairlist updates; use of SHAKE on all bonds (LINCS in Gromacs) versus SHAKE only on hydrogens. I would further note that AMBER has changed in significant ways since 1997 (> 20 years ago). Some issues with energy comparisons among programs is described in JCAMD 31, 147 (2017). Page 3, lines 125-126: Small effect on energy but due to chaos small differences can be strongly amplified – Do you have evidence for this? Although CHARMM and AMBER by default do not give equivalent energies and forces (although they can be made to get accuracy on forces to 10**-6 if constants are changed), they can give equivalent results on converged conformational sampling in the limits of sufficient sampling. Additionally, equilibrium properties can be converged in MD simulations starting from vastly different initial conditions given sufficient sampling. Force field errors can occur when re-implemented quite easily leading to anomalous results (for example see recent work of Hayatshahi et al. on dinucleotides JPCB 121, 451 (2017) who could not reproduce the work of Brown et al. JCTC 11, 2315 (2015)). Page 4, line 163: What is the evidence that “Peer review tends to be shallow”? Provide citation or consider softening the statement – for example, “Given time constraints on reviewers and the complexity of the modern scientific workflow and software that often may involve very large-scale calculations, I would speculate that in some cases initial peer review can be shallow.” Evidence that few reviewers re-do experiments or computations? In discussion of software only as binary, software not published, you may want to note that in the 90’s publishing of force field parameters became requirement for publication, yet some commercial force fields (Schrodinger) are not published and are proprietary (not available outside of their codes). Cite this review as
Anonymous Reviewer ( 2018 ) Peer Review #2 of "Verifiability in computer-aided research: the role of digital scientific notations at the human-computer interface (v0.1)" . PeerJ Computer Science https://doi.org/10.7287/peerj-cs.158v0.1/reviews/2 Download Original Submission (PDF)
- submitted Mar 5, 2018 All text and materials provided via this peer-review history page are made available under a Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.
About us - PeerJ team | Our publications | Benefits | Partnerships | Endorsements Awards Resources - FAQ | Careers | Pressroom | Terms of use | Privacy | Contact Academic boards - Advisors | Editors | Subject areas Follow us - PeerJ blog | Twitter | Facebook | LinkedIn | Pinterest Submission guides - PeerJ – Life and Environment | PeerJ Computer Science | PeerJ Chemistry Spread the word - Activities | Resources PeerJ feeds - Atom | RSS 1.0 | RSS 2.0 | JSON PeerJ Computer Science feeds - Atom | RSS 1.0 | RSS 2.0 | JSON Archives - PeerJ – Life and Environment | PeerJ Computer Science
©2012-2019 PeerJ, Inc | Public user content licensed CC BY 4.0 unless otherwise specified. PeerJ ISSN: 2167-8359 PeerJ Comput. Sci. ISSN: 2376-5992 PeerJ Preprints ISSN: 2167-9843
computer science
