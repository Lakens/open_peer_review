Review History for Comparison and benchmark of name-to-gender inference services [PeerJ]
PeerJ Computer Science PeerJ – the Journal of Life & Environmental Sciences PeerJ Computer Science PeerJ Physical Chemistry PeerJ Organic Chemistry PeerJ Inorganic Chemistry PeerJ Analytical Chemistry PeerJ Materials Science Visit PeerJ.org and get involved About PeerJ Journals Overview PeerJ Journals FAQ What we publish 5 Years publishing Solutions for authors Reputation High quality peer review Fast publishing Indexing and Impact Factor Global readership Feature comparison Reduced cost publishing Author feedback Early career researcher benefits Senior researcher benefits Open review (optional) Rebuttal letters Sections About the journal Sections Aquatic Biology Biochemistry, Biophysics and Molecular Biology Biodiversity and Conservation Bioinformatics and Genomics Brain and Cognition Ecology Environmental Science Microbiology Paleontology and Evolutionary Science Plant Biology Zoological Science About PeerJ Journals Overview PeerJ Journals FAQ What we publish 5 Years publishing Solutions for authors Reputation High quality peer review Fast publishing Indexing and Impact Factor Global readership Feature comparison Reduced cost publishing Author feedback Early career researcher benefits Senior researcher benefits Open review (optional) Rebuttal letters More Subjects Search articles Peer-reviewed Journals PeerJ (Life, Biological, Environmental and Health Sciences) PeerJ Computer Science PeerJ Physical Chemistry PeerJ Organic Chemistry PeerJ Inorganic Chemistry PeerJ Analytical Chemistry PeerJ Materials Science Preprints PeerJ Preprints Table of contents Table of Contents - current and archives PeerJ - Medicine articles PeerJ - Biology & Life science articles PeerJ - Environmental Science articles PeerJ - General bio (stats, legal, policy, edu) PeerJ Computer Science PeerJ Preprints Academic advisors Volunteer to review Collections Job listings Discussions Blog Institutional plans Reviews and awards Spread the word Who are we? Contact Login AUTHORS Peer Journals Overview Submission Guidelines Subject Areas Editorial Board Editorial Criteria Pricing General FAQ Computer Science FAQ Aims and Scope Author Interviews Policies and Procedures SUBMIT ARTICLE
Review History Comparison and benchmark of name-to-gender inference services To increase transparency, PeerJ operates a system of 'optional signed reviews and history'. This takes two forms: (1) peer reviewers are encouraged, but not required, to provide their names (if they do so, then their profile page records the articles they have reviewed), and (2) authors are given the option of reproducing their entire peer review history alongside their published article (in which case the complete peer review process is provided, including revisions, rebuttal letters and editor decision letters). New to public reviews? Learn more about optional signed reviews and how to write a better rebuttal letter .
Summary
The initial submission of this article was received on February 19th, 2018 and was peer-reviewed by 2 reviewers and the Academic Editor. The Academic Editor made their initial decision on April 27th, 2018. The first revision was submitted on May 31st, 2018 and was reviewed by the Academic Editor. The article was Accepted by the Academic Editor on June 4th, 2018.
label_version_1
Version 0.2 (accepted)
Ciro Cattuto
·
Jun 4, 2018
label_recommendation_1
·
Academic Editor
Accept
The revised manuscript satisfactorily addresses all of the minor points raised by the Reviewers, hence it is now suitable for publication.
Download Version 0.2 (PDF)
Download author's rebuttal letter
- submitted May 31, 2018
label_version_2
Version 0.1 (original submission)
Ciro Cattuto
·
Apr 27, 2018
label_recommendation_2
·
Academic Editor
Minor Revisions
This is a nicely written and useful paper evaluating the relative performance of different techniques / APIs for gender inference from names. It is a good fit for PeerJ CS in both scope and quality. The Reviewers point out some limitations and possible improvements that I believe can be carried out as minor modification to the existing manuscript. Addressing those minor modifications should be considered a necessary condition for an eventual final acceptance for publication. Some comments about the limitations of previously published work should be more carefully phrased, as they do not appear fully supported neither by the actual claims of the cited work, nor by the evidence presented by the Authors. This applies in particular to the comments about the cited work by Karimi et al.
label_author_1
Reviewer 1 ·
Mar 24, 2018
Basic reporting
label_br_1
This paper describes an evaluation of available methods (mostly web APIs) that allow to infer the gender of a person based on its name. The paper is overall written well and can be easily understood. Related work is described sufficiently (although given my background I cannot guarantee completeness). The paper structure is adequate. Raw data and source code for the paper is available and makes the impression to be complete. Personally, I find it somewhat strange to cite the address of the raw data of a paper as a regular citation.
Experimental design
label_ed_1
The paper pursues a clear and well defined goal. The outcome of the paper can be helpful to researchers in various academic disciplines. The paper is currently missing a statistical analysis of the findings, i.e., it is unclear which differences are statistically significant. I suggest to add the respective tests, see e.g., "Japkowicz, Nathalie, and Mohak Shah. Evaluating learning algorithms: a classification perspective. Cambridge University Press, 2011."
Validity of the findings
label_votf_1
The discussion should go into more detail with respect to bias potentially be implied by data used for the study (e.g., what are the supposed origins of the names, what is the share of Asian/African names, etc...). Looking at the raw data, I have a loose assumption that parts of the dataset (zbmath) are focused on western names. Indeed, it would be also interesting to split the analysis into the different data sources and report results. That would give the reader an impression how strongly results vary for different (sub-) datasets. The setting of Benchmark 2 is not completely clear to me. Is it "at most 5% of all names" (as in the paper, line 400) or "at most 5% of all classified names"? I would like this Benchmark to be described in more detail. For the setting in Benchmark 3, reporting the RoC curve would be the appropriate measure to use in my opinion. I strongly suggest to add that to the paper. See: "Bradley, Andrew P. "The use of the area under the ROC curve in the evaluation of machine learning algorithms." Pattern recognition 30.7 (1997): 1145-1159." Table 4, Table 5, and Table 6 contain very little information. I would suggest to just report mean _and standard deviation_ of the iterations.
Comments for the author
label_cfta_1
Figure 1 and its meaning/relevance should be described in more detail. Tables use German instead of English floating point separators (i.e. "0,002" instead of "0.002") Cite this review as
Anonymous Reviewer ( 2018 ) Peer Review #1 of "Comparison and benchmark of name-to-gender inference services (v0.1)" . PeerJ Computer Science https://doi.org/10.7287/peerj-cs.156v0.1/reviews/1
label_author_2
Reviewer 2 ·
Apr 19, 2018
Basic reporting
label_br_2
This paper represents a very interesting and worthy contribution to the field of automatic gender inference. Written English quality is consistent and of high quality, and both the paper and the overall argument are well-structured. Its problematization within the context of academic publishing and bibliometrics is both extensive and relatively up-to-date. However, all publications cited in the references section strictly pertain to automatic gender inference; given the importance and sophistication of the performance evaluation metrics and procedures used in this research, reference to seminal works pertaining to predictive analytics and confusion matrices would have been most welcome (at least some of them must have been consulted in doing this research). Finally, both analysis and discussions are directly related to the objective and problematics of the paper.
Experimental design
label_ed_2
This article fits nicely within the aims and scope of this journal. The investigation procedure is well described and could easily be replicated. The contribution of this research to the existing body of knowledge is also well stated. In sum, nothing negative to report regarding experimental design.
Validity of the findings
label_votf_2
Interpretation of the results is prudent, yet unambiguous. The authors have done a nice work collecting, compiling, and cleaning all the data necessary for the project. Two points need to be emphasized, however. In the data collection phase, the authors used nltk's Named Entity Recognizer to identify names in bibliographic records drawn from WoS. We know from experience that this algorithm has non-negligible efficiency and accuracy problems, especially in comparison to other, freely available alternatives such as Stanford's CoreNLP package. A few comments thereon would be desirable. Also, while the authors do mention the time- and country-dependency of automatic gender detection based on first names, the impact of these characteristics on the performance of the algorithms they evaluate as well as on the significance and scope of their research seems somewhat downplayed. To give one example (surely known by the authors), China represents the second biggest country in the world in terms of research output, but existing gender detection algorithms based on first names often perform poorly when it comes to disambiguating the gender of researchers of Asian origin. The fact that this isn't addressed in the evaluation procedure certainly affects the scope and impact of the research results, and the absence of any thorough discussion thereof reflects somewhat badly on the paper.
Comments for the author
label_cfta_2
No comments. Cite this review as
Anonymous Reviewer ( 2018 ) Peer Review #2 of "Comparison and benchmark of name-to-gender inference services (v0.1)" . PeerJ Computer Science https://doi.org/10.7287/peerj-cs.156v0.1/reviews/2 Download Original Submission (PDF)
- submitted Feb 19, 2018 All text and materials provided via this peer-review history page are made available under a Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.
About us - PeerJ team | Our publications | Benefits | Partnerships | Endorsements Awards Resources - FAQ | Careers | Pressroom | Terms of use | Privacy | Contact Academic boards - Advisors | Editors | Subject areas Follow us - PeerJ blog | Twitter | Facebook | LinkedIn | Pinterest Submission guides - PeerJ – Life and Environment | PeerJ Computer Science | PeerJ Chemistry Spread the word - Activities | Resources PeerJ feeds - Atom | RSS 1.0 | RSS 2.0 | JSON PeerJ Computer Science feeds - Atom | RSS 1.0 | RSS 2.0 | JSON Archives - PeerJ – Life and Environment | PeerJ Computer Science
©2012-2019 PeerJ, Inc | Public user content licensed CC BY 4.0 unless otherwise specified. PeerJ ISSN: 2167-8359 PeerJ Comput. Sci. ISSN: 2376-5992 PeerJ Preprints ISSN: 2167-9843
computer science
