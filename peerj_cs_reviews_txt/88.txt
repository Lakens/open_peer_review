Review History for Large-scale comparative visualisation of sets of multidimensional data [PeerJ]
PeerJ Computer Science PeerJ – the Journal of Life & Environmental Sciences PeerJ Computer Science PeerJ Physical Chemistry PeerJ Organic Chemistry PeerJ Inorganic Chemistry PeerJ Analytical Chemistry PeerJ Materials Science Visit PeerJ.org and get involved About PeerJ Journals Overview PeerJ Journals FAQ What we publish 5 Years publishing Solutions for authors Reputation High quality peer review Fast publishing Indexing and Impact Factor Global readership Feature comparison Reduced cost publishing Author feedback Early career researcher benefits Senior researcher benefits Open review (optional) Rebuttal letters Sections About the journal Sections Aquatic Biology Biochemistry, Biophysics and Molecular Biology Biodiversity and Conservation Bioinformatics and Genomics Brain and Cognition Ecology Environmental Science Microbiology Paleontology and Evolutionary Science Plant Biology Zoological Science About PeerJ Journals Overview PeerJ Journals FAQ What we publish 5 Years publishing Solutions for authors Reputation High quality peer review Fast publishing Indexing and Impact Factor Global readership Feature comparison Reduced cost publishing Author feedback Early career researcher benefits Senior researcher benefits Open review (optional) Rebuttal letters More Subjects Search articles Peer-reviewed Journals PeerJ (Life, Biological, Environmental and Health Sciences) PeerJ Computer Science PeerJ Physical Chemistry PeerJ Organic Chemistry PeerJ Inorganic Chemistry PeerJ Analytical Chemistry PeerJ Materials Science Preprints PeerJ Preprints Table of contents Table of Contents - current and archives PeerJ - Medicine articles PeerJ - Biology & Life science articles PeerJ - Environmental Science articles PeerJ - General bio (stats, legal, policy, edu) PeerJ Computer Science PeerJ Preprints Academic advisors Volunteer to review Collections Job listings Discussions Blog Institutional plans Reviews and awards Spread the word Who are we? Contact Login AUTHORS Peer Journals Overview Submission Guidelines Subject Areas Editorial Board Editorial Criteria Pricing General FAQ Computer Science FAQ Aims and Scope Author Interviews Policies and Procedures SUBMIT ARTICLE
Review History Large-scale comparative visualisation of sets of multidimensional data To increase transparency, PeerJ operates a system of 'optional signed reviews and history'. This takes two forms: (1) peer reviewers are encouraged, but not required, to provide their names (if they do so, then their profile page records the articles they have reviewed), and (2) authors are given the option of reproducing their entire peer review history alongside their published article (in which case the complete peer review process is provided, including revisions, rebuttal letters and editor decision letters). New to public reviews? Learn more about optional signed reviews and how to write a better rebuttal letter .
Summary
The initial submission of this article was received on May 20th, 2016 and was peer-reviewed by 2 reviewers and the Academic Editor. The Academic Editor made their initial decision on June 27th, 2016. The first revision was submitted on August 25th, 2016 and was reviewed by the Academic Editor. The article was Accepted by the Academic Editor on September 2nd, 2016.
label_version_1
Version 0.2 (accepted)
Luciano Sánchez
·
Sep 2, 2016
label_recommendation_1
·
Academic Editor
Accept
The authors have made substantial changes to the initial draft. In particular, a new timing experiment has been conducted for neuroscience and astronomical datasets, where CAVE2 is compared to a personal desktop computer. There are also minor changes in section 5.3, and also a compared analysis with other systems, along with many amendments and improvements of the explanation. Not all the suggestions of rev 2 have been followed. In particular, figures for standard techniques have not been included, however the justification given by the authors is admissible. Generally speaking, explanations in the rebuttal letter are reasonable.
Download Version 0.2 (PDF)
Download author's rebuttal letter
- submitted Aug 25, 2016
label_version_2
Version 0.1 (original submission)
Luciano Sánchez
·
Jun 27, 2016
label_recommendation_2
·
Academic Editor
Major Revisions
The technical quality of the draft could be further improved. The scientific contributions should be clearly stated and supported by data thus the reader can perceive the novelty of this work.
label_author_1
Luis Junco ·
Jun 7, 2016
Basic reporting
label_br_1
The article is clear and has a correct structure. Motivation is duly substantiated and related work is extensive and updated.
Experimental design
label_ed_1
Section 3 is somewhat generic in terms of the design description. Section 4, and more specifically 4.3, provides much space to the description of the control software implementation, but only describes a few examples. It could be explained in this section the appropriate set of tools and functionalities for the analysis of the two proposed problems: Magnetic Resonance Imaging (MRI) data from IMAGE-HD and large-scale systematic morphological classification of the kinematic structures of galaxies. Regard this second problem is cited in the article but later nothing is explained about him.
Validity of the findings
label_votf_1
The developed system, ENCUBE, has some excellent features such as: It allows comparative visualization and analysis of large amounts of data, actions aplied to data cubes in parallel, large display visualization area, very high processing power, collaborative workspace, stereoscopic capability and workflow serialization.
Comments for the author
label_cfta_1
It is a good tool for scientific visualization. Cite this review as
Junco L ( 2016 ) Peer Review #1 of "Large-scale comparative visualisation of sets of multidimensional data (v0.1)" . PeerJ Computer Science https://doi.org/10.7287/peerj-cs.88v0.1/reviews/1
label_author_2
Reviewer 2 ·
Jun 13, 2016
Basic reporting
label_br_2
No Comments.
Experimental design
label_ed_2
The submission must describe original primary research within the Scope of the journal. -> Ok The submission should clearly define the research question, which must be relevant and meaningful. The knowledge gap being investigated should be identified, and statements should be made as to how the study contributes to filling that gap. -> There is no research question. The investigation must have been conducted rigorously and to a high technical standard. -> There is no experiment. Methods should be described with sufficient information to be reproducible by another investigator. -> Ok The research must have been conducted in conformity with the prevailing ethical standards in the field. -> Ok
Validity of the findings
label_votf_2
The data should be robust, statistically sound, and controlled. -> There is no data gathering. The data on which the conclusions are based must be provided or made available in an acceptable discipline-specific repository. -> All conclusions are speculative. The conclusions should be appropriately stated, should be connected to the original question investigated, and should be limited to those supported by the results. -> This is not ok in the writing. Speculation is welcomed, but should be identified as such. -> It is not identified as speculative. Decisions are not made based on any subjective determination of impact, degree of advance, novelty, being of interest to only a niche audience, etc. Replication experiments are encouraged (provided the rationale for the replication, and how it adds value to the literature, is clearly described); however, we do not allow the ‘pointless’ repetition of well known, widely accepted results. -> The conclusions are mostly subjective. Negative / inconclusive results are acceptable. -> There are no experiments.
Comments for the author
label_cfta_2
The article describes encube, a data visualization deviced aimed at the exploration of large data. There is no doubt that the authors have put great effort into this work, and that it seems to present relevant features. However, the text has some serious flaws. Most of them seem to be linked to the excessive description of the system, which makes the paper miss points that are relevant in scientific literature: evaluation and the detection of the contributions of the work using objective data. These are my comments: “For a comprehensive review of the variety of standard techniques see, for example, Akenine-M ¨oller et al. (2008), Toriwaki and Yoshida (2009) and Szeliski (2010).” > It would be very useful for the reader to have some (two? Three?) figures with examples of standard techniques and how they work. Also, why are there three review papers on the subject in three consecutive years? It can be the case that there was a lot of innovation in the field in these years. Could the authors state what is the contribution of each of these reviews? “Structured three dimensional (3D) images or data cubes are ubiquitous in scientific research.” > I disagree with that. There are many fields in science that are oblivious about data cubes. Again, some figures and a deeper discussion would greatly increase the appeal of this paper. > Along “Related Work”: again, I miss some figures that could highlight the differences between visualization systems proposals and the preceeding ones. It would probably be a good idea to only select the two or three that provided inspiration for encube. In Section 5.3 > Without an user study, all of these discussions are based on anedoctal data or speculation. This article needs an user study. My next note also regards this question. > Although the authors state that there are three important questions (“1) how to integrate comparative visualisation and analysis into a unified system; 2) how to document the discovery process; and 3) how to enable scientists to continue the research process once back at their desktop.”). However: > 1) I am not convinced that this was discussed along the article. Again, the reader needs at least some comparative figures displaying how encube is different from previous systems. Also, this only accounts for the proposal of the system: proving that this point was properly addressed requires an user study. > 2) Again, an user study is necessary to validate the proposed system. How did users interact with the discovery process history? > 3) By research, I am guessing the authors mean “discovery” or “exploration”? The server-based service is a good solution for this. The user study, however, should highlight whether the server-based approach actually improved the data exploration experience. I believe that the core of the work (the proposed system) is of good quality, and that the authors should calmly address the issues above prior to publishing. Cite this review as
Anonymous Reviewer ( 2016 ) Peer Review #2 of "Large-scale comparative visualisation of sets of multidimensional data (v0.1)" . PeerJ Computer Science https://doi.org/10.7287/peerj-cs.88v0.1/reviews/2 Download Original Submission (PDF)
- submitted May 20, 2016 All text and materials provided via this peer-review history page are made available under a Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.
About us - PeerJ team | Our publications | Benefits | Partnerships | Endorsements Awards Resources - FAQ | Careers | Pressroom | Terms of use | Privacy | Contact Academic boards - Advisors | Editors | Subject areas Follow us - PeerJ blog | Twitter | Facebook | LinkedIn | Pinterest Submission guides - PeerJ – Life and Environment | PeerJ Computer Science | PeerJ Chemistry Spread the word - Activities | Resources PeerJ feeds - Atom | RSS 1.0 | RSS 2.0 | JSON PeerJ Computer Science feeds - Atom | RSS 1.0 | RSS 2.0 | JSON Archives - PeerJ – Life and Environment | PeerJ Computer Science
©2012-2019 PeerJ, Inc | Public user content licensed CC BY 4.0 unless otherwise specified. PeerJ ISSN: 2167-8359 PeerJ Comput. Sci. ISSN: 2376-5992 PeerJ Preprints ISSN: 2167-9843
computer science
