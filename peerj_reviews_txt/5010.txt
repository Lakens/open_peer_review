Review History for Unfamiliar face matching with photographs of infants and children [PeerJ]
PeerJ Journals Peer-reviewed PeerJ – the Journal of Life & Environmental Sciences PeerJ Computer Science PeerJ Physical Chemistry PeerJ Organic Chemistry PeerJ Inorganic Chemistry PeerJ Analytical Chemistry PeerJ Materials Science Preprints PeerJ Preprints Visit PeerJ.org and get involved About PeerJ Journals Overview PeerJ Journals FAQ What we publish 5 Years publishing Solutions for authors Reputation High quality peer review Fast publishing Indexing and Impact Factor Global readership Feature comparison Reduced cost publishing Author feedback Early career researcher benefits Senior researcher benefits Open review (optional) Rebuttal letters Sections About the journal Sections Aquatic Biology Biochemistry, Biophysics and Molecular Biology Biodiversity and Conservation Bioinformatics and Genomics Brain and Cognition Ecology Environmental Science Microbiology Paleontology and Evolutionary Science Plant Biology Zoological Science About PeerJ Journals Overview PeerJ Journals FAQ What we publish 5 Years publishing Solutions for authors Reputation High quality peer review Fast publishing Indexing and Impact Factor Global readership Feature comparison Reduced cost publishing Author feedback Early career researcher benefits Senior researcher benefits Open review (optional) Rebuttal letters More Subjects Search articles Advanced search of articles & preprints PeerJ - Medicine articles PeerJ - Biology & Life science articles PeerJ Computer Science PeerJ Preprints Table of contents Table of Contents - current and archives PeerJ - Medicine articles PeerJ - Biology & Life science articles PeerJ - Environmental Science articles PeerJ - General bio (stats, legal, policy, edu) PeerJ Computer Science PeerJ Preprints Academic advisors Volunteer to review Collections Job listings Discussions Blog Institutional plans Reviews and awards Spread the word Who are we? Contact Login AUTHORS Peer Journals Overview Submission Guidelines Subject Areas Editorial Board Editorial Criteria Pricing General FAQ Computer Science FAQ Aims and Scope Author Interviews Policies and Procedures SUBMIT ARTICLE
Review History Unfamiliar face matching with photographs of infants and children To increase transparency, PeerJ operates a system of 'optional signed reviews and history'. This takes two forms: (1) peer reviewers are encouraged, but not required, to provide their names (if they do so, then their profile page records the articles they have reviewed), and (2) authors are given the option of reproducing their entire peer review history alongside their published article (in which case the complete peer review process is provided, including revisions, rebuttal letters and editor decision letters). New to public reviews? Learn more about optional signed reviews and how to write a better rebuttal letter .
Summary
The initial submission of this article was received on March 28th, 2018 and was peer-reviewed by 2 reviewers and the Academic Editor. The Academic Editor made their initial decision on April 23rd, 2018. The first revision was submitted on May 29th, 2018 and was reviewed by the Academic Editor. The article was Accepted by the Academic Editor on May 29th, 2018.
label_version_1
Version 0.2 (accepted)
Lydia Hopper
·
May 29, 2018
label_recommendation_1
·
Academic Editor
Accept
Thank you for submitting your revised manuscript to PeerJ. I have reviewed all the edits you made, including the inclusion of new analyses of your existing datasets and the addition of data from two new experiments (3b and 4). I believe your revisions have made throughout addressed my and the reviewers' previous recommendations, resulting in a stronger article. It is my pleasure to recommend it for publication in PeerJ. # PeerJ Staff Note - this decision was reviewed and approved by Claire Fletcher-Flinn, a PeerJ Section Editor covering this Section #
Download Version 0.2 (PDF)
Download author's rebuttal letter
- submitted May 29, 2018
label_version_2
Version 0.1 (original submission)
Lydia Hopper
·
Apr 23, 2018
label_recommendation_2
·
Academic Editor
Minor Revisions
Thank you for your submission to PeerJ. Two experts in your field have provided reviews of your article, and I have also reviewed it. Both reviewers were complimentary about your studies and both provide detailed and thoughtful feedback. I will not reiterate their comments here, but I do encourage you to respond to each of their questions in turn. Additionally, I have a few points of feedback of my own. In particular, I feel that the descriptions of the three experiments could each benefit from the provision of some additional details to aid with the clarity of their presentation. Specifically, regarding Exp 1 I note that 86.7% of participants were female. Do you think this sampling bias might affect your results (i.e. is anything known about sex biases in facial recognition)? Please acknowledge this limitation. Additionally, please can you provide more detailed information regarding what instructions you gave participants prior to them starting the task and where and how the task was conducted. Regarding Exp 2, were the participants the same individuals who completed Exp 1? I assume not, but please state this explicitly. Additionally, as for Exp 1, the majority of participants were female, again, please acknowledge this limitation. Finally, were the 30 children used as the stimuli male and female? If so, what proportion were female? Regarding Exp 3, in general I found the methods for this experiment a little unclear. For example, were the participants shown both adult-adult pairings (new stimuli) as well as infant-infant pairings (from Exp 1) and infant-child pairings (from Exp 2)? If so, please clarify this and also say what number of pairings came from Exp 1 vs Exp 2 in addition to the novel 40 adult face stimuli. Additionally, what % of the 40 adult face stimuli were female? In addition to my comments regarding the description of your methods, I also felt that the Discussion was overly long. For example, the first paragraph felt very repetitious from the Introduction and could probably be omitted. I believe that if you can respond to the reviewers’ comments, in addition to my own, it is likely that I will be able to accept your article for publication in PeerJ, although of course this is not guaranteed.
label_author_1
Reviewer 1 ·
Apr 8, 2018
Basic reporting
label_br_1
This paper describes important research currently missing from the research literature and I believe it should be published in PeerJ once the issues listed below are addressed. I commend the authors for writing a very interesting and well thought out paper. It is simple but effective. Please ensure you report effect sizes throughout. There are some missing. Lines 207-213 – this section is slightly confusing as you report results of a ‘benchmark test’ – I would move this sentence down to the next paragraph? You then report the t-tests - % correct, sensitivity and criterion. Please additionally report whether hits and false alarms significantly differed from chance – sometimes these effects can be illuminating, and after all you report the descriptives in Table 1. Line 213 – you then return to the GFMT. Is this the long version of the GFMT? If so, please report trial numbers. And also explain why here you use the long version, later the short version (p 338). Line 224 and Table 1 – why not report hit and FA rates and run t-tests comparing with criterion? Line 230 – would it be possible to report confidence intervals when making this passport officer estimation? Line 309 – also report hits and false alarms t-tests and effect sizes.
Experimental design
label_ed_1
Meets all standards
Validity of the findings
label_votf_1
Meets all standards.
Comments for the author
label_cfta_1
Line 234 and 321– I think here you should have run a quick follow up study testing why sex of the infants was not informative in assisting ID judgements, as the results suggest there may be something unusual about the images you used. I would ask a few participants to make sex judgements to each infant image. It should not take long, and it would not need many participants but would answer the question as to whether sex information was somehow missing from your images, that was available in the other studies listed in Line 234. It might suggest that with a different set of images of infants, the results for the sex-incongruent results would differ. However, if the editor feels this is not necessary for publication then I am happy for you to leave as is. Cite this review as
Anonymous Reviewer ( 2018 ) Peer Review #1 of "Unfamiliar face matching with photographs of infants and children (v0.1)" . PeerJ https://doi.org/10.7287/peerj.5010v0.1/reviews/1
label_author_2
Matthew Fysh ·
Apr 13, 2018
Basic reporting
label_br_2
The article is clear and well-written. However, I have several suggestions for improving the quality of the manuscript further. 1. The review of the current face matching literature in the introduction is rather sparse and could be more comprehensive. For example, some key studies are missing from the introduction that set the context of the study, such as Burton et al.’s (2010) work with the GFMT showing baseline accuracy rates of 80% as a best-case scenario, and White et al.’s (2014) work with passport officers. Likewise, there is room for discussion of within-person variability in relation to Experiment 2 (see, e.g., Fysh & Bindemann, 2018; Megreya et al., 2013), and how this impacts face matching performance. 2. In the General Discussion (Lines 486-487), the authors write that “children’s faces are more homogenous than adult faces, displaying lower levels of between-face variability.” This is a logical point, but difficult to reconcile with the findings of Experiment 3 – if infant faces are homogenous, then couldn’t we logically expect a response bias in the direction of classifying faces as the same identity, rather than as different? 3. Would it be possible to also provide an example mismatch trial for the Infant-Infant matching task? 4. The link between Experiments 1 and 2 is not explicitly stated, although the rationale is straightforward. A few lines at the end of the discussion for Experiment 1 to introduce the motivation for Experiment 2 would help clarify this. 5. There is little discussion of the findings of Experiment 2. These could be contextualised in line with the current face matching literature (e.g., within-person variability).
Experimental design
label_ed_2
The research question is timely and relevant to face matching in practical settings such as passport control. There is clear value in understanding the extent to which child and infant photographs can be accurately matched. However, I find some aspects of the current experimental design to be problematic. These concerns are specified in further detail below. 1. For Experiment 3, the authors note that they did not restrict themselves to same-sex pairings for mismatch trials in the infant-infant and infant-child matching tasks (Line 373). If this is the case, why are differences between same-sex and different-sex pairings not reported as they are in Experiments 1 and 2? In general, this consistent aspect of the design across experiments is problematic, given that this introduces a clear distinction between match (always same-sex) and mismatch trials (sometimes same-sex), and it isn’t clear as to why the researchers chose to construct their mismatch trials in this way. 2. In line with the above point – given that the trial types are so different here, I do not think that it is appropriate to collapse accuracy across match and mismatch trials when analysing performance. Instead, Experiment 3 would be far more informative if the data were analysed via a 2 (trial: match vs. mismatch) x 3 (matching task: infant-infant vs. infant-child vs. GFMT) repeated measures ANOVA. 3. I also have some further concerns about the selection criteria used for stimuli in Experiment 3, namely that items in the new infant-infant and infant-child matching tasks were based on by-item accuracy from Experiments 1 and 2. If my understanding of the first two experiments is correct, then this means that for each match trial, 30 data points were available, but only 1 data point was available per mismatch trial because these were randomly generated for each participant. Is this correct? If so, then these seem to be rather loose criteria for selection of mismatching stimuli to be included in the final test. 4. In Experiment 3, it is reported that 37 participants were recruited, and then in the results it is reported that data from three participants was excluded, with the analysis being based on the remaining 37 participants. This detail should be amended and/or clarified in the ‘Participants’ section of Experiment 3.
Validity of the findings
label_votf_2
The findings are interesting and novel. Considered together, this research suggests that it is extremely challenging to match images of two infants (Experiment 1), as well as infant-to-child photographs (Experiment 2), and that these are notably more difficult than matching two (highly optimised) images of adults (Experiment 3). There are some caveats to this work that should be addressed, however, and there is room to extend these findings further. Again, suggestions for improving the current set of results are provided below. 1. The researchers should consider including some reliability measures for the infant-infant and infant-child matching tasks (e.g., Cronbach's alpha or Split-Half Reliability). 2. In general, the sample sizes are rather small across all experiments, and differences between the construction of the tasks used makes it difficult to assess whether the results are actually replicating between Experiments 1 and 2 to Experiment 3. It would be worth re-running Experiment 3 with a greater number of participants to replicate the main findings (i.e. that infant-infant and infant-child matching is more difficult than matching adult faces), as well as clarify the marginal findings (e.g., correlations between tests), which may have been unstable due to the low sample sizes (e.g., Schönbrodt & Perugini, 2013). 3. The researchers may also want to consider examining how accuracy for infant-infant and infant-child matching relates to additional, more difficult tests of face matching (e.g., Kent Face Matching Test, Model Face Matching Test, Good Bad Ugly Face Test, etc.). 4. In Experiment 1, why was accuracy compared to performance in the GFMT as found by Bobak, Dowsett, et al. (2016), as opposed to Burton et al. (2010)? Likewise, why is performance in Experiment 2 also not compared to GFMT levels of accuracy?
Comments for the author
label_cfta_2
In general, this research explores an important set of questions and I would like to see the findings published, following some revisions. I hope that the suggestions that are supplied here will be of some value moving forward with the manuscript. Cite this review as
Fysh M ( 2018 ) Peer Review #2 of "Unfamiliar face matching with photographs of infants and children (v0.1)" . PeerJ https://doi.org/10.7287/peerj.5010v0.1/reviews/2 Download Original Submission (PDF)
- submitted Mar 28, 2018 All text and materials provided via this peer-review history page are made available under a Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.
About us - PeerJ team | Our publications | Benefits | Partnerships | Endorsements Awards Resources - FAQ | Careers | Pressroom | Terms of use | Privacy | Contact Academic boards - Advisors | Editors | Subject areas Follow us - PeerJ blog | Twitter | Facebook | LinkedIn | Pinterest Submission guides - PeerJ (life - bio - med) | Computer Science | Chemistry | PeerJ Preprints instructions Spread the word - Activities | Resources PeerJ feeds - Atom | RSS 1.0 | RSS 2.0 | JSON PeerJ Computer Science feeds - Atom | RSS 1.0 | RSS 2.0 | JSON PeerJ Preprint feeds - Atom | RSS 1.0 | RSS 2.0 | JSON Archives - PeerJ | PeerJ Computer Science | PeerJ Preprints
©2012-2019 PeerJ, Inc | Public user content licensed CC BY 4.0 unless otherwise specified. PeerJ ISSN: 2167-8359 PeerJ Comput. Sci. ISSN: 2376-5992 PeerJ Preprints ISSN: 2167-9843
brain cognition
