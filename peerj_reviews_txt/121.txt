Review History for Towards objectively quantifying sensory hypersensitivity: a pilot study of the “Ariana effect” [PeerJ]
PeerJ Journals Peer-reviewed PeerJ – the Journal of Life & Environmental Sciences PeerJ Computer Science PeerJ Physical Chemistry PeerJ Organic Chemistry PeerJ Inorganic Chemistry PeerJ Analytical Chemistry PeerJ Materials Science Preprints PeerJ Preprints Visit PeerJ.org and get involved About PeerJ Journals Overview PeerJ Journals FAQ What we publish 5 Years publishing Solutions for authors Reputation High quality peer review Fast publishing Indexing and Impact Factor Global readership Feature comparison Reduced cost publishing Author feedback Early career researcher benefits Senior researcher benefits Open review (optional) Rebuttal letters Sections About the journal Sections Aquatic Biology Biochemistry, Biophysics and Molecular Biology Biodiversity and Conservation Bioinformatics and Genomics Brain and Cognition Ecology Environmental Science Microbiology Paleontology and Evolutionary Science Plant Biology Zoological Science About PeerJ Journals Overview PeerJ Journals FAQ What we publish 5 Years publishing Solutions for authors Reputation High quality peer review Fast publishing Indexing and Impact Factor Global readership Feature comparison Reduced cost publishing Author feedback Early career researcher benefits Senior researcher benefits Open review (optional) Rebuttal letters More Subjects Search articles Advanced search of articles & preprints PeerJ - Medicine articles PeerJ - Biology & Life science articles PeerJ Computer Science PeerJ Preprints Table of contents Table of Contents - current and archives PeerJ - Medicine articles PeerJ - Biology & Life science articles PeerJ - Environmental Science articles PeerJ - General bio (stats, legal, policy, edu) PeerJ Computer Science PeerJ Preprints Academic advisors Volunteer to review Collections Job listings Discussions Blog Institutional plans Reviews and awards Spread the word Who are we? Contact Login AUTHORS Peer Journals Overview Submission Guidelines Subject Areas Editorial Board Editorial Criteria Pricing General FAQ Computer Science FAQ Aims and Scope Author Interviews Policies and Procedures SUBMIT ARTICLE
Review History Towards objectively quantifying sensory hypersensitivity: a pilot study of the “Ariana effect” To increase transparency, PeerJ operates a system of 'optional signed reviews and history'. This takes two forms: (1) peer reviewers are encouraged, but not required, to provide their names (if they do so, then their profile page records the articles they have reviewed), and (2) authors are given the option of reproducing their entire peer review history alongside their published article (in which case the complete peer review process is provided, including revisions, rebuttal letters and editor decision letters). New to public reviews? Learn more about optional signed reviews and how to write a better rebuttal letter .
Summary
The initial submission of this article was received on June 11th, 2013 and was peer-reviewed by 3 reviewers and the Academic Editor. The Academic Editor made their initial decision on July 1st, 2013. The first revision was submitted on July 9th, 2013 and was reviewed by 1 reviewer and the Academic Editor. The article was Accepted by the Academic Editor on July 14th, 2013.
label_version_1
Version 0.2 (accepted)
Marco Iacoboni
·
Jul 14, 2013
label_recommendation_1
·
Academic Editor
Accept
Thanks for addressing all our concerns and for submitting your work to PeerJ.
label_author_1
T. Sigi Hale ·
Jul 11, 2013
Basic reporting
label_br_1
nothing
Experimental design
label_ed_1
nothing
Validity of the findings
label_votf_1
nothing
Comments for the author
label_cfta_1
nothing Cite this review as
Hale TS ( 2013 ) Peer Review #1 of "Towards objectively quantifying sensory hypersensitivity: a pilot study of the “Ariana effect” (v0.2)" . PeerJ https://doi.org/10.7287/peerj.121v0.2/reviews/1 Download Version 0.2 (PDF)
Download author's rebuttal letter
- submitted Jul 9, 2013
label_version_2
Version 0.1 (original submission)
Marco Iacoboni
·
Jul 1, 2013
label_recommendation_2
·
Academic Editor
Major Revisions
I am concerned that “The researcher applying the tactile stimulation was not always blind to the degree of SH of the participants.” This may have created unconscious bias in hand-delivered tactile stimulation. I think that what you should do is to run additional subjects with research assistants that are completely blind to the degree of SH and even to the rationale of the study. You should also probably remove from data analysis the subjects that have been stimulated by researchers not blind to the SH of the participants. Having a larger number of subjects tested in each group and by research assistants blind to SH level of participants should address some of the concerns of the reviewers. You should also carefully address the excellent constructive criticisms of the reviewers, including the ones on statistical analyses and potentially distracting effects of movements from the person delivering the tactile stimulation.
label_author_2
Moya Kinnealey ·
Jul 1, 2013
Basic reporting
label_br_2
Please check the Title of the figures for clarity
Experimental design
label_ed_2
No comments
Validity of the findings
label_votf_2
No Comments
Cite this review as
Kinnealey M ( 2013 ) Peer Review #1 of "Towards objectively quantifying sensory hypersensitivity: a pilot study of the “Ariana effect” (v0.1)" . PeerJ https://doi.org/10.7287/peerj.121v0.1/reviews/1
label_author_3
T. Sigi Hale ·
Jul 1, 2013
Basic reporting
label_br_3
Abstract: Point 1: Authors states: “In an attempt to objectively quantify SH severity, the authors tested whether a steady tactile stimulus distracts subjects with SH from a cognitive task more than it does subjects without SH.” It seems obvious that patients with SH should be more distracted. It’s like asking if people with a snake phobia will be more distracted by a snake on their desk than people without a snake phobia. Perhaps, it makes more sense to state something to the effect of: “In an attempt to objectively quantify SH severity, the authors tested the extent to which steady tactile stimulus distracts subjects with SH versus subjects without SH during the performance of choice response task.” The key contribution here seems to be to demonstrate a developing method to quantify SH severity. I don’t know this literature, but it seems obvious that a tactile stimulus should distract an SH sample more so than controls. Introduction: Point 1: It is not clear whether the main objective is to test the hypothesis that tactile stimulation will distract SH patients more than controls, or to assess a potential method for quantifying SH severity that may have clinical and investigational utility. Perhaps, the authors could clarify that a positive relationship between SH severity and distractibility via tactile stimulation is expected, and that the primary goal of this work is to assess whether that dynamic can be behaviorally quantified. Figures Figure 1 The authors’ state: “The line graph shows for each group of 10 trials the mean over subjects of the median reaction time for that block within each subject.” I am very confused by this image. Blocks were 30 trials long. RT scores were pulled from the median within those blocks and then averaged per condition. This figure is not showing trail-based data, nor is it showing block based data, but seems to be showing RT data for successive groupings of 10 trials? I don’t see how this is relevant to the reported results. Perhaps, I am missing something here?
Experimental design
label_ed_3
Method: Point 1 It would be nice to know the inter-trial interval. Point 2 The interpretation of results requires considering potential attentional confounds associated with sensory expectation versus active SH effects alone (stimulations condition was not randomized). Perhaps this should be mentioned in the limitations. Point 3 Why was the ASQ administered after the CRT task? The task-taking experience may have biased the ratings. This should perhaps be addressed in the limitations. Point 4 Seems that block one served as a training block. Why not just call it that? Point 5 I am very confused as to why the authors used median RT per block to create their averaged RT per condition. More typically, one would average across trials per condition. In any case, I don’t see this as wrong per se, but it is unusual, and as such, the authors may want to provide some basis for choosing that approach. Point 6 Regarding the assessment of group differences on stimulation effects (i.e., the difference score), it seems a repeated measures ANOVA would be suitable (group x condition, with RT as the dependent). That being said, the methods seem ok, just unusual. Still, given the total sample is only 19, the authors may want to report directly in the methods what the resultant n-sizes per group are after making this split. Also, some information here about how the threshold was chosen may be helpful (i.e., based on clinical diagnostic cut-offs?). It would also be useful to note the gender ratios and ages in the resultant groups. I imagine they are different, and it would be nice to be convinced that these are not driving the results. Point 7 Because the sample size is small, it seems optimal to assess the relationship between stimulation-distraction and SH rating using the ASQ as a continuous measure. This was done with the correlation analysis. However, this doesn’t tell us if the correlation is driven by one condition in particular (i.e., hopefully by the stim condition). It would be nice (not necessary) to see separate correlation analyses between ASQ and each condition. Alternatively, one might consider a regression looking at the interaction of block-type x ASQ predicting RTs. Point 8 Why was median RT collected within blocks (and then averaged), but accuracy assessed across blocks? Point 9 In the analysis of within block effects, again, why use median versus mean? If there are large effects loaded in particular aspects of a block (e.g., last few and/or first few trials), it seems median scores would reduce the chance of finding such effects.
Validity of the findings
label_votf_3
Results Point 1 With the small n size in the no-SH group, I’m wondering how the analyses would have turned out lumping the ‘fence sitters’ into a ‘low-SH’ group (no-SH+moderate-SH vs. clinical-SH). In any case, given the very low sample size of the no-SH group, the continuous approach makes the most sense (i.e., used for the correlation analysis, or the regression I suggested). Also, the authors might consider reporting the mean difference score for each of the 3 diagnostic groups (no SH, marginal SH, and definite SH) so the readers can see the pattern themselves. Finally, if there’s any reason to suspect gender and age differences might be impacting the group analysis, this should be addressed. Discussion Point 1 The authors state, “All these limitations, along with our modest sample size, would not bias the results, but would tend to reduce our power to find significant effects.” I disagree. With only 5 subjects in the no-SH group, variability in the delivery of stimulation could have biased results, as well as, other factors that were not controlled (e.g., response hand relative to side of stimulation, gender, age, etc.). Point 2 The discussion seems to suggest that evaluating whether SH subjects would show a larger interference effect is the main objective. Perhaps it is, but this should be clarified in relation to the core objective of developing a means to quantify SH severity. In my view, the methods development narrative is more compelling. With only 5 subjects in the no SH group, and the correlation being weakly significant, the relevance seems more about the development of a new approach to quantify SH severity.
Cite this review as
Hale TS ( 2013 ) Peer Review #2 of "Towards objectively quantifying sensory hypersensitivity: a pilot study of the “Ariana effect” (v0.1)" . PeerJ https://doi.org/10.7287/peerj.121v0.1/reviews/2
label_author_4
Reviewer 3 ·
Jun 29, 2013
Basic reporting
label_br_4
This is a pilot study testing the hypothesis that somatic hypersensitivity influences the performance on cognitive tasks requiring sustained attention. The idea behind the study rationale is logical and interesting. The study is sufficiently well written and clear in all its parts. Results are potentially interesting, although I have some reservations on the study design.
Experimental design
label_ed_4
My major concern is with the way the distracting tactile stimuli were administered. If these were delivered by hand by one of the researchers, how were participants blinded to movements performed by the person who delivered the stimuli? In other words, how did the authors make sure that distraction was uniquely dependent on the tactile stimuli and not also by the close presence of the person delivering the stimuli? More details on the experimental setting are necessary.
Validity of the findings
label_votf_4
Another major concern comes from the heterogeneous clinical sample. Some patients have ADHD, others OCD, others TS, others apparently none of these neurodevelopmental disorders. There was no attempt to adjust for potential confounding of primary diagnosis (or, even better, of the degree of inattention) on the performance on a reaction time task that is structured like a continuous performance task, thus potentially impaired at baseline in patients with attention deficit.
Comments for the author
Hand-delivered stimuli represent a great source of variability, and definitely less than ideal for a sound paradigm. In improving the paradigm for future studies, the authors should avoid this limitation. Cite this review as
Anonymous Reviewer ( 2013 ) Peer Review #3 of "Towards objectively quantifying sensory hypersensitivity: a pilot study of the “Ariana effect” (v0.1)" . PeerJ https://doi.org/10.7287/peerj.121v0.1/reviews/3 Download Original Submission (PDF)
- submitted Jun 11, 2013 All text and materials provided via this peer-review history page are made available under a Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.
About us - PeerJ team | Our publications | Benefits | Partnerships | Endorsements Awards Resources - FAQ | Careers | Pressroom | Terms of use | Privacy | Contact Academic boards - Advisors | Editors | Subject areas Follow us - PeerJ blog | Twitter | Facebook | LinkedIn | Pinterest Submission guides - PeerJ (life - bio - med) | Computer Science | Chemistry | PeerJ Preprints instructions Spread the word - Activities | Resources PeerJ feeds - Atom | RSS 1.0 | RSS 2.0 | JSON PeerJ Computer Science feeds - Atom | RSS 1.0 | RSS 2.0 | JSON PeerJ Preprint feeds - Atom | RSS 1.0 | RSS 2.0 | JSON Archives - PeerJ | PeerJ Computer Science | PeerJ Preprints
©2012-2019 PeerJ, Inc | Public user content licensed CC BY 4.0 unless otherwise specified. PeerJ ISSN: 2167-8359 PeerJ Comput. Sci. ISSN: 2376-5992 PeerJ Preprints ISSN: 2167-9843
brain cognition
