Review History for SKIMMR: facilitating knowledge discovery in life sciences by machine-aided skim reading [PeerJ]
PeerJ Journals Peer-reviewed PeerJ – the Journal of Life & Environmental Sciences PeerJ Computer Science PeerJ Physical Chemistry PeerJ Organic Chemistry PeerJ Inorganic Chemistry PeerJ Analytical Chemistry PeerJ Materials Science Preprints PeerJ Preprints Visit PeerJ.org and get involved About PeerJ Journals Overview PeerJ Journals FAQ What we publish 5 Years publishing Solutions for authors Reputation High quality peer review Fast publishing Indexing and Impact Factor Global readership Feature comparison Reduced cost publishing Author feedback Early career researcher benefits Senior researcher benefits Open review (optional) Rebuttal letters Sections About the journal Sections Aquatic Biology Biochemistry, Biophysics and Molecular Biology Biodiversity and Conservation Bioinformatics and Genomics Brain and Cognition Ecology Environmental Science Microbiology Paleontology and Evolutionary Science Plant Biology Zoological Science About PeerJ Journals Overview PeerJ Journals FAQ What we publish 5 Years publishing Solutions for authors Reputation High quality peer review Fast publishing Indexing and Impact Factor Global readership Feature comparison Reduced cost publishing Author feedback Early career researcher benefits Senior researcher benefits Open review (optional) Rebuttal letters More Subjects Search articles Advanced search of articles & preprints PeerJ - Medicine articles PeerJ - Biology & Life science articles PeerJ Computer Science PeerJ Preprints Table of contents Table of Contents - current and archives PeerJ - Medicine articles PeerJ - Biology & Life science articles PeerJ - Environmental Science articles PeerJ - General bio (stats, legal, policy, edu) PeerJ Computer Science PeerJ Preprints Academic advisors Volunteer to review Collections Job listings Discussions Blog Institutional plans Reviews and awards Spread the word Who are we? Contact Login AUTHORS Peer Journals Overview Submission Guidelines Subject Areas Editorial Board Editorial Criteria Pricing General FAQ Computer Science FAQ Aims and Scope Author Interviews Policies and Procedures SUBMIT ARTICLE
Review History SKIMMR: facilitating knowledge discovery in life sciences by machine-aided skim reading To increase transparency, PeerJ operates a system of 'optional signed reviews and history'. This takes two forms: (1) peer reviewers are encouraged, but not required, to provide their names (if they do so, then their profile page records the articles they have reviewed), and (2) authors are given the option of reproducing their entire peer review history alongside their published article (in which case the complete peer review process is provided, including revisions, rebuttal letters and editor decision letters). New to public reviews? Learn more about optional signed reviews and how to write a better rebuttal letter .
Summary
The initial submission of this article was received on December 6th, 2013 and was peer-reviewed by 2 reviewers and the Academic Editor. The Academic Editor made their initial decision on December 19th, 2013. The first revision was submitted on April 8th, 2014 and was reviewed by 2 reviewers and the Academic Editor. A further revision was submitted on June 20th, 2014 and was reviewed by the Academic Editor. The article was Accepted by the Academic Editor on June 23rd, 2014.
label_version_1
Version 0.3 (accepted)
Harry Hochheiser
·
Jun 23, 2014
label_recommendation_1
·
Academic Editor
Accept
Thank you for addressing the concerns raised by reviewer 1 - these revisions have clearly made the paper stronger.
Download Version 0.3 (PDF)
Download author's rebuttal letter
- submitted Jun 20, 2014
label_version_2
Version 0.2
Harry Hochheiser
·
May 6, 2014
label_recommendation_2
·
Academic Editor
Minor Revisions
Reviewer 1 has made several suggestions regarding the organization of the paper (Basic reporting); formatting - particularly with regards to LaTeX (Validity of the findings) and comments for the author. Comments under "Validity of the findings" raise some points that might be addressed in the discussion. Addressing these concerns should not be terribly difficult. The resulting revision is likely to be substantially stronger.
label_author_1
Richard Boyce ·
May 5, 2014
Basic reporting
label_br_1
I applaud the effort that the authors have taken to address the first set of critiques. The addition of an evaluation is a great improvement. Also, the review of related work is much more comprehensive. Unfortunately, the structure of the article is now a bit tangled with the addition of the new evaluations. As it is, the article is cumbersome to read and far too technically detailed for the reader to get the important points and adequately assess the validity of results. The methods for the evaluation should be placed in the "methods" section rather than the results. There is now a great excess of formulas and other technical information that would be best placed in a supplement. Noted limitations (e.g, the use of the GENIA corpus) would be better placed in a "Potential Lmitations" section. For readibility, I think that all formulas should be numbered and many of them should go into the supplement since they seem pretty typical for the domain.
Experimental design
label_ed_1
The simulations are intriguing, creative, and reportable. The results do not seem overstated and its clear that further work requires a solid user study which is a stated goal of future work.
Validity of the findings
label_votf_1
The results seem to have face validity and this reviewer can find no obvious source of bias that would limit the author's finding beyond what they have stated. The precision and recall analysis yielded apparently poor results and deriving these statistics from overlap of related articles requires some more thought. This is because the original algorithm for PubMed related articles is not really a gold standard because it is known to produce false associations ( see its "precision at 5" in the original evaluation). So, its just a comparative algorithm which might suggest that using agreement over chance (e.g., Kappa) might be a better metric than Prec and Recall and more informative than correlation. That being said, this reviewer did not have time to work through the numerous formulas to check for proper usage and accuracy. It seems that numerous equations have a dangling or non-relevant apostrophe which might be a side-effect of the use of commas at the end of formulas written in LaTeX. For example, the first formula in section 2.1: $$ cooc((e_x,e_y),PubMed_{PMID})=sum_{i,j in S(e_x,e_y)} rac{1}{1+|i - j|}, $$ This seems to be a very common problem in the paper.
Comments for the author
label_cfta_1
Minor comments: Abstract - 2nd sentence of results - missing 'the': 'that are <the> most relevant' Abstract - I think that "formal comparison" should be broken into the methods and results. Describing a methodology in the results is not typical. The authors did an evaluation so report it as a scientific evaluations 2.2.1 third sentence - co-occuring terms <are> associated Footnotes: the use of raw IP addresses to link to materials is not robust over time. Suggest changing to PURLS before publication line 390 - wrong single quote symbol before 'Magnetic p22, par 2: 'we construct *an* edge' (currently 'and') near line 460: 'we use either <an> index...' Figures - the y axes are bit hard to see and the X axis nearly impossible. Cite this review as
Boyce R ( 2014 ) Peer Review #1 of "SKIMMR: facilitating knowledge discovery in life sciences by machine-aided skim reading (v0.2)" . PeerJ https://doi.org/10.7287/peerj.483v0.2/reviews/1
label_author_2
Reviewer 2 ·
May 6, 2014
Basic reporting
label_br_2
see below
Experimental design
label_ed_2
see below
Validity of the findings
label_votf_2
see below
Comments for the author
label_cfta_2
The authors have addressed all of the issues brought up by the reviewers. Of most concern and most importantly, they have devised an empirical test which instills more confidence in their conclusions. Although the test is based on anticipated behavior, I am satisfied that it provides evidence of the system performance to back up some of the claims. In addition, they have substantially rewritten the literature review, grounding their work in the context of other similar work. I therefore recommend publication of this revised version. Cite this review as
Anonymous Reviewer ( 2014 ) Peer Review #2 of "SKIMMR: facilitating knowledge discovery in life sciences by machine-aided skim reading (v0.2)" . PeerJ https://doi.org/10.7287/peerj.483v0.2/reviews/2 Download Version 0.2 (PDF)
Download author's rebuttal letter
- submitted Apr 8, 2014
label_version_3
Version 0.1 (original submission)
Harry Hochheiser
·
Dec 19, 2013
label_recommendation_3
·
Academic Editor
Major Revisions
The reviewers have identified several significant concerns that should be addressed in a revision. As I read the reviews, the following items seem to me to be the most critical: 1. Background discussions omit important prior work. 2. Inaccuracies in discussion of Pubmed and some biomedical ontologies. 3. Lack of detail on concept extraction. 4. Lack of evaluation. Although the paper describes ongoing user tests, no results are provided. Perhaps most critically, this paper does not present a complete evaluation. The evaluation framework presented in the paper fail to meet the requirements for experimental design and validity of findings described in the PeerJ editorial criteria (https://peerj.com/about/editorial-criteria/). A revised version should addresss these concerns.
label_author_3
Richard Boyce ·
Dec 18, 2013
Basic reporting
label_br_3
The manuscript is very nicely written using clear and understandable English with very few spelling or grammatical errors. The Introduction and related work sections do demonstrate how the work fits into the broader field of knowledge. Also, the sections cite literature appropriately but are missing some important related systems [1-3]. I have more to say on this in the discussion of the validity of findings. Some statements and points could be presented more accurately. One is that on line 55 the authors imply that PubMed ranks its search results. This is not true since PubMed uses Boolean retrieval. The initial results are *sorted* by date of indexing within PubMed. Users have many other options for sorting. PubMed Related Article Search (see [4]) does rank by document "eliteness" but this is not the default search interface. In the relate work section, the authors correctly compare their work to GoPubMed but do not do a good enough job of describing the system (e.g., that it shows users text snippets along with tags from ontologies) or the ontologies that are used. GoPubMed uses Medical Subject Headings (MeSH) and the Gene Ontology (GO) both of which cannot simply be dismissed as "rigid, manually built ontologies". Rather, they are quite comprehensive for the biomedical and biological domains respectively, are the product of many thousands of man hours of collaborative development among domain experts, and their benefits for information retrieval have been well studied. Figure seem relevant and at an acceptable resolution. Some of the tables shown in the examples contain font sizes that are too small. 1. W. Pratt and H. Wasserman. Querycat: Automatic categorization of medline queries, 2000. 2. Wanda Pratt. Dynamic organization of search results using the UMLS. In AMIA Proceeding Fall 1997, pages 480-484, 1997. 3. http://www-01.ibm.com/software/data/information-optimization/ 4. Jimmy Lin and W John Wilbur, “PubMed related articles: a probabilistic topic-based model for content similarity,” BMC Bioinformatics 8, no. 1 (2007): 423.
Experimental design
label_ed_3
Its not clear to me that the work described fits with the scope of the journal because an actual experiment is not being reported. The manuscript reports in very nice detail the design of a novel information retrieval system, providing several examples to motivate the methods discussion. The authors also suggest an experimental design and point to tooling on the system's website to support the proposed experiment. However, I am not sure if these laudible efforts satisfy the scope of the journal. For example, the system design might be better placed in a journal like the Journal for Biomedical Informatics or IEEE Transactions on Information Technology in Biomedicine [2]. Likewise, the experimental design might be better placed in the JMIR Research Protocols [3]. Apart from these significant concerns, the methods writeup and the downloadable/accessible software artifacts are all good quality. Issues that could easily addressed include 1) the authors should make clear what the colored edges represent in the graph figures, the use of PURLs for URIs to the project artifacts would make the links easier to use and more permanent, No ethical concerns. 1. www.journals.elsevier.com/journal-of-biomedical-informatics/<U+200E> 2. http://ieeexplore.ieee.org/xpl/RecentIssue.jsp?reload=true&punumber=4233 3. www.researchprotocols.org/<U+200E>
Validity of the findings
label_votf_3
I have no concerns about the actual software artifacts or the computational methods reported. The biggest concerns have to do with the conclusions. The authors claim that the importance of their work is that it contributes a novel paradigm for search, skimming. However, the proposed methods seems to me to be quite similar to "spreading activation search" (aka "associative retrieval") discussed in [1-3]. Thus, the novelty is questionable. Important aspect of the target documents (PubMed abstracts) are not discussed such as that all PubMed abstracts are already tagged with topical concepts from the Medical Subject Headings vocabulary. It would seem that this high-quality, curated, data would be very valuable to the use case. Another serious concern is the lack of detail provided on how concepts are extracted from documents. All that is provided is a like to the LingPipe toolkit. The obvious question is how accurate the extaraction approach is. After some searching of my own, I found that, for the biomedical domain, LingPipe uses HMM classifiers based on the GENIA corpus. This is an old corpus and one question is how useful and valid the extraction is compared to modern tools such as the UMLS MetaMap and SemRep tools [4] or the Bioportal Annotator which uses MGrep and a few hundred ontologies [5]. In the discussion, the comments on the difficulty of acquiring participants for user studies seem anecdotal and detract from the paper. Also, I question why a more conventional IR evaluation could not be designed using existing corpora such as that developed for the TREC Genomics Track [6] that was used in evaluation of both GoPubMed and PubMed Related Article Search. 1. R. K. Belew. Adaptive information retrieval: using a connectionist representation to retrieve and learn about documents. In SIGIR '89: Proceedings of the 12th annual international ACM SIGIR conference on Research and development in information retrieval, pages 11-20, New York, NY, USA, 1989. ACM Press. 2. Richard K. Belew. Finding out about: a cognitive perspective on search engine technology and the WWW. Cambridge, 2000. 3. F. Crestani. Application of spreading activation techniques in informationretrieval. Artif. Intell. Rev., 11(6):453-482, 1997. 4. http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3540517/ 5. Noy et al. BioPortal: Ontologies and integrated data resources at the click of a mouse. Nucleic Acids Research 37:W170-173, 2009 6. Cohen AM, Hersh WR. The TREC 2004 genomics track categorization task: classifying full text biomedical documents. J Biomed Discov Collab. 2006 Mar 14;1:4. PubMed PMID: 16722582; PubMed Central PMCID: PMC1440303
Cite this review as
Boyce R ( 2014 ) Peer Review #1 of "SKIMMR: facilitating knowledge discovery in life sciences by machine-aided skim reading (v0.1)" . PeerJ https://doi.org/10.7287/peerj.483v0.1/reviews/1
label_author_4
Reviewer 2 ·
Dec 9, 2013
Basic reporting
label_br_4
The writing style, structure, and figures in the sections through 3.2 are good. Starting with 3.3, the text begins to sound like an advertisement for the software package and any content or analysis is minimal. Although the literature review covers development literature on similar systems, the literature on reading behavior/reading patterns is missing. For example, Tenopir & King, Vakkari, Nicholas and Rowlands.
Experimental design
label_ed_4
The problem for which this system was developed is clearly defined, although hypotheses should be more explicitly stated. This is an important issue, how have you tested that your system solves the human problem described in the introduction? The development and testing of the system with documents appears to be conducted with rigour and to a high standard. Then something puzzling occurs—the authors speak about methods for ongoing user studies and evaluation. Results are mostly absent, however and we are left with methods, but no corresponding analysis. It is at this point that the reader begins to lose confidence in what had been presented well until this point. It seems that internal testing of the algorithms occurred during software development, but planned testing of the software with human subjects didn’t happen or didn’t succeed. It becomes unclear.
Validity of the findings
Again, the system design, algorithms, and description are interesting. For user tests that were proposed the results are missing. Conclusions seem to be at a software sales level, rather than a rigorous test level. Will the code be made available for replication or further testing? Should this paper wait to be published until the user tests that are described get done?
Comments for the author
This paper starts out in a very promising way. Introduction to the problem, discussion of your system and approach are all interesting and detailed. Then, starting at section 3.3 it falls apart. Analysis and findings start sounding like a software advertisement, user studies are alluded to but findings are absent, and it just peters out. Cite this review as
Anonymous Reviewer ( 2014 ) Peer Review #2 of "SKIMMR: facilitating knowledge discovery in life sciences by machine-aided skim reading (v0.1)" . PeerJ https://doi.org/10.7287/peerj.483v0.1/reviews/2 Download Original Submission (PDF)
- submitted Dec 6, 2013 All text and materials provided via this peer-review history page are made available under a Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.
About us - PeerJ team | Our publications | Benefits | Partnerships | Endorsements Awards Resources - FAQ | Careers | Pressroom | Terms of use | Privacy | Contact Academic boards - Advisors | Editors | Subject areas Follow us - PeerJ blog | Twitter | Facebook | LinkedIn | Pinterest Submission guides - PeerJ (life - bio - med) | Computer Science | Chemistry | PeerJ Preprints instructions Spread the word - Activities | Resources PeerJ feeds - Atom | RSS 1.0 | RSS 2.0 | JSON PeerJ Computer Science feeds - Atom | RSS 1.0 | RSS 2.0 | JSON PeerJ Preprint feeds - Atom | RSS 1.0 | RSS 2.0 | JSON Archives - PeerJ | PeerJ Computer Science | PeerJ Preprints
©2012-2019 PeerJ, Inc | Public user content licensed CC BY 4.0 unless otherwise specified. PeerJ ISSN: 2167-8359 PeerJ Comput. Sci. ISSN: 2376-5992 PeerJ Preprints ISSN: 2167-9843
bioinformatics genomics
