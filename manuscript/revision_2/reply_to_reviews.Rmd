---
title: "Reply_to_reviews"
author: "Daniel Lakens"
date: "5/24/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## To do:

# Reviewer 1

1 & 2: A breakdown of their findings by discipline
3: Reviewers who gave more than 1 review. 
> We can show multiple signed reviews, but we do not know if people sign sometimes but not other times. We can see if there is a "always sign" comment? Otherwise, asked to discuss as a limitation, which it is. 
4: Changes of time. We did originally look at this, but there were none. However, this is an obvious question (since we explored it) and we should have included it. 
5: Do RSOS and PEERJ allow reviewers to recommend reviewers? 
6: Opening up the review process could also lead to showing how some researchers game the system by targeting colleagues likely to give positive reviews. Discuss this possibility. 

7: Please ensure the manuscript is proofread for clerical/grammatical errors. The word
"individidual" is misspelt, "researchers decision" is missing an apostrophe
8: Please check the link to the following reference, it appears to be broken in the pdf I reviewed:
Bastian, H. (2018). Signing Critical Peer Reviews & the Fear of Retaliation: What Should We
Do? |Absolutely Maybe. https://blogs.plos.org/absolutely- maybe/2018/03/22/signing-criticalpeer-
reviews- the-fear-of-retaliation-what-should-we-do/.

## Reviewer 2

1: it would be helpful to add a one paragraph description of PeerJ and RSOS. It might
include their scope, reputation, or any other information that would allow the reader to
contextualize the sample. [I think we can combine this with point 5 above]

2: Add information to make the figures more distinct. Figure 2 can include the words “Royal
Society”. This will make clear using the figures alone that figures 1 and 2 represent entirely
separate samples.

3: *Under the subheading “Additional Analyses”, the authors offer some analysis regarding “how
often reviewers agree.” This is an interesting approach to a common-place but under-studied
phenomena in meta-science. That said, these analyses don’t contribute to the main theme of the
paper, are not referenced in any other part of the paper, are not situated in the reviewed literature,
and are not interpreted in any way. I suggest that this section be either removed or expanded and
incorporated into the larger theme. 
[I agree - it is more like a motivation for people to explore that data further. Maybe remove, maybe make clearer it is just to show the data is available]

4: Reputations could easily be damaged by this type of transparency if it became known one reviewed retracted papers. Combine with point 6 by reviewer 1.

5: The authors acknowledge that the causal relationship for the relationship they observe could run
two different ways. Given the (well-acknowledged) incomplete nature of the data, considering at
least one more counter-narrative would make for a more complete treatment of the issue.

Editor comments: 

2: There are no quantitative analyses in the manuscript other than proportions. Why is this?
Maybe the authors think their findings are too exploratory to justify the presentation of p values, but some indication of the magnitude of the effects would surely be useful.

[There are no p-values, because the work is descriptive. We do not intend to make any claims - or do we? MAgnitude of effect can be seen in %, no?

3: I would be interested to see a short discussion of the possible consequences of the fact
that at PeerJ, readers do not get to see the individual reviewers’ recommendations.

4: “Our data support the idea that researchers [sic] decision to sign is related to their
recommendation across a wide range of scientific disciplines” (p. 4)—this language could
suggest that a robust effect has been shown for each of several disciplines. However, all that has actually been shown (I think) is that there is some effect (cf. my earlier remark about effect sizes) across a sample that encompasses a wide range of disciplines. It could be that the effect is
stronger for psychology and weaker or absent for biomedical sciences, for example. 
[Combine with point 1 and 2 of Reviewer 1]

5: “For positive recommendations, reviewers will get credit for their reviews, while for
negative reviews they do not run the risk of receiving any backlash from colleagues in their
field” (p. 4)—I’m not sure if “credit” is the right word. Reviewers who write a glowing report on
a manuscript can certainly expect to be liked by its author(s), but whether or not the rest of their
colleagues (and the field as a whole) will give them any sort of “credit” will depend on whether
the article later turns out to be flawed (and/or whether it upsets some powerful people); and the
same could apply in reverse for a negative review. So perhaps this comment should be rephrased
to emphasise that any “credit” is restricted to approval felt from the author(s) of the accepted
article. 
[True, combine with point 4 by reviewer 2]


6: I downloaded the code and data associated with the manuscript via the OSF link and was
able to reproduce the main numerical results and figures. However, the percentages in the first
paragraph on page 4 were printed with a lot more decimal places than in the PDF file (e.g., “For
41.7218543% of the manuscripts the maximum deviation was one category”). I wonder if your R
setup has a different global rounding option from mine? For best reproducibility, rounding
should probably be done explicitly in the code.