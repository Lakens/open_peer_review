---
title             : "Reviewers' Decision to Sign Reviews is Related to Their Recommendation"
shorttitle        : "Signing Open Reviews"
author: 
  - name          : "Nino van Sambeek"
    affiliation   : "1"
  - name          : "Daniel Lakens"
    affiliation   : "1"
    corresponding : yes
    address       : "ATLAS 9.402, 5600 MB, Eindhoven, The Netherlands"
    email         : "D.Lakens@tue.nl"
affiliation:
  - id            : "1"
    institution   : "Eindhoven University of Technology, The Netherlands"
author_note: |
 Accepted for publication at Meta-Psychology. This work was supported by the Netherlands Organization for Scientific Research (NWO) VIDI grant 452-17-013. All data underlying this reproducible manuscript are available at https://osf.io/9526a/. A Code Ocean capsule that computationally reproduces this manuscript is available at https://codeocean.com/capsule/6067104.
abstract: |
 Surveys indicate that researchers generally have a positive attitude towards open peer review when this consists of making reviews available alongside published articles. Researchers are more negative about revealing the identity of reviewers. They worry reviewers will be less likely to express criticism if their identity is known to authors. Experiments suggest that reviewers are somewhat less likely to recommend rejection when they are told their identity will be communicated to authors, than when they will remain anonymous. One recent study revealed reviewers in five journals who voluntarily signed their reviews gave more positive recommendations than those who did not sign their reviews. We replicate and extend this finding by analyzing 12010 open reviews in PeerJ and 4188 reviews in the Royal Society Open Science where authors can voluntarily sign their reviews. These results based on behavioral data from real peer reviews across a wide range of scientific disciplines demonstrate convincingly that reviewers' decision to sign is related to their recommendation. The proportion of signed reviews was higher for more positive recommendations, than for more negative recommendations. We also share all 23649 text-mined reviews as raw data underlying our results that can be re-used by researchers interested in peer review.
  
keywords          : "Peer Review, Open Reviews, Transparency, Open Science"
wordcount         : 3282
bibliography      : ["signing_open_peer_review_.bib"]
figsintext        : yes
figurelist        : no
tablelist         : no
footnotelist      : no
lineno            : no
mask              : no
class             : "jou, a4paper"
output            : papaja::apa6_pdf
editor_options: 
  chunk_output_type: console
---

```{r setup, include = FALSE}

library(dplyr)
library(tidyr)
library(stringr)
library(ggplot2)
library(gridExtra)

setwd("C:/Users/dlakens/surfdrive/R/open_peer_review")
#### READ IN DATA
# Read open science and open biology rds datafiles
TRS_data_os <- readRDS(file = "royal_society_data_os.rds")
TRS_data_ob <- readRDS(file = "royal_society_data_ob.rds")
# Combine both datasets into one.
TRS_data <- rbind(TRS_data_os, TRS_data_ob)

#Read in PeerJ data
PeerJ_main_data <- readRDS(file = "peerj_data.rds")
PeerJ_cs_data <- readRDS(file = "peerj_cs_data.rds")
# Combine both datasets into one.
PeerJ_data <- rbind(PeerJ_main_data, PeerJ_cs_data)

# For manual checks, sort TRS data on doi number
TRS_data_sorted <- TRS_data[ with( TRS_data , order ( df_link)),]

#Create dataframe for TRS and PeerJ of unique rows (e.g., for days)
TRS_unique <- TRS_data[!duplicated(TRS_data$df_link),]
PeerJ_unique <- PeerJ_data[!duplicated(PeerJ_data$df_link),]
TRS_unique_os <- TRS_data_os[!duplicated(TRS_data_os$df_link),]
TRS_unique_ob <- TRS_data_ob[!duplicated(TRS_data_ob$df_link),]


# We can see how many reviews are open in PeerJ by counting all txt files in the peer_reviews_txt folder, and seeing how many end up in our dataframe:
open_review_PeerJ <- nrow(PeerJ_unique)/length(list.files(path = "peerj_reviews_txt"))

# We can see how many reviews are open in TRS by counting all doi in scopus, and seeing how many end up in our list of open reviews:
OS_pdf_list <- readLines(file("royal_society_pdf_files/OS_pdf_list.txt", open = "r"))

open_review_TRS <- length(OS_pdf_list) / length(read.csv("scopus_export_rsos.csv", stringsAsFactors = FALSE)$DOI)

# How many entries in RSOS in total? Read in scopus datafile
TRS_scopus_OS <- read.csv("scopus_export_rsos.csv", stringsAsFactors = FALSE)
table(TRS_scopus_OS$Document.Type)
errata_editorial <- sum(table(TRS_scopus_OS$Document.Type)[c(3,4)])
# Excude Errata and Editorial (that have no reviews)
TRS_scopus_OS <- TRS_scopus_OS[!TRS_scopus_OS$Document.Type == "Erratum" & !TRS_scopus_OS$Document.Type == "Editorial",]
table(TRS_scopus_OS$Document.Type)

# How many entries in RSOS in total? Read in scopus datafile
TRS_scopus_OB <- read.csv("scopus_export_rsob.csv", stringsAsFactors = FALSE)
table(TRS_scopus_OB$Document.Type)
errata_editorial <- errata_editorial + sum(table(TRS_scopus_OB$Document.Type)[c(2,3,5)])
# Exclude Errata and Editorial (that have no reviews)
TRS_scopus_OB <- TRS_scopus_OB[!TRS_scopus_OB$Document.Type == "Erratum" & !TRS_scopus_OB$Document.Type == "Editorial" & !TRS_scopus_OB$Document.Type == "Retracted",]
table(TRS_scopus_OB$Document.Type)

# One submission to PeerJ was accepted but no reviewer info. It means they are accepted by the editor? Or just an early mistake.
sum(is.na(PeerJ_unique$df_days))

# Mean review time (
TRS_mean_review_time <- mean(TRS_unique$df_days)
PeerJ_mean_review_time <- mean(PeerJ_unique$df_days, na.rm = TRUE)

# How many reviews are anonymous?
mean(TRS_data$df_anonymous)
mean(PeerJ_data$df_anonymous)

# How many reviews in total in the raw txt files?
length(TRS_data$df_anonymous)+length(PeerJ_data$df_anonymous)

# Adding year from scopus to TRS data
# First add doi column to merge on
TRS_data$DOI <- gsub('http://dx.doi.org/','',TRS_data$df_link)
# Create joint SCOPUS dataframe
TRS_scopus <- rbind(TRS_scopus_OS, TRS_scopus_OB)
# Merge datasets - we lose 1 row for missing DOI (figshare doi)
TRS_data <- merge(TRS_data, TRS_scopus, by = 'DOI')

# Select only those reviews for each paper with the highest version for PeerJ (first review)
PeerJ_data_R1 <- group_by(PeerJ_data, df_link)
# Make recommendation a factor
PeerJ_data_R1$df_recommendation <-  recode_factor(PeerJ_data_R1$df_recommendation, `1` = "accept", `2` = "minor revision", `3` = "major revision")
# Slightly shorten 1 label PeerJ section for plot
PeerJ_data_R1$df_section <- recode(PeerJ_data_R1$df_section, "biochemistry, biophysics and molecular biology"="biochemistry biophysics molecular biology", "NA" = "no section assigned")
PeerJ_data_R1 <- top_n(PeerJ_data_R1, 1, df_version)
mean(count(PeerJ_data_R1, df_link)$n) #count reviews per article.

# Do the same for TRS
TRS_data_R1 <- group_by(TRS_data, df_link)
# Make recommendation a factor
TRS_data_R1$df_recommendation <-  recode_factor(TRS_data_R1$df_recommendation, `1` = "accept", `2` = "minor revision", `3` = "major revision", `4` = "reject")
TRS_data_R1 <- top_n(TRS_data_R1, 1, df_version) 

# We can easily check whether reviewers agree or not by looking at their recommendations.
TRS_review_agree <- group_by(TRS_data_R1, df_link)
TRS_review_agree <- mutate(TRS_review_agree, df_unique = n_distinct(df_recommendation))
TRS_review_agree <- TRS_review_agree[!duplicated(TRS_review_agree$df_link),]

# Are there really reviws with 4 distinct recommendations?
sum(TRS_review_agree$df_unique == 4) # Reported as a fun fact in the additional analyses

#Select the top recommendation for each review
TRS_data_R1_top <- top_n(TRS_data_R1, 1, as.numeric(df_recommendation)) 
TRS_data_R1_top <- TRS_data_R1_top[!duplicated(TRS_data_R1_top$df_link),]
#Select the bottom recommendation for each review
TRS_data_R1_bottom <- top_n(TRS_data_R1, -1, as.numeric(df_recommendation)) 
TRS_data_R1_bottom <- TRS_data_R1_bottom[!duplicated(TRS_data_R1_bottom$df_link),]

#Compute the difference between top and bottom
rev_dif <- as.numeric(TRS_data_R1_top$df_recommendation) - as.numeric(TRS_data_R1_bottom$df_recommendation)
#We table the difference - many are similar, many differ 1 category, some 2 few 3.
table(rev_dif)

# It is useful to select only cases that have a recommendation
TRS_data_complete <- TRS_data[complete.cases(TRS_data$df_recommendation),]

```

As technology advances, science advances. The rise of the internet has made it possible to transparently share all steps in the scientific process [@spellman_short_2015]. This includes opening up the peer review process. An increasing number of journals have started to make peer review reports available alongside published articles as part of ongoing experiments that aim to improve peer review [@bruce_impact_2016]. Open peer review can be implemented by making peer reviews available, but also by revealing the identity of reviewers during or after the peer review process. An important argument in favour of revealing the identity of reviewers is that they can receive credit for their work [@godlee_making_2002]. However, scientists do not feel these benefits outweigh possible costs, and are worried that criticism on manuscripts might lead to backlash from the authors in the future. Some reviewers might accept these negative consequences, while other might choose to strategically reveal their identity only for positive reviews they write.

Researchers self-report that they would be less likely to review for a journal if their identity is made public, and anecdotally mention that signed reviews would make it more difficult to be honest about manuscripts they believe are poor quality [@mulligan_peer_2013]. A more recent survey found that 50.8% of almost 3000 scientists believe that revealing the identity of reviewers would make peer review worse [@ross-hellauer_survey_2017]. Almost two-thirds of respondents believed reviewers would be less likely to deliver strong criticisms if their identity became known to the authors.

These self-report studies are complemented by experiments in which reviewers are randomly assigned to a condition where their identity would be revealed during the peer review process [@walsh_open_2000]. Reviewers in the condition where their identity was revealed were less likely to recommend rejection (n = 30) than reviewers who remained anonymous (n = 51). This suggests that a causal effect exists between knowing your identity will be revealed, and the recommendation that is made during the peer review process. Based on a small-scale meta-analysis of four studies Bruce and colleagues (2016) found support for the conclusion that reviewers are somewhat less likely to recommend rejection when they have to sign their reviews.

Although the self-report studies and the experiments clearly suggest that reviewers worry about having their name attached to more critical reviews they write, so far little is known about what reviewers actually do when given the opportunity to sign their reviews. The trade-off between the benefit of getting credit when performing peer reviews and the risk of negative consequences when signing critical reviews might lead to strategic behavior where authors become more likely to sign reviews the more positive their recommendation is. If this strategic behavior occurs in practice, we should see a different pattern of recommendations for signed and unsigned reviews. One recent study revealed such a pattern when analyzing data from an Elsevier trial on publishing peer review reports in the journal *Agricultural and Forest Meteorology*, *Annals of Medicine and Surgery*, *Engineering Fracture Mechanics*, the *Journal of Hydrology: Regional Studies*, and the *International Journal of Surgery* [@bravo_effect_2019]. Although only 8.1% of reviewers voluntarily disclosed their identity in these reviews, the data revealed a clear difference between the recommendations by reviewers who chose to sign their reviews, compared to reviewers who did not sign. 

# The Current Study

We examined the relationship between the recommendations peer reviewers made and the proportion of signed reviews in two large open access journals, PeerJ (including reviews for PeerJ Computer Science) and Royal Society Open journals (Royal Society Open Science and Royal Society Open Biology). We ignored more recently launched PeerJ journals in the field of Chemistry due to the small number of articles published to date in these journals. PeerJ and Royal Society Open journals publish articles across a wide range of scientific disciplines, including biology, chemistry, engineering, life sciences, mathematics, and medicine, thus allowing us to replicate and extend the analysis by Bravo and colleagues (2019). PeerJ launched in 2012 and PeerJ Computer Science launched in 2015. PeerJ provides reviewers the possibility to sign, and authors the possibility to make peer reviews available with the final publication. Royal Society Open Science (RSOS) launched in 2014 and strongly encouraged authors to make the peer reviews available with the final publication, and made this mandatory in January 2019. Royal Society Open Biology (RSOB) made sharing reviews with the final publication mandatory in May 2017. Peer reviewers have the option to make their identity known when submitting their review to RSOS or RSOB. Because of their broad scope, the large number of publications in each journal, and their early focus on open reviews, the reviews for PeerJ and Royal Society Open journals provide insights into the peer review behavior of scientists across a wide range of disciplines.

# Accessing Open Reviews

PeerJ assigns all articles a number, increasing consecutively with each published manuscript. Reviews are always accessible in HTML (e.g., reviews for the first article published in PeerJ are available at https://peerj.com/articles/1/reviews). For Royal Society Open journals reviews are published online as a PDF file. A list of Digital Object Identifiers (DOIs) for every article published in RSOS and RSOB was retrieved through Scopus. All available reviews were downloaded, and the PDF files were converted to plain text files using pdftools for R [@r_core_team_r_2013; @ooms_pdftools_2019]. These text files were mined for recommendations, reviewer names, submission and acceptance dates, and the review content, using the stringr package in R [@wickham_stringr_2019]. 

For each article we extracted the number of revisions, and for each revision we saved whether each of the reviewers signed, the word count for their review, and their recommendation for that review round. Note that for PeerJ the editor makes the recommendation for each submission based on the reviews. We therefore do not directly know which recommendation each reviewer provided, but we analyze the data based on the assumption that the decision by the editor is correlated with the underlying reviews. For Royal Society Open journals reviewers make a recommendation, which may be to 'accept as is', 'accept with minor revisions', 'major revision', or 'reject'. Because PeerJ and Royal Society Open journals only share reviews for published articles there are few 'reject' recommendations for Royal Society Open journals and no 'reject' recommendations by editors among PeerJ reviews. Searching all reviews for PeerJ for the words 'appealed on' revealed 47 articles that were initially rejected, appealed, received a 'major revision' recommendation, and were eventually published. We have coded these papers as 'major revisions'. All scripts to download and analyze the reviews, and computationally reproduce this manuscript, are available at https://osf.io/9526a/.

# Results

We examined `r 7930+225` articles published in PeerJ (7930 in PeerJ, 225 in PeerJ Computer Science), as well as `r length(TRS_scopus_OS$DOI)+length(TRS_scopus_OB$DOI)` articles from Royal Society Open journals (`r length(TRS_scopus_OS$DOI)` from RSOS, `r length(TRS_scopus_OB$DOI)` from RSOB, `r errata_editorial` of which were editorials or errata without reviews) published up to October 2019. We retrieved all reviews when these were made available (the reviews were available for `r nrow(PeerJ_unique)` articles in PeerJ, and `r nrow(TRS_unique)` articles in Royal Society). Articles can, of course, go through multiple rounds of review. However, we focus only on the first review round in our analyses as this review reflects the initial evaluation of reviewers, before the handling editor has made any decision, following @bravo_effect_2019. On average initial submissions at PeerJ received `r round(mean(count(PeerJ_data_R1, df_link)$n), 2)` reviews. Articles in the Royal Society Open journals received on average `r round(mean(count(TRS_data_R1, df_link)$n), 2)` reviews for the original submission. 

```{r, include = FALSE}
# sum number of anonymous responses by subgroup using the by command in base R
PeerJ_anonymous_by_recommendation <- by(PeerJ_data_R1$df_anonymous == 1, PeerJ_data_R1$df_recommendation, sum)
PeerJ_notanonymous_by_recommendation <- by(PeerJ_data_R1$df_anonymous == 0, PeerJ_data_R1$df_recommendation, sum)

# Reviewers ask for overview per subfield
PeerJ_notanonymous_by_recommendation_by_field <- by(PeerJ_data_R1$df_anonymous == 0, list(recommendation = PeerJ_data_R1$df_recommendation, section = PeerJ_data_R1$df_section), sum)

PeerJ_data_R1 %>% 
  group_by(df_recommendation) %>% 
  summarise(unsigned = sum(df_anonymous == 0), signed = sum(df_anonymous == 1))

# Reproducing proportions using dplyr
PeerJ_data_R1 %>% 
  group_by(df_recommendation) %>% 
  summarise(signed = sum(df_anonymous == 0)/sum(PeerJ_notanonymous_by_recommendation), unsigned = sum(df_anonymous == 1)/sum(PeerJ_anonymous_by_recommendation))

# Reproducing proportions using dplyr per field
PeerJ_data_R1 %>% 
  group_by(df_recommendation, df_section) %>% 
  summarise(signed = sum(df_anonymous == 0)/sum(PeerJ_notanonymous_by_recommendation), unsigned = sum(df_anonymous == 1)/sum(PeerJ_anonymous_by_recommendation))



##########
# Analysis for The Royal Society (TRS)
##########

# sum number of anonymous responses by subgroup using the by command in base R
TRS_anonymous_by_recommendation <- by(TRS_data_R1$df_anonymous == 1, TRS_data_R1$df_recommendation, sum)
TRS_notanonymous_by_recommendation <- by(TRS_data_R1$df_anonymous == 0, TRS_data_R1$df_recommendation, sum)

```

# Signed reviews as a function of the recommendation

For all `r nrow(PeerJ_unique)` articles published in PeerJ where reviews were available we retrieved `r length(PeerJ_data_R1$df_anonymous)` unique reviews for the initial submission (as each article is typically reviewed by multiple reviewers). In total `r sum(PeerJ_data_R1$df_anonymous == 0)` reviewers signed their review for the initial submission, and `r sum(PeerJ_data_R1$df_anonymous == 1)` reviewers did not. In Royal Society Open journals we analyzed `r nrow(TRS_unique)` articles for which we retrieved `r length(TRS_data_R1$df_anonymous)` unique reviews for the first submission, where `r sum(TRS_data_R1$df_anonymous == 0)` reviewers signed their review and `r sum(TRS_data_R1$df_anonymous == 1)` did not. The percentages of people who signed (`r 100*sum(PeerJ_data_R1$df_anonymous == 0)/(sum(PeerJ_data_R1$df_anonymous == 0)+sum(PeerJ_data_R1$df_anonymous == 1))`% for PeerJ, `r 100*sum(TRS_data_R1$df_anonymous == 0)/(sum(TRS_data_R1$df_anonymous == 0)+sum(TRS_data_R1$df_anonymous == 1))`% for Royal Society Open journals) are slightly lower than the 43.23% reported by @wang_open_2016 who analyzed the first 1214 articles published in PeerJ.

To answer our main research question we plotted the signed and unsigned reviews for PeerJ as a function of the recommendation in the first review round (see Figure \ref{fig:PeerJrec}). Remember that for PeerJ these recommendations are made by the editor, and thus only indirectly capture the evaluation of the reviewer. For minor revisions, a greater proportion of reviews was signed than unsigned, but for major revisions, more reviews were unsigned than signed. Too few articles are immediately accepted after the first round of reviews in PeerJ (`r sum(as.numeric(PeerJ_data_R1$df_recommendation) == 1, na.rm = TRUE)` in total) to impact the proportions in the other two categories.

```{r, PeerJrec, fig.cap = "Proportion of 'minor revisions' or 'major revisions' recommendations by the handling editor at PeerJ conditioned on whether reviews were signed or unsigned."}

# #create plot data for accept, minor, major recommendations
# plot_data <- data.frame(factor(c("accept", "minor revision", "major revision", "accept", "minor revision", "major revision"), levels = c("accept", "minor revision", "major revision")),
#                    c("unsigned", "unsigned", "unsigned", "signed", "signed", "signed"),
#                    c(PeerJ_anonymous_by_recommendation, PeerJ_notanonymous_by_recommendation))
# 
# colnames(plot_data) <- c("recommendation", "signed", "count")
# 
# ggplot(plot_data, aes(x = recommendation, y = count, fill = signed)) + 
#   geom_bar(position="dodge", stat="identity") +
#   theme_linedraw(base_size = 12)

# Makes more sense to plot proportions
PeerJ_anonymous_by_recommendation_proportion <- PeerJ_anonymous_by_recommendation/sum(PeerJ_anonymous_by_recommendation)
                                                       
PeerJ_notanonymous_by_recommendation_proportion <- PeerJ_notanonymous_by_recommendation/sum(PeerJ_notanonymous_by_recommendation)


# create plot data for only minor, major recommendations
# Remove _proportion for raw frequencies plot
plot_data <- data.frame(factor(c("minor revision", "major revision", "minor revision", "major revision"), levels = c("minor revision", "major revision")),
                   c("unsigned", "unsigned", "signed", "signed"),
                   c(PeerJ_anonymous_by_recommendation_proportion[2:3], PeerJ_notanonymous_by_recommendation_proportion[2:3]))
colnames(plot_data) <- c("recommendation", "signed", "proportion")

ggplot(plot_data, aes(x = recommendation, y = proportion, fill = signed)) +
  geom_bar(position="dodge", stat="identity") +
  theme_linedraw(base_size = 20) +  
  theme(legend.position = "bottom",
        legend.title=element_blank(),
        plot.title = element_text(hjust = 0.5),
        panel.grid.major.x = element_blank()) + 
  scale_fill_manual(values = c("#000000", "#808080")) + 
  coord_cartesian(ylim = c(0, 1)) +
  ggtitle("PeerJ recommendations") +
  geom_text(aes(label = sprintf('%.2f',proportion)), 
              size = 6, 
              color = "black",
              position = position_dodge(width = 0.9),
              vjust = -.5)


```

Analyzing the reviews at Royal Society Open journals provides a more direct answer to our question, since each individual reviewer is asked to provide a recommendation of 'accept', 'minor revisions', 'major revisions', or 'reject'. We can therefore directly compare how recommendations are related to the decision to sign reviews (see Figure \ref{fig:TRSrec}). The overall pattern clearly shows that the proportion of signed reviews is larger for more positive recommendations (accept and minor revisions) whereas the proportion of unsigned reviews is larger for more negative reviews (major revisions and reject). 

```{r, TRSrec, fig.cap = "Proportion of 'accept', 'minor revisions', 'major revisions', or 'reject' recommendations by reviewers of Royal Society Open journals conditioned on whether reviews were signed or unsigned."}

# Makes more sense to plot proportions
TRS_anonymous_by_recommendation_proportion <- TRS_anonymous_by_recommendation/sum(TRS_anonymous_by_recommendation)
TRS_notanonymous_by_recommendation_proportion <- TRS_notanonymous_by_recommendation/sum(TRS_notanonymous_by_recommendation)

#create plot data for accept, minor, major recommendations
plot_data <- data.frame(factor(c("accept", "minor", "major", "reject", "accept", "minor", "major", "reject"), levels = c("accept", "minor", "major", "reject")),
                   c("unsigned", "unsigned", "unsigned", "unsigned", "signed", "signed", "signed", "signed"),
                   c(TRS_anonymous_by_recommendation_proportion, TRS_notanonymous_by_recommendation_proportion))
colnames(plot_data) <- c("recommendation", "signed", "proportion")

ggplot(plot_data, aes(x = recommendation, y = proportion, fill = signed)) + 
  geom_bar(position="dodge", stat="identity") +
  theme_linedraw(base_size = 20) + 
  theme(legend.position = "bottom",
        legend.title=element_blank(),
        plot.title = element_text(hjust = 0.5),
        panel.grid.major.x = element_blank()) + 
  scale_fill_manual(values = c("#000000", "#808080")) + 
  coord_cartesian(ylim = c(0, 1)) +
  ggtitle("Royal Society recommendations") +  
  geom_text(aes(label = sprintf('%.2f',proportion)), 
              size = 6, 
              color = "black",
              position = position_dodge(width = 0.9),
              vjust = -.5) 


# #create plot data for only minor, major recommendations
# plot_data <- data.frame(c("minor revision", "major revision", "minor revision", "major revision"),
#                    c("unsigned", "unsigned", "signed", "signed"),
#                    c(TRS_anonymous_by_recommendation[2:3], TRS_notanonymous_by_recommendation[2:3]))
# colnames(plot_data) <- c("recommendation", "signed", "count")
# 
# ggplot(plot_data, aes(x = recommendation, y = count, fill = signed)) + 
#   geom_bar(position="dodge", stat="identity") +
#   theme_linedraw(base_size = 20)

```

We cannot draw causal conclusions based on this correlational data. It is possible that reviewers are less likely to sign more negative reviews. It is also possible that people who sign their reviews generally give more positive recommendations, and therefore the distribution of signed reviews differs from non-signed reviews. These are just two of many possible explanations for the observed pattern. Based on the literature reviewed in the introduction we know researchers are hesitant to voice criticism when their identity will be known, and experimental evidence suggests that if identities are shared with authors, recommendations become somewhat more positive. Therefore, it seems plausible that at least part of the pattern we observed can be explained by reviewers being more likely to sign more positive reviews. Although we had access to few 'reject' recommendations because we could only access reviews for published manuscripts, the difference between signed and unsigned reviews for major revisions, minor revisions, and accept recommendations replicates the findings by @bravo_effect_2019 across a larger range of research fields, based on a larger dataset, and in journals where a larger percentage of reviewers volunteer to disclose their identity. This replication suggests that the difference in recommendations depending on whether reviews are signed or not is a rather reliable observation.

Both PeerJ and Royal Society Open journals publish articles in a wide range of disciplines. The open reviews at PeerJ specify the subject area the article was submitted to, but this information is not available from meta-data or the bibliographic record for Royal Society Open journals. 

\onecolumn

```{r, warning=FALSE, PeerJrecperfield, fig.width=15, fig.height=16, fig.cap = "Total number of 'minor revisions' or 'major revisions' recommendations by the handling editor at PeerJ conditioned on whether reviews were signed or unsigned across different scientific disciplines."}

# As asked by reviewers, how does this differ over subfields?

# Reproducing proportions using dplyr per field
plot_data_by_field <- PeerJ_data_R1 %>% 
  group_by(df_recommendation, df_section) %>% 
  summarise(signed = sum(df_anonymous == 0)/sum(PeerJ_notanonymous_by_recommendation), unsigned = sum(df_anonymous == 1)/sum(PeerJ_anonymous_by_recommendation))

# Reproducing counts using dplyr per field
plot_data_by_field <- PeerJ_data_R1 %>%
  group_by(df_recommendation, df_section) %>%
  summarise(signed = sum(df_anonymous == 0), unsigned = sum(df_anonymous == 1))


# Filter NA and direct accepts
plot_data_by_field <- plot_data_by_field %>% filter(as.integer(df_recommendation) > 1)

plot_data_by_field_long <- gather(plot_data_by_field, signed, count, signed:unsigned, factor_key = TRUE)

ggplot(plot_data_by_field_long, aes(x = df_recommendation, y = count, fill = signed)) +
  geom_bar(position="dodge", stat="identity") +
  theme_linedraw(base_size = 20) +  
  theme(legend.position = "bottom",
        legend.title=element_blank(),
        panel.grid.major.x = element_blank(),
        plot.title = element_text(hjust = 0.5),
        axis.title.x=element_blank()) + 
  scale_fill_manual(values = c("#000000", "#808080")) + 
  coord_cartesian(ylim = c(0, 900)) +
  geom_text(aes(label = count), # to add rounding: sprintf('%.2f',proportion)
              size = 6, 
              color = "black",
              position = position_dodge(width = 0.9),
              vjust = -.5) + 
  ggtitle("PeerJ recommendations") +
  facet_wrap(~df_section, ncol = 3)

```

<!-- ```{r, TRSrecpergroup, fig.width=15, fig.height=4, fig.cap = "Proportion of 'accept', 'minor revisions', 'major revisions', or 'reject' recommendations by reviewers of Royal Society Open journals conditioned on whether reviews were signed or unsigned."} -->

<!-- # Makes more sense to plot proportions -->
<!-- TRS_anonymous_by_recommendation_proportion <- TRS_anonymous_by_recommendation/sum(TRS_anonymous_by_recommendation) -->
<!-- TRS_notanonymous_by_recommendation_proportion <- TRS_notanonymous_by_recommendation/sum(TRS_notanonymous_by_recommendation) -->


<!-- # As asked by reviewers, how does this differ over subfields? -->

<!-- # Reproducing proportions using dplyr per field -->
<!-- plot_data_by_field <- TRS_data_R1 %>%  -->
<!--   group_by(df_recommendation, df_section) %>%  -->
<!--   summarise(signed = sum(df_anonymous == 0)/sum(TRS_notanonymous_by_recommendation), unsigned = sum(df_anonymous == 1)/sum(TRS_anonymous_by_recommendation)) -->

<!-- # Reproducing counts using dplyr per field -->
<!-- plot_data_by_field <- TRS_data_R1 %>% -->
<!--   group_by(df_recommendation, df_section) %>% -->
<!--   summarise(signed = sum(df_anonymous == 0), unsigned = sum(df_anonymous == 1)) -->

<!-- plot_data_by_field_long <- gather(plot_data_by_field, signed, proportion, signed:unsigned, factor_key = TRUE) -->

<!-- ggplot(plot_data_by_field_long, aes(x = df_recommendation, y = proportion, fill = signed)) + -->
<!--   geom_bar(position="dodge", stat="identity") + -->
<!--   theme_linedraw(base_size = 20) +   -->
<!--   theme(legend.position = "bottom", -->
<!--         legend.title=element_blank(), -->
<!--         panel.grid.major.x = element_blank(), -->
<!--         plot.title = element_text(hjust = 0.5), -->
<!--         axis.title.x=element_blank()) +  -->
<!--   scale_fill_manual(values = c("#000000", "#808080")) +  -->
<!--   coord_cartesian(ylim = c(0, 1200)) + -->
<!--   geom_text(aes(label = proportion), # to add rounding: sprintf('%.2f',proportion) -->
<!--               size = 6,  -->
<!--               color = "black", -->
<!--               position = position_dodge(width = 0.9), -->
<!--               vjust = -.5) +  -->
<!--   ggtitle("Royal Society Open recommendations") + -->
<!--   facet_wrap(~df_section, ncol = 4) -->

<!-- ``` -->

```{r, warning=FALSE, recperyear, fig.width=15, fig.height=17, fig.cap = "Total number of signed and unsigned reviews for each of the different possible recommendations for PeerJ and Royal Society Open journals per year."}

#As requested by reviewers, show pattern over time. 

# Reproducing counts using dplyr per field
plot_data_by_year <- PeerJ_data_R1 %>%
  group_by(df_recommendation, df_year) %>%
  summarise(signed = sum(df_anonymous == 0), unsigned = sum(df_anonymous == 1))


# Filter NA and direct accepts
plot_data_by_year <- plot_data_by_year %>% filter(as.integer(df_year) > 1, as.integer(df_recommendation) > 1)

plot_data_by_year_long <- gather(plot_data_by_year, signed, count, signed:unsigned, factor_key = TRUE)

p1 <- ggplot(plot_data_by_year_long, aes(x = df_recommendation, y = count, fill = signed)) +
  geom_bar(position="dodge", stat="identity") +
  theme_linedraw(base_size = 20) +  
  theme(legend.position = "bottom",
        legend.title=element_blank(),
        panel.grid.major.x = element_blank(),
        plot.title = element_text(hjust = 0.5),
        axis.title.x=element_blank()) + 
  scale_fill_manual(values = c("#000000", "#808080")) + 
  coord_cartesian(ylim = c(0, 1500)) +
  geom_text(aes(label = count), # to add rounding: sprintf('%.2f',proportion)
              size = 6, 
              color = "black",
              position = position_dodge(width = 0.9),
              vjust = -.5) + 
  ggtitle("PeerJ recommendations per year") +
  facet_wrap(~df_year, ncol = 4)

# Reproducing counts using dplyr per field
plot_data_by_year <- TRS_data_R1 %>%
  group_by(df_recommendation, Year) %>%
  summarise(signed = sum(df_anonymous == 0), unsigned = sum(df_anonymous == 1))


# Filter NA and direct accepts
plot_data_by_year <- plot_data_by_year %>% filter(as.integer(Year) > 1, as.integer(df_recommendation) > 1)

plot_data_by_year_long <- gather(plot_data_by_year, signed, count, signed:unsigned, factor_key = TRUE)

p2 <- ggplot(plot_data_by_year_long, aes(x = df_recommendation, y = count, fill = signed)) +
  geom_bar(position="dodge", stat="identity") +
  theme_linedraw(base_size = 20) +  
  theme(legend.position = "bottom",
        legend.title=element_blank(),
        panel.grid.major.x = element_blank(),
        plot.title = element_text(hjust = 0.5),
        axis.title.x=element_blank()) + 
  scale_fill_manual(values = c("#000000", "#808080")) + 
  coord_cartesian(ylim = c(0, 500)) +
  geom_text(aes(label = count), # to add rounding: sprintf('%.2f',proportion)
              size = 6, 
              color = "black",
              position = position_dodge(width = 0.9),
              vjust = -.5) + 
  ggtitle("Royal Society Open recommendations per year") +
  facet_wrap(~Year, ncol = 3)


grid.arrange(p1, p2, ncol = 1)

```

\twocolumn

To examine the extent to which there is variation in signed and unsigned reviews across subject areas in PeerJ, Figure \ref{fig:PeerJrecperfield} shows the number of signed and unsigned reviews, as a function of the recommendation provided by the editor, across 11 subject areas (and one left-over category for articles not assigned a section). We see that the general pattern holds across all fields. In general, reviews are less likely to be signed than unsigned, except for the paleontology and evolutionary science section, where reviews are more likely to be signed than unsigned. Beyond this main effect, we see a clear interaction across all sections, where the ratio of signed versus unsigned reviews is consistently greater for major revisions than for minor revisions. Across all fields covered by PeerJ, it is relatively less likely to see a signed review for major revisions than for minor revisions, suggesting that the observed effect is present across disciplines.

Figure \ref{fig:recperyear} shows the number of signed and unsigned reviews as a function of the recommendation for each year in which articles were published (since 2012 for PeerJ, and since 2014 for Royal Society Open journals). As already noted, the proportion of signed reviews reduced over time compared to earlier analyses by @wang_open_2016. Figure \ref{fig:recperyear} reveals that this conclusion holds for both PeerJ and Royal Society Open journals. It seems that in the early years of their existence more reviewers who accepted invitations to review for PeerJ and Royal Society Open journals also agreed to sign their reviews. We don't have the data to explain this pattern, but one might speculate that these open access journals attracted authors and reviewers especially interested in transparency and openness when they started to publish manuscripts. As the journals became more established, and the number of submissions increased, the reviewer population increased beyond the early adopters of open science, to include reviewers less likely to sign their reviews. 

# Discussion

Our analysis shows that when authors are given the choice to sign their reviews, signed reviews have more positive recommendations than unsigned reviews. This pattern is clearly present for reviews in Royal Society Open Science and Open Biology, a large multi-disciplinary journal that publishes articles across a wide range of scientific domains. The pattern is also visible in a second large multi-disciplinary journal, PeerJ, under the assumption that recommendations by editors at PeerJ are correlated with the recommendations by reviewers. Our results replicate and extend earlier findings by @bravo_effect_2019, and complement self-report and experimental results in the literature. 

It might be that some reviewers consistently sign their reviews, while others sign selectively. Some researchers have contributed multiple signed reviews to our dataset, but it is unknown whether they also contributed unsigned reviews. The confidentiality of peer review makes it impossible to retrieve whether reviewers selectively sign from open reviews, and this question will need to be studied by surveying researchers directly. It is possible to indicate suggested and opposed reviewers when submitting a manuscript at PeerJ and RSOS. It is unknown how often suggested reviewers end up reviewing submissions, or whether suggested or non-suggested reviewers differ in how often they sign their reviews, depending on their recommendation. It therefore is unknown whether the current results will generalize to journals that do not ask authors to recommend reviewers. It is interesting to note that PeerJ asks reviewers for a recommendation, but only shares the reviews and the editor's decision with the authors. It would be interesting to examine whether this feature of the review process impacts reviewer's decision to sign.

Regrettably, neither PeerJ nor Royal Society Open journals make peer reviews available for manuscripts that were rejected. As a consequence, we have analyzed a biased sample of the literature. Few scientific journals make peer reviews available for all submitted articles (two notable exceptions are Meta-Psychology and F1000). Although open reviews enable us to look in more detail at the peer review process, it would be extremely interesting to be able to follow manuscripts through the peer review process even when they are rejected at one specific journal. Despite this limitation, the pattern of results we observe is very similar to that reported by @bravo_effect_2019 who had access to the reviews for accepted and rejected manuscripts.

Peer review is generally seen as an important quality control mechanism in science, yet researchers can rarely evaluate the quality of peer review at journals. Open reviews allow researchers to examine meta-scientific questions that give insights into the peer review process. The dataset we are sharing has information about the recommendations of reviewers (RSOS and RSOB) or editors (PeerJ) after each round of peer review, the names of reviewers who signed their review, and the time in review (`r round(mean(PeerJ_unique$df_days, na.rm = TRUE),0)` days for PeerJ, `r round(mean(TRS_unique$df_days, na.rm = TRUE),0)` days for Royal Society Open journals). Using the DOI, researchers can link this data to other sources of information such as citation counts. Because the reviews themselves are included in our dataset, researchers can use the text files to answer more detailed questions about the content of peer reviews across different domains. Since we know the individual recommendation of each reviewer for Royal Society Open journals, one example of the insights that open reviews provide is how often reviewers agree. For the `r sum(table(rev_dif))` papers where the reviews were published, all reviewers agreed on the recommendation for `r table(rev_dif)[1]` articles (`r 100 * table(rev_dif)[1]/sum(table(rev_dif))`% of the time). For `r 100 *  table(rev_dif)[2]/sum(table(rev_dif))`% of the manuscripts the maximum deviation was one category (e.g., minor and major revisions), for `r 100 *  table(rev_dif)[3]/sum(table(rev_dif))`% of the manuscripts the maximum deviation was two categories (e.g., accept and major revision), and for `r 100 *  table(rev_dif)[4]/sum(table(rev_dif))`% of the manuscripts the maximum deviation was three categories (i.e., accept and reject). There were `r sum(TRS_review_agree$df_unique == 4)` articles where researchers received all four possible recommendations (accept, minor revisions, major revisions, reject) from at least four different reviewers. We hope the dataset we share with this manuscript will allow other meta-scientists to examine additional questions about the peer review process.
 
Reviewers might have different reasons to sign or not sign their reviews. They might be worried about writing a positive review for a paper that is later severely criticized, and receiving blame for missing these flaws when reviewing the submission. Reviewers might not want to reveal that their recommendations are unrelated to the scientific quality of the paper, but based on personal biases or conflicts of interest. Finally, reviewers might fear backlash from authors who receive their negative reviews. Our data supports the idea that reviewers' decisions to sign are related to their recommendation across a wide range of scientific disciplines. Due to the correlational nature of the data, this relationship could emerge because reviewers who are on average more likely to sign also give more positive recommendations, reviewers are more likely to sign more positive recommendations, or both these effects could be true at the same time. Together with self-report data and experiments reported in the literature, our data increase the plausibility that in real peer reviews at least some reviewers are more likely to sign if their recommendation is more positive. This type of strategic behavior also follows from a purely rational goal to optimize the benefits of peer review while minimizing the costs. For positive recommendations, reviewers will get credit for their reviews, while for negative reviews they do not run the risk of receiving any backlash from colleagues in their field.

It is worthwhile to examine whether this fear of retaliation has an empirical basis, and if so, to consider developing guidelines to counteract such retaliation [@bastian_signing_2018]. Based on all available research it seems plausible that at least some reviewers hesitate to sign if they believe doing so could have negative consequences, but will sign reviews with more positive recommendations to get credit for their work. Therefore, it seems worthwhile to explore ways in which reviewers could be enabled to feel comfortable to claim credit for all their reviews, regardless of whether their recommendation is positive or negative.

## Author Contributions

N. van Sambeek and D. Lakens developed the idea, and jointly created the R code to generate and analyze the data. N. van Sambeek drafted the initial version of the manuscript as a Bachelor's thesis, D. Lakens drafted the final version, and both authors revised the final version of the manuscript. 

## Conflict of Interest Statement

The authors report no conflict of interest.

## Funding

This work was funded by VIDI Grant 452-17-013 from the Netherlands Organisation for Scientific Research.

## ORCID ID

Daniel Lakens ![](orcid.png) https://orcid.org/0000-0002-0247-239X

# References

\setlength{\parindent}{-0.5in}
\setlength{\leftskip}{0.5in}


```{r, include = FALSE}
# Easter Egg: Reviewer 2 is not evil

# sum number of responses by subgroup using the by command in base R
TRS_reviewer1_recommendation <- by(TRS_data_R1$df_reviewer_number == 1, TRS_data_R1$df_recommendation, sum)

TRS_reviewer2_recommendation <- by(TRS_data_R1$df_reviewer_number == 2, TRS_data_R1$df_recommendation, sum)

# Makes more sense to plot proportions
TRS_reviewer1_recommendation_proportion <- TRS_reviewer1_recommendation/sum(TRS_reviewer1_recommendation)
TRS_reviewer2_recommendation_proportion <- TRS_reviewer2_recommendation/sum(TRS_reviewer2_recommendation)

#create plot data for accept, minor, major, accept recommendations
plot_data <- data.frame(factor(c("accept", "minor", "major", "reject", "accept", "minor", "major", "reject"), levels = c("accept", "minor", "major", "reject")),
                   c("reviewer 1", "reviewer 1", "reviewer 1", "reviewer 1", "reviewer 2", "reviewer 2", "reviewer 2", "reviewer 2"),
                   c(TRS_reviewer1_recommendation_proportion, TRS_reviewer2_recommendation_proportion))
colnames(plot_data) <- c("recommendation", "reviewer", "count")

ggplot(plot_data, aes(x = recommendation, y = count, fill = reviewer)) + 
  geom_bar(position="dodge", stat="identity") +
  theme_linedraw(base_size = 20) + 
  theme(legend.position = "top",
        legend.title=element_blank(),
        panel.grid.major.x = element_blank()) + 
  scale_fill_manual(values = c("#000000", "#808080")) + 
  coord_cartesian(ylim = c(0, 1)) +
  geom_text(aes(label = sprintf('%.2f',count)), 
              size = 6, 
              color = "black",
              position = position_dodge(width = 0.9),
              vjust = -.5) 

# Is there a difference in recommendation between reviewer numbers? 
prop_data <- cbind(TRS_reviewer1_recommendation[2:3], TRS_reviewer2_recommendation[2:3]) #bind as matrix
rownames(prop_data) <- c("minor revision", "major revision") #add column names
colnames(prop_data) <- c("reviewer1", "reviewer2") #add row names
# Test of these different proportions
prop.test(prop_data) #perform test of proportions
```