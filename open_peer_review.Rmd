---
title             : "The Proportion of Signed Open Reviews Depends on the Recommendation"
shorttitle        : "Recommendation Dependent Signing"
author: 
  - name          : "Daniel Lakens"
    affiliation   : "1"
    corresponding : yes
    address       : "ATLAS 9.402, 5600 MB, Eindhoven, The Netherlands"
    email         : "D.Lakens@tue.nl"
  - name          : "Nino van Sambeek"
    affiliation   : "1"
affiliation:
  - id            : "1"
    institution   : "Eindhoven University of Technology, The Netherlands"
author_note: |
  This work was supported by the Netherlands Organization for Scientific Research (NWO) VIDI grant 452-17-013. 
abstract: |
  Open Peer Review is becoming increasingly common, and when reviewers decide whether or not to sign, they take their recommendation into account.
  
keywords          : "Peer Review, Open Science, Transparency"
wordcount         : 4926
bibliography      : ["signing_open_peer_review_.bib"]
figsintext        : yes
figurelist        : no
tablelist         : no
footnotelist      : no
lineno            : no
mask              : no
class             : "jou, a4paper"
output            : papaja::apa6_pdf
editor_options: 
  chunk_output_type: console
---

```{r setup, include = FALSE}

library(dplyr)
library(stringr)
library(ggplot2)
library(here)

here::here()
#### READ IN DATA
# Read open science and open biology rds datafiles
TRS_data_os <- readRDS(file = "royal_society_data_os.rds")
TRS_data_ob <- readRDS(file = "royal_society_data_ob.rds")
# Combine both datasets into one.
TRS_data <- rbind(TRS_data_os, TRS_data_ob)

#Read in PeerJ data
PeerJ_data <- readRDS(file = "peerj_data.rds")

# For manual checks, sort TRS data on doi number
TRS_data_sorted <- TRS_data[ with( TRS_data , order ( df_link)),]

#Create dataframe for TRS and PeerJ of unique rows (e.g., for days)
TRS_unique <- TRS_data[!duplicated(TRS_data$df_link),]
PeerJ_unique <- PeerJ_data[!duplicated(PeerJ_data$df_link),]
TRS_unique_os <- TRS_data_os[!duplicated(TRS_data_os$df_link),]
TRS_unique_ob <- TRS_data_ob[!duplicated(TRS_data_ob$df_link),]


# We can see how many reviews are open in PeerJ by counting all txt files in the peer_reviews_txt folder, and seeing how many end up in our dataframe:
open_review_PeerJ <- nrow(PeerJ_unique)/length(list.files(path = "peerj_reviews_txt"))

# We can see how many reviews are open in TRS by counting all doi in scopus, and seeing how many end up in our list of open reviews:
OS_pdf_list <- readLines(file("royal_society_pdf_files/OS_pdf_list.txt", open = "r"))


open_review_TRS <- length(OS_pdf_list) / length(read.csv("scopus_export_rsos.csv", stringsAsFactors = FALSE)$DOI)
# In this blog Royal Society says around 2/3 of reviews are published alongside the article in 2017: http://blogs.royalsociety.org/publishing/publication-of-peer-review-reports/ 
TRS_2017 <- read.csv("scopus_export_rsos.csv", stringsAsFactors = FALSE)
TRS_2017 <- filter(TRS_2017, str_sub(TRS_2017$DOI, -6) > 170000 & str_sub(TRS_2017$DOI, -6) < 180000)

TRS_2017_open <- OS_pdf_list[OS_pdf_list > 170000 & OS_pdf_list < 180000]
# Seems close - I am not excluding corrections (that have no peer review history), I think the blog post is. 
length(TRS_2017_open)/nrow(TRS_2017)

# open_review_TRS <- length(length(read.csv("scopus_export_rsos.csv", stringsAsFactors = FALSE)$DOI))>170000 & readLines(file("royal_society_pdf_files/OS_pdf_list.txt", open = "r")<180000)) / length(read.csv("scopus_export_rsos.csv", stringsAsFactors = FALSE)$DOI)



XXX <- read.csv("scopus_export_rsos.csv", stringsAsFactors = FALSE)
# One submission to PeerJ was accepted but no reviewer info. It means they are accepted by the editor? Or just an early mistake.
sum(is.na(PeerJ_unique$df_days))

# Mean review time (
TRS_mean_review_time <- mean(TRS_unique$df_days)
PeerJ_mean_review_time <- mean(PeerJ_unique$df_days, na.rm = TRUE)

# How many reviews are anonymous?
mean(TRS_data$df_anonymous)
mean(PeerJ_data$df_anonymous)


# # Using data.table command
# group <- as.data.table(PeerJ_data)
# TEST <- group[group[, .I[df_version == max(df_version)], by=df_link]$V1]

# Select only those reviews for each paper with the highest version (first review)
PeerJ_data_R1 <- group_by(PeerJ_data, df_link)
PeerJ_data_R1 <- top_n(PeerJ_data_R1, 1, df_version)
mean(count(PeerJ_data_R1, df_link)$n) #count reviews per article.


TRS_data_R1 <- group_by(TRS_data, df_link)
TRS_data_R1 <- top_n(TRS_data_R1, 1, df_version) 

TRS_review_agree <- group_by(TRS_data_R1, df_link)
unique(TRS_review_agree$df_recommendation)


# We can easily check whether reviewers agree or not by looking at their recommendations.
TRS_review_agree <- group_by(TRS_data_R1, df_link)
TRS_review_agree <- mutate(TRS_review_agree, df_unique = n_distinct(df_recommendation))
TRS_review_agree <- TRS_review_agree[!duplicated(TRS_review_agree$df_link),]

ggplot(TRS_review_agree, aes(x = as.factor(df_unique))) + 
  geom_bar() +
  theme_linedraw(base_size = 20)

# Are there really reviws with 4 distinct recommendations?
sum(TRS_review_agree$df_unique == 4)

#Select the top recommendation for each review
TRS_data_R1_top <- top_n(TRS_data_R1, 1, df_recommendation) 
TRS_data_R1_top <- TRS_data_R1_top[!duplicated(TRS_data_R1_top$df_link),]
#Select the bottom recommendation for each review
TRS_data_R1_bottom <- top_n(TRS_data_R1, -1, df_recommendation) 
TRS_data_R1_bottom <- TRS_data_R1_bottom[!duplicated(TRS_data_R1_bottom$df_link),]

#Compute the difference between top and bottom
rev_dif <- TRS_data_R1_top$df_recommendation - TRS_data_R1_bottom$df_recommendation
#We table the difference - many are similar, many differ 1 category, some 2 few 3.
table(rev_dif)

# It is useful to select only cases that have a recommendation
TRS_data_complete <- TRS_data[complete.cases(TRS_data$df_recommendation),]

```

In a previous study where researchers were randomly assigned to a condition where their reviews would be signed, or not signed [@walsh_open_2000], reviewers in the signed condition were less likely to recommend rejection (n = 30) than researchers in the unsigned condition (n = 51). This suggests that a causal effect exists between knowing your name will be known or not and the recommendation reviewers provide. Another study that randomly assigned reviewers to a condition where reviewer names would be shared alongside the reviews or a control condition did not find differences between recommendations that were made, but in this study a much larger percentage of reviewers (55%) declined to participate in the trial, raising the possibility of self-selection of participants [@rooyen_effect_2010]. Reviewers indicate they are less likely to review for a journal if their names will be made public, and anecdotally mention that signed reviews would make it more difficult to be honest about poor quaity papers [@mulligan_peer_2013]. A more recent large survey among almost 3000 scientists found that 50.8% of reviewers believe signed open reviews would make peer review worse, with almost two-thirds of respondents indicating they believed reviewers would be less likely to devliver strong criticisms if their identity became known to the authors [@ross-hellauer_survey_2017]. 


## Accessing Open Peer Reviews

PeerJ assigns all articles a number from 1, increasing consecutively with each published manuscript. Reviews are always accessible in html (i.e., at https://peerj.com/articles/1/reviews for the first article). Using text-mining tools in R (cite R, RCurl, XML, stringer) we downloaded all reviews and stored them as individual text files. Reviews of articles published in The Royal Society Open Science and Open Biology are published online with the articles as PDF files. A list of the DOI for every article published in these two outlets was retrieved through Scopus. All reviews were downloaded, and the PDF files were exported to plain text files using pdftools [@ooms_pdftools_2019]. Text files were textmined using the stringr package [@wickham_stringr_2019]. 

For each article we extracted the nuber of revisions, and for each revision we saved for each reviewer whether the reviewer signed the review or not, the word count for their review, and the recommendation for that review round. Note that for PeerJ recommendations each round are giving by the editor. We therefore do not directly know which recommendation each reviewer provided. For RSOS and RSOB reviewers do recommend to 'accept as is', 'accept with minor revisions', 'major revision', or 'reject'. Although 'reject' recommendations occur for RSOS and RSOB, both PeerJ and RSOS and RSOB only share reviews for published articles, and therefore we have very few reject recommendation in RSOS and RSOB, and none in PeerJ. Searching all text files for PeerJ for 'appealed on' reveals 44 articles that were initially rejected, appealed, received a 'major revision' recommendation, and were eventually published. We have coded these papers as 'major revisions'. We also extracted the names of all reviewers when they signed, and calculated the days each manuscript was under peer review from initial submission to acceptance for publication. 

## Peer Review Reports

PeerJ provides authors the option to reproduce the complete peer review history of their article alongside the final publication of the article. 
The Royal Society Open Science and Open Biology also provided authors with the option to publish peer reviews alongside the article, and made this mandatory from May 2017 (Open Biology) and January 2019 (Open Science). 
Peer reviewers have the option to provide their names to the authors when submitting their peer review for both PeerJ and Royal Society Open Science and Open Biology.

We examined the first 7223 articles pubished in PeerJ, as well as the first `r length(read.csv("scopus_export_rsos.csv", stringsAsFactors = FALSE)$DOI)` articles in RSOS and the first `r length(read.csv("scopus_export_rsob.csv", stringsAsFactors = FALSE)$DOI)` articles in RSOB. We retrieved reviews for `r nrow(PeerJ_unique)` articles in PeerJ, `r nrow(TRS_unique_os)` articles in RSOS, and `r nrow(TRS_unique_ob)` articles in RSOB. WE NEED TO REMOVE CORRECTIONS FROM THE NUMBERS ABOVE. Articles can go through multiple rounds of review. We focus on the first review round as this review reflects the initial evaluation of reviewers, before the handling editor has made a decision. On average initial submissions at PeerJ received `r round(mean(count(PeerJ_data_R1, df_link)$n), 2)` reviews. Articles in the Royal Society journals received on average `r round(mean(count(TRS_data_R1, df_link)$n), 2)` reviews for the original submission. 

```{r, include = FALSE}
# sum number of anonymous responses by subgroup using the by command in base R
PeerJ_anonymous_by_recommendation <- by(PeerJ_data_R1$df_anonymous == 1, PeerJ_data_R1$df_recommendation, sum)
PeerJ_notanonymous_by_recommendation <- by(PeerJ_data_R1$df_anonymous == 0, PeerJ_data_R1$df_recommendation, sum)

##########
# Analysis for The Royal Society (TRS)
##########

# sum number of anonymous responses by subgroup using the by command in base R
TRS_anonymous_by_recommendation <- by(TRS_data_R1$df_anonymous == 1, TRS_data_R1$df_recommendation, sum)
TRS_notanonymous_by_recommendation <- by(TRS_data_R1$df_anonymous == 0, TRS_data_R1$df_recommendation, sum)

```

## Signed reviews as a function of the recommendation

When analzing all reviews of original submissions in PeerJ `r sum(PeerJ_data_R1$df_anonymous == 0)` reviewers signed their reviews and `r sum(PeerJ_data_R1$df_anonymous == 1)` did not. In The Royal Society journals `r sum(TRS_data_R1$df_anonymous == 0)` reviewers signed their first review while `r sum(TRS_data_R1$df_anonymous == 1)` did not. The percentages of people who sign (`r 100*sum(PeerJ_data_R1$df_anonymous == 0)/(sum(PeerJ_data_R1$df_anonymous == 0)+sum(PeerJ_data_R1$df_anonymous == 1))`% for PeerJ, `r 100*sum(TRS_data_R1$df_anonymous == 0)/(sum(TRS_data_R1$df_anonymous == 0)+sum(TRS_data_R1$df_anonymous == 1))`% for the Royal Society) are slightly lower than the 43.23% reported by [@wang_open_2016], who analyzed only the first 1214 articles published in PeerJ.


To answer our main research question we plotted the signed an unsigned reviews as a function of the recommendation in the first review round. Remember that for PeerJ these recommendations are made by the editor, and thus only indirectly capture the evaluation of the reviewer. The percentage of signed peer reviews is larger for minor revisions (`r 100 * as.vector(PeerJ_notanonymous_by_recommendation/(PeerJ_notanonymous_by_recommendation + PeerJ_anonymous_by_recommendation))[2]`%) than it is for major revisions (`r 100 * as.vector(PeerJ_notanonymous_by_recommendation/(PeerJ_notanonymous_by_recommendation + PeerJ_anonymous_by_recommendation))[3]`%). Too few articles are accepted after the initial revision in PeerJ (`r sum(PeerJ_data_R1$df_recommendation == 1, na.rm = TRUE)` in total) to provide reliable estimates for this category.

```{r, PeerJrec, fig.cap = "Frequencies of signed and unsigned reviews as a function of whether the handling editor at PeerJ recommended 'minor revisions' or 'major revisions'."}

# #create plot data for accept, minor, major recommendations
# plot_data <- data.frame(factor(c("accept", "minor revision", "major revision", "accept", "minor revision", "major revision"), levels = c("accept", "minor revision", "major revision")),
#                    c("anonymous", "anonymous", "anonymous", "not anonymous", "not anonymous", "not anonymous"),
#                    c(PeerJ_anonymous_by_recommendation, PeerJ_notanonymous_by_recommendation))
# 
# colnames(plot_data) <- c("recommendation", "signed", "count")
# 
# ggplot(plot_data, aes(x = recommendation, y = count, fill = signed)) + 
#   geom_bar(position="dodge", stat="identity") +
#   theme_linedraw(base_size = 12)


#create plot data for only minor, major recommendations
plot_data <- data.frame(factor(c("minor revision", "major revision", "minor revision", "major revision"), levels = c("minor revision", "major revision"))
                               ,
                   c("anonymous", "anonymous", "not anonymous", "not anonymous"),
                   c(PeerJ_anonymous_by_recommendation[2:3], PeerJ_notanonymous_by_recommendation[2:3]))
colnames(plot_data) <- c("recommendation", "signed", "count")

ggplot(plot_data, aes(x = recommendation, y = count, fill = signed)) +
  geom_bar(position="dodge", stat="identity") +
  theme_linedraw(base_size = 20) +  
  theme(legend.position = "top",
        legend.title=element_blank(),
        panel.grid.major.x = element_blank()) + 
  scale_fill_manual(values = c("#000000", "#808080")) + 
  coord_cartesian(ylim = c(0, 5000)) +
  geom_text(aes(label = count), 
              size = 6, 
              color = "black",
              position = position_dodge(width = 0.9),
              vjust = -.5)
```

Analyzing the reviews at the Royal Society provides a more direct answer to our question, since each individual reviewer is asked to provide a recommendation of 'accept', 'minor revisions', 'major revisions', or 'reject'. We can therefore directly compare how recommendations are related to the decision to sign reviews. The percentage of signed peer reviews is decreases as recommendations become more negative, from `r 100 * as.vector(TRS_notanonymous_by_recommendation/(TRS_notanonymous_by_recommendation + TRS_anonymous_by_recommendation))[1]`% for 'accept' recommendations, `r 100 * as.vector(TRS_notanonymous_by_recommendation/(TRS_notanonymous_by_recommendation + TRS_anonymous_by_recommendation))[2]`% for 'minor revisions', `r 100 * as.vector(TRS_notanonymous_by_recommendation/(TRS_notanonymous_by_recommendation + TRS_anonymous_by_recommendation))[3]`% for 'major revisions', and `r 100 * as.vector(TRS_notanonymous_by_recommendation/(TRS_notanonymous_by_recommendation + TRS_anonymous_by_recommendation))[4]`% for 'reject' recommendations. 

Since these data are correlational we can not draw causal conclusions. It is possible that reviewers are less likely to sign more negative reviews. It is also possible that people who sign their reviews are in general less negative in their recommendations, and therefore the distribution of signed reviews differs from non-signed reviews. Given the literature described in the introduction that provides anecdotal evidence researchers worry they will be able to provide open criticism if their names are public, and experimental evidence suggesting that if names are made public, recommendation become somewhat more positive, it seems plausible at least part of the pattern we observed can be explained by reviewers being more likely to sign their more positive reviews. 

# Additional Analyses

Open reviews make it possible to examine additional interesting questions. The dataset we are sharing has information about the recommendations of reviewers or editors after each round of peer review, and the names of reviewers who signed their review. Through the DOI, researchers can link this data to, for example, citation counts for each manuscript. This makes it possible to explore whether initial evaluations of a manuscript are related to future citations counts. By manually coding which career stage authors and reviewers belong to during the review process it is possible to gain insights into who is more likely practice open science. It is possible to extract information about the time between submission and acceptance for each article (`r round(mean(PeerJ_unique$df_days, na.rm = TRUE),0)` days for PeerJ, `r round(mean(TRS_unique$df_days, na.rm = TRUE),0)` days for RSOS and RSOB). Because the reviews themselves are included in our dataset, researchers interested can use the text files to answer more details questions about the content of peer reviews across different domains. 

For example, since multiple reviewers for The Royal Society Open Science and Open Biology make their individual recommendations known, we can see how often reviewers agree. For the `r sum(table(rev_dif))` individual papers that we were able to retrieve reviews from, reviewers agreed on the recommendation in `r table(rev_dif)[1]` (`r 100 * table(rev_dif)[1]/sum(table(rev_dif))`%) of the time. For `r 100 *  table(rev_dif)[2]/sum(table(rev_dif))`% of the manuscripts the maximum deviation was one category (e.g., minor and major revisions), for `r 100 *  table(rev_dif)[3]/sum(table(rev_dif))`% of the manuscripts the maximum deviation was two categories (e.g., accept and major revision), and for `r 100 *  table(rev_dif)[4]/sum(table(rev_dif))`% of the manuscripts the maximum deviation was three categories (i.e., accept and reject). There were `r sum(TRS_review_agree$df_unique == 4)` articles where researchers received all four possible recommendations (accept, minor revisions, major revisions, reject) from at least four different reviewers. 

Regrettably, neither PeerJ nor RSOS make peer reviews available for manuscripts that were rejected. As a consequence, we have analyzed a biased sample of the literature. At the moment only very few scientific journals (e.g., Meta-Psychology, F1000) make peer reviews available for all submitted articles. Although open reviews enable us to look in more detail at the peer review process, it would be extremely interesting to be able to follow manuscripts through the peer review process even when they are rejected at specific journals.



```{r, TRSrec, fig.cap = "Frequencies of signed and unsigned reviews as a function of whether the reviewer at the Royal Society Open Science and Open Biology recommended 'accept', 'minor revisions', 'major revisions', or 'reject'."}


#create plot data for accept, minor, major recommendations
plot_data <- data.frame(factor(c("accept", "minor", "major", "reject", "accept", "minor", "major", "reject"), levels = c("accept", "minor", "major", "reject")),
                   c("anonymous", "anonymous", "anonymous", "anonymous", "not anonymous", "not anonymous", "not anonymous", "not anonymous"),
                   c(TRS_anonymous_by_recommendation, TRS_notanonymous_by_recommendation))
colnames(plot_data) <- c("recommendation", "signed", "count")

ggplot(plot_data, aes(x = recommendation, y = count, fill = signed)) + 
  geom_bar(position="dodge", stat="identity") +
  theme_linedraw(base_size = 20) + 
  theme(legend.position = "top",
        legend.title=element_blank(),
        panel.grid.major.x = element_blank()) + 
  scale_fill_manual(values = c("#000000", "#808080")) + 
  coord_cartesian(ylim = c(0, 1250)) +
  geom_text(aes(label = count), 
              size = 6, 
              color = "black",
              position = position_dodge(width = 0.9),
              vjust = -.5) 


# #create plot data for only minor, major recommendations
# plot_data <- data.frame(c("minor revision", "major revision", "minor revision", "major revision"),
#                    c("anonymous", "anonymous", "not anonymous", "not anonymous"),
#                    c(TRS_anonymous_by_recommendation[2:3], TRS_notanonymous_by_recommendation[2:3]))
# colnames(plot_data) <- c("recommendation", "signed", "count")
# 
# ggplot(plot_data, aes(x = recommendation, y = count, fill = signed)) + 
#   geom_bar(position="dodge", stat="identity") +
#   theme_linedraw(base_size = 20)

```




```{r}
prop_data <- cbind(PeerJ_anonymous_by_recommendation[2:3], PeerJ_notanonymous_by_recommendation[2:3]) #bind as matrix
rownames(prop_data) <- c("minor revision", "major revision") #add column names
colnames(prop_data) <- c("anonymous", "not anonymous") #add row names
# Test of these different proportions
prop.test(prop_data) #perform test of proportions


# In proportions
# 2061 is 53.6% of a total of 3843 articles that receive a minor revision at the first review are anonymous (and 46.4% is not anonymous), for a major revision, 65.4% (4495 out of 6876) is anonymous, 34.6% is not anonymous.
# Calculate proportions
1-as.vector(PeerJ_notanonymous_by_recommendation/(PeerJ_notanonymous_by_recommendation + PeerJ_anonymous_by_recommendation))

```



```{r}
# Word counts PeerJ

words <- by(PeerJ_data_R1$df_word_count, PeerJ_data_R1[,c(5,7)], mean, simplify = TRUE)

plot_data <- data.frame(factor(c("accept", "minor revision", "major revision", "accept", "minor revision", "major revision"), levels = c("accept", "minor revision", "major revision")),
                   c("not anonymous", "not anonymous", "not anonymous", "anonymous", "anonymous", "anonymous"),
                   c(words))
colnames(plot_data) <- c("recommendation", "signed", "wordcount")

ggplot(plot_data, aes(x = recommendation, y = wordcount, fill = signed)) + 
  geom_bar(position="dodge", stat="identity") +
  theme_linedraw(base_size = 20)

ggplot(PeerJ_data_R1, aes(x = df_recommendation, y = df_word_count, fill = as.factor(df_anonymous))) + 
  geom_bar(position="dodge", stat="identity") +
  theme_linedraw(base_size = 20)


ggplot(PeerJ_data_R1, aes(x = df_recommendation, y = df_word_count, fill = as.factor(df_anonymous))) + 
  geom_jitter() +
  theme_linedraw(base_size = 20)


t.test(PeerJ_data_R1$df_word_count ~ PeerJ_data_R1$df_anonymous)


```


In Nino's BEP there were  3552 reviews in RSOS. I have 3629 after running all his code. 


Older Royal Society journals have the comment: Note: This manuscript was transferred from another Royal Society journal without peer review.
What does this mean?

Word counts are not perfectly reliable for several reasons (e.g., addition of references, but also more importantly comments in an attached pdf - maybe we can search for attached and pdf in short comments to identify these. )


" This result is similar to the findings in Bornmann, Wolf,
and Daniel (2012) that the comments in public peer review are much longer than
the comments in closed peer review." 



## Future Research





## NOTES

Searching all text files of PeerJ for 'appealed on' reveals 44 articles that were initially appealed, and subsequently published. 
From Nino's thesis for RSOS: "Interestingly, for 39 of the reviews we had
access to (and were thus published) the final recommendation given by the editor is to reject the manuscript. For these articles to still get published, the author has to appeal the rejection and change the decision made by the editor." 
Not sure how he got this - my code does not suggest this.
sum(PeerJ_data$df_recommendation == 4, na.rm = TRUE) # how many reviews for first submissions 

From Wang et al: " Of the 3,569 review reports, 85 were submitted as
attachments in various formats (comments inserted to the manuscripts, PDF or Word
DOC) or the reviews contained no substantive content such as “no further comments.”" 

# Author Contributions

N. van Sambeek and D. Lakens developed the idea. Both authors generated the R code to generate the data and anayze the results. N van Sambeek drafted the initial version of the manuscript as a Bachelor thesis, D. Lakens wrote the final version, both authors revised the manuscript.

# Conflict of Interest Statement

The authors report no conflicts of interest.

# References

\setlength{\parindent}{-0.5in}
\setlength{\leftskip}{0.5in}


```{r, include = FALSE}
### general numbers
sum(PeerJ_data_R1$df_anonymous == 0) # how many reviews for first submissions are not anonymous?
sum(PeerJ_data_R1$df_anonymous == 1) # how many reviews for first submissions are anonymous?



prop_data <- cbind(PeerJ_anonymous_by_recommendation[2:3], PeerJ_notanonymous_by_recommendation[2:3]) #bind as matrix
rownames(prop_data) <- c("minor revision", "major revision") #add column names
colnames(prop_data) <- c("anonymous", "not anonymous") #add row names
# Test of these different proportions
prop.test(prop_data) #perform test of proportions


# In proportions
# 2061 is 53.6% of a total of 3843 articles that receive a minor revision at the first review are anonymous (and 46.4% is not anonymous), for a major revision, 65.4% (4495 out of 6876) is anonymous, 34.6% is not anonymous.
# Calculate proportions
1-as.vector(PeerJ_notanonymous_by_recommendation/(PeerJ_notanonymous_by_recommendation + PeerJ_anonymous_by_recommendation))


prop_data <- cbind(TRS_anonymous_by_recommendation[2:3], TRS_notanonymous_by_recommendation[2:3]) #bind as matrix
rownames(prop_data) <- c("minor revision", "major revision") #add column names
colnames(prop_data) <- c("anonymous", "not anonymous") #add row names

# In proportions
# 2061 is 53.6% of a total of 3843 articles that receive a minor revision at the first review are anonymous (and 46.4% is not anonymous), for a major revision, 65.4% (4495 out of 6876) is anonymous, 34.6% is not anonymous.
680/(680+524)
178/(178+83)

# Test of these different proportions
prop.test(prop_data) #perform test of proportions


prop_data <- cbind(TRS_anonymous_by_recommendation[c(1,3)], TRS_notanonymous_by_recommendation[c(1,3)]) #bind as matrix
rownames(prop_data) <- c("minor revision", "major revision") #add column names
colnames(prop_data) <- c("anonymous", "notanonymous") #add row names
prop.test(prop_data) #perform test of proportions


prop_data <- cbind(TRS_anonymous_by_recommendation[c(1,2)], TRS_notanonymous_by_recommendation[c(1,2)]) #bind as matrix
rownames(prop_data) <- c("minor revision", "major revision") #add column names
colnames(prop_data) <- c("anonymous", "notanonymous") #add row names
prop.test(prop_data) #perform test of proportions

# Calculate proportions
1-as.vector(TRS_notanonymous_by_recommendation/(TRS_notanonymous_by_recommendation + TRS_anonymous_by_recommendation))

# Analyze words
# TRS
words <- by(TRS_data_R1$df_word_count, TRS_data_R1[,c(5,7)], mean, simplify = TRUE)

plot_data <- data.frame(factor(c("accept", "minor revision", "major revision", "reject", "accept", "minor revision", "major revision", "reject"), levels = c("accept", "minor revision", "major revision", "reject")),
                   c("not anonymous", "not anonymous", "not anonymous", "not anonymous", "anonymous", "anonymous", "anonymous", "anonymous"),
                   c(words))
colnames(plot_data) <- c("recommendation", "signed", "wordcount")

ggplot(plot_data, aes(x = recommendation, y = wordcount, fill = signed)) + 
  geom_bar(position="dodge", stat="identity") +
  theme_linedraw(base_size = 20)

t.test(TRS_data_R1$df_word_count ~ TRS_data_R1$df_anonymous)


# Exploratory
# Randomly split anonymous over 100 subgroups to see progression over time - seems stable.
df_over_time <- data.frame(TRS_data$df_anonymous[1:3600], rep(seq(1,100,1),each = 360))
colnames(df_over_time) <- c("anonymous", "time") # specify columns names
x <- group_by(df_over_time, time)
x <- summarize(x, m = mean(anonymous))

df_over_time <- data.frame(PeerJ_data$df_anonymous[1:12300], rep(seq(1,100,1),each = 1230))
colnames(df_over_time) <- c("anonymous", "time") # specify columns names
x <- group_by(df_over_time, time)
x <- summarize(x, m = mean(anonymous))

# Analysis total review time
# TRS
review_time <- by(TRS_data_R1$df_days, TRS_data_R1[,c(5,7)], mean, simplify = TRUE)

plot_data <- data.frame(factor(c("accept", "minor revision", "major revision", "reject", "accept", "minor revision", "major revision", "reject"), levels = c("accept", "minor revision", "major revision", "reject")),
                   c("not anonymous", "not anonymous", "not anonymous", "not anonymous", "anonymous", "anonymous", "anonymous", "anonymous"),
                   c(review_time))
colnames(plot_data) <- c("recommendation", "signed", "days")

ggplot(plot_data, aes(x = recommendation, y = days, fill = signed)) + 
  geom_bar(position="dodge", stat="identity") +
  theme_linedraw(base_size = 20)

t.test(TRS_data_R1$df_word_count ~ TRS_data_R1$df_anonymous)



heroes <- filter(TRS_data_R1, df_anonymous == 0 & df_recommendation == 4)
filter(PeerJ_data, df_anonymous == 0 & df_reviewer_name == "Daniel Lakens")

me <- filter(TRS_data, df_anonymous == 0)
me <- me[ with( me , order ( df_reviewer_name)),]



##data from Walsh et al 
##
41/(41+36) #accept
53/(53+35) #accept revision
43/(43+31) #resubmit with revision
30/(30+51) #reject

```
