---
title             : "Researchers Decision to Sign is Related to Their Recommendation"
shorttitle        : "Recommendation Relates to Signing"
author: 
  - name          : "Nino van Sambeek"
    affiliation   : "1"
  - name          : "DaniÃ«l Lakens"
    affiliation   : "1"
    corresponding : yes
    address       : "ATLAS 9.402, 5600 MB, Eindhoven, The Netherlands"
    email         : "D.Lakens@tue.nl"
affiliation:
  - id            : "1"
    institution   : "Eindhoven University of Technology, The Netherlands"
author_note: |
 This work was supported by the Netherlands Organization for Scientific Research (NWO) VIDI grant 452-17-013. 
abstract: |
 Open review is becoming increasingly common. Surveys indicate that researchers are generally positive about making peer reviews available. Researchers are more negative about revealing the identity of reviewers. In part this is because of the possibility that reviewers will be less likely to express criticisms if their identity is known to authors. Experiments suggest that reviewers might be somewhat more likely to recommend to accept articles when they are told their name will be shared alongside the reviews, than when they will remain anonymous. So far, there is surprisingly little information about what reviewers actually do in practice. We analyzed XXXX open reviews in PeerJ and XXXX reviews in the Royal Society Open Science and Open Biology to examine whether in practice the propability that a reviewer reveals their identity is related to their recommendation. When comparing XXXX signed and XXXX unsigned reviews, we observed a clear pattern where the more positive a recommendation, the more likely it was signed. Although our data does not allow for inferences about causal relationships, it provides behavioral data from real peer reviews that demonstrates convincingly that reviewers who sign their reviews give more positive recommendations.
  
keywords          : "Peer Review, Open Science, Transparency"
wordcount         : 4926
bibliography      : ["signing_open_peer_review_.bib"]
figsintext        : yes
figurelist        : no
tablelist         : no
footnotelist      : no
lineno            : no
mask              : no
class             : "jou, a4paper"
output            : papaja::apa6_pdf
editor_options: 
  chunk_output_type: console
---

```{r setup, include = FALSE}

library(dplyr)
library(stringr)
library(ggplot2)
library(here)

here::here()
#### READ IN DATA
# Read open science and open biology rds datafiles
TRS_data_os <- readRDS(file = "royal_society_data_os.rds")
TRS_data_ob <- readRDS(file = "royal_society_data_ob.rds")
# Combine both datasets into one.
TRS_data <- rbind(TRS_data_os, TRS_data_ob)

#Read in PeerJ data
PeerJ_data <- readRDS(file = "peerj_data.rds")

# For manual checks, sort TRS data on doi number
TRS_data_sorted <- TRS_data[ with( TRS_data , order ( df_link)),]

#Create dataframe for TRS and PeerJ of unique rows (e.g., for days)
TRS_unique <- TRS_data[!duplicated(TRS_data$df_link),]
PeerJ_unique <- PeerJ_data[!duplicated(PeerJ_data$df_link),]
TRS_unique_os <- TRS_data_os[!duplicated(TRS_data_os$df_link),]
TRS_unique_ob <- TRS_data_ob[!duplicated(TRS_data_ob$df_link),]


# We can see how many reviews are open in PeerJ by counting all txt files in the peer_reviews_txt folder, and seeing how many end up in our dataframe:
open_review_PeerJ <- nrow(PeerJ_unique)/length(list.files(path = "peerj_reviews_txt"))

# We can see how many reviews are open in TRS by counting all doi in scopus, and seeing how many end up in our list of open reviews:
OS_pdf_list <- readLines(file("royal_society_pdf_files/OS_pdf_list.txt", open = "r"))


open_review_TRS <- length(OS_pdf_list) / length(read.csv("scopus_export_rsos.csv", stringsAsFactors = FALSE)$DOI)

# How many entries in RSOS in total? Read in scopus datafile
TRS_scopus_OS <- read.csv("scopus_export_rsos.csv", stringsAsFactors = FALSE)
table(TRS_scopus_OS$Document.Type)
# Excude Errata and Editorial (that have no reviews)
TRS_scopus_OS <- TRS_scopus_OS[!TRS_scopus_OS$Document.Type == "Erratum" & !TRS_scopus_OS$Document.Type == "Editorial",]
table(TRS_scopus_OS$Document.Type)

# How many entries in RSOS in total? Read in scopus datafile
TRS_scopus_OB <- read.csv("scopus_export_rsob.csv", stringsAsFactors = FALSE)
table(TRS_scopus_OB$Document.Type)
# Excude Errata and Editorial (that have no reviews)
TRS_scopus_OB <- TRS_scopus_OB[!TRS_scopus_OB$Document.Type == "Erratum" & !TRS_scopus_OB$Document.Type == "Editorial" & !TRS_scopus_OB$Document.Type == "Retracted",]
table(TRS_scopus_OB$Document.Type)

# One submission to PeerJ was accepted but no reviewer info. It means they are accepted by the editor? Or just an early mistake.
sum(is.na(PeerJ_unique$df_days))

# Mean review time (
TRS_mean_review_time <- mean(TRS_unique$df_days)
PeerJ_mean_review_time <- mean(PeerJ_unique$df_days, na.rm = TRUE)

# How many reviews are anonymous?
mean(TRS_data$df_anonymous)
mean(PeerJ_data$df_anonymous)

# Select only those reviews for each paper with the highest version for PeerJ (first review)
PeerJ_data_R1 <- group_by(PeerJ_data, df_link)
PeerJ_data_R1 <- top_n(PeerJ_data_R1, 1, df_version)
mean(count(PeerJ_data_R1, df_link)$n) #count reviews per article.

# Do the same for TRS
TRS_data_R1 <- group_by(TRS_data, df_link)
TRS_data_R1 <- top_n(TRS_data_R1, 1, df_version) 

# We can easily check whether reviewers agree or not by looking at their recommendations.
TRS_review_agree <- group_by(TRS_data_R1, df_link)
TRS_review_agree <- mutate(TRS_review_agree, df_unique = n_distinct(df_recommendation))
TRS_review_agree <- TRS_review_agree[!duplicated(TRS_review_agree$df_link),]

ggplot(TRS_review_agree, aes(x = as.factor(df_unique))) + 
  geom_bar() +
  theme_linedraw(base_size = 20)

# Are there really reviws with 4 distinct recommendations?
sum(TRS_review_agree$df_unique == 4)

#Select the top recommendation for each review
TRS_data_R1_top <- top_n(TRS_data_R1, 1, df_recommendation) 
TRS_data_R1_top <- TRS_data_R1_top[!duplicated(TRS_data_R1_top$df_link),]
#Select the bottom recommendation for each review
TRS_data_R1_bottom <- top_n(TRS_data_R1, -1, df_recommendation) 
TRS_data_R1_bottom <- TRS_data_R1_bottom[!duplicated(TRS_data_R1_bottom$df_link),]

#Compute the difference between top and bottom
rev_dif <- TRS_data_R1_top$df_recommendation - TRS_data_R1_bottom$df_recommendation
#We table the difference - many are similar, many differ 1 category, some 2 few 3.
table(rev_dif)

# It is useful to select only cases that have a recommendation
TRS_data_complete <- TRS_data[complete.cases(TRS_data$df_recommendation),]

```

As technology advances, science advances. The rise of the internet has made it possible to share all aspects of the scientific process. This includes opening up the peer review process. An increasing number of journals has started to make peer review reports available alongside published articles as part of ongoing experiments that aim to improve the peer review process [@bruce_impact_2016]. Having access to these peer review reports makes it possible to gain new insights about how scientists perform reviews. 

Some journals have chosen to not just share reviews publicly when an article is published, but in addition require reviewers to sign their reviews. An important argument for such a policy is that reviewers can receive credit for their work [@godlee_making_2002]. However, scientists do not feel these benefits outweigh possible costs. Researchers indicate they would be less likely to review for a journal if their names are made public, and anecdotally mention that signed reviews would make it more difficult to be honest about poor quaity papers [@mulligan_peer_2013]. A more recent survey among almost 3000 scientists found that 50.8% of reviewers believe signed open reviews would make peer review worse [@ross-hellauer_survey_2017]. Almost two-thirds of respondents believed reviewers would be less likely to deliver strong criticisms if their identity became known to the authors.

These self-report studies are complemented by experiments in which reviewers are been randomly assigned to a condition where their names would be added to their reviews or not [@walsh_open_2000]. Reviewers in the signed condition were less likely to recommend rejection (n = 30) than researchers in the unsigned condition (n = 51). This suggests that a causal effect exists between knowing your name will be known or not and the recommendation reviewers provide. Across four studies, a meta-analysis by Bruce and colleagues (2016) supports the conclusion that reviewers are somewhat less likely to recommend rejection when they have to sign their reviews.

Although the self-report studies and the experiments clearly suggest reviewers worry about having their name attached to highly critical reviews, so far little is known about what reviewers actually do when given the opportunity to sign their reviews. The trade-off between the benefit of getting credit for peer review work and the risk of negative consequences when signing critical reviews might lead to strategic behavior where authors become more likely to sign reviews the most positive their recommendation is. If this strategic behavior occurs in practice, we should see a different pattern of recommendations for signed and unsigned reviews. One recent study revealed such a pattern when analyzing data from an Elsevier trial on publishing peer review reports in the journal Agricultural and Forest Meteorology, Annals of Medicine and Surgery, Engineering Fracture Mechanics, Journal of Hydrology: Regional Studies, and the International Journal of Surgery [@bravo_effect_2019]. Although only 8.1% of reviewers choose to disclose their identity in these reviews, but the data revealed a clear difference between the recommendations by reviewers who chose to sign their reviews, compared to reviewers who did not choose to sign. Here, we examine this pattern in two large open access journals that publish across a wide range of scientific disciplines. In this replication and extension of the analysis by Bravo and colleagues (2019) we examine the relationship between recommendations and signed reviews in open peer reviews in mega-journals that publish scientific work across a wide range of scientific disciplines. 

# Accessing Open Peer Reviews

To examine the pattern of recommendations as a function of whether reviewers signed their review or not we analyzed `r length(TRS_scopus_OS$DOI)+length(TRS_scopus_OB$DOI)` articles published in The Royal Society Open Science and Open Biology, and 7223 articles published in PeerJ. PeerJ launched in 2012 and gave reviewers the possibility to sign, and authors to make the peer reviews available with the final publication. The Royal Society Open science launched in 2014 and strongly encouraged authors to make the peer reviews available with the final publication. Royal Society Open Biology made sharing reviews with the final publication mandatory in 2017. Because of their broad scope, the large number of publications in each journal, and their early focus on open reviews, these reviews provide insights into the peer review behavior of scientists across a wide range of disciplines.

PeerJ assigns all articles a number, increasing consecutively with each published manuscript. Reviews are always accessible in html (i.e., at https://peerj.com/articles/1/reviews for the first article can be found). Reviews of articles published in The Royal Society Open Science and Open Biology are published online with the articles as PDF files. A list of Digital Object Identifiers (DOIs) for every article published in these two outlets was retrieved through Scopus. All reviews were downloaded, and the PDF files were exported to plain text files using pdftools [@ooms_pdftools_2019]. These text files were mined for recommendations, reviewers names, submission and acceptance dates, and the review content, using the stringr package [@wickham_stringr_2019]. 

For each article we extracted the number of revisions, and for each revision we saved for each review whether the reviewer signed or not, the word count for their review, and the recommendation for that review round. Note that for PeerJ recommendations each round are giving by the editor. We therefore do not directly know which recommendation each reviewer provided, but analyze the data based on the assumption that the decision by the editor is correlated with the underlying reviews. For RSOS and RSOB reviewers do recommend to 'accept as is', 'accept with minor revisions', 'major revision', or 'reject'. Although 'reject' recommendations occur for RSOS and RSOB, both PeerJ and RSOS and RSOB only share reviews for published articles, and therefore we have very few reject recommendation in RSOS and RSOB, and none in PeerJ. Searching all reviews for PeerJ for the words 'appealed on' reveals 44 articles that were initially rejected, appealed, received a 'major revision' recommendation, and were eventually published. We have coded these papers as 'major revisions'. All scripts to download and analyze the reviews, and computationally reproduce this manuscript, are available at https://osf.io/jkbmw/. 

# Peer Review Reports

PeerJ provides authors the option to publish the peer reviews of their article alongside the final publication of the article. The Royal Society Open Science and Open Biology also provided authors with the option to publish peer reviews alongside the article, and made this mandatory from May 2017 (Open Biology) and January 2019 (Open Science). Peer reviewers have the option to provide their names to the authors when submitting their peer review for both PeerJ and Royal Society Open Science and Open Biology.

We examined the first 7223 articles pubished in PeerJ, as well as the first `r length(TRS_scopus_OS$DOI)` articles in RSOS and the first `r length(TRS_scopus_OB$DOI)` articles in RSOB. We retrieved all reviews for all review rounds of `r nrow(PeerJ_unique)` articles in PeerJ, `r nrow(TRS_unique_os)` articles in RSOS, and `r nrow(TRS_unique_ob)` articles in RSOB. Articles can go through multiple rounds of review. We focus on the first review round as this review reflects the initial evaluation of reviewers, before the handling editor has made a decision, following [@bravo_effect_2019]. On average initial submissions at PeerJ received `r round(mean(count(PeerJ_data_R1, df_link)$n), 2)` reviews. Articles in the Royal Society journals received on average `r round(mean(count(TRS_data_R1, df_link)$n), 2)` reviews for the original submission. 

```{r, include = FALSE}
# sum number of anonymous responses by subgroup using the by command in base R
PeerJ_anonymous_by_recommendation <- by(PeerJ_data_R1$df_anonymous == 1, PeerJ_data_R1$df_recommendation, sum)
PeerJ_notanonymous_by_recommendation <- by(PeerJ_data_R1$df_anonymous == 0, PeerJ_data_R1$df_recommendation, sum)

##########
# Analysis for The Royal Society (TRS)
##########

# sum number of anonymous responses by subgroup using the by command in base R
TRS_anonymous_by_recommendation <- by(TRS_data_R1$df_anonymous == 1, TRS_data_R1$df_recommendation, sum)
TRS_notanonymous_by_recommendation <- by(TRS_data_R1$df_anonymous == 0, TRS_data_R1$df_recommendation, sum)

```

# Signed reviews as a function of the recommendation

When analyzing all reviews of original submissions in PeerJ `r sum(PeerJ_data_R1$df_anonymous == 0)` reviewers signed their reviews and `r sum(PeerJ_data_R1$df_anonymous == 1)` did not. In The Royal Society journals `r sum(TRS_data_R1$df_anonymous == 0)` reviewers signed their first review while `r sum(TRS_data_R1$df_anonymous == 1)` did not. The percentages of people who sign (`r 100*sum(PeerJ_data_R1$df_anonymous == 0)/(sum(PeerJ_data_R1$df_anonymous == 0)+sum(PeerJ_data_R1$df_anonymous == 1))`% for PeerJ, `r 100*sum(TRS_data_R1$df_anonymous == 0)/(sum(TRS_data_R1$df_anonymous == 0)+sum(TRS_data_R1$df_anonymous == 1))`% for the Royal Society) are slightly lower than the 43.23% reported by [@wang_open_2016], who analyzed the first 1214 articles published in PeerJ.

To answer our main research question we plotted the signed an unsigned reviews as a function of the recommendation in the first review round. Remember that for PeerJ these recommendations are made by the editor, and thus only indirectly capture the evaluation of the reviewer. The percentage of signed peer reviews is larger for minor revisions (`r 100 * as.vector(PeerJ_notanonymous_by_recommendation/(PeerJ_notanonymous_by_recommendation + PeerJ_anonymous_by_recommendation))[2]`%) than it is for major revisions (`r 100 * as.vector(PeerJ_notanonymous_by_recommendation/(PeerJ_notanonymous_by_recommendation + PeerJ_anonymous_by_recommendation))[3]`%). Too few articles are accepted after the initial revision in PeerJ (`r sum(PeerJ_data_R1$df_recommendation == 1, na.rm = TRUE)` in total) to provide reliable estimates for this category.


```{r, PeerJrec, fig.cap = "Frequencies of signed and unsigned reviews as a function of whether the handling editor at PeerJ recommended 'minor revisions' or 'major revisions'."}

# #create plot data for accept, minor, major recommendations
# plot_data <- data.frame(factor(c("accept", "minor revision", "major revision", "accept", "minor revision", "major revision"), levels = c("accept", "minor revision", "major revision")),
#                    c("unsigned", "unsigned", "unsigned", "signed", "signed", "signed"),
#                    c(PeerJ_anonymous_by_recommendation, PeerJ_notanonymous_by_recommendation))
# 
# colnames(plot_data) <- c("recommendation", "signed", "count")
# 
# ggplot(plot_data, aes(x = recommendation, y = count, fill = signed)) + 
#   geom_bar(position="dodge", stat="identity") +
#   theme_linedraw(base_size = 12)


#create plot data for only minor, major recommendations
plot_data <- data.frame(factor(c("minor revision", "major revision", "minor revision", "major revision"), levels = c("minor revision", "major revision"))
                               ,
                   c("unsigned", "unsigned", "signed", "signed"),
                   c(PeerJ_anonymous_by_recommendation[2:3], PeerJ_notanonymous_by_recommendation[2:3]))
colnames(plot_data) <- c("recommendation", "signed", "count")

ggplot(plot_data, aes(x = recommendation, y = count, fill = signed)) +
  geom_bar(position="dodge", stat="identity") +
  theme_linedraw(base_size = 20) +  
  theme(legend.position = "top",
        legend.title=element_blank(),
        panel.grid.major.x = element_blank()) + 
  scale_fill_manual(values = c("#000000", "#808080")) + 
  coord_cartesian(ylim = c(0, 5000)) +
  geom_text(aes(label = count), 
              size = 6, 
              color = "black",
              position = position_dodge(width = 0.9),
              vjust = -.5)
```


Analyzing the reviews at the Royal Society provides a more direct answer to our question, since each individual reviewer is asked to provide a recommendation of 'accept', 'minor revisions', 'major revisions', or 'reject'. We can therefore directly compare how recommendations are related to the decision to sign reviews. The percentage of signed peer reviews is decreases as recommendations become more negative, from `r 100 * as.vector(TRS_notanonymous_by_recommendation/(TRS_notanonymous_by_recommendation + TRS_anonymous_by_recommendation))[1]`% for 'accept' recommendations, `r 100 * as.vector(TRS_notanonymous_by_recommendation/(TRS_notanonymous_by_recommendation + TRS_anonymous_by_recommendation))[2]`% for 'minor revisions', `r 100 * as.vector(TRS_notanonymous_by_recommendation/(TRS_notanonymous_by_recommendation + TRS_anonymous_by_recommendation))[3]`% for 'major revisions', and `r 100 * as.vector(TRS_notanonymous_by_recommendation/(TRS_notanonymous_by_recommendation + TRS_anonymous_by_recommendation))[4]`% for 'reject' recommendations. 


```{r, TRSrec, fig.cap = "Frequencies of signed and unsigned reviews as a function of whether the reviewer at the Royal Society Open Science and Open Biology recommended 'accept', 'minor revisions', 'major revisions', or 'reject'."}


#create plot data for accept, minor, major recommendations
plot_data <- data.frame(factor(c("accept", "minor", "major", "reject", "accept", "minor", "major", "reject"), levels = c("accept", "minor", "major", "reject")),
                   c("unsigned", "unsigned", "unsigned", "unsigned", "signed", "signed", "signed", "signed"),
                   c(TRS_anonymous_by_recommendation, TRS_notanonymous_by_recommendation))
colnames(plot_data) <- c("recommendation", "signed", "count")

ggplot(plot_data, aes(x = recommendation, y = count, fill = signed)) + 
  geom_bar(position="dodge", stat="identity") +
  theme_linedraw(base_size = 20) + 
  theme(legend.position = "top",
        legend.title=element_blank(),
        panel.grid.major.x = element_blank()) + 
  scale_fill_manual(values = c("#000000", "#808080")) + 
  coord_cartesian(ylim = c(0, 1250)) +
  geom_text(aes(label = count), 
              size = 6, 
              color = "black",
              position = position_dodge(width = 0.9),
              vjust = -.5) 


# #create plot data for only minor, major recommendations
# plot_data <- data.frame(c("minor revision", "major revision", "minor revision", "major revision"),
#                    c("unsigned", "unsigned", "signed", "signed"),
#                    c(TRS_anonymous_by_recommendation[2:3], TRS_notanonymous_by_recommendation[2:3]))
# colnames(plot_data) <- c("recommendation", "signed", "count")
# 
# ggplot(plot_data, aes(x = recommendation, y = count, fill = signed)) + 
#   geom_bar(position="dodge", stat="identity") +
#   theme_linedraw(base_size = 20)

```

Since these data are correlational we can not draw causal conclusions. It is possible that reviewers are less likely to sign more negative reviews. It is also possible that people who sign their reviews are in general less negative in their recommendations, and therefore the distribution of signed reviews differs from non-signed reviews. Given the literature described in the introduction that provides anecdotal evidence researchers worry they will be able to provide open criticism if their names are public, and experimental evidence suggesting that if names are made public, recommendation become somewhat more positive, it seems plausible at least part of the pattern we observed can be explained by reviewers being more likely to sign their more positive reviews. Our results replicate the observations by Bravo and colleagues (2019).

# Additional Analyses

The dataset we are sharing has information about the recommendations of reviewers or editors after each round of peer review, the names of reviewers who signed their review, and the time in review  (`r round(mean(PeerJ_unique$df_days, na.rm = TRUE),0)` days for PeerJ, `r round(mean(TRS_unique$df_days, na.rm = TRUE),0)` days for RSOS and RSOB). Through the DOI, researchers can link this data to other sources of information such as citation counts. Because the reviews themselves are included in our dataset, researchers interested can use the text files to answer more details questions about the content of peer reviews across different domains. For example, we can examine the word count for signed and unsigned reviews as a function of the recommendation. With the caveat that the graph below present uncleaned raw word counts, we replicate the results reported by (@wang_open_2016) for PeerJ in the dataset for Royal Society Open Science and Open Biology that there is no statistical difference between the word count for signed and unsigned reviews, despite a small numerical difference in the same direction as in their analysis of PeerJ reviews. 

```{r wordcount}
# Word counts PeerJ

# words <- by(PeerJ_data_R1$df_word_count, PeerJ_data_R1[,c(5,7)], mean, simplify = TRUE)
# 
# plot_data <- data.frame(factor(c("accept", "minor revision", "major revision", "accept", "minor revision", "major revision"), levels = c("accept", "minor revision", "major revision")),
#                    c("signed", "signed", "signed", "unsigned", "unsigned", "unsigned"),
#                    c(words))
# colnames(plot_data) <- c("recommendation", "signed", "word count")
# 
# ggplot(plot_data, aes(x = recommendation, y = wordcount, fill = signed)) + 
#   geom_bar(position="dodge", stat="identity") +
#   theme_linedraw(base_size = 20)
# 
# ggplot(PeerJ_data_R1, aes(x = df_recommendation, y = df_word_count, fill = as.factor(df_anonymous))) + 
#   geom_bar(position="dodge", stat="identity") +
#   theme_linedraw(base_size = 20)
# 
# t.test(PeerJ_data_R1$df_word_count ~ PeerJ_data_R1$df_anonymous)

# Words count RSOS
# 

words <- by(TRS_data_R1$df_word_count, TRS_data_R1[,c(5,7)], mean, simplify = TRUE)

plot_data <- data.frame(factor(c("accept", "minor", "major", "reject", "accept", "minor", "major", "reject"), levels = c("accept", "minor", "major", "reject")),
                   c("signed", "signed", "signed", "signed", "unsigned", "unsigned", "unsigned", "unsigned"),
                   c(words))
colnames(plot_data) <- c("recommendation", "signed", "wordcount")

ggplot(plot_data, aes(x = recommendation, y = wordcount, fill = signed)) + 
  geom_bar(position="dodge", stat="identity") +
  theme_linedraw(base_size = 20) + 
  theme(legend.position = "top",
        legend.title=element_blank(),
        panel.grid.major.x = element_blank()) + 
  scale_fill_manual(values = c("#000000", "#808080")) + 
  coord_cartesian(ylim = c(0, 750)) +
  geom_text(aes(label = round(wordcount,0)), 
              size = 6, 
              color = "black",
              position = position_dodge(width = 0.9),
              vjust = -.5) +
  labs(y = "word count")

# When tested, the difference is not significant
# t.test(TRS_data_R1$df_word_count ~ TRS_data_R1$df_anonymous)
```

Since multiple reviewers for The Royal Society Open Science and Open Biology make their individual recommendations known, one example of the insights open reviews provide is how often reviewers agree. For the `r sum(table(rev_dif))` individual papers that we were able to retrieve reviews from, reviewers agreed on the recommendation in `r table(rev_dif)[1]` (`r 100 * table(rev_dif)[1]/sum(table(rev_dif))`%) of the time. For `r 100 *  table(rev_dif)[2]/sum(table(rev_dif))`% of the manuscripts the maximum deviation was one category (e.g., minor and major revisions), for `r 100 *  table(rev_dif)[3]/sum(table(rev_dif))`% of the manuscripts the maximum deviation was two categories (e.g., accept and major revision), and for `r 100 *  table(rev_dif)[4]/sum(table(rev_dif))`% of the manuscripts the maximum deviation was three categories (i.e., accept and reject). There were `r sum(TRS_review_agree$df_unique == 4)` articles where researchers received all four possible recommendations (accept, minor revisions, major revisions, reject) from at least four different reviewers. 

Regrettably, neither PeerJ nor RSOS make peer reviews available for manuscripts that were rejected. As a consequence, we have analyzed a biased sample of the literature. At the moment only very few scientific journals (e.g., Meta-Psychology, F1000) make peer reviews available for all submitted articles. Although open reviews enable us to look in more detail at the peer review process, it would be extremely interesting to be able to follow manuscripts through the peer review process even when they are rejected at specific journals. Despite this limitation, the pattern of results is very similar to that observed by Bravo and colleagues (2019) who had access to the reviews for accepted and rejected manuscripts. Thus, even though the proportion of reject recommendation in our dataset is small, the relative difference with which this recommendation is given in signed and unsigned reviews seems comparable to datsets that include rejected manuscripts.

# Discussion

Our analysis shows that when authors are given the choice to sign their reviews, signed reviews have more positive recommendations than unsigned reviews. This is true for reviews in Royal Society Open Science, a large multi-disciplinary journal that published across a wide range of scientific domains. The effect is also visible in a second large multi-disciplinary journal, PeerJ, under the assumption that recommendations by editors at PeerJ are correlated with the recommendations by reviewers. Our results replicate and extend earlier findings by @bravo_effect_2019, and complement self-report and experimental results. Although our results do not allow for causal claims, they are in line with experimental results that reveal researchers give more positive recommendations when their name is published alongside their reviews [@bruce_impact_2016].

Open reviews allow researchers to adress meta-scientific questions that give insights into the peer review process. Our data provide further support for the idea that researchers decision to sign is related to their recommendation across a wide range of scientific disciplines. Our correlational data, interpreted together with self-report data and experiments in the literature, increases the plausibility that in real peer reviews at least some researchers are more likely to sign if their recommendation is more positive. This type of strategic behavior also follows from a purely rational strategy to optimize the benefits of peer review while minimizing the costs. For positive recommendations, reviewers will get credit for theor reviews, while for negative reviews, they do not run the risk of receiving any backlash from collegaues in their field. 

It is worthwhile to examine whether this fear of retaliation has an empirical basis, and if so, to consider developing guidelines to counteract such retaliation [@bastian_signing_2018]. Althoughit looks increasingly plausible that at least some reviewers feel hesitant to sign if they believe doing so could have negative consequences, but are willing to sign to gain credit for their work for more positive recommendations, it seems wortwhile to explore ways in which they can receive credit for all their peer review work, also when their recommendations are more negative. If more journals share open reviews, also for rejected manuscripts, future meta-scientific work might be able to study this important topic in more detail. 


```{r, include = FALSE}
# We can in principle report p-values for tests for proportions, but I don't think this adds omething to the manuscript. 

prop_data <- cbind(PeerJ_anonymous_by_recommendation[2:3], PeerJ_notanonymous_by_recommendation[2:3]) #bind as matrix
rownames(prop_data) <- c("minor revision", "major revision") #add column names
colnames(prop_data) <- c("anonymous", "not anonymous") #add row names
# Test of these different proportions
prop.test(prop_data) #perform test of proportions


# In proportions
# 2061 is 53.6% of a total of 3843 articles that receive a minor revision at the first review are anonymous (and 46.4% is not anonymous), for a major revision, 65.4% (4495 out of 6876) is anonymous, 34.6% is not anonymous.
# Calculate proportions
1-as.vector(PeerJ_notanonymous_by_recommendation/(PeerJ_notanonymous_by_recommendation + PeerJ_anonymous_by_recommendation))

```



# Author Contributions

N. van Sambeek and D. Lakens developed the idea, and jointly created the R code to generate and analyze the data. N van Sambeek drafted the initial version of the manuscript as a Bachelor thesis, D. Lakens drafted the final version, and both authors revised the final version of the manuscript.

# Conflict of Interest Statement

The authors report no conflicts of interest.

# References

\setlength{\parindent}{-0.5in}
\setlength{\leftskip}{0.5in}


```{r, include = FALSE}
### general numbers
sum(PeerJ_data_R1$df_anonymous == 0) # how many reviews for first submissions are not anonymous?
sum(PeerJ_data_R1$df_anonymous == 1) # how many reviews for first submissions are anonymous?



prop_data <- cbind(PeerJ_anonymous_by_recommendation[2:3], PeerJ_notanonymous_by_recommendation[2:3]) #bind as matrix
rownames(prop_data) <- c("minor revision", "major revision") #add column names
colnames(prop_data) <- c("anonymous", "not anonymous") #add row names
# Test of these different proportions
prop.test(prop_data) #perform test of proportions


# In proportions
# 2061 is 53.6% of a total of 3843 articles that receive a minor revision at the first review are anonymous (and 46.4% is not anonymous), for a major revision, 65.4% (4495 out of 6876) is anonymous, 34.6% is not anonymous.
# Calculate proportions
1-as.vector(PeerJ_notanonymous_by_recommendation/(PeerJ_notanonymous_by_recommendation + PeerJ_anonymous_by_recommendation))


prop_data <- cbind(TRS_anonymous_by_recommendation[2:3], TRS_notanonymous_by_recommendation[2:3]) #bind as matrix
rownames(prop_data) <- c("minor revision", "major revision") #add column names
colnames(prop_data) <- c("unsigned", "signed") #add row names

# In proportions
# 2061 is 53.6% of a total of 3843 articles that receive a minor revision at the first review are anonymous (and 46.4% is not anonymous), for a major revision, 65.4% (4495 out of 6876) is anonymous, 34.6% is not anonymous.
680/(680+524)
178/(178+83)

# Test of these different proportions
prop.test(prop_data) #perform test of proportions


prop_data <- cbind(TRS_anonymous_by_recommendation[c(1,3)], TRS_notanonymous_by_recommendation[c(1,3)]) #bind as matrix
rownames(prop_data) <- c("minor revision", "major revision") #add column names
colnames(prop_data) <- c("unsigned", "notanonymous") #add row names
prop.test(prop_data) #perform test of proportions


prop_data <- cbind(TRS_anonymous_by_recommendation[c(1,2)], TRS_notanonymous_by_recommendation[c(1,2)]) #bind as matrix
rownames(prop_data) <- c("minor revision", "major revision") #add column names
colnames(prop_data) <- c("unsigned", "notanonymous") #add row names
prop.test(prop_data) #perform test of proportions

# Calculate proportions
1-as.vector(TRS_notanonymous_by_recommendation/(TRS_notanonymous_by_recommendation + TRS_anonymous_by_recommendation))

# Analyze words
# TRS
words <- by(TRS_data_R1$df_word_count, TRS_data_R1[,c(5,7)], mean, simplify = TRUE)

plot_data <- data.frame(factor(c("accept", "minor revision", "major revision", "reject", "accept", "minor revision", "major revision", "reject"), levels = c("accept", "minor revision", "major revision", "reject")),
                   c("signed", "signed", "signed", "signed", "unsigned", "unsigned", "unsigned", "unsigned"),
                   c(words))
colnames(plot_data) <- c("recommendation", "signed", "wordcount")

ggplot(plot_data, aes(x = recommendation, y = wordcount, fill = signed)) + 
  geom_bar(position="dodge", stat="identity") +
  theme_linedraw(base_size = 20)

t.test(TRS_data_R1$df_word_count ~ TRS_data_R1$df_anonymous)


# Exploratory
# Randomly split anonymous over 100 subgroups to see progression over time - seems stable.
df_over_time <- data.frame(TRS_data$df_anonymous[1:3600], rep(seq(1,100,1),each = 360))
colnames(df_over_time) <- c("anonymous", "time") # specify columns names
x <- group_by(df_over_time, time)
x <- summarize(x, m = mean(anonymous))

df_over_time <- data.frame(PeerJ_data$df_anonymous[1:12300], rep(seq(1,100,1),each = 1230))
colnames(df_over_time) <- c("anonymous", "time") # specify columns names
x <- group_by(df_over_time, time)
x <- summarize(x, m = mean(anonymous))

# Analysis total review time
# TRS
review_time <- by(TRS_data_R1$df_days, TRS_data_R1[,c(5,7)], mean, simplify = TRUE)

plot_data <- data.frame(factor(c("accept", "minor revision", "major revision", "reject", "accept", "minor revision", "major revision", "reject"), levels = c("accept", "minor revision", "major revision", "reject")),
                   c("signed", "signed", "signed", "signed", "unsigned", "unsigned", "unsigned", "unsigned"),
                   c(review_time))
colnames(plot_data) <- c("recommendation", "signed", "days")

ggplot(plot_data, aes(x = recommendation, y = days, fill = signed)) + 
  geom_bar(position="dodge", stat="identity") +
  theme_linedraw(base_size = 20)

t.test(TRS_data_R1$df_word_count ~ TRS_data_R1$df_anonymous)



heroes <- filter(TRS_data_R1, df_anonymous == 0 & df_recommendation == 4)
filter(PeerJ_data, df_anonymous == 0 & df_reviewer_name == "Daniel Lakens")

me <- filter(TRS_data, df_anonymous == 0)
me <- me[ with( me , order ( df_reviewer_name)),]



##data from Walsh et al 
##
41/(41+36) #accept
53/(53+35) #accept revision
43/(43+31) #resubmit with revision
30/(30+51) #reject

```
