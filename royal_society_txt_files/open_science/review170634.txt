How robust is familiar face recognition? A repeat detection
study of > 1000 faces
Angus F. Chapman, Hannah Hawkins-Elder and Tirta Susilo
Article citation details
R. Soc. open sci. 5: 170634.
http://dx.doi.org/10.1098/rsos.170634
Review timeline
Original submission: 17 March 2017 Note: Reports are unedited and appear as
1st revised submission: 18 May 2017 submitted by the referee. The review history
2nd revised submission: 8 June 2017 appears in chronological order.
3rd revised submission: 15 March 2018
4th revised submission: 19 April 2018
Final acceptance: 24 April 2018
Review History
label_version_1
RSOS-170250.R0 (Original submission)
label_author_1
Review form: Reviewer 1 (Wilma Bainbridge)
Is the manuscript scientifically sound in its present form?
Are the interpretations and conclusions justified by the results?
Is the language acceptable?
Yes
Is it clear how to access all supporting data?
Do you have any ethical concerns with this paper?
No
© 2018 The Authors. Published by the Royal Society under the terms of the Creative Commons
Attribution License http://creativecommons.org/licenses/by/4.0/, which permits unrestricted use,
provided the original author and source are credited
2
Have you any concerns about statistical analyses in this paper?
No
Recommendation?
label_recommendation_1
Major revision
Comments to the Author(s)
label_comment_1
The current study looks to replicate previous findings that familiar face recognition is more
invariant to changes in the face (i.e., viewpoint) than unfamiliar face recognition, using a
previously untested continuous recognition task that more closely approximates the face
recognition tasks people perform in daily life - with many faces of varied style and (for half of the
participants) with repeat detection on a different image of the same person.
Generally, the methodology and the analyses that bring about the pilot results are sound. The
hypotheses are straightforward and well supported by previous literature. There are some
questions I have and some suggestions of additional measures or analyses you could try that
could provide further insight into the data. I would like to hear how you plan to address these
before fully accepting this preregistration.
Also for full disclosure as promoted by this journal (Royal Society Open Science), I will identify
myself: I am Dr. Wilma Bainbridge, lead author on one of the works you cite which uses this
continuous recognition paradigm to answer different questions about face memory.
Major Comments
- Labeling the general category of the celebrity is likely a very different level of familiarity in
comparison to correctly knowing the full name of the person, yet you group these responses
together. While you can do an analysis grouping them together, I would be interested to see an
additional analysis looking at a more fine-grained scale of familiarity. Would it be possible to get
numbered (e.g., Likert scale) familiarity ratings for each image? Or if not - how do the results pan
out if you separate out people who only know the career type versus the full name?
- Since hair is generally cropped here, and as you acknowledge from previous literature, people
tend to use more external cues for recognizing unfamiliar faces, the stimuli may have a built-in
bias against the unfamiliar faces due to an elimination of their cues from the oval cropping. You
may consider allowing more of the external cues to be shown, or acknowledge this as a potential
confound in the analysis of your results.
-The hit rates for the vigilance repeats and target repeats are surprisingly low. As a comparison,
in the Bainbridge et al. 2013 study, the average hit rate was 51.6% for the target trials, and the
vigilance hit rate was probably even higher, and these are all unfamiliar faces. The fact that your
participants are only getting 58.3% of the same vigilance repeats only a handful of seconds later
leads me to believe that there might be something going on. Specifically, I wonder if your brief
image presentation time could partially be a culprit - perhaps the images are going by so quickly
that participants don't have time to process the images and respond? You currently present them
for 500 ms while Bainbridge et al. 2013 and Isola et al. 2011 present them for 1s each.
- I'm curious - are there more false alarms to familiar faces than unfamiliar faces?
Minor Comments
- It is great to see you explicitly acknowledge the technical issue in the familiarity task and that
you will avoid that moving forward!
3
- There are citations missing in the data checks section (after "consistent with previous
literature").
- "All images for fillers (including the first presentation of a later target) were randomly selected
from the two images created per face" (p.4) -- Just to make sure I understand this correctly, does it
mean that for a given celebrity, it was randomized whether a subject first saw face A of that
celebrity (and depending on condition was then later tested on face A or face B of that person) or
first saw face B? Counterbalancing that may be important (e.g., so that it's not always a forward-
facing first presentation and then a rotated second presentation as in Figure 1).
- This wasn't completely clear in the methods until later, but for the "different" group vigilant
repeats, people were also responding to a different image of the same celebrity, correct?
- Also just as a reference, we have some new work that seems directly relevant to your work (no
need to cite unless if you find this actually useful!):
Bainbridge, 2016. The memorability of people: Intrinsic memorability across transformations of a
person's face. Journal of Experimental Psychology: Learning, Memory, and Cognition.
It similarly does a continuous recognition task with participants split into "same" and "different"
image groups for a face identity, though it only looks at unfamiliar face recognition. While I also
find that people do worse with the different image task for unfamiliar faces, the key finding is
that there is high consistency across people in which faces they do well on (even in the "different"
task) and which they do poorly on.
label_author_2
Review form: Reviewer 2 (David White)
Is the manuscript scientifically sound in its present form?
Are the interpretations and conclusions justified by the results?
Is the language acceptable?
Yes
Is it clear how to access all supporting data?
Do you have any ethical concerns with this paper?
No
Have you any concerns about statistical analyses in this paper?
No
Recommendation?
label_recommendation_2
Accept in principle
Comments to the Author(s)
label_comment_2
The authors propose to examine the extend to which detecting duplicate faces is facilitated by
repetition of the same image, relative to when two different of the same person are repeated.
4
This task is really very interesting. The paradigm is taken from a previous study, but this is a
novel application that promises to address a theoretically important issue in the study of face
recognition -- to what extent are image-level details of images encoded in memory?
My main comment is that the authors do not discuss repetition priming effects. This is an
important topic in the context of the work and the results should be interpreted in the context of
this work (for a useful background summary see Johnston & Barry, 2001). The authors were
probably already intending to do so, but I would expect to see a discussion of that body of work
in a full write up, and so the work should be mentioned in the introduction.
Logic of the rationale, study design and analysis were very well thought out and clearly
described in an impressive level of detail. I have no concerns about the proposed methodology.
The authors also present detailed pilot data and analysis confirming that the method is a valid
test of their hypothesis.
MINOR
Page 2, lines 36 to 48: This passage appears to discuss findings from Burton et al. 2005 but doesn't
cite.
Page 4, line 28: Was internet search actually random? Or highest page rank images on Google?
REFERENCES
Johnston, R. A., & Barry, C. (2001). Best face forward: similarity effects in repetition priming of
face recognition. The Quarterly Journal of Experimental Psychology: Section A, 54(2), 383-396.
Burton, A. M., Jenkins, R., Hancock, P. J., & White, D. (2005). Robust representations for face
recognition: The power of averages. Cognitive psychology, 51(3), 256-284.
=============
David White
UNSW Sydney
label_author_3
Review form: Reviewer 3
Is the manuscript scientifically sound in its present form?
Are the interpretations and conclusions justified by the results?
Is the language acceptable?
Yes
Is it clear how to access all supporting data?
Do you have any ethical concerns with this paper?
No
5
Have you any concerns about statistical analyses in this paper?
Yes
Recommendation?
label_recommendation_3
Major revision
Comments to the Author(s)
label_comment_3
I found this manuscript largely well written and a relatively novel way to address a core and
significant question in face recognition. The method is relatively neat and a good addition to the
literature (following some modifications). The results of the pilot are as anticipated. I have a few
suggestions for how to improve the introduction to sell the core novelty more. My most major
concerns are methodological and suggest major changes to the methods for the final report.
Introduction
A more clear definition of familiarity is required at the very start - some may consider unfamiliar
faces are ones not encountered before, therefore they cannot be recognised. In fact, by definition,
when they are recognised, they are familiar - therefore, I suggest being clear in this definition
from the start of the introduction all the way through (Pre-experimental unfamiliar faces?).
Likewise, familiar faces can be personally familiar, experimentally-induced, or famous and there
are clear dissociations between these types. Given that this task compares famous faces that the
person knows something about with famous faces that they don't (which might still have some
residual familiarity).
Page 2 - paragraph two, include more citations for these comments
Page 2 - paragraph 3, the authors say "familiar face recognition should be more robust" - robust in
what way? Recognition abilities remain despite changes in?
Page 3 - paragraph 2, I don't think the paradigm that is presented better reflects face recognition
in a natural setting: how can viewing a stream of faces for an hour reflect natural face
recognition? I'm not criticising the paradigm, just the justification. Remove all phrases indicating
that the procedure better reflects face recognition in a natural setting. Indeed, the justification for
this procedure is not well thought out (not using personally-familiar faces because of the
methodological issues is too vague, for example - what this implies is that the use of well-
matched personally familiar faces is too time-consuming rather than an issue). I suggest either
being honest about the justification or removing it. The task itself is an interesting addition to the
literature, and this could be expanded upon.
Page 3 - reference to figure 1: explain and justify the reference to two types of repeats indicated in
the figure and make predictions for this. Or, do not include Figure 1 until the method section.
For the Pilot Methods, I have a number of critical concerns. These will need to be fixed for the
registered report to be accepted. Some of the methodological descriptions require more detail in
order to replicate the study.
Participants - provide details of age, gender, and ethnicity. The study uses a between-subjects
design over the most critical variable. This should be a repeated measures variable. I realise this
increases the number of trials, but will reduce the number of participants required.
Materials - provide details of age, gender, and ethnicity to show that these match to the
participants given the own-group biases. Were the images of the celebrities taken from when they
were most famous according to the participants (i.e., Harrison Ford is most famous as a 30 year
old to many participants currently over the age of 30, but is most famous as a 60+ year old to
many participants under the age of 30 due to films). All of the stimulus details need to be
provided. The justification of needing to recognise the same people over different contexts is only
partially supported: in natural circumstances, we do not need to recognise faces of someone who
is younger than when we last saw them (except in rare cases). Is this taken into consideration (i.e.,
6
are the two images of each celebrity very different or were they taken around a similar sort of
time?)
The cropping of the images is not an appropriate technique when using celebrity images as the
hair is a vital feature for recognition especially for famous faces.
Procedure - The familiarity task is insufficient for measuring familiarity of the faces. The
familiarity task should be done for all faces to ensure that the target faces were not more familiar
than the non-target faces (indeed, the target faces should be counterbalanced so all are used). I
would also recommend collecting familiarity ratings to statistically show the targets are not more
familiar than non-targets.
A design section would help as it would clarify how the authors have defined what is a familiar
face and what isn't (i.e., it is done on the basis of participant ratings after the experiment).
The exclusion of participants who were unfamiliar with the famous faces is very lax. If
participants were familiar with less than 10% of the faces, this means that for most of the faces in
the trials, they were unfamiliar. In which case, this is really a task measuring some sort of pop out
effect (the rare instances of familiar faces). Based on this, I would suggest either excluding when
participants show familiarity of less than 30%, and/or analysing accuracy across faces of different
levels of familiarity.
Results
Page 6 - line 29, Replace (citations) with the citations.
Given the importance of a non-significant t-test result (the fact that familiar faces are recognised
equally well in both same and different images, Bayesian statistics should be performed to show
how much confidence we should have in the null results.
Given my suggestion to collect familiarity ratings for all faces, I would suggest running
correlations between familiarity ratings and detection accuracy for same image and different
image. A comparison of the correlation coefficients would then highlight whether familiar faces
how more of a robust representation.
There is also a large amount of potential analyses that could be run on these data, including
reaction time analyses, analyses across blocks in the experiment. I would recommend that the
authors consider potential further analyses that they may want to run.
label_author_4
Review form: Reviewer 4
Is the manuscript scientifically sound in its present form?
Are the interpretations and conclusions justified by the results?
Is the language acceptable?
Yes
Is it clear how to access all supporting data?
Do you have any ethical concerns with this paper?
No
Have you any concerns about statistical analyses in this paper?
No
7
Recommendation?
label_recommendation_4
Major revision
Comments to the Author(s)
label_comment_4
MAJOR
- Overall, I think this is a well-designed study, with clear methodology and an analysis pipeline
which would allow replication. However, I do not think it is framed in the best context.
Specifically, several previous studies have consistently shown that familiar faces are better
recognized across image changes as compared to unfamiliar faces. The authors say that these
studies are limited due to the short experiment time and small number of stimuli used, and that
“we don’t really know whether familiar face recognition remains robust to image changes under
more demanding experimental conditions that better reflect face recognition in a natural setting”.
Given that consistent effects have been observed across multiple studies with few stimuli, this
already suggests that these effects of familiarity are robust; therefore, from this perspective, I
cannot see how this study would contribute anything further to our current understanding of
participants abilities to recognize familiar and unfamiliar faces across changes in viewpoint.
Instead, the unique aspect of this study is very much the memory load that the participants will
encounter. The study needs to be better framed in the context of a face memory study with a
perceptual component, rather than a perceptual task alone – in its’ current form the novelty (of
the heavy memory load) is lost. I suggest that the authors reframe this study, which in turn will
strengthen the importance of the research question and their rationale for doing the study.
- Given the variability in performance between the groups (indeed due to the nature of the
study), in addition to the % hits, it would be useful to examine (and view) the results as
d’prime’s. I suspect the results will still hold, but if not then this would require some re-
interpretation of the results.
MINOR
- For the familiarity check – are the same images used as in the experiment?
- Unfortunately, due to the nature of the design, the authors cannot determine how the size of the
gap between presentations is affected by the ability to identify faces across images. I’m not
suggesting you do this here, but this might be really interesting to look at in a future study using
your design.
- As a side-note, I’m not convinced that this study examines face recognition in a “natural setting”
– for example, if one wanted to examine more natural face recognition, then wouldn’t dynamic
faces be more appropriate?
label_end_comment
Decision letter (RSOS-170250.R0)
26-Apr-2017
Dear Mr Chapman,
The Editors assigned to your Stage 1 Registered Report ("How robust is familiar face recognition?
A repeat detection study of >1,000 photographs") have now received comments from reviewers.
We would like you to revise your paper in accordance with the referee and editors suggestions
which can be found below (not including confidential reports to the Editor). Please note this
decision does not guarantee eventual acceptance.
Please submit a copy of your revised paper within three weeks (i.e. by the 18-May-2017). If
8
deemed necessary by the Editors, your manuscript will be sent back to one or more of the original
reviewers for assessment. If the original reviewers are not available we may invite new reviewers.
To revise your manuscript, log into http://mc.manuscriptcentral.com/rsos and enter your
Author Centre, where you will find your manuscript title listed under "Manuscripts with
Decisions." Under "Actions," click on "Create a Revision." Your manuscript number has been
appended to denote a revision. Revise your manuscript and upload a new version through your
Author Centre.
When submitting your revised manuscript, you must respond to the comments made by the
referees and upload a file "Response to Referees" in "Section 6 - File Upload". Please use this to
document how you have responded to the comments, and the adjustments you have made. In
order to expedite the processing of the revised manuscript, please be as specific as possible in
your response.
Once again, thank you for submitting your manuscript to Royal Society Open Science and I look
forward to receiving your revision. If you have any questions at all, please do not hesitate to get
in touch.
Kind regards,
Alice Power
Editorial Coordinator
Royal Society Open Science
on behalf of Chris Chambers
Registered Reports Editor, Royal Society Open Science
openscience@royalsociety.org
Associate Editor Comments to Author:
Comments to the Author:
Four expert reviewers have now assessed the submission. All are broadly positive but offer a
wide range of suggestions for revision, spanning the full breadth of the Stage 1 review criteria,
from clarifying and strengthening the study rationale to broader inclusion of dependent
measures and statistical approaches (e.g. Bayesian hypothesis testing, as suggested by Reviewer
3). Concerning one comment by Reviewer 3: "I would recommend that the authors consider
potential further analyses that they may want to run", please note that to avoid blurring the line
beween confirmatory pre-registered analyses and post hoc exploratory analyses, any exploratory
analyses that do not serve an a priori hypothesis or other pre-specified purpose should not be
included in Stage 1 manuscripts but should instead be reported at Stage 2 in an "Exploratory
Analyses" section of the Results. In responding to this reviewer's comment you are, however,
welcome to discuss some of the broader plans you may have for considering extra analyses.
In addition to addressing the comments of the reviewers, please confirm in the manuscript what
the minimum analysed sample size will be in the event that the participant exclusion rate in the
pre-registered experiment turns out higher than expected. Ideally this should be the figure
indicated in the power analysis (N=124), thus guaranteeing to replace excluded participants until
the stopping rule is triggered. If so, your protocol could be altered to simply state that
participants will tested until N=124 non-excluded participants are reached, as opposed to pre-
specifying an estimated total sample of N=142.
9
Comments to Author:
Reviewer: 1
Comments to the Author(s)
The current study looks to replicate previous findings that familiar face recognition is more
invariant to changes in the face (i.e., viewpoint) than unfamiliar face recognition, using a
previously untested continuous recognition task that more closely approximates the face
recognition tasks people perform in daily life - with many faces of varied style and (for half of the
participants) with repeat detection on a different image of the same person.
Generally, the methodology and the analyses that bring about the pilot results are sound. The
hypotheses are straightforward and well supported by previous literature. There are some
questions I have and some suggestions of additional measures or analyses you could try that
could provide further insight into the data. I would like to hear how you plan to address these
before fully accepting this preregistration.
Also for full disclosure as promoted by this journal (Royal Society Open Science), I will identify
myself: I am Dr. Wilma Bainbridge, lead author on one of the works you cite which uses this
continuous recognition paradigm to answer different questions about face memory.
Major Comments
- Labeling the general category of the celebrity is likely a very different level of familiarity in
comparison to correctly knowing the full name of the person, yet you group these responses
together. While you can do an analysis grouping them together, I would be interested to see an
additional analysis looking at a more fine-grained scale of familiarity. Would it be possible to get
numbered (e.g., Likert scale) familiarity ratings for each image? Or if not - how do the results pan
out if you separate out people who only know the career type versus the full name?
- Since hair is generally cropped here, and as you acknowledge from previous literature, people
tend to use more external cues for recognizing unfamiliar faces, the stimuli may have a built-in
bias against the unfamiliar faces due to an elimination of their cues from the oval cropping. You
may consider allowing more of the external cues to be shown, or acknowledge this as a potential
confound in the analysis of your results.
-The hit rates for the vigilance repeats and target repeats are surprisingly low. As a comparison,
in the Bainbridge et al. 2013 study, the average hit rate was 51.6% for the target trials, and the
vigilance hit rate was probably even higher, and these are all unfamiliar faces. The fact that your
participants are only getting 58.3% of the same vigilance repeats only a handful of seconds later
leads me to believe that there might be something going on. Specifically, I wonder if your brief
image presentation time could partially be a culprit - perhaps the images are going by so quickly
that participants don't have time to process the images and respond? You currently present them
for 500 ms while Bainbridge et al. 2013 and Isola et al. 2011 present them for 1s each.
- I'm curious - are there more false alarms to familiar faces than unfamiliar faces?
Minor Comments
- It is great to see you explicitly acknowledge the technical issue in the familiarity task and that
you will avoid that moving forward!
10
- There are citations missing in the data checks section (after "consistent with previous
literature").
- "All images for fillers (including the first presentation of a later target) were randomly selected
from the two images created per face" (p.4) -- Just to make sure I understand this correctly, does it
mean that for a given celebrity, it was randomized whether a subject first saw face A of that
celebrity (and depending on condition was then later tested on face A or face B of that person) or
first saw face B? Counterbalancing that may be important (e.g., so that it's not always a forward-
facing first presentation and then a rotated second presentation as in Figure 1).
- This wasn't completely clear in the methods until later, but for the "different" group vigilant
repeats, people were also responding to a different image of the same celebrity, correct?
- Also just as a reference, we have some new work that seems directly relevant to your work (no
need to cite unless if you find this actually useful!):
Bainbridge, 2016. The memorability of people: Intrinsic memorability across transformations of a
person's face. Journal of Experimental Psychology: Learning, Memory, and Cognition.
It similarly does a continuous recognition task with participants split into "same" and "different"
image groups for a face identity, though it only looks at unfamiliar face recognition. While I also
find that people do worse with the different image task for unfamiliar faces, the key finding is
that there is high consistency across people in which faces they do well on (even in the "different"
task) and which they do poorly on.
Reviewer: 2
Comments to the Author(s)
The authors propose to examine the extend to which detecting duplicate faces is facilitated by
repetition of the same image, relative to when two different of the same person are repeated.
This task is really very interesting. The paradigm is taken from a previous study, but this is a
novel application that promises to address a theoretically important issue in the study of face
recognition -- to what extent are image-level details of images encoded in memory?
My main comment is that the authors do not discuss repetition priming effects. This is an
important topic in the context of the work and the results should be interpreted in the context of
this work (for a useful background summary see Johnston & Barry, 2001). The authors were
probably already intending to do so, but I would expect to see a discussion of that body of work
in a full write up, and so the work should be mentioned in the introduction.
Logic of the rationale, study design and analysis were very well thought out and clearly
described in an impressive level of detail. I have no concerns about the proposed methodology.
The authors also present detailed pilot data and analysis confirming that the method is a valid
test of their hypothesis.
MINOR
Page 2, lines 36 to 48: This passage appears to discuss findings from Burton et al. 2005 but doesn't
cite.
Page 4, line 28: Was internet search actually random? Or highest page rank images on Google?
11
REFERENCES
Johnston, R. A., & Barry, C. (2001). Best face forward: similarity effects in repetition priming of
face recognition. The Quarterly Journal of Experimental Psychology: Section A, 54(2), 383-396.
Burton, A. M., Jenkins, R., Hancock, P. J., & White, D. (2005). Robust representations for face
recognition: The power of averages. Cognitive psychology, 51(3), 256-284.
=============
David White
UNSW Sydney
Reviewer: 3
Comments to the Author(s)
I found this manuscript largely well written and a relatively novel way to address a core and
significant question in face recognition. The method is relatively neat and a good addition to the
literature (following some modifications). The results of the pilot are as anticipated. I have a few
suggestions for how to improve the introduction to sell the core novelty more. My most major
concerns are methodological and suggest major changes to the methods for the final report.
Introduction
A more clear definition of familiarity is required at the very start - some may consider unfamiliar
faces are ones not encountered before, therefore they cannot be recognised. In fact, by definition,
when they are recognised, they are familiar - therefore, I suggest being clear in this definition
from the start of the introduction all the way through (Pre-experimental unfamiliar faces?).
Likewise, familiar faces can be personally familiar, experimentally-induced, or famous and there
are clear dissociations between these types. Given that this task compares famous faces that the
person knows something about with famous faces that they don't (which might still have some
residual familiarity).
Page 2 - paragraph two, include more citations for these comments
Page 2 - paragraph 3, the authors say "familiar face recognition should be more robust" - robust in
what way? Recognition abilities remain despite changes in?
Page 3 - paragraph 2, I don't think the paradigm that is presented better reflects face recognition
in a natural setting: how can viewing a stream of faces for an hour reflect natural face
recognition? I'm not criticising the paradigm, just the justification. Remove all phrases indicating
that the procedure better reflects face recognition in a natural setting. Indeed, the justification for
this procedure is not well thought out (not using personally-familiar faces because of the
methodological issues is too vague, for example - what this implies is that the use of well-
matched personally familiar faces is too time-consuming rather than an issue). I suggest either
being honest about the justification or removing it. The task itself is an interesting addition to the
literature, and this could be expanded upon.
Page 3 - reference to figure 1: explain and justify the reference to two types of repeats indicated in
the figure and make predictions for this. Or, do not include Figure 1 until the method section.
For the Pilot Methods, I have a number of critical concerns. These will need to be fixed for the
registered report to be accepted. Some of the methodological descriptions require more detail in
order to replicate the study.
Participants - provide details of age, gender, and ethnicity. The study uses a between-subjects
design over the most critical variable. This should be a repeated measures variable. I realise this
increases the number of trials, but will reduce the number of participants required.
Materials - provide details of age, gender, and ethnicity to show that these match to the
participants given the own-group biases. Were the images of the celebrities taken from when they
12
were most famous according to the participants (i.e., Harrison Ford is most famous as a 30 year
old to many participants currently over the age of 30, but is most famous as a 60+ year old to
many participants under the age of 30 due to films). All of the stimulus details need to be
provided. The justification of needing to recognise the same people over different contexts is only
partially supported: in natural circumstances, we do not need to recognise faces of someone who
is younger than when we last saw them (except in rare cases). Is this taken into consideration (i.e.,
are the two images of each celebrity very different or were they taken around a similar sort of
time?)
The cropping of the images is not an appropriate technique when using celebrity images as the
hair is a vital feature for recognition especially for famous faces.
Procedure - The familiarity task is insufficient for measuring familiarity of the faces. The
familiarity task should be done for all faces to ensure that the target faces were not more familiar
than the non-target faces (indeed, the target faces should be counterbalanced so all are used). I
would also recommend collecting familiarity ratings to statistically show the targets are not more
familiar than non-targets.
A design section would help as it would clarify how the authors have defined what is a familiar
face and what isn't (i.e., it is done on the basis of participant ratings after the experiment).
The exclusion of participants who were unfamiliar with the famous faces is very lax. If
participants were familiar with less than 10% of the faces, this means that for most of the faces in
the trials, they were unfamiliar. In which case, this is really a task measuring some sort of pop out
effect (the rare instances of familiar faces). Based on this, I would suggest either excluding when
participants show familiarity of less than 30%, and/or analysing accuracy across faces of different
levels of familiarity.
Results
Page 6 - line 29, Replace (citations) with the citations.
Given the importance of a non-significant t-test result (the fact that familiar faces are recognised
equally well in both same and different images, Bayesian statistics should be performed to show
how much confidence we should have in the null results.
Given my suggestion to collect familiarity ratings for all faces, I would suggest running
correlations between familiarity ratings and detection accuracy for same image and different
image. A comparison of the correlation coefficients would then highlight whether familiar faces
how more of a robust representation.
There is also a large amount of potential analyses that could be run on these data, including
reaction time analyses, analyses across blocks in the experiment. I would recommend that the
authors consider potential further analyses that they may want to run.
Reviewer: 4
Comments to the Author(s)
MAJOR
- Overall, I think this is a well-designed study, with clear methodology and an analysis pipeline
which would allow replication. However, I do not think it is framed in the best context.
Specifically, several previous studies have consistently shown that familiar faces are better
recognized across image changes as compared to unfamiliar faces. The authors say that these
studies are limited due to the short experiment time and small number of stimuli used, and that
“we don’t really know whether familiar face recognition remains robust to image changes under
more demanding experimental conditions that better reflect face recognition in a natural setting”.
Given that consistent effects have been observed across multiple studies with few stimuli, this
already suggests that these effects of familiarity are robust; therefore, from this perspective, I
cannot see how this study would contribute anything further to our current understanding of
participants abilities to recognize familiar and unfamiliar faces across changes in viewpoint.
Instead, the unique aspect of this study is very much the memory load that the participants will
13
encounter. The study needs to be better framed in the context of a face memory study with a
perceptual component, rather than a perceptual task alone – in its’ current form the novelty (of
the heavy memory load) is lost. I suggest that the authors reframe this study, which in turn will
strengthen the importance of the research question and their rationale for doing the study.
- Given the variability in performance between the groups (indeed due to the nature of the
study), in addition to the % hits, it would be useful to examine (and view) the results as
d’prime’s. I suspect the results will still hold, but if not then this would require some re-
interpretation of the results.
MINOR
- For the familiarity check – are the same images used as in the experiment?
- Unfortunately, due to the nature of the design, the authors cannot determine how the size of the
gap between presentations is affected by the ability to identify faces across images. I’m not
suggesting you do this here, but this might be really interesting to look at in a future study using
your design.
- As a side-note, I’m not convinced that this study examines face recognition in a “natural setting”
– for example, if one wanted to examine more natural face recognition, then wouldn’t dynamic
faces be more appropriate?
Author's Response to Decision Letter for (RSOS-170250.R0)
See Appendix A.
label_version_2
RSOS-170530.R0 (Revision)
label_author_5
Review form: Reviewer 1 (Wilma Bainbridge)
Is the manuscript scientifically sound in its present form?
Are the interpretations and conclusions justified by the results?
Is the language acceptable?
Yes
Is it clear how to access all supporting data?
Do you have any ethical concerns with this paper?
No
Have you any concerns about statistical analyses in this paper?
No
14
Recommendation?
label_recommendation_5
Accept in principle
Comments to the Author(s)
label_comment_5
The authors have done a good job addressing all of my comments.
label_author_6
Review form: Reviewer 3
Is the manuscript scientifically sound in its present form?
Are the interpretations and conclusions justified by the results?
Is the language acceptable?
Yes
Is it clear how to access all supporting data?
Do you have any ethical concerns with this paper?
No
Have you any concerns about statistical analyses in this paper?
Yes
Recommendation?
label_recommendation_6
Accept with minor revision
Comments to the Author(s)
label_comment_6
I appreciate the authors have gone into a great deal of effort in an attempt to improve the
manuscript and I do believe it is much improved. However, I believe there are areas that still
warrant improvement.
I do believe that the lack of meta-data about the images is a concern (point 11). I realise that it is
very difficult to collate this information after collection. I recommend discussing this fully and
highlighting this as a potential issue for future studies.
Regarding point 13 - I think that it will be worth collecting more extensive familiarity ratings. I
think the three-point scale for measuring familiarity may be insufficient for more elaborate and
interesting analyses. Potentially a seven-point Likert-type familiarity rating for each face would
allow for further analyses. This follows on from Reviewer 1's second comment and I would
strongly encourage such an analysis. Such an analysis would mean that there would be less need
to exclude participants.
While the authors have carefully chosen the target faces, I wonder if there is potential for a
stimulus-as-fixed-fallacy effect. Would the effects replicate with other faces? I realise the
matching of the demographics is important. This might be worth a comment for future analyses.
Page 14 - The authors claim that they "perform a familiarity check at the end of their experiment"
of their stimuli set - this is slightly misleading as the familiarity rating is only on the limited
number of target faces not on the entire 1000 faces (which would be better, but the authors point
out would not be the most practical method).
15
I commend the authors for engaging with the Bayesian statistics, but suggest incorporating them
into every analysis (including the data checks and the F ratios). The use of Bayesian statistics also
revealed a mistake in the interpretation of the results. On page 18, the authors claim "familiar
faces were detected equally well regardless of image" but the Bayes factor is 0.44. This means
there is barely anecdotal evidence for the null hypothesis. Typically, we would consider evidence
for the null hypothesis only to be present if the Bayes Factor was less than 0.33 and then this is
only weak evidence (Dienes, 2014). This sentence needs to be rewritten. The same issue is present
on page 19. Similarly, on page 19, the Bayes factor for the significant result was 1.66 which is
barely anecdotal evidence for the hypothesis - these results are in no way convincing. In other
words, the study is too insensitive to actually provide evidence. Given the importance of this
result to the conclusions of the paper, I think that the conclusions will need to be toned down
considerably and more participants will need to be run than is evidenced by the power analysis.
label_author_7
Review form: Reviewer 4
Is the manuscript scientifically sound in its present form?
Are the interpretations and conclusions justified by the results?
Is the language acceptable?
Yes
Is it clear how to access all supporting data?
Do you have any ethical concerns with this paper?
No
Have you any concerns about statistical analyses in this paper?
Yes
Recommendation?
label_recommendation_7
Accept with minor revision
Comments to the Author(s)
label_comment_7
The authors have addressed the majority of my concerns. However, below are 2 unresolved
points - one which is minor and would be fine to be addressed in the final paper rather than
immediately (point 4), and the other which requires the authors to record participant responses
and analyze the data differently in order for the results to be interpreted appropriately (point 5).
If the authors are willing to address this last point, I would be happy to accept this paper (in
principle) for publication upon completion. Below are the points in the context of the previous
comments and replies (new comments indicated by **).
Point 4.
Reviewer: ...Instead, the unique aspect of this study is very much the memory load that the
participants will encounter. The study needs to be better framed in the context of a face memory
study with a perceptual component, rather than a perceptual task alone – in its’ current form the
novelty (of the heavy memory load) is lost. I suggest that the authors reframe this study, which in
16
turn will strengthen the importance of the research question and their rationale for doing the
study.
Authors: As noted in our response to point #6 of Reviewer #3, we now describe our task as
having “demanding experimental conditions with significant memory load.” We hope this
adequately acknowledges the novelty of our task in this respect, while retaining the points about
other advantages (number of stimuli, experiment duration).
**Reviewer: I agree that this study would make a nice addition to the literature. To clarify, I think
that the benefit of this study can be strengthened if the authors discuss face working memory,
and how matching faces over longer time durations (rather than simultaneous or sequential) has
not been previously tested. I’m happy for this to be passed at this stage, but the final paper will
benefit from including references to relevant studies on face/object working memory. For
example, Jackson & Raymond, 2008, JEP; or Brady, Konkle, Alvarez, & Oliva (2008) which
presented participants with a long series of images and tested participants ability to remember
the images.
Point 5.
Reviewer: Given the variability in performance between the groups (indeed due to the nature of
the study), in addition to the % hits, it would be useful to examine (and view) the results as
d’prime’s. I suspect the results will still hold, but if not then this would require some
reinterpretation of the results.
Authors: As we noted in a response to point #5 of Reviewer #1, familiarity ratings were not
collected for filler trials. Because of this, we cannot calculate false alarms or d’ for familiar vs.
unfamiliar images.
**Reviewer: This is fine. However, for the final study, it is important to consider d’primes due to
possible participant response biases. This is important when comparing, not only across
conditions, but also across participants – particularly when one condition is more difficult than
another. If participants in one group have a different response bias to another group (i.e. due to
task difficulty), then this will produce differences when just examining the hit rates. For example,
using this design, participants can press the space bar throughout the entirety of the experiment,
and the results are recorded as 100%. In instances where this is a possibility, it is important to
consider participants' response biases - in this case, false alarms to the unfamiliar faces. I
therefore strongly urge the authors to collect this data, and analyze the data using d'primes,
alongside percent hits. In this circumstance, there is no benefit to analyzing only the hit rates
when you collect your real data, but rather, the hit rates should be observed in the context of false
alarms for the results to be appropriately interpreted across conditions/participant groups.
label_end_comment
Decision letter (RSOS-170530.R0)
01-Jun-2017
Dear Mr Chapman
On behalf of the Editors, I am pleased to inform you that your Manuscript RSOS-170530 entitled
"How robust is familiar face recognition? A repeat detection study of >1,000 photographs" has
been accepted in principle for publication in Royal Society Open Science subject to minor revision
17
in accordance with the referee and editor suggestions. Please find their comments at the end of
this email.
The reviewers and handling editors have recommended publication, but also suggest some minor
revisions to your manuscript. Therefore, I invite you to respond to the comments and revise your
manuscript.
Please you submit the revised version of your manuscript within 7 days (i.e. by the 09-Jun-2017).
If you do not think you will be able to meet this date please let me know immediately.
To revise your manuscript, log into https://mc.manuscriptcentral.com/rsos and enter your
Author Centre, where you will find your manuscript title listed under "Manuscripts with
Decisions". Under "Actions," click on "Create a Revision." You will be unable to make your
revisions on the originally submitted version of the manuscript. Instead, revise your manuscript
and upload a new version through your Author Centre.
When submitting your revised manuscript, you will be able to respond to the comments made by
the referees and upload a file "Response to Referees" in "Section 6 - File Upload". You can use this
to document any changes you make to the original manuscript. In order to expedite the
processing of the revised manuscript, please be as specific as possible in your response to the
referees.
Full author guidelines can be found here
http://rsos.royalsocietypublishing.org/content/registered-reports.
Once again, thank you for submitting your manuscript to Royal Society Open Science and I look
forward to receiving your revision. If you have any questions at all, please do not hesitate to get
in touch.
Best wishes
Alice Power
Royal Society Open Science
on behalf of Chris Chambers
Subject Editor, Royal Society Open Science
openscience@royalsociety.org
Associate Editor Comments to Author:
The manuscript was returned to three of the original reviewers, whose assessments are generally
positive. However, a number of specific issues remain to be addressed. The two most salient
points are clarifying (and potentially extending) the application of Bayesian statistical methods,
and including a calculation of d-prime in the pre-registered experiment to disambiguate
perceptual sensitivity from response bias. This latter point, especially, is crucial. If the authors are
able to address these points to the editors' satisfication in their next revision, then provisional
acceptance will awarded without further review.
18
Reviewer comments to Author:
Reviewer: 3
Comments to the Author(s)
I appreciate the authors have gone into a great deal of effort in an attempt to improve the
manuscript and I do believe it is much improved. However, I believe there are areas that still
warrant improvement.
I do believe that the lack of meta-data about the images is a concern (point 11). I realise that it is
very difficult to collate this information after collection. I recommend discussing this fully and
highlighting this as a potential issue for future studies.
Regarding point 13 - I think that it will be worth collecting more extensive familiarity ratings. I
think the three-point scale for measuring familiarity may be insufficient for more elaborate and
interesting analyses. Potentially a seven-point Likert-type familiarity rating for each face would
allow for further analyses. This follows on from Reviewer 1's second comment and I would
strongly encourage such an analysis. Such an analysis would mean that there would be less need
to exclude participants.
While the authors have carefully chosen the target faces, I wonder if there is potential for a
stimulus-as-fixed-fallacy effect. Would the effects replicate with other faces? I realise the
matching of the demographics is important. This might be worth a comment for future analyses.
Page 14 - The authors claim that they "perform a familiarity check at the end of their experiment"
of their stimuli set - this is slightly misleading as the familiarity rating is only on the limited
number of target faces not on the entire 1000 faces (which would be better, but the authors point
out would not be the most practical method).
I commend the authors for engaging with the Bayesian statistics, but suggest incorporating them
into every analysis (including the data checks and the F ratios). The use of Bayesian statistics also
revealed a mistake in the interpretation of the results. On page 18, the authors claim "familiar
faces were detected equally well regardless of image" but the Bayes factor is 0.44. This means
there is barely anecdotal evidence for the null hypothesis. Typically, we would consider evidence
for the null hypothesis only to be present if the Bayes Factor was less than 0.33 and then this is
only weak evidence (Dienes, 2014). This sentence needs to be rewritten. The same issue is present
on page 19. Similarly, on page 19, the Bayes factor for the significant result was 1.66 which is
barely anecdotal evidence for the hypothesis - these results are in no way convincing. In other
words, the study is too insensitive to actually provide evidence. Given the importance of this
result to the conclusions of the paper, I think that the conclusions will need to be toned down
considerably and more participants will need to be run than is evidenced by the power analysis.
Reviewer: 4
Comments to the Author(s)
The authors have addressed the majority of my concerns. However, below are 2 unresolved
points - one which is minor and would be fine to be addressed in the final paper rather than
immediately (point 4), and the other which requires the authors to record participant responses
and analyze the data differently in order for the results to be interpreted appropriately (point 5).
If the authors are willing to address this last point, I would be happy to accept this paper (in
principle) for publication upon completion. Below are the points in the context of the previous
comments and replies (new comments indicated by **).
Point 4.
Reviewer: ...Instead, the unique aspect of this study is very much the memory load that the
participants will encounter. The study needs to be better framed in the context of a face memory
study with a perceptual component, rather than a perceptual task alone – in its’ current form the
novelty (of the heavy memory load) is lost. I suggest that the authors reframe this study, which in
19
turn will strengthen the importance of the research question and their rationale for doing the
study.
Authors: As noted in our response to point #6 of Reviewer #3, we now describe our task as
having “demanding experimental conditions with significant memory load.” We hope this
adequately acknowledges the novelty of our task in this respect, while retaining the points about
other advantages (number of stimuli, experiment duration).
**Reviewer: I agree that this study would make a nice addition to the literature. To clarify, I think
that the benefit of this study can be strengthened if the authors discuss face working memory,
and how matching faces over longer time durations (rather than simultaneous or sequential) has
not been previously tested. I’m happy for this to be passed at this stage, but the final paper will
benefit from including references to relevant studies on face/object working memory. For
example, Jackson & Raymond, 2008, JEP; or Brady, Konkle, Alvarez, & Oliva (2008) which
presented participants with a long series of images and tested participants ability to remember
the images.
Point 5.
Reviewer: Given the variability in performance between the groups (indeed due to the nature of
the study), in addition to the % hits, it would be useful to examine (and view) the results as
d’prime’s. I suspect the results will still hold, but if not then this would require some
reinterpretation of the results.
Authors: As we noted in a response to point #5 of Reviewer #1, familiarity ratings were not
collected for filler trials. Because of this, we cannot calculate false alarms or d’ for familiar vs.
unfamiliar images.
**Reviewer: This is fine. However, for the final study, it is important to consider d’primes due to
possible participant response biases. This is important when comparing, not only across
conditions, but also across participants – particularly when one condition is more difficult than
another. If participants in one group have a different response bias to another group (i.e. due to
task difficulty), then this will produce differences when just examining the hit rates. For example,
using this design, participants can press the space bar throughout the entirety of the experiment,
and the results are recorded as 100%. In instances where this is a possibility, it is important to
consider participants' response biases - in this case, false alarms to the unfamiliar faces. I
therefore strongly urge the authors to collect this data, and analyze the data using d'primes,
alongside percent hits. In this circumstance, there is no benefit to analyzing only the hit rates
when you collect your real data, but rather, the hit rates should be observed in the context of false
alarms for the results to be appropriately interpreted across conditions/participant groups.
Reviewer: 1
Comments to the Author(s)
The authors have done a good job addressing all of my comments.
Author's Response to Decision Letter for (RSOS-170530.R0)
See Appendix B.
20
label_end_comment
Decision letter (RSOS-170634.R0)
09-Jun-2017
Dear Mr Chapman
On behalf of the Editor, I am pleased to inform you that your Manuscript RSOS-170634 entitled
"How robust is familiar face recognition? A repeat detection study of >1,000 photographs" has
been accepted in principle for publication in Royal Society Open Science.
You may now progress to Stage 2 and complete the study as approved. We would be grateful if
you could now update the journal office as to the anticipated completion date of your study.
Following completion of your study, we invite you to resubmit your paper for peer review as a
Stage 2 Registered Report. Please note that your manuscript can still be rejected for publication at
Stage 2 if the Editors consider any of the following conditions to be met:
• The results were unable to test the authors’ proposed hypotheses by failing to meet the
approved outcome-neutral criteria
• The authors altered the Introduction, rationale, or hypotheses, as approved in the Stage 1
submission
• The authors failed to adhere closely to the registered experimental procedures
• Any post-hoc (unregistered) analyses were either unjustified, insufficiently caveated, or overly
dominant in shaping the authors’ conclusions
• The authors’ conclusions were not justified given the data obtained
We encourage you to read the complete guidelines for authors concerning Stage 2 submissions at
http://rsos.royalsocietypublishing.org/content/registered-reports. Please especially note the
requirements for data sharing and that withdrawing your manuscript will result in publication of
a Withdrawn Registration.
Once again, thank you for submitting your manuscript to Royal Society Open Science and I look
forward to receiving your stage 2 submission. If you have any questions at all, please do not
hesitate to get in touch. We look forward to hearing from you shortly with the anticipated
submission date for your stage two manuscript.
Kind regards,
Alice Power
Royal Society Open Science
on behalf of Chris Chambers
Registered Reports Editor, Royal Society Open Science
openscience@royalsociety.org
Author's Response to Decision Letter for (RSOS-170634.R0)
We have completed the experiment under the registered protocol that was decided on following
Stage 1 review. We confirm that no data other than our pilot data included at Stage 1 was
collected prior to the date of the in-principle acceptance.
21
label_version_3
RSOS-170634.R1 (Revision)
label_author_8
Review form: Reviewer 1 (Wilma Bainbridge)
Is the manuscript scientifically sound in its present form?
Are the interpretations and conclusions justified by the results?
Is the language acceptable?
Yes
Is it clear how to access all supporting data?
Do you have any ethical concerns with this paper?
No
Have you any concerns about statistical analyses in this paper?
No
Recommendation?
label_recommendation_8
Accept with minor revision
Comments to the Author(s)
label_comment_8
I find it incredibly interesting that you were ultimately not able to replicate the finding in your
pilot study and suggested by prior work, that while repetition detection performance across
different image types for unfamiliar faces should be lower than for same images, this difference
should not show for familiar faces. Pre-registering the study (combined with your power
analyses, large sample size, and Bayesian statistics) make this null finding particularly
compelling, and I believe this result to be of importance to the face memory community. This sort
of result is a great example of the importance of pre-registration, as it encourages researchers to
report null results and gives us a better look into phenomena previously thought to be incredibly
robust.
I have a few comments on items I'd like clarification on, or to see edits for.
- When doing the analyses using the 7-point rating scale of familiarity, if I understand correctly
(the grammar is a bit confusing in that sentence), you include both self-reported and coder-scored
familiarity in the binomial mixed-effects model. You then seem to describe it as coder-scored
familiarity fully accounting for the effects ultimately found in the model, and that self-reported
familiarity has no relation to ultimate recognition. Does a correlation between self-reported
familiarity and recognition rate come out as non-significant as well (i.e., I'd like to see a statistic
specifically supporting the claim in Figure 7)?
- In the discussion, make more explicit the link you are hypothesizing between familiar face
recognition and task demands ("familiar face recognition has limits" is a bit vague).
- I don't quite understand your possible semantic description in the discussion. If participants are
using semantic information for the familiar faces (i.e., remembering the name of the celebrity),
shouldn't you predict there to be no difference in recognition rate for the same image versus
different image conditions?
22
label_author_9
Review form: Reviewer 2 (David White)
Is the manuscript scientifically sound in its present form?
Are the interpretations and conclusions justified by the results?
Is the language acceptable?
Yes
Is it clear how to access all supporting data?
Do you have any ethical concerns with this paper?
No
Have you any concerns about statistical analyses in this paper?
No
Recommendation?
label_recommendation_9
Accept with minor revision
Comments to the Author(s)
label_comment_9
This registered report describes a single study that used a repeat-detection task. The study
examines the important question of whether repeat detection of faces presented in a continuous
stream is facilitated when precisely the same image of a face is repeated, relative to when a
different image of the same face is repeated. The critical test in this paper is whether this
facilitation interacts with the familiarity of a face. This is a theoretically important test as it has a
bearing on the differences in how familiar and unfamiliar faces are represented.
The proposal that familiar face perception is relatively less reliant on image-level representation
than unfamiliar face perception has been influential in understanding the cognitive processes
underlying face perception, and there is substantial evidence from face matching tasks to support
this view. The results of this experiment show that image changes had a similar effect on familiar
and unfamiliar repetition detection task, suggesting that perception of familiar and unfamiliar
faces is equally reliant on image-level details.
Overall this is a neat study and the report is very well written. It makes an important contribution
to the literature on this topic. I have some points that should be relatively straightforward to
address in revision.
1. One potential reason why the critical interaction was not observed are the floor effects that are
observed in the Unfamiliar face, different-image condition. This is reflective of the difficulty in
developing a task for which the measurement scale enables a researcher to capture the full range
of variance observed on familiar and unfamiliar face processing tasks (unfamiliar face processing
is poor, familiar is much better, and so it is difficult to create a scale that captures both). This
difficulty is compounded by the fact that there are large individual differences in face processing
ability.
This challenge is by no means unique to this study, and similar floor/ ceiling effects are apparent
in other studies in this literature. Nevertheless, I think this should be acknowledged directly in
23
the manuscript. The exploratory analysis shown in Figure 4 goes some way to revolve this, but
the unfamiliar distributions do not look normal in this analysis either.
2. There were many analyses performed on the data. This vigilant approach to confirming their
null interaction result is a key strength of the paper, but the lengthy results section could perhaps
be streamlined by moving some analysis to supplementary materials.
In particular, the value of the d-prime analysis was not clear to me. False alarm rates in this
analysis are filler images incorrectly identified as repeats. Why would one expect performance on
these items to differ in the same image/ different image condition? The same/different image
condition does not appear to be relevant to these items, as this is the first time that the Ps in both
conditions had seen filler faces. Because I was unclear on the mechanism by which the
experimental manipulation could affect false alarm rates, I did not learn much from this analysis.
3. In my previous review of this paper (pre-registration), I mentioned repetition priming effects as
being relevant literature in relation to the author's research question. On reviewing these
comments I am concerned that I was not specific enough, as the authors appear to have
misinterpreted the relevance of this work to their research in the full manuscript.
I did not mean that to say that the repeat detection method is like a repetition priming
experiment (it may be in some ways, but also different in other key aspects). Rather, I drew
attention to the findings reported by Johnston and Barry because they are relevant here. They
show what is known in priming literature as 'graded similarity effects' (see also Ellis, Young,
Flude & Hay, 1987). These effects index facilitation of face identification by prior exposure to an
image of the same face (i.e. repetition priming strength), as a graded function of the similarity
between the prime image and the test image. When an identical image is used at prime and test,
the priming effects are stronger than when different images are used. Critically for the author's
study, graded similarity is found for familiar faces. This basic phenomenon has been shown in
other strands of research (for example, iconic images of familiar faces are recognised better), and
together they show that familiar face recognition is sensitive to image-level properties.
Based on these studies, one may predict that detection of familiar face repetitions would be
facilitated by repeats of identical images. So, it is possible that the hypothesis tested in this study
was based too narrowly on evidence from studies of face matching.
MINOR
-- P11, line 22 -- Not clear from the start of this section how face knowledge is operationalised -
should state that it is the percentage of faces recognised at the beginning of section.
=============
David White
UNSW Sydney
label_author_10
Review form: Reviewer 3
Is the manuscript scientifically sound in its present form?
Are the interpretations and conclusions justified by the results?
24
Is the language acceptable?
Yes
Is it clear how to access all supporting data?
Do you have any ethical concerns with this paper?
No
Have you any concerns about statistical analyses in this paper?
No
Recommendation?
label_recommendation_10
Accept as is
Comments to the Author(s)
label_comment_10
The exploratory analyses are a really positive addition to the planned analysis and the
explanation of the results is entirely appropriate.
label_end_comment
Decision letter (RSOS-170634.R1)
12-Apr-2018
Dear Mr Chapman:
On behalf of the Editor, I am pleased to inform you that your Stage 2 Registered Report RSOS-
170634.R1 entitled "How robust is familiar face recognition? A repeat detection study of >1,000
faces" has been deemed suitable for publication in Royal Society Open Science subject to minor
revision in accordance with the referee suggestions. Please find the referees' comments at the end
of this email.
The reviewers and Subject Editor have recommended publication, but also suggest some minor
revisions to your manuscript. Therefore, I invite you to respond to the comments and revise your
manuscript.
Please also ensure that all the below editorial sections are included where appropriate -- if any
section is not applicable to your manuscript, please can we ask you to nevertheless include the
heading, but explicitly state that the heading is inapplicable. An example of these sections is
attached with this email.
• Ethics statement
If your study uses humans or animals please include details of the ethical approval received,
including the name of the committee that granted approval. For human studies please also detail
whether informed consent was obtained. For field studies on animals please include details of all
permissions, licences and/or approvals granted to carry out the fieldwork.
• Data accessibility
It is a condition of publication that all supporting data are made available either as
supplementary information or preferably in a suitable permanent repository. The data
25
accessibility section should state where the article's supporting data can be accessed. This section
should also include details, where possible of where to access other relevant research materials
such as statistical tools, protocols, software etc can be accessed. If the data has been deposited in
an external repository this section should list the database, accession number and link to the DOI
for all data from the article that has been made publicly available. Data sets that have been
deposited in an external repository and have a DOI should also be appropriately cited in the
manuscript and included in the reference list.
If you wish to submit your supporting data or code to Dryad (http://datadryad.org/), or modify
your current submission to dryad, please use the following link:
http://datadryad.org/submit?journalID=RSOS&manu=(Document not available)
• Competing interests
Please declare any financial or non-financial competing interests, or state that you have no
competing interests.
• Authors’ contributions
All submissions, other than those with a single author, must include an Authors’ Contributions
section which individually lists the specific contribution of each author. The list of Authors
should meet all of the following criteria; 1) substantial contributions to conception and design, or
acquisition of data, or analysis and interpretation of data; 2) drafting the article or revising it
critically for important intellectual content; and 3) final approval of the version to be published.
All contributors who do not meet all of these criteria should be included in the
acknowledgements.
We suggest the following format:
AB carried out the molecular lab work, participated in data analysis, carried out sequence
alignments, participated in the design of the study and drafted the manuscript; CD carried out
the statistical analyses; EF collected field data; GH conceived of the study, designed the study,
coordinated the study and helped draft the manuscript. All authors gave final approval for
publication.
• Acknowledgements
Please acknowledge anyone who contributed to the study but did not meet the authorship
criteria.
• Funding statement
Please list the source of funding for each author.
Because the schedule for publication is very tight, it is a condition of publication that you submit
the revised version of your manuscript within 7 days (i.e. by the 20-Apr-2018). If you do not think
you will be able to meet this date please let me know immediately.
To revise your manuscript, log into https://mc.manuscriptcentral.com/rsos and enter your
Author Centre, where you will find your manuscript title listed under "Manuscripts with
Decisions". Under "Actions," click on "Create a Revision." You will be unable to make your
revisions on the originally submitted version of the manuscript. Instead, revise your manuscript
and upload a new version through your Author Centre.
When submitting your revised manuscript, you will be able to respond to the comments made by
the referees and upload a file "Response to Referees" in "Section 6 - File Upload". You can use this
to document any changes you make to the original manuscript. In order to expedite the
26
processing of the revised manuscript, please be as specific as possible in your response to the
referees.
When uploading your revised files please make sure that you have:
1) A text file of the manuscript (tex, txt, rtf, docx or doc), references, tables (including captions)
and figure captions. Do not upload a PDF as your "Main Document".
2) A separate electronic file of each figure (EPS or print-quality PDF preferred (either format
should be produced directly from original creation package), or original software format)
3) Included a 100 word media summary of your paper when requested at submission. Please
ensure you have entered correct contact details (email, institution and telephone) in your user
account
4) Included the raw data to support the claims made in your paper. You can either include your
data as electronic supplementary material or upload to a repository and include the relevant doi
within your manuscript
5) All supplementary materials accompanying an accepted article will be treated as in their final
form. Note that the Royal Society will neither edit nor typeset supplementary material and it will
be hosted as provided. Please ensure that the supplementary material includes the paper details
where possible (authors, article title, journal name).
Supplementary files will be published alongside the paper on the journal website and posted on
the online figshare repository (https://figshare.com). The heading and legend provided for each
supplementary file during the submission process will be used to create the figshare page, so
please ensure these are accurate and informative so that your files can be found in searches. Files
on figshare will be made available approximately one week before the accompanying article so
that the supplementary material can be attributed a unique DOI.
Once again, thank you for submitting your manuscript to Royal Society Open Science and I look
forward to receiving your revision. If you have any questions at all, please do not hesitate to get
in touch.
Kind regards,
Andrew Dunn
Royal Society Open Science
openscience@royalsociety.org
on behalf of Professor Chris Chambers (Registered Reports Editor, Royal Society Open Science)
openscience@royalsociety.org
Associate Editor Comments to Author (Professor Chris Chambers):
Associate Editor: 1
Comments to the Author:
The Stage 2 submission was returned to three of the original reviewers who provided
assessments at Stage 1. All are positive and the extent of required revisions is straightforward.
Reviewer 3 is entirely satisfied, while Reviewer 1 notes some specific claims that require further
clarification and justification. Reviewer 2 offers a more extensive critique, suggesting an
explanation for the observed negative results and questioning the inclusion some exploratory
analyses. The reviewer (in their point 3) also questions the rationale of the hypothesis. In
responding specifically to this issue, please be sure to confine all consideration of this point to the
Discussion section; for a Registered Report, relitigation of study rationale at Stage 2 is only
appropriate in the Discussion and must not lead to changes in the framing or presentation of the
hypotheses in the Introduction.
27
Comments to Author:
Reviewer: 1
Comments to the Author(s)
I find it incredibly interesting that you were ultimately not able to replicate the finding in your
pilot study and suggested by prior work, that while repetition detection performance across
different image types for unfamiliar faces should be lower than for same images, this difference
should not show for familiar faces. Pre-registering the study (combined with your power
analyses, large sample size, and Bayesian statistics) make this null finding particularly
compelling, and I believe this result to be of importance to the face memory community. This sort
of result is a great example of the importance of pre-registration, as it encourages researchers to
report null results and gives us a better look into phenomena previously thought to be incredibly
robust.
I have a few comments on items I'd like clarification on, or to see edits for.
- When doing the analyses using the 7-point rating scale of familiarity, if I understand correctly
(the grammar is a bit confusing in that sentence), you include both self-reported and coder-scored
familiarity in the binomial mixed-effects model. You then seem to describe it as coder-scored
familiarity fully accounting for the effects ultimately found in the model, and that self-reported
familiarity has no relation to ultimate recognition. Does a correlation between self-reported
familiarity and recognition rate come out as non-significant as well (i.e., I'd like to see a statistic
specifically supporting the claim in Figure 7)?
- In the discussion, make more explicit the link you are hypothesizing between familiar face
recognition and task demands ("familiar face recognition has limits" is a bit vague).
- I don't quite understand your possible semantic description in the discussion. If participants are
using semantic information for the familiar faces (i.e., remembering the name of the celebrity),
shouldn't you predict there to be no difference in recognition rate for the same image versus
different image conditions?
Reviewer: 2
Comments to the Author(s)
This registered report describes a single study that used a repeat-detection task. The study
examines the important question of whether repeat detection of faces presented in a continuous
stream is facilitated when precisely the same image of a face is repeated, relative to when a
different image of the same face is repeated. The critical test in this paper is whether this
facilitation interacts with the familiarity of a face. This is a theoretically important test as it has a
bearing on the differences in how familiar and unfamiliar faces are represented.
The proposal that familiar face perception is relatively less reliant on image-level representation
than unfamiliar face perception has been influential in understanding the cognitive processes
underlying face perception, and there is substantial evidence from face matching tasks to support
this view. The results of this experiment show that image changes had a similar effect on familiar
and unfamiliar repetition detection task, suggesting that perception of familiar and unfamiliar
faces is equally reliant on image-level details.
Overall this is a neat study and the report is very well written. It makes an important contribution
to the literature on this topic. I have some points that should be relatively straightforward to
address in revision.
1. One potential reason why the critical interaction was not observed are the floor effects that are
28
observed in the Unfamiliar face, different-image condition. This is reflective of the difficulty in
developing a task for which the measurement scale enables a researcher to capture the full range
of variance observed on familiar and unfamiliar face processing tasks (unfamiliar face processing
is poor, familiar is much better, and so it is difficult to create a scale that captures both). This
difficulty is compounded by the fact that there are large individual differences in face processing
ability.
This challenge is by no means unique to this study, and similar floor/ ceiling effects are apparent
in other studies in this literature. Nevertheless, I think this should be acknowledged directly in
the manuscript. The exploratory analysis shown in Figure 4 goes some way to revolve this, but
the unfamiliar distributions do not look normal in this analysis either.
2. There were many analyses performed on the data. This vigilant approach to confirming their
null interaction result is a key strength of the paper, but the lengthy results section could perhaps
be streamlined by moving some analysis to supplementary materials.
In particular, the value of the d-prime analysis was not clear to me. False alarm rates in this
analysis are filler images incorrectly identified as repeats. Why would one expect performance on
these items to differ in the same image/ different image condition? The same/different image
condition does not appear to be relevant to these items, as this is the first time that the Ps in both
conditions had seen filler faces. Because I was unclear on the mechanism by which the
experimental manipulation could affect false alarm rates, I did not learn much from this analysis.
3. In my previous review of this paper (pre-registration), I mentioned repetition priming effects as
being relevant literature in relation to the author's research question. On reviewing these
comments I am concerned that I was not specific enough, as the authors appear to have
misinterpreted the relevance of this work to their research in the full manuscript.
I did not mean that to say that the repeat detection method is like a repetition priming
experiment (it may be in some ways, but also different in other key aspects). Rather, I drew
attention to the findings reported by Johnston and Barry because they are relevant here. They
show what is known in priming literature as 'graded similarity effects' (see also Ellis, Young,
Flude & Hay, 1987). These effects index facilitation of face identification by prior exposure to an
image of the same face (i.e. repetition priming strength), as a graded function of the similarity
between the prime image and the test image. When an identical image is used at prime and test,
the priming effects are stronger than when different images are used. Critically for the author's
study, graded similarity is found for familiar faces. This basic phenomenon has been shown in
other strands of research (for example, iconic images of familiar faces are recognised better), and
together they show that familiar face recognition is sensitive to image-level properties.
Based on these studies, one may predict that detection of familiar face repetitions would be
facilitated by repeats of identical images. So, it is possible that the hypothesis tested in this study
was based too narrowly on evidence from studies of face matching.
MINOR
-- P11, line 22 -- Not clear from the start of this section how face knowledge is operationalised -
should state that it is the percentage of faces recognised at the beginning of section.
=============
David White
UNSW Sydney
29
Reviewer: 3
Comments to the Author(s)
The exploratory analyses are a really positive addition to the planned analysis and the
explanation of the results is entirely appropriate.
Author's Response to Decision Letter for (RSOS-170634.R1)
See Appendix C.
label_end_comment
Decision letter (RSOS-170634.R2)
24-Apr-2018
Dear Mr Chapman:
It is a pleasure to accept your manuscript entitled "How robust is familiar face recognition? A
repeat detection study of >1,000 faces" in its current form for publication in Royal Society Open
Science.
Thank you for your fine contribution. On behalf of the Editors of Royal Society Open Science, we
look forward to your continued contributions to the Journal.
Kind regards,
Andrew Dunn
Royal Society Open Science
openscience@royalsociety.org
on behalf of Professor Chris Chambers (Subject Editor)
openscience@royalsociety.org
Appendix A
ASSOCIATE EDITOR
1. Four expert reviewers have now assessed the submission. All are broadly positive but offer a
wide range of suggestions for revision, spanning the full breadth of the Stage 1 review criteria,
from clarifying and strengthening the study rationale to broader inclusion of dependent
measures and statistical approaches (e.g. Bayesian hypothesis testing, as suggested by
Reviewer 3). Concerning one comment by Reviewer 3: "I would recommend that the authors
consider potential further analyses that they may want to run", please note that to avoid
blurring the line between confirmatory pre-registered analyses and post hoc exploratory
analyses, any exploratory analyses that do not serve an a priori hypothesis or other pre-
specified purpose should not be included in Stage 1 manuscripts but should instead be
reported at Stage 2 in an "Exploratory Analyses" section of the Results. In responding to this
reviewer's comment you are, however, welcome to discuss some of the broader plans you may
have for considering extra analyses.
Thank you for the opportunity to respond to reviewers and resubmit our work. We believe our
manuscript is strengthened by the changes we’ve made following the reviews. We agree that our
registered analysis should focus on our a priori hypotheses, but we hope to run exploratory analyses
later (such as those suggested by Reviewer #3).
2. In addition to addressing the comments of the reviewers, please confirm in the manuscript
what the minimum analysed sample size will be in the event that the participant exclusion rate
in the pre-registered experiment turns out higher than expected. Ideally this should be the
figure indicated in the power analysis (N=124), thus guaranteeing to replace excluded
participants until the stopping rule is triggered. If so, your protocol could be altered to simply
state that participants will tested until N=124 non-excluded participants are reached, as
opposed to pre-specifying an estimated total sample of N=142.
Yes, this is an important clarification. The Method of our main experiment now says:
“We will test participants until a sample of 124 non-excluded participants is reached.”
REVIEWER #1
1. The current study looks to replicate previous findings that familiar face recognition is more
invariant to changes in the face (i.e., viewpoint) than unfamiliar face recognition, using a
previously untested continuous recognition task that more closely approximates the face
recognition tasks people perform in daily life - with many faces of varied style and (for half of
the participants) with repeat detection on a different image of the same person. Generally, the
methodology and the analyses that bring about the pilot results are sound. The hypotheses are
straightforward and well supported by previous literature. There are some questions I have
and some suggestions of additional measures or analyses you could try that could provide
further insight into the data. I would like to hear how you plan to address these before fully
accepting this preregistration. Also for full disclosure as promoted by this journal (Royal
Society Open Science), I will identify myself: I am Dr. Wilma Bainbridge, lead author on one of
the works you cite which uses this continuous recognition paradigm to answer different
questions about face memory.
We thank Dr Bainbridge for her thoughtful and positive comments on our work.
2. Labeling the general category of the celebrity is likely a very different level of familiarity in
comparison to correctly knowing the full name of the person, yet you group these responses
together. While you can do an analysis grouping them together, I would be interested to see
an additional analysis looking at a more fine-grained scale of familiarity. Would it be possible
to get numbered (e.g., Likert scale) familiarity ratings for each image? Or if not - how do the
results pan out if you separate out people who only know the career type versus the full name?
We agree that a more fine-grained analysis of familiarity could be useful. We tried to do this,
however we have some concerns as below.
A score of 3 was given to responses that provided the correct full name (both first and last name) of
the famous face. If participants provided a specific example that identified the individual (e.g., a
character played in a TV show or film), or if they made a minor error in naming the individual (e.g.,
first or last name provided correctly), this was scored as 2. If participants provided a general
category (e.g., actor, singer) that the famous individual belonged to, this was scored as 1. If
participants left the response blank, or made an incorrect identification, this was scored as 0. This
rating scheme led to responses being predominantly scored 0’s (M = 73.5%, SD = 14.3), followed by
3’s (M = 15.9%, SD = 9.5), 2’s (M = 9.4%, SD = 7.9), and 1’s (M = 1.2%, SD = 2.7).
The figure below shows that there is more variability in performance when analysing familiarity this
way, however there is very little difference between familiar faces scored 2 or 3. However, our
participants were not given explicit instructions of how much information to provide about the
famous faces and so may have responded conservatively. It’s possible that prompting participants to
provide as much detail as possible, or guess, would lead to a more graded effect of familiarity on
recognition, but at least for our data this doesn’t appear to be the case. Because of the low number
of items when splitting familiar faces into 3 categories, we prefer to average all correct responses
(scores > 0) together as ‘familiar’ faces. We can run this ‘fine-grained’ analysis as part of later
exploration if this is requested by the reviewers, however we believe that for the registered analysis
our current method (grouping into familiar vs unfamiliar) seems the most appropriate.
3. Since hair is generally cropped here, and as you acknowledge from previous literature, people
tend to use more external cues for recognizing unfamiliar faces, the stimuli may have a built-in
bias against the unfamiliar faces due to an elimination of their cues from the oval cropping.
You may consider allowing more of the external cues to be shown, or acknowledge this as a
potential confound in the analysis of your results.
We apologise for not being clear in our previous description, but we’ve made it clear now that our
oval cropping doesn’t exclude all external face cues, only those that may enable matching of ‘same’
images without actually processing the face (e.g., background colour). Our oval cropping is actually
quite similar to what Dr Bainbridge did in her studies of unfamiliar faces (Bainbridge et al., 2013;
Bainbridge, 2017), so we don’t think our stimuli are biased against unfamiliar faces. Below are some
examples.
Example faces from Bainbridge et al. Example faces from Bainbridge
(2013) (2017)
Example faces from our study, showing both images for each identity:
Example FILLER faces Example VIGILANCE faces Example TARGET faces
We prefer to keep our stimuli as they are, because we are mainly interested in stable cues in faces.
However, we will address this issue in the Discussion.
4. The hit rates for the vigilance repeats and target repeats are surprisingly low. As a
comparison, in the Bainbridge et al. 2013 study, the average hit rate was 51.6% for the target
trials, and the vigilance hit rate was probably even higher, and these are all unfamiliar faces.
The fact that your participants are only getting 58.3% of the same vigilance repeats only a
handful of seconds later leads me to believe that there might be something going on.
Specifically, I wonder if your brief image presentation time could partially be a culprit -
perhaps the images are going by so quickly that participants don't have time to process the
images and respond? You currently present them for 500 ms while Bainbridge et al. 2013 and
Isola et al. 2011 present them for 1s each.
We too wondered about why this difference arose. Brief presentation time is one potential reason,
but we note two other differences between our study and Bainbridge et al. (2013). First,
Bainbridge’s participants were able to quit the experiment at any time, so there might have been
self-selection for better participants. Second, Bainbridge’s experiment automatically ended for
participants who performed poorly on vigilance trials. All these differences might explain our lower
hit rates, which is something we will touch upon in the Discussion.
5. I'm curious - are there more false alarms to familiar faces than unfamiliar faces?
Participants were only assessed on familiarity for the 100 target images, not fillers, so unfortunately
we have no way of analysing false alarms separately for familiar and unfamiliar faces.
6. It is great to see you explicitly acknowledge the technical issue in the familiarity task and that
you will avoid that moving forward!
Thanks.
7. There are citations missing in the data checks section (after "consistent with previous
literature").
Fixed.
8. "All images for fillers (including the first presentation of target and vigilance repeats) were
randomly selected from the two images created per face" (p.4) -- Just to make sure I
understand this correctly, does it mean that for a given celebrity, it was randomized whether a
subject first saw face A of that celebrity (and depending on condition was then later tested on
face A or face B of that person) or first saw face B? Counterbalancing that may be important
(e.g., so that it's not always a forward-facing first presentation and then a rotated second
presentation as in Figure 1).
Yes, for vigilance and target repeats the first presentation of a face was randomly chosen from the
two images. We’ve now made this clearer:
“The first image presented was randomly selected from the two images created per celebrity (see
Materials), while the second image was determined by condition.”
9. This wasn't completely clear in the methods until later, but for the "different" group vigilant
repeats, people were also responding to a different image of the same celebrity, correct?
Yes. We have now clarified this:
“The critical manipulation was whether the repeated face (for both vigilance and target repeats) was
the exact same image as the original face (‘same’ condition), or whether it was a different image of
the same face (‘different’ condition).”
10. Also just as a reference, we have some new work that seems directly relevant to your work (no
need to cite unless if you find this actually useful!):
Bainbridge, 2016. The memorability of people: Intrinsic memorability across transformations
of a person's face. Journal of Experimental Psychology: Learning, Memory, and Cognition.
It similarly does a continuous recognition task with participants split into "same" and
"different" image groups for a face identity, though it only looks at unfamiliar face
recognition. While I also find that people do worse with the different image task for unfamiliar
faces, the key finding is that there is high consistency across people in which faces they do well
on (even in the "different" task) and which they do poorly on.
It is useful, thanks. We’ve now included it as a citation.
REVIEWER #2
1. The authors propose to examine the extent to which detecting duplicate faces is facilitated by
repetition of the same image, relative to when two different of the same person are repeated.
This task is really very interesting. The paradigm is taken from a previous study, but this is a
novel application that promises to address a theoretically important issue in the study of face
recognition -- to what extent are image-level details of images encoded in memory?
We thank Dr White for his positive comments.
2. My main comment is that the authors do not discuss repetition priming effects. This is an
important topic in the context of the work and the results should be interpreted in the context
of this work (for a useful background summary see Johnston & Barry, 2001). The authors were
probably already intending to do so, but I would expect to see a discussion of that body of
work in a full write up, and so the work should be mentioned in the introduction.
Thanks for bringing this up. We have now mentioned priming in the Introduction:
“This task is similar to those used in studies of repetition priming (Bruce & Valentine, 1985;
Johnston & Barry, 2001), in which presentation of face images enhances recognition of those
faces on a later task.“
3. Logic of the rationale, study design and analysis were very well thought out and clearly
described in an impressive level of detail. I have no concerns about the proposed methodology.
The authors also present detailed pilot data and analysis confirming that the method is a valid
test of their hypothesis.
Thank you.
4. Page 2, lines 36 to 48: This passage appears to discuss findings from Burton et al. 2005 but
doesn't cite.
Fixed.
5. Page 4, line 28: Was internet search actually random? Or highest page rank images on
Google?
Images were selected from Google Images based on the following criteria: 1) they had high
resolution (face was approximately 180 x 250 pixels at minimum); 2) they provided a full view of the
person’s face (e.g., not occluded by a hat); 3) the two selected images for each person varied in
viewpoint, expression, and/or lighting. We have now removed the second paragraph of the
Materials section, and shifted some of that detail into the first paragraph:
“For each celebrity, we downloaded several face images from a Google Image search. Two images
were selected for each celebrity to make up the stimulus set. The two images for each celebrity varied
in terms of lighting, viewpoint, expression, and date taken. Images were cropped in an oval to
exclude non-face, background information (as in e.g., Bainbridge et al., 2013; Bainbridge, 2017).”
REVIEWER #3
1. I found this manuscript largely well written and a relatively novel way to address a core and
significant question in face recognition. The method is relatively neat and a good addition to
the literature (following some modifications). The results of the pilot are as anticipated. I have
a few suggestions for how to improve the introduction to sell the core novelty more. My most
major concerns are methodological and suggest major changes to the methods for the final
report.
We thank this reviewer for their detailed comments and suggestions.
2. A more clear definition of familiarity is required at the very start - some may consider
unfamiliar faces are ones not encountered before, therefore they cannot be recognised. In fact,
by definition, when they are recognised, they are familiar - therefore, I suggest being clear in
this definition from the start of the introduction all the way through (Pre-experimental
unfamiliar faces?).
We agree that a more explicit definition of familiarity is useful. We have therefore added the
following sentence to the opening paragraph of the Introduction:
“Familiar faces, by definition, are those we recognise and can identify.”
3. Likewise, familiar faces can be personally familiar, experimentally-induced, or famous and
there are clear dissociations between these types. Given that this task compares famous faces
that the person knows something about with famous faces that they don't (which might still
have some residual familiarity).
We believe we have made it clear that in this study that we are using famous faces, and we provide
a rationale for this choice. We agree that different types of familiarity might produce different
results. This point is something that we can talk about later in Discussion.
4. Page 2 - paragraph two, include more citations for these comments
In addition to the Burton et al. (2005) paper suggested by Dr White, we have included other specific
references in this section (Bruce & Young, 1986; Jenkins & Burton, 2011). Collectively, the references
in this paragraph cover much of the theoretical distinction between familiar and unfamiliar faces,
while the paragraph following it describes empirical findings of how familiarity affects face
recognition across a number of manipulations.
5. Page 2 - paragraph 3, the authors say "familiar face recognition should be more robust" -
robust in what way? Recognition abilities remain despite changes in?
We have clarified this point in our Introduction:
“The distinction above predicts that changes to images will impair recognition of familiar faces less
than unfamiliar faces.”
6. Page 3 - paragraph 2, I don't think the paradigm that is presented better reflects face
recognition in a natural setting: how can viewing a stream of faces for an hour reflect natural
face recognition? I'm not criticising the paradigm, just the justification. Remove all phrases
indicating that the procedure better reflects face recognition in a natural setting. Indeed, the
justification for this procedure is not well thought out (not using personally-familiar faces
because of the methodological issues is too vague, for example - what this implies is that the
use of well-matched personally familiar faces is too time-consuming rather than an issue). I
suggest either being honest about the justification or removing it. The task itself is an
interesting addition to the literature, and this could be expanded upon.
This point is well taken. We have adjusted our description of the task throughout the Introduction
and Method. Specifically, we have removed reference to our task reflecting “face recognition in a
natural setting” in favour of “more demanding experimental conditions with significant memory
load”. We have also changed our wording of using personally-familiar faces to a “practical” issue
rather than a “methodological” one.
7. Page 3 - reference to figure 1: explain and justify the reference to two types of repeats
indicated in the figure and make predictions for this. Or, do not include Figure 1 until the
method section.
We have added the following clarifying sentence to this paragraph:
“Repeated images could appear at one of two different times: vigilance repeats occurred shortly
following the initial presentation of a face, and were included to assess participants’ engagement with
the task; target repeats occurred much longer after the initial face was presented, and were the main
concern of our analysis.”
For the Pilot Methods, I have a number of critical concerns. These will need to be fixed for the
registered report to be accepted. Some of the methodological descriptions require more detail in
order to replicate the study.
8. Participants - provide details of age, gender, and ethnicity.
We didn’t collect demographic information from our pilot participants, but we intend to assess age,
gender, and ethnicity in our main experiment as part of post-hoc, exploratory analysis. We have
added the following to the Method for the Registered Study:
“We will ask these participants of their age, gender, and ethnicity.”
9. The study uses a between-subjects design over the most critical variable. This should be a
repeated measures variable. I realise this increases the number of trials, but will reduce the
number of participants required.
Ideally, we would manipulate image type within-subjects. However, with our experimental paradigm
and participant pool there are some limitations that render within-subject design less desirable. As it
is, the task is very long and demanding. Based on our pilot results, it’s almost certain that accuracy
will drop more if the task is twice as long, and we will likely hit floor in the unfamiliar/different
condition (which will make our findings hard to interpret). We therefore prefer to use a between-
subject design, which our pilot data suggest is suitable to address our question (with a properly
powered sample of 124 people).
10. Materials - provide details of age, gender, and ethnicity to show that these match to the
participants given the own-group biases.
We have provided more detail of the demographics in the Method:
“Each face was coded on the four demographic attributes: glasses (4.7% of faces); skin colour (10.1%
dark, 89.9% light); gender (53.7% male, 46.3% female); and age (0.8% child, 91.6% adult, 7.6%
elderly).”
The same 100 target faces are used for all participants, who are randomly assigned to each image
type condition, so this should protect against own-group bias.
11. Were the images of the celebrities taken from when they were most famous according to the
participants (i.e., Harrison Ford is most famous as a 30 year old to many participants currently
over the age of 30, but is most famous as a 60+ year old to many participants under the age of
30 due to films). All of the stimulus details need to be provided.
The justification of needing to recognise the same people over different contexts is only
partially supported: in natural circumstances, we do not need to recognise faces of someone
who is younger than when we last saw them (except in rare cases). Is this taken into
consideration (i.e., are the two images of each celebrity very different or were they taken
around a similar sort of time?)
Unfortunately we don’t have full metadata of the images (e.g., year the picture was taken) and in
many cases this exact information was unavailable. However, most image pairs are taken from
similar times, and for celebrities who are currently well-known the images are reasonably recent
(within the last 5 years; and not from before they were famous). For some celebrities, who were
more well-known in the past, or who are now deceased, the images are taken from a time when
they were most well-known. Most importantly, we don’t think this is an issue for our study given
that the same 100 target faces are used for all participants, who are randomly assigned to the two
conditions.
12. The cropping of the images is not an appropriate technique when using celebrity images as the
hair is a vital feature for recognition especially for famous faces.
We think this reviewer meant “…vital feature for recognition especially for unfamiliar faces”, as also
pointed out by Reviewer #1 and as we note in our Introduction “Familiar faces are better recognised
using internal features (e.g., eyes, mouth) than external features (e.g., hair) (Clutterbuck & Johnston,
2005; Ellis et al., 1979; Osborne & Stevenage, 2008; Young et al., 1985)”. Please see our response to
point #3 by Reviewer #1.
13. Procedure - The familiarity task is insufficient for measuring familiarity of the faces. The
familiarity task should be done for all faces to ensure that the target faces were not more
familiar than the non-target faces (indeed, the target faces should be counterbalanced so all
are used).
For practical reasons this is unfeasible. Measuring familiarity of the 100 targets takes approximately
10 minutes, so measuring familiarity of all 1006 faces would take nearly 2 hours on its own. As we
describe in the Method, we carefully chose the target faces to sample the demographics of the full
stimulus set. We could have randomly generated a target set for each participant, but this does not
allow us to balance demographics, and might lead to differences in stimuli between our groups.
14. I would also recommend collecting familiarity ratings to statistically show the targets are not
more familiar than non-targets.
We’re unsure what this additional rating would contribute to our analysis. The target faces are the
same for all participants and across groups, so if the targets are more familiar than non-targets, this
might improve performance (e.g., greater hit rate for targets, and/or reduced false alarms for fillers)
but it would do so across the board, and would not bias our results.
15. A design section would help as it would clarify how the authors have defined what is a familiar
face and what isn't (i.e., it is done on the basis of participant ratings after the experiment).
We have clarified this point in our Analytic Procedure:
“To test our hypothesis… we conducted a 2 × 2 mixed ANOVA with image (same vs different) and
familiarity (familiar vs unfamiliar, based on responses of each participant during the familiarity task)
as fixed factors…”
16. The exclusion of participants who were unfamiliar with the famous faces is very lax. If
participants were familiar with less than 10% of the faces, this means that for most of the
faces in the trials, they were unfamiliar. In which case, this is really a task measuring some sort
of pop out effect (the rare instances of familiar faces). Based on this, I would suggest either
excluding when participants show familiarity of less than 30%, and/or analysing accuracy
across faces of different levels of familiarity.
Please see our response to point #2 of Reviewer #1. Although there were a limited number of trials
at each level, the data suggest that familiarity with a face in a specific context (e.g., TV show
character) leads to similar levels of performance as for high familiarity.
Our participants were familiar with 30% of the faces on average, so setting the familiarity criteria at
this level would lead to exclusion of half of our participants. We think this is overly conservative, as
the range of familiarity in our pilot data was enough to demonstrate our effect.
17. Page 6 - line 29, Replace (citations) with the citations.
Noted, and fixed.
18. Given the importance of a non-significant t-test result (the fact that familiar faces are
recognised equally well in both same and different images, Bayesian statistics should be
performed to show how much confidence we should have in the null results.
This is an excellent suggestion, and we have now included this:
“Analysis of null results was complemented by Bayesian statistics. Using the BayesFactor package
(Morey & Rouder, 2015), we conducted Bayesian t-tests alongside the traditional t-tests used to
decompose the results of repeated measures ANOVAs. We report the Bayes Factor in favour of the
alternative hypothesis (BF10) based on analysis using a ‘default’ prior with scale = 0.707 (Morey,
Romeijn, & Rouder, 2016).”
The Pilot Results have been updated to reflect this analysis:
“While unfamiliar faces were better detected with ‘same’ than ‘different’ images, t(43) = 3.74, p <
.001, ds = 1.14, BF10 = 52.10, familiar faces were detected equally well regardless of image, t(43) =
0.97, p = .325, ds = 0.30, BF10 = 0.44.”
and when accounting for differences in task difficulty:
“As in the full sample, unfamiliar faces were detected better with ‘same’ (M = 32.4%) than ‘different’
images (M = 22.7%), t(23) = 2.07, p = .049, ds = 0.87, BF10 = 1.66. In contrast, familiar faces were
detected equally well with ‘same’ (M = 57.4%) and ‘different’ images (M = 63.0%), t(23) = 0.96, p =
.349, ds = 0.40, BF10 = 0.51.“
We have also updated the analysis scripts in our OSF storage to include this additional analysis.
19. Given my suggestion to collect familiarity ratings for all faces, I would suggest running
correlations between familiarity ratings and detection accuracy for same image and different
image. A comparison of the correlation coefficients would then highlight whether familiar
faces how more of a robust representation.
Again, please refer to our response to point #2 of Reviewer #1. Inspection of the figure suggests that
there is not much difference across different levels of familiarity, and the most noticeable effect is
when comparing unfamiliar recognition to that with any level of familiarity.
20. There is also a large amount of potential analyses that could be run on these data, including
reaction time analyses, analyses across blocks in the experiment. I would recommend that the
authors consider potential further analyses that they may want to run.
We agree with the Editor with respect to only specifying registered analyses about our a priori
hypotheses. We will consider other, exploratory analyses once full data collection is complete. We
agree with this reviewer that block-level and RT analyses might be revealing. We also wish to
examine whether participants’ overall familiarity (i.e., the number of identities they report as
familiar) is related to their performance on the task; do participants who know more celebrities have
a greater hit rate for target repeats, and fewer false alarms for fillers?
REVIEWER #4
1. Overall, I think this is a well-designed study, with clear methodology and an analysis pipeline
which would allow replication.
We thank this reviewer for their positive comments.
2. However, I do not think it is framed in the best context. Specifically, several previous studies
have consistently shown that familiar faces are better recognized across image changes as
compared to unfamiliar faces. The authors say that these studies are limited due to the short
experiment time and small number of stimuli used, and that “we don’t really know whether
familiar face recognition remains robust to image changes under more demanding
experimental conditions that better reflect face recognition in a natural setting”.
Please see our response to point #6 of Reviewer #3 regarding description of our study as reflecting
“face recognition in a natural setting”.
3. Given that consistent effects have been observed across multiple studies with few stimuli, this
already suggests that these effects of familiarity are robust; therefore, from this perspective, I
cannot see how this study would contribute anything further to our current understanding of
participants abilities to recognize familiar and unfamiliar faces across changes in viewpoint.
We agree that the current literature presents the impression that recognition is more robust for
familiar than unfamiliar faces, but we think our study can extend this literature in several ways. First,
as we outline in our Introduction, we aim to test recognition using a longer and more demanding
task, with a greater number of stimuli than has been done previously. Second, and perhaps more
importantly, none of the published studies was preregistered, and so we have no idea how much
this area of research suffers from publication bias. Our Registered Report will provide an adequately
powered study that readers can be confident is not subject to the many issues that have come to
light during the replication crisis.
4. Instead, the unique aspect of this study is very much the memory load that the participants
will encounter. The study needs to be better framed in the context of a face memory study
with a perceptual component, rather than a perceptual task alone – in its’ current form the
novelty (of the heavy memory load) is lost. I suggest that the authors reframe this study, which
in turn will strengthen the importance of the research question and their rationale for doing
the study.
As noted in our response to point #6 of Reviewer #3, we now describe our task as having
“demanding experimental conditions with significant memory load.” We hope this adequately
acknowledges the novelty of our task in this respect, while retaining the points about other
advantages (number of stimuli, experiment duration).
5. Given the variability in performance between the groups (indeed due to the nature of the
study), in addition to the % hits, it would be useful to examine (and view) the results as
d’prime’s. I suspect the results will still hold, but if not then this would require some re-
interpretation of the results.
As we noted in a response to point #5 of Reviewer #1, familiarity ratings were not collected for filler
trials. Because of this, we cannot calculate false alarms or d’ for familiar vs. unfamiliar images.
6. For the familiarity check – are the same images used as in the experiment?
We realise that this was not clear in our Method, and thank the reviewer for catching this. Yes – the
reviewer is correct, and we have added the following sentence to our Method:
“The face images shown were the same as the first presentation of the target in the repeat detection
task.”
7. Unfortunately, due to the nature of the design, the authors cannot determine how the size of
the gap between presentations is affected by the ability to identify faces across images. I’m
not suggesting you do this here, but this might be really interesting to look at in a future study
using your design.
If by this the reviewer means that the duration between the first and second presentation of images
might affect recognition performance, we agree that this is an interesting question. Comparison of
vigilance (short gap) and target (long gap) trials gives some insight. Although we don’t have
familiarity ratings for the faces used as vigilance repeats, collapsing across familiarity shows that
‘same’ images were detected better on vigilance trials (M = 58.3%) than target trials (M = 44.4%),
while detection of ‘different’ images was comparable on vigilance (M = 32.8%) and target trials (M =
32.3%). As the reviewer suggests, manipulating the duration between presentations would be
interesting for future studies.
8. As a side-note, I’m not convinced that this study examines face recognition in a “natural
setting” – for example, if one wanted to examine more natural face recognition, then wouldn’t
dynamic faces be more appropriate?
As noted in our response to point #6 of Reviewer #3, we have removed description of our task as
reflecting more “real-world” or “natural” recognition.
Appendix B
ASSOCIATE EDITOR
The manuscript was returned to three of the original reviewers, whose assessments are generally
positive. However, a number of specific issues remain to be addressed. The two most salient
points are clarifying (and potentially extending) the application of Bayesian statistical methods,
and including a calculation of d-prime in the pre-registered experiment to disambiguate
perceptual sensitivity from response bias. This latter point, especially, is crucial. If the authors are
able to address these points to the editors' satisfication in their next revision, then provisional
acceptance will awarded without further review.
We’re glad that the reviewers felt positive about our revised paper. We have addressed the two
remaining concerns as follow:
<U+F0B7> Bayes: We have added Bayes factors for all analyses reported. We have also tempered our
interpretation of the effects based on the evidence these Bayes factors provide, however we
are confident that our registered study will find less ambiguous evidence for these effects.
<U+F0B7> d-prime: We will now assess familiarity for a subset of the filler trials, which allows us to
analyse d’ for familiar and unfamiliar trials separately. Because our registered study is
powered on an effect size for hit rates, our main analysis will still focus on hits; however, we
will use d’ to confirm that this result is not accounted for by alternate explanations (e.g.,
response bias). Other analyses we have already run (see our response to points 3 & 4 by
Reviewer #4) also allow us to disentangle these explanations.
We hope our paper is now suitable for acceptance. Our point-by-point response is below.
REVIEWER #3
1. I appreciate the authors have gone into a great deal of effort in an attempt to improve the
manuscript and I do believe it is much improved. However, I believe there are areas that still
warrant improvement. I do believe that the lack of meta-data about the images is a concern
(point 11). I realise that it is very difficult to collate this information after collection. I
recommend discussing this fully and highlighting this as a potential issue for future studies.
We agree that having richer metadata on our image set would be beneficial, and are happy to
incorporate this into our discussion.
2. Regarding point 13 - I think that it will be worth collecting more extensive familiarity ratings. I
think the three-point scale for measuring familiarity may be insufficient for more elaborate
and interesting analyses. Potentially a seven-point Likert-type familiarity rating for each face
would allow for further analyses. This follows on from Reviewer 1's second comment and I
would strongly encourage such an analysis. Such an analysis would mean that there would be
less need to exclude participants.
We agree that having a 7-point Likert scale for familiarity would be useful. However, because we did
not have this in our piloting, we would like to keep our main analysis the same: participants will still
be asked to identify each of the faces, and their responses will be coded as familiar or unfamiliar as
in the pilot study. We will then run additional exploratory analyses using the Likert rating as
suggested. We have added the following to the Method of our Registered Study:
“In addition to identifying each of the target faces, participants will be asked to rate their familiarity
with the face on a 7-point Likert scale, from 1 (‘not at all familiar with this person’) to 7 (‘highly
familiar with this person’).”
3. While the authors have carefully chosen the target faces, I wonder if there is potential for a
stimulus-as-fixed-fallacy effect. Would the effects replicate with other faces? I realise the
matching of the demographics is important. This might be worth a comment for future
analyses.
We had thought about randomly selecting target faces for each participant, but in the end decided
against this. Fixing target faces allows us to balance demographics (as the reviewer notes) as well as
allowing us to compare familiarity across participants. We’re happy to comment on this in our
Discussion.
4. Page 14 - The authors claim that they "perform a familiarity check at the end of their
experiment" of their stimuli set - this is slightly misleading as the familiarity rating is only on
the limited number of target faces not on the entire 1000 faces (which would be better, but
the authors point out would not be the most practical method).
This is a fair point, and now that we will also assess familiarity for a subset of fillers (see our
response to point 5 by Reviewer #4), we have revised the relevant sentence to read:
“We perform a familiarity check … to assess each participant’s familiarity with a subset of the stimuli
…”
5. I commend the authors for engaging with the Bayesian statistics, but suggest incorporating
them into every analysis (including the data checks and the F ratios).
Done. We have also clarified in the Method:
“Using the BayesFactor package (Morey & Rouder, 2015), we conducted Bayesian t-tests alongside the
traditional t-tests; F-ratios were first converted to t-values (Dienes, 2014).”
6. The use of Bayesian statistics also revealed a mistake in the interpretation of the results. On
page 18, the authors claim "familiar faces were detected equally well regardless of image"
but the Bayes factor is 0.44. This means there is barely anecdotal evidence for the null
hypothesis. Typically, we would consider evidence for the null hypothesis only to be present if
the Bayes Factor was less than 0.33 and then this is only weak evidence (Dienes, 2014). This
sentence needs to be rewritten. The same issue is present on page 19. Similarly, on page 19,
the Bayes factor for the significant result was 1.66 which is barely anecdotal evidence for the
hypothesis - these results are in no way convincing.
We have added cautionary statements to our analyses where Bayes factors do not provide sufficient
evidence either way. We agree in general that our Bayes factors do not suggest strong evidence for
the null. However, our Bayes factors are consistent with our interpretation of the data, and give
further justification for conducting our registered study. Notably, an effect of zero in our Pilot Study
would have given a Bayes factor of only 0.30; with the larger, preregistered sample, the Bayes
factors are likely to provide less ambiguous evidence for our conclusions (whichever direction they
may go in).
7. In other words, the study is too insensitive to actually provide evidence. Given the importance
of this result to the conclusions of the paper, I think that the conclusions will need to be toned
down considerably and more participants will need to be run than is evidenced by the power
analysis.
We prefer not to use Bayes factors to make dichotomous decisions (e.g., accept/reject the null) but
rather as additional evidence for our conclusions. Our understanding is that Bayes factors and power
analysis come from entirely separate theories of statistics, and so it’s not clear how Bayes factors
would inform calculation of sample size. If we were to use Bayes factor as a decision criterion then
we could use a “stopping rule” whereby we stop data collection once the Bayes factor reaches a
certain level of evidence in either direction (BF > 10, e.g.; Dienes, 2011; Rouder, 2014), but here we
prefer to use standard power analysis to compute the required sample size. We will consider Bayes
factors when we make our final conclusions.
REVIEWER #4
1. The authors have addressed the majority of my concerns. However, below are 2 unresolved
points - one which is minor and would be fine to be addressed in the final paper rather than
immediately (point 4), and the other which requires the authors to record participant
responses and analyze the data differently in order for the results to be interpreted
appropriately (point 5). If the authors are willing to address this last point, I would be happy
to accept this paper (in principle) for publication upon completion. Below are the points in the
context of the previous comments and replies (new comments indicated by **).
Thanks.
2. I agree that this study would make a nice addition to the literature. To clarify, I think that the
benefit of this study can be strengthened if the authors discuss face working memory, and
how matching faces over longer time durations (rather than simultaneous or sequential) has
not been previously tested. I’m happy for this to be passed at this stage, but the final paper
will benefit from including references to relevant studies on face/object working memory. For
example, Jackson & Raymond, 2008, JEP; or Brady, Konkle, Alvarez, & Oliva (2008) which
presented participants with a long series of images and tested participants ability to
remember the images.
Yes, we will refer to these papers in the Discussion.
3. This is fine. However, for the final study, it is important to consider d’primes due to possible
participant response biases. This is important when comparing, not only across conditions,
but also across participants – particularly when one condition is more difficult than another. If
participants in one group have a different response bias to another group (i.e. due to task
difficulty), then this will produce differences when just examining the hit rates.
We agree that d’ offers advantages over hit rates (see our response to point 5). But in terms of task
difficulty, we’d like to emphasise that we attempted to address this in our follow-up analysis using a
median split on performance for vigilance repeats. We found the same pattern of results when the
groups were equated on task performance, suggesting that our findings are not confounded by task
difficulty.
4. For example, using this design, participants can press the space bar throughout the entirety of
the experiment, and the results are recorded as 100%.
This is true, but then the false alarm rate would’ve been 100%, which is not the case (we found
12.5%). Importantly, there was no difference between the groups on false alarm rates, suggesting
that participants were no more likely to falsely detect a repeat when the target images were
different than when they were the same.
5. In instances where this is a possibility, it is important to consider participants' response biases
- in this case, false alarms to the unfamiliar faces. I therefore strongly urge the authors to
collect this data, and analyze the data using d'primes, alongside percent hits. In this
circumstance, there is no benefit to analyzing only the hit rates when you collect your real
data, but rather, the hit rates should be observed in the context of false alarms for the results
to be appropriately interpreted across conditions/participant groups.
Collecting familiarity ratings for all >1,000 images after an already long and demanding experiment
doesn’t seem a good idea, so we came up with an alternative solution: We will assess familiarity for
a fixed, pre-randomised set of 100 of the filler faces, in addition to the 100 target faces. This will
allow us to calculate d’ for familiar and unfamiliar faces separately, without adding too much work
for our participants and risking a decrease in performance quality. We have added this to the
Method of our Registered Study:
“So that we can calculate false alarm rates and d’ for familiar and unfamiliar faces separately, we will
also ask participants to identify 100 filler faces. The faces will be preselected from the set of 881
images that were shown only once in the experiment, such that they match the overall demographics
(as for target faces).”
We have also adjusted our exclusion criteria:
“We will exclude participants who are familiar with <10% of either target or filler faces, …”
References
Dienes, Z. (2011). Bayesian versus orthodox statistics: which side are you on? Psychological Science,
6(3), 274-290.
Rouder, J. N. (2014). Optional stopping: no problem for Bayesians. Psychonomic Bulletin & Review,
21, 301-308.
Appendix C
ASSOCIATE EDITOR
The Stage 2 submission was returned to three of the original reviewers who provided
assessments at Stage 1. All are positive and the extent of required revisions is
straightforward. Reviewer 3 is entirely satisfied, while Reviewer 1 notes some specific
claims that require further clarification and justification. Reviewer 2 offers a more
extensive critique, suggesting an explanation for the observed negative results and
questioning the inclusion some exploratory analyses. The reviewer (in their point 3) also
questions the rationale of the hypothesis. In responding specifically to this issue, please be
sure to confine all consideration of this point to the Discussion section; for a Registered
Report, relitigation of study rationale at Stage 2 is only appropriate in the Discussion and
must not lead to changes in the framing or presentation of the hypotheses in the
Introduction.
Thank you for yours and the reviewers’ comments and feedback on the manuscript. We
have addressed each of the comments below. In response to Reviewer 2’s point 5, we have
revisited the relevant points in discussion while leaving our introduction and hypotheses
unchanged.
REVIEWER #1
1. I find it incredibly interesting that you were ultimately not able to replicate the finding
in your pilot study and suggested by prior work, that while repetition detection
performance across different image types for unfamiliar faces should be lower than for
same images, this difference should not show for familiar faces. Pre-registering the
study (combined with your power analyses, large sample size, and Bayesian statistics)
make this null finding particularly compelling, and I believe this result to be of
importance to the face memory community. This sort of result is a great example of
the importance of pre-registration, as it encourages researchers to report null results
and gives us a better look into phenomena previously thought to be incredibly robust.
I have a few comments on items I'd like clarification on, or to see edits for.
Thank you very much for your kind words.
2. When doing the analyses using the 7-point rating scale of familiarity, if I understand
correctly (the grammar is a bit confusing in that sentence), you include both self-
reported and coder-scored familiarity in the binomial mixed-effects model. You then
seem to describe it as coder-scored familiarity fully accounting for the effects
ultimately found in the model, and that self-reported familiarity has no relation to
ultimate recognition. Does a correlation between self-reported familiarity and
recognition rate come out as non-significant as well (i.e., I'd like to see a statistic
specifically supporting the claim in Figure 7)?
Yes, Figure 7 shows there was no relationship between self-reported familiarity and
detection performance. In the model, self-reported familiarity was added as the first
effect, so the lack of significance is not due simply to the effects being accounted for by
the coder-scored familiarity or image type. We realize this was not clear in the
manuscript, so we have rearranged the text slightly to clarify this (p. 8).
“Fixed effects were included for self-reported familiarity, as well as the main effects for and
interaction between coder-scored familiarity and image condition (similar to the models used for hit
rate and d’).”
We have also added a table to the supplementary materials showing the successive
model comparisons, including BIC and chi-squared tests.
The mixed-effects model essentially shows the correlation between self-reported
familiarity and repeat detection in a trial-by-trial manner, accounting for within-subjects
variability. If the reviewer is instead asking for a correlation between individual’s mean
reported familiarity and repeat detection across the entire experiment, there is also no
relationship, r(122) = -.039, p = .669. While this correlation is consistent with the mixed-
effects model, we find it less convincing because it averages out any differences in
detection that might be driven by self-reported familiarity with specific images.
3. In the discussion, make more explicit the link you are hypothesizing between familiar
face recognition and task demands ("familiar face recognition has limits" is a bit
vague).
We have adjusted the sentence indicated to make our proposal clearer to the reader. It
now reads (p. 9):
“If correct, our finding suggests that the robustness of familiar face recognition may falter under
more demanding tasks.”
4. I don't quite understand your possible semantic description in the discussion. If
participants are using semantic information for the familiar faces (i.e., remembering
the name of the celebrity), shouldn't you predict there to be no difference in
recognition rate for the same image versus different image conditions?
If individuals were solely using semantic information for familiar faces, then yes,
performance would be the same regardless of image. However, we consider it more
likely that semantic information is used in addition to visual codes. This view suggests
that semantic information would enhance overall performance for familiar faces relative
to unfamiliar faces without any interaction with image conditions, which is what we
found. However, we see now that this argument is actually better placed as a potential
limitation of our study in that the use of semantic information by the participants may
limit conclusions about pictorial and structural codes. We have now moved this point
further down alongside other limitations (p. 10):
“Finally, familiar face recognition might rely not only on visual information, but also semantic
knowledge associated with the face (e.g., name, occupation). Because semantic information is only
available to familiar faces, they are better detected overall than unfamiliar faces. However,
detection still makes use of visual codes, which explains why we see better performance with same
than different images. This interpretation suggests an important role of semantic knowledge in
studies of familiar vs unfamiliar face recognition, an important area for future work.”
REVIEWER #2
1. This registered report describes a single study that used a repeat-detection task. The
study examines the important question of whether repeat detection of faces presented
in a continuous stream is facilitated when precisely the same image of a face is
repeated, relative to when a different image of the same face is repeated. The critical
test in this paper is whether this facilitation interacts with the familiarity of a face. This
is a theoretically important test as it has a bearing on the differences in how familiar
and unfamiliar faces are represented.
The proposal that familiar face perception is relatively less reliant on image-level
representation than unfamiliar face perception has been influential in understanding
the cognitive processes underlying face perception, and there is substantial evidence
from face matching tasks to support this view. The results of this experiment show that
image changes had a similar effect on familiar and unfamiliar repetition detection
task, suggesting that perception of familiar and unfamiliar faces is equally reliant on
image-level details.
Overall this is a neat study and the report is very well written. It makes an important
contribution to the literature on this topic. I have some points that should be relatively
straightforward to address in revision.
Thank you very much for your commendation.
2. One potential reason why the critical interaction was not observed are the floor effects
that are observed in the Unfamiliar face, different-image condition. This is reflective of
the difficulty in developing a task for which the measurement scale enables a
researcher to capture the full range of variance observed on familiar and unfamiliar
face processing tasks (unfamiliar face processing is poor, familiar is much better, and
so it is difficult to create a scale that captures both). This difficulty is compounded by
the fact that there are large individual differences in face processing ability.
This challenge is by no means unique to this study, and similar floor/ ceiling effects are
apparent in other studies in this literature. Nevertheless, I think this should be
acknowledged directly in the manuscript. The exploratory analysis shown in Figure 4
goes some way to revolve this, but the unfamiliar distributions do not look normal in
this analysis either.
We agree that performance in the unfamiliar/different condition might be limited by
floor effects and have now addressed this point in discussion (p. 10):
“Our ability to identify an interaction between familiarity and image type might have been limited
by potential floor effects in the unfamiliar/different condition, where performance was particularly
low. Our analyses on the subset of participants equated for vigilance and on d’ account for this
somewhat, as the floor effects were less apparent (see Figures 4 & 5). This analysis produced a
similar pattern of results to our main analysis. Floor effects aside, it is important to note that we
found a large effect of image type for familiar faces. This finding is inconsistent with the notion that
familiar face recognition is robust regardless of image.”
3. There were many analyses performed on the data. This vigilant approach to
confirming their null interaction result is a key strength of the paper, but the lengthy
results section could perhaps be streamlined by moving some analysis to
supplementary materials.
While we appreciate the reviewer’s concern about the length of the results section, we
believe that each of the analyses produce results that might be interesting to readers.
Additionally, each of the analyses are the result of the Stage 1 review process: the
addition of Bayes factors, ratings for filler faces, and the rating scale were all suggestions
by reviewers, while the relationship between face knowledge and repeat detection
performance was one exploratory analysis we proposed during Stage 1. For this reason,
we are unsure which, if any, of these analyses should be removed, or shifted to
supplementary materials. We would appreciate the editor’s guidance on this decision.
4. In particular, the value of the d-prime analysis was not clear to me. False alarm rates
in this analysis are filler images incorrectly identified as repeats. Why would one
expect performance on these items to differ in the same image/ different image
condition? The same/different image condition does not appear to be relevant to
these items, as this is the first time that the Ps in both conditions had seen filler faces.
Because I was unclear on the mechanism by which the experimental manipulation
could affect false alarm rates, I did not learn much from this analysis.
Because we manipulated image type between-subjects, we wanted to be sure that the
groups did not have different decision criteria as reflected by different false alarm rates,
which might affect our interpretation of hit rates. False alarm rates did not differ across
groups, but the interaction showed that false alarms were most common for familiar
faces in the different image group. We think the simplest explanation is that participants
in this condition were aware that repeated images were not identical to the original, and
so were more likely to respond to faces that were related (visually or semantically) to an
actual target face, mistaking this familiarity for a repeat.
5. In my previous review of this paper (pre-registration), I mentioned repetition priming
effects as being relevant literature in relation to the author's research question. On
reviewing these comments I am concerned that I was not specific enough, as the
authors appear to have misinterpreted the relevance of this work to their research in
the full manuscript.
I did not mean that to say that the repeat detection method is like a repetition priming
experiment (it may be in some ways, but also different in other key aspects). Rather, I
drew attention to the findings reported by Johnston and Barry because they are
relevant here. They show what is known in priming literature as 'graded similarity
effects' (see also Ellis, Young, Flude & Hay, 1987). These effects index facilitation of
face identification by prior exposure to an image of the same face (i.e. repetition
priming strength), as a graded function of the similarity between the prime image and
the test image. When an identical image is used at prime and test, the priming effects
are stronger than when different images are used. Critically for the author's study,
graded similarity is found for familiar faces. This basic phenomenon has been shown in
other strands of research (for example, iconic images of familiar faces are recognised
better), and together they show that familiar face recognition is sensitive to image-
level properties.
Based on these studies, one may predict that detection of familiar face repetitions
would be facilitated by repeats of identical images. So, it is possible that the
hypothesis tested in this study was based too narrowly on evidence from studies of
face matching.
We thank the reviewer for this helpful clarification. We have added a section to the
discussion relating to the literature of repetition priming (p. 10):
“Instead, this finding [the difference in repeat detection for same/different images of a
familiar face] is more consistent with those from repetition priming studies, where image
similarity between the first and repeated presentation of a face predicts participants’ ability to
recognise the face (Ellis, Young, Flude, & Hay, 1987; Johnston & Barry, 2001). Our study supports
these findings, suggesting that image-level differences contribute to familiar face recognition.”
6. P11, line 22 -- Not clear from the start of this section how face knowledge is
operationalised - should state that it is the percentage of faces recognised at the
beginning of section.
Thank you for pointing this out. We have added in a sentence operationalizing face
knowledge to make this clear to the reader (p. 8):
“A participant’s ‘face knowledge’ was defined as the percentage of faces they recognized in the
familiarity check.”
REVIEWER #3
1. The exploratory analyses are a really positive addition to the planned analysis and the
explanation of the results is entirely appropriate.
Thank you for your positive response.
Society Open
