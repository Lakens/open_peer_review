Calibration with confidence: a principled method for panel
assessment
R. S. MacKay, R. Kenna, R. J. Low and S. Parker
Article citation details
R. Soc. open sci. 4: 160760.
http://dx.doi.org/10.1098/rsos.160760
Review timeline
Original submission: 29 September 2016 Note: Reports are unedited and appear as
Revised submission: 7 January 2017 submitted by the referee. The review history
Final acceptance: 9 January 2017 appears in chronological order.
Review History
label_version_1
RSOS-160760.R0 (Original submission)
label_author_1
Review form: Reviewer 1
Is the manuscript scientifically sound in its present form?
Yes
Are the interpretations and conclusions justified by the results?
Yes
Is the language acceptable?
Yes
Is it clear how to access all supporting data?
Yes
Do you have any ethical concerns with this paper?
No
Have you any concerns about statistical analyses in this paper?
I do not feel qualified to assess the statistics
© 2017 The Authors. Published by the Royal Society under the terms of the Creative Commons
Attribution License http://creativecommons.org/licenses/by/4.0/, which permits unrestricted use,
provided the original author and source are credited
2
Recommendation?
label_recommendation_1
Accept with minor revision (please list in comments)
Comments to the Author(s)
label_comment_1
Please see attached file. (Appendix A)
label_author_2
Review form: Reviewer 2 (Adrian Barnett)
Is the manuscript scientifically sound in its present form?
Yes
Are the interpretations and conclusions justified by the results?
Yes
Is the language acceptable?
Yes
Is it clear how to access all supporting data?
Yes
Do you have any ethical concerns with this paper?
No
Have you any concerns about statistical analyses in this paper?
No
Recommendation?
label_recommendation_2
Accept with minor revision (please list in comments)
Comments to the Author(s)
label_comment_2
I enjoyed reading this paper which tackles an issue of great importance to science, but where
there is surprisingly little research. The paper made a good argument for investigating this area
and the flow of ideas and presentation of results was very clear. The simulation and case studies
were appropriate and interesting.
The overall message is that adding reviewers' confidence has added useful information and
improved the reliability of rankings. This is not surprising given the anecdotes about people
reviewing outside their field (Malice's wonderland: research funding and peer review. Journal of
neurobiology, Vol. 14, No. 2. (March 1983), pp. 95-112), which becomes worse with conflicts of
interest often causing those familiar with the field to have to recuse.
One issue that this approach may well encounter is an over-confident reviewer who gives a
sigma_ao as zero (or close to zero). It may be worth recommending a sensitivity analysis with
sigma_ao^* = max(f, sigma_ao) where f is some pre-specified constant.
When comparing the three systems (Figure 5) it would be better to use the Bland-Altman
agreement plots rather than correlation.
3
Minor comments
- Introduction, 1st paragraph. Another approach I've experienced is that grant reviewers are only
allowed to give the same number of low, medium and high scores, which forces them to have
some spread and creates a balanced spread across reviewers.
- Introduction, 1st paragraph. The difference is standard between reviewers should be small for
large samples.
- Introduction, last paragraph. We did an unpublished experiment on grant reviewers' confidence
https://eprints.qut.edu.au/77513/
- There is a paper on the ideal number of reviewers:
http://journals.plos.org/plosone/article?id=10.1371/journal.pone.0120838
- Methods. What about using a Bayesian model which would allow shrinkage in b_a?
- The need for a highly connected network (last paragraph page 6) is useful information for
funding agencies. However, a well designed network can become "moth-eaten" by conflicts of
interest.
- Results or Discussion. It might be worth noting the opportunity costs of adding more reviewers.
More reviewers also adds to the burden of the funding agencies, who already struggle to find
reviewers: doi:10.1186/1741-7015-8-62
- Discussion. The most interesting alternative approach I have seen lately is to include the
variance in reviewers' scores as well as the mean: doi:10.1016/j.respol.2016.07.004
- Discussion. It may be worth mentioning this paper on using training to improve reviews
oi:10.1371/journal.pone.0130450
- Discussion. The ultimate test of the method could be an experiment where deliberate errors are
inserted into applications and reviewers randomised to receive different versions of the same
grant as this creates a known truth
label_author_3
Review form: Reviewer 3 (Julia Higgins)
Is the manuscript scientifically sound in its present form?
Yes
Are the interpretations and conclusions justified by the results?
No
Is the language acceptable?
Yes
Is it clear how to access all supporting data?
Yes
Do you have any ethical concerns with this paper?
No
Have you any concerns about statistical analyses in this paper?
I do not feel qualified to assess the statistics
Recommendation?
label_recommendation_3
Accept with minor revision (please list in comments)
Comments to the Author(s)
label_comment_3
See attached files (Appendix B).
4
label_end_comment
Decision letter (RSOS-160760)
20-Dec-2016
Dear Dr Kenna
On behalf of the Editors, I am pleased to inform you that your Manuscript RSOS-160760 entitled
"Calibration with confidence: A robust and efficient method for panel assessment" has been
accepted for publication in Royal Society Open Science subject to minor revision in accordance
with the referee suggestions. Please find the referees' comments at the end of this email.
The reviewers and handling editors have recommended publication, but also suggest some minor
revisions to your manuscript. Therefore, I invite you to respond to the comments and revise your
manuscript.
• Ethics statement
If your study uses humans or animals please include details of the ethical approval received,
including the name of the committee that granted approval. For human studies please also detail
whether informed consent was obtained. For field studies on animals please include details of all
permissions, licences and/or approvals granted to carry out the fieldwork.
• Data accessibility
It is a condition of publication that all supporting data are made available either as
supplementary information or preferably in a suitable permanent repository. The data
accessibility section should state where the article's supporting data can be accessed. This section
should also include details, where possible of where to access other relevant research materials
such as statistical tools, protocols, software etc can be accessed. If the data has been deposited in
an external repository this section should list the database, accession number and link to the DOI
for all data from the article that has been made publicly available. Data sets that have been
deposited in an external repository and have a DOI should also be appropriately cited in the
manuscript and included in the reference list.
If you wish to submit your supporting data or code to Dryad (http://datadryad.org/), or modify
your current submission to dryad, please use the following link:
http://datadryad.org/submit?journalID=RSOS&manu=RSOS-160760
• Competing interests
Please declare any financial or non-financial competing interests, or state that you have no
competing interests.
• Authors’ contributions
All submissions, other than those with a single author, must include an Authors’ Contributions
section which individually lists the specific contribution of each author. The list of Authors
should meet all of the following criteria; 1) substantial contributions to conception and design, or
acquisition of data, or analysis and interpretation of data; 2) drafting the article or revising it
critically for important intellectual content; and 3) final approval of the version to be published.
All contributors who do not meet all of these criteria should be included in the
acknowledgements.
We suggest the following format:
AB carried out the molecular lab work, participated in data analysis, carried out sequence
alignments, participated in the design of the study and drafted the manuscript; CD carried out
the statistical analyses; EF collected field data; GH conceived of the study, designed the study,
5
coordinated the study and helped draft the manuscript. All authors gave final approval for
publication.
• Acknowledgements
Please acknowledge anyone who contributed to the study but did not meet the authorship
criteria.
• Funding statement
Please list the source of funding for each author.
Please note that we cannot publish your manuscript without these end statements included. We
have included a screenshot example of the end statements for reference. If you feel that a given
heading is not relevant to your paper, please nevertheless include the heading and explicitly state
that it is not relevant to your work.
Because the schedule for publication is very tight, it is a condition of publication that you submit
the revised version of your manuscript within 7 days (i.e. by the 29-Dec-2016). If you do not think
you will be able to meet this date please let me know immediately.
To revise your manuscript, log into https://mc.manuscriptcentral.com/rsos and enter your
Author Centre, where you will find your manuscript title listed under "Manuscripts with
Decisions". Under "Actions," click on "Create a Revision." You will be unable to make your
revisions on the originally submitted version of the manuscript. Instead, revise your manuscript
and upload a new version through your Author Centre.
When submitting your revised manuscript, you will be able to respond to the comments made by
the referees and upload a file "Response to Referees" in "Section 6 - File Upload". You can use this
to document any changes you make to the original manuscript. In order to expedite the
processing of the revised manuscript, please be as specific as possible in your response to the
referees.
When uploading your revised files please make sure that you have:
1) A text file of the manuscript (tex, txt, rtf, docx or doc), references, tables (including captions)
and figure captions. Do not upload a PDF as your "Main Document".
2) A separate electronic file of each figure (EPS or print-quality PDF preferred (either format
should be produced directly from original creation package), or original software format)
3) Included a 100 word media summary of your paper when requested at submission. Please
ensure you have entered correct contact details (email, institution and telephone) in your user
account
4) Included the raw data to support the claims made in your paper. You can either include your
data as electronic supplementary material or upload to a repository and include the relevant doi
within your manuscript
5) All supplementary materials accompanying an accepted article will be treated as in their final
form. Note that the Royal Society will neither edit nor typeset supplementary material and it will
be hosted as provided. Please ensure that the supplementary material includes the paper details
where possible (authors, article title, journal name).
Supplementary files will be published alongside the paper on the journal website and posted on
the online figshare repository (https://figshare.com). The heading and legend provided for each
supplementary file during the submission process will be used to create the figshare page, so
please ensure these are accurate and informative so that your files can be found in searches. Files
on figshare will be made available approximately one week before the accompanying article so
that the supplementary material can be attributed a unique DOI.
6
Once again, thank you for submitting your manuscript to Royal Society Open Science and I look
forward to receiving your revision. If you have any questions at all, please do not hesitate to get
in touch.
Kind regards,
Andrew Dunn
Senior Publishing Editor
Royal Society Open Science
on behalf of Mark Chaplain
Subject Editor, Royal Society Open Science
openscience@royalsociety.org
Associate Editor Comments to Author:
Please revise the manuscript to address the comments of the three reviewers.
Reviewer comments to Author:
Reviewer: 1
Comments to the Author(s)
Please see attached file.
Reviewer: 2
Comments to the Author(s)
I enjoyed reading this paper which tackles an issue of great importance to science, but where
there is surprisingly little research. The paper made a good argument for investigating this area
and the flow of ideas and presentation of results was very clear. The simulation and case studies
were appropriate and interesting.
The overall message is that adding reviewers' confidence has added useful information and
improved the reliability of rankings. This is not surprising given the anecdotes about people
reviewing outside their field (Malice's wonderland: research funding and peer review. Journal of
neurobiology, Vol. 14, No. 2. (March 1983), pp. 95-112), which becomes worse with conflicts of
interest often causing those familiar with the field to have to recuse.
One issue that this approach may well encounter is an over-confident reviewer who gives a
sigma_ao as zero (or close to zero). It may be worth recommending a sensitivity analysis with
sigma_ao^* = max(f, sigma_ao) where f is some pre-specified constant.
When comparing the three systems (Figure 5) it would be better to use the Bland-Altman
agreement plots rather than correlation.
Minor comments
- Introduction, 1st paragraph. Another approach I've experienced is that grant reviewers are only
allowed to give the same number of low, medium and high scores, which forces them to have
some spread and creates a balanced spread across reviewers.
- Introduction, 1st paragraph. The difference is standard between reviewers should be small for
large samples.
- Introduction, last paragraph. We did an unpublished experiment on grant reviewers' confidence
https://eprints.qut.edu.au/77513/
- There is a paper on the ideal number of reviewers:
http://journals.plos.org/plosone/article?id=10.1371/journal.pone.0120838
- Methods. What about using a Bayesian model which would allow shrinkage in b_a?
7
- The need for a highly connected network (last paragraph page 6) is useful information for
funding agencies. However, a well designed network can become "moth-eaten" by conflicts of
interest.
- Results or Discussion. It might be worth noting the opportunity costs of adding more reviewers.
More reviewers also adds to the burden of the funding agencies, who already struggle to find
reviewers: doi:10.1186/1741-7015-8-62
- Discussion. The most interesting alternative approach I have seen lately is to include the
variance in reviewers' scores as well as the mean: doi:10.1016/j.respol.2016.07.004
- Discussion. It may be worth mentioning this paper on using training to improve reviews
oi:10.1371/journal.pone.0130450
- Discussion. The ultimate test of the method could be an experiment where deliberate errors are
inserted into applications and reviewers randomised to receive different versions of the same
grant as this creates a known truth
Reviewer: 3
Comments to the Author(s)
See attached files
Author's Response to Decision Letter for (RSOS-160760)
See Appendix C.
label_end_comment
Decision letter (RSOS-160760.R1)
09-Jan-2017
Dear Dr Kenna,
I am pleased to inform you that your manuscript entitled "Calibration with confidence: A
principled method for panel assessment" is now accepted for publication in Royal Society Open
Science.
You can expect to receive a proof of your article in the near future. Please contact the editorial
office (openscience_proofs@royalsociety.org and openscience@royalsociety.org) to let us know if
you are likely to be away from e-mail contact. Due to rapid publication and an extremely tight
schedule, if comments are not received, your paper may experience a delay in publication.
Royal Society Open Science operates under a continuous publication model
(http://bit.ly/cpFAQ). Your article will be published straight into the next open issue and this
will be the final version of the paper. As such, it can be cited immediately by other researchers.
As the issue version of your paper will be the only version to be published I would advise you to
check your proofs thoroughly as changes cannot be made once the paper is published.
In order to raise the profile of your paper once it is published, we can send through a PDF of your
paper to selected colleagues. If you wish to take advantage of this, please reply to this email with
the name and email addresses of up to 10 people who you feel would wish to read your article.
8
On behalf of the Editors of Royal Society Open Science, we look forward to your continued
contributions to the Journal.
Best wishes,
Alice Power
Editorial Coordinator
Royal Society Open Science
openscience@royalsociety.org
Appendix A
Calibration with confidence: a robust and efficient method for panel
assessment
by R.S. MacKay, S. Parker, R. Kenna & R.J. Low.
General comments
There is no doubt at all that this paper is addressing an interesting and
important question. I am not qualified to assess the originality/ validity of
the statistics and so I leave that to others. I have some questions and com-
ments for the authors which I believe, if addressed, would greatly increase
the impact. They could be summarised by saying that I think it is worth
being really honest about, and discussing, the shortcomings of the approach
(not least as I suspect that you have a proposed workaround for many of
them). Even if the principal effect of the paper is to provide a starting point
for e.g. REF panels in thinking about calibration, that would be a major
contribution.
1. I think you can be more upfront about the limitations of your ap-
proach. For example, it is probably never going to be realistic to
expect that you will have a sufficiently highly connected graph to ap-
ply to funding panels for grants (at least in the UK), but the other
examples such as RAE/REF are more than enough in their own right.
2. The potential readership for a paper like this is very big - this is a
question that affects us all. In view of that, a brief explanation of
Fisher’s IBA would be very valuable.
3. I lack conviction that it is realistic to train assessors in assigning con-
fidence scores (alluded to at the bottom of page 4). Your work-around
for high-medium-low should be included in the main text. There are,
of course, major differences not only between research fields, but I
would expect also between genders, in the way that individuals assess
their confidence levels. Couldn’t this skew things quite considerably?
4. I would conjecture that low or medium confidence is accompanied by a
regression to the mean - assessors only give very high or very low scores
if they are confident in their views. So will bias be correlated with
confidence. How robust is the model to changes like that? Also, do
you have a proposal for dealing with the issue that very often different
panel members use different proportions of the available scale?
5. You look at robustness to small changes in scores in Appendix C,
but how does it behave with respect to outliers? Maybe also some
discussion earlier about the choice of least squares (beyond it leads
to a neat solution). Exercises like the REF use much coarser scores,
so that small changes don’t really exist (and weird things happen at
boundaries!). It would be nice to see examples with that sort of data.
6. At the bottom of page 8, there seems to be an implication that if
we are interested in maximum errors, until there are six assessors per
object, the improvements obtained by using CWC in place of IBA are
marginal, but at the bottom of page 10 the improvements in mean
errors seem to be independent of the number of assessors. Maybe a
small discussion of, for example, what that might mean for a REF
panel (where double reading is already a huge burden).
7. Page 14, there is a remark that improvements would require assessors
to evaluate more than two proposals each. I very much doubt that
there is any robust approach to calibration if assessors are only eval-
uating two proposals! I find myself unsure what to think about this
data set.
8. In Section 4.3, I did not understand why CATS points would be a
surrogate for confidences. (Indeed, I could only guess what CATS
points were...).
9. The discussion could be much more thorough. I think that the tradeoff
between levels of confidence/expertise and connectivity of the graph is
an important one to draw out and explore - in particular, panels must
be aware that the design of the calibration exercise has to take place
before the evaluations. It doesn’t do the question justice to defer it to
an Appendix (where it receives a very cursory treatment).
10. I found the last paragraph of the discussion confusing. Should I be
thinking, for example, of the discontinuities that might arise if a math-
ematical sciences panel were to split the assessment into ‘pure’, ‘ap-
plied’ and ‘statistics’ ?
11. As you say, when applied to real data, the usefulness of your approach
was not so clear - this also reflects the need to design the calibration
in advance, whereas you, necessarily, were doing it retrospectively.
Nonetheless, I think your analysis points to clear advantages of at
least adopting Fisher’s approach (and there is no shame in publishing
a conclusion that Fisher was right and his method should be more
widely adopted!).
Appendix B
This paper reports a statistical method for accounting for multiple reviewers/assessors with varying
levels of confidence in the relevant field. It is a problem with which I am very familiar. The authors
compare four different approaches from simple averaging to their favoured algorithm for calibrating
the confidence level to be attributed to each assessor. They then test the methods against each other
on, firstly, some computer generated data, and then on some real but anonymised data.
The paper shows very clearly the variability in score which follows on the different averaging
procedures, and for this alone I think it is worth publishing. It presents a real caveat with respect to
attributing too much certainty to assessor scores.
Unfortunately the title implies that the favoured algorithm, which they call “Calibration with
Confidence” (CWC) is “best”, however in the test case the CWC does not show any great advantage
over simpler calibration methods. The authors state this quite clearly in the discussion section:
“A test on some real data was only marginally conclusive, however suggesting that the assessment
procedure for that context needed more robust design.”
I also note that using this method will require more work from the assessors in that they have to make
a numerical estimate of their confidence in the field being assessed. In my experience this will very
likely lead to referees rejecting the request to asses. Not because they object but because they are
usually very busy people!
While recommending that the paper be published, assuming that a reviewer with the relevant expertise
in statistical methods is happy, I would request that the title uses a more measured statement such as
“a potentially robust and efficient calibration method”, and that the abstract includes comments on the
fact that the robustness and efficiency are only marginally demonstrated by the test cases.
Thanks for your e-mail and the comments of the Referees on our manuscript RSOS-160760 . We are
pleased with the very positive reviews. You asked for minor revisions and we have made these. Here are
our detailed responses.
Reviewer 1- General comment: “I think it is worth being really honest about, and discussing, the
shortcomings of the approach”
Our Response: We have added a substantial discussion of the shortcomings to the Discussion section.
Reviewer 1- Detailed Comment 1: I think you can be more upfront about the limitations of your
approach. For example, it is probably never going to be realistic to expect that you will have a
sufficiently highly connected graph to apply to funding panels for grants (at least in the UK), but the
other examples such as RAE/REF are more than enough in their own right.
EPSRC panels require each application to be scored by 3 panel members and they attempt to make the
graph fairly well connected by assigning one of the scorers to be a “generalist”. We refrain from
discussing the cases of individual funding agencies in this paper, fearing that such a discussion may
obfuscate the main message. We will discuss matters such as this (RAE, REF, EPSRC) in a less technical
follow-on paper.
Reviewer 1- Detailed Comment 2. The potential readership for a paper like this is very big - this is a
question that affects us all. In view of that, a brief explanation of Fisher’s IBA would be very valuable.
We added some text on page 3.
Reviewer 1- Detailed Comment 3. I lack conviction that it is realistic to train assessors in assigning
confidence scores (alluded to at the bottom of page 4). Your work-around for high-medium-low should
be included in the main text. There are, of course, major differences not only between research fields,
but I would expect also between genders, in the way that individuals assess their confidence levels.
Couldn’t this skew things quite considerably?
We agree that the Referee has a point here. But it is an issue for further research. Indeed, an
exploration of gender differences would be interesting but perhaps it is rather an issue for psychologists
or sociologists. We do hope that our paper may inspire such research. Notwithstanding this, we have
moved forward text about HML between Eqs.(3) and (4). The question of training assessors to assign
uncertainties is addressed by the reference we cite.
Reviewer 1- Detailed Comment 4. I would conjecture that low or medium confidence is accompanied by
a regression to the mean - assessors only give very high or very low scores if they are confident in their
views. So will bias be correlated with confidence. How robust is the model to changes like that? Also, do
you have a proposal for dealing with the issue that very often different panel members use different
proportions of the available scale?
These issues are already commented on in App F, but we’ve expanded slightly.
Reviewer 1- Detailed Comment 5. You look at robustness to small changes in scores in Appendix C, but
how does it behave with respect to outliers? Maybe also some discussion earlier about the choice of least
squares (beyond it leads to a neat solution). Exercises like the REF use much coarser scores, so that
small changes don’t really exist (and weird things happen at boundaries!). It would be nice to see
examples with that sort of data.
We have added a small discussion about outliers and have pointed out the problem that some
organisations insist on a discrete scale. But we defer more substantial discussion of issues like these to a
follow-on paper.
Reviewer 1- Detailed Comment 6. At the bottom of page 8, there seems to be an implication that if we
are interested in maximum errors, until there are six assessors per object, the improvements obtained
by using CWC in place of IBA are marginal, but at the bottom of page 10 the improvements in mean
what that might mean for a REF panel (where double reading is already a huge burden).
Again, we prefer to discuss the peculiarities of REF in a different paper.
Reviewer 1- Detailed Comment 7. Page 14, there is a remark that improvements would require
assessors to evaluate more than two proposals each. I very much doubt that there is any robust
approach to calibration if assessors are only evaluating two proposals! I find myself unsure what to think
about this data set.
Sorry, there was a typo here. Instead of 2 proposals per assessor we meant 2 assessors per proposal.
We have corrected it now (page 13).
Reviewer 1- Detailed Comment 8. In Section 4.3, I did not understand why CATS points would be a
surrogate for confidences. (Indeed, I could only guess what CATS points were...).
We added explanatory text here.
Reviewer 1- Detailed Comment 9. The discussion could be much more thorough. I think that the tradeoff
between levels of confidence/expertise and connectivity of the graph is an important one to draw out
and explore - in particular, panels must be aware that the design of the calibration exercise has to take
place before the evaluations. It doesn’t do the question justice to defer it to an Appendix (where it
receives a very cursory treatment).
We have made the Discussion more thorough. Design of the assessment graph, however, is a
substantial question that needs a separate paper.
Reviewer 1- Detailed Comment 10. I found the last paragraph of the discussion confusing. Should I be
thinking, for example, of the discontinuities that might arise if a mathematical sciences panel were to
split the assessment into ‘pure’, ‘applied’ and ‘statistics’?
Yes. There is clear evidence that, at least at RAE2008, different panels had different standards – see,
e.g., the new Ref.[20]. Our approach takes advantage of cross-panel referring to infer relative standards
and normalise them. We have added text along these lines (and cited Ref.[20]).
Reviewer 1- Detailed Comment 11. As you say, when applied to real data, the usefulness of your
approach was not so clear - this also reflects the need to design the calibration in advance, whereas you,
necessarily, were doing it retrospectively. Nonetheless, I think your analysis points to clear advantages
of at least adopting Fisher’s approach (and there is no shame in publishing a conclusion that Fisher was
right and his method should be more widely adopted!).
Yes. We have added a remark to this effect.
Reviewer: 2
Reviewer 2- Main Comment 1. One issue that this approach may well encounter is an over-confident
reviewer who gives a sigma_ao as zero (or close to zero). It may be worth recommending a sensitivity
analysis with sigma_ao^* = max(f, sigma_ao) where f is some pre-specified constant.
The over-confident reviewer issue may be a problem in practice, though we’d like to think it is addressed
by training in estimating uncertainties. See our response to Point 3 of Referee 1.
Reviewer 2- Main Comment 2. When comparing the three systems (Figure 5) it would be better to use
the Bland-Altman agreement plots rather than correlation.
We include this in the revised version.
Minor comments
Reviewer 2- Minor Comment 1. Introduction, 1st paragraph. Another approach I've experienced is that
grant reviewers are only allowed to give the same number of low, medium and high scores, which forces
Forcing reviewers to give same numbers of HML scores is akin to the dangers of SA, so bias removal is
still required. But it might help make (48) less necessary.
Reviewer 2- Minor Comment 2. Introduction, 1st paragraph. The difference is standard between
reviewers should be small for large samples.
Difference in standards small for large samples? No, unless the previous tactic is used.
Reviewer 2- Minor Comment 3. Introduction, last paragraph. We did an unpublished experiment on
grant reviewers' confidence https://eprints.qut.edu.au/77513/
Interesting, though not clear how to fit this in. But we have added a citation to another of your papers
that fits in well for another point.
Reviewer 2- Minor Comment 4. There is a paper on the ideal number of reviewers:
http://journals.plos.org/plosone/article?id=10.1371/journal.pone.0120838
We cite this now – thanks!
Reviewer 2- Minor Comment 5. Methods. What about using a Bayesian model which would allow
shrinkage in b_a?
We are sorry but we don’t understand what is meant here.
Reviewer 2- Minor Comment 6. The need for a highly connected network (last paragraph page 6) is
useful information for funding agencies. However, a well designed network can become "moth-eaten" by
conflicts of interest.
Indeed, this is one of the things we meant by “constraints” so we’ve made it more explicit.
Reviewer 2- Minor Comment 7. Results or Discussion. It might be worth noting the opportunity costs of
adding more reviewers. More reviewers also adds to the burden of the funding agencies, who already
struggle to find reviewers: doi:10.1186/1741-7015-8-62
Yes, adding more reviewers is not without cost!
Reviewer 2- Minor Comment 8. Discussion. The most interesting alternative approach I have seen lately
is to include the variance in reviewers' scores as well as the mean: doi:10.1016/j.respol.2016.07.004
An interesting paper! Not clear to us where to link it though.
Reviewer 2- Minor Comment 9. Discussion. It may be worth mentioning this paper on using training to
improve reviews oi:10.1371/journal.pone.0130450
Thanks, we’ve added a reference to it.
Reviewer 2- Minor Comment 10. Discussion. The ultimate test of the method could be an experiment
where deliberate errors are inserted into applications and reviewers randomised to receive different
versions of the same grant as this creates a known truth
Yes, could be a good test. Again, we think this is best dealt with in a future publication.
Reviewer: 3
Reviewer 3- Comment 1. Unfortunately the title implies that the favoured algorithm, which they call
“Calibration with Confidence” (CWC) is “best”, however in the test case the CWC does not show any
great advantage over simpler calibration methods…. I would request that the title uses a more measured
statement
We agree and have changed the title.
Reviewer 3- Comment 2. I also note that using this method will require more work from the assessors in
that they have to make a numerical estimate of their confidence in the field being assessed. In my
experience this will very likely lead to referees rejecting the request to asses. Not because they object
but because they are usually very busy people!
Yes, reporting uncertainties adds a bit of work but many agencies require HML already
Nonetheless, in the Abstract we change “The robustness and efficiency of the process are demonstrated
by comparing” -> “The process is compared...”and append “and to Fisher’s additive incomplete block
analysis.”
And before “The algorithm is tested” insert: Reliability estimates for the resulting true values are
obtained.
Further Changes: Apart from these, we have made a few minor improvements and inserted a dedication
at the front.
Society Open
