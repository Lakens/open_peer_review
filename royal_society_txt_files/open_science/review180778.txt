What can associative learning do for planning?
Johan Lind
Article citation details
R. Soc. open sci. 5: 180778.
http://dx.doi.org/10.1098/rsos.180778
Review timeline
Original submission: 30 May 2018 Note: Reports are unedited and appear as
1st revised submission: 3 September 2018 submitted by the referee. The review history
2nd revised submission: 23 October 2018 appears in chronological order.
Final acceptance: 29 October 2018
Review History
label_version_1
RSOS-180778.R0 (Original submission)
label_author_1
Review form: Reviewer 1 (Lucy Cheke)
Is the manuscript scientifically sound in its present form?
No
Are the interpretations and conclusions justified by the results?
No
Is the language acceptable?
Yes
Is it clear how to access all supporting data?
Yes
Do you have any ethical concerns with this paper?
No
Reports © 2018 The Reviewers; Decision Letters © 2018 The Reviewers and Editors;
Responses © 2018 The Reviewers, Editors and Authors. Published by the Royal Society under the
terms of the Creative Commons Attribution License http://creativecommons.org/licenses/by/4.0/,
which permits unrestricted use, provided the original author and source are credited
2
Have you any concerns about statistical analyses in this paper?
Yes
Recommendation?
label_recommendation_1
Major revision is needed (please make suggestions in comments)
Comments to the Author(s)
label_comment_1
The authors present an attempt to model the performance of apes and corvids on two tests
putatively assessing foresight and planning using associative learning models that rely on
secondary reinforcement. I am extremely enthusiastic about such an attempt and am in general
very keen on this study. There are, however, a number of issues that I think need to be addressed
before this manuscript is suitable for publication.
My critiques are detailed below. In general, I believe that there needs to be greater clarity and
detail in both the review of the animal cognition literature in this field and of the model being
used. In particular, the authors have omitted some key studies (osvath & Osvath 2008 in
particular) which may speak to their results, and alternative methodologies for assessing
planning (e.g. Correia et al., 2007; Cheke et al., 2012) which may be less easily modelled. The
studies being modelled need to be more clearly explained and in particular it should be noted
which experiments were performed as “controls”. The results section needs to be significantly
clarified, in particular the graphs are extremely difficult to interpret, and the overall reporting of
results appears to be very “gist-based” when, unless I’ve completely misunderstood (which is
possible!) it would be possible to conduct some statistical tests to address the hypotheses directly.
Detailed comments
Abstract
Some spelling errors (“exible”)
Introduction
Beating human players on a specific game does not equate to providing a “attractive account for
human behaviour” or an example of “human-like” behaviour. These games are not reflections of
any particular cognitive skill in humans, and indeed most humans find them very difficult, while
modern computers find some of them (e.g. chess) relatively simple. In general, associative
learning as an explanation for human behaviour is even more dismissed than it is in animal
behaviour. The paradox might thus be more accurately described as one of great achievement in
producing complex flexible behaviour in the field of AI, while dismissed as a model for flexible
behaviour in biological systems (both human and animal).
I would suggest that the introduction needs to be significantly expanded to allow the authors to
go into more detail on a number of issues.
- Firstly, its important to recognise the degree of discussion regarding associative learning in
flexible animal behaviour. These do not fall entirely into the boxes of “ignored” or “dismissed” –
there are plenty of associative controls out there. The very valid criticism of these is that they
usually assume the simplest possible form of associative learning and often don’t take account of
the more complex factors that the authors’ model relies upon, such as secondary reinforcement
(infact, this distinction would be a good one to make, since the authors explicitly assess the
different prediction of a “simplistic” associative learning model relying on primary reinforcement
vs. a chaining one). However, some DO take a more nuanced approach (see Osvath & Osvath,
2008, for a secondary reinforcement control within a very similar task to Mulcahy & Call – i.e. the
two-choice task in which the subjects should choose the tool, but once they have the tool (because
they do not need more than one) should then choose the small reward – the authors argued that
had the functional tool acquired secondary reinforcement properties, then the apes would choose
3
the tool in both choices).
- Secondly the associative learning literature itself (and the theories regarding secondary
reinforcement) seems to be completely ignored. I recognise that this is explored in depth in their
previous work (Enquist et al., 2016), but both the background literature and the model itself need
more introduction here, and not to assume that the readers have read that paper (which I had to
do to follow this ms).
- It is important to distinguish between conclusions about animal cognition and conclusions
about tests of animal cognition. While in the introduction the aim of this study in modelling
particular animal cognition tasks is mostly clear, in places (particularly the discussion) it reads
much more like they are modelling animal planning itself. The methods of the modelled studies
have been criticised in the animal cognition literature for years as solvable through associative
learning (see Cheke & Clayton, 2010). But that doesn’t tell us anything about animal planning –
just that these tasks are unsuitable for testing it. This is in contrast to the “planning for a future
motivational state” style studies, which are less vulnerable to this critique (although there are
plenty of different critiques of those!). For example, while the studies with scrubjays and
Eurasian jays may indeed be using a “specialised memory repertoire” (what kind of repertoire
this might be has never been defined, so it is not at present a valid reason to dismiss them), there
are two studies (Correia et al., 2007 and Cheke et el., 2012) that specifically modulate the current
and future value of certain rewards. It would be interesting to have a perspective on these types
of planning experiments, rather than avoiding mentioning them altogether.
Methods
The available actions doesn’t include moving between rooms, which was a key feature in the
original study. Why was this?
The details of the original empirical studies needs to be made clearer. For example, the
significance of experiment 4 (in mulcahy & call) where the tool is “rewarded” but not
“functional”.
Results
The results as presented are not at all easily interpretable. They are presented entirely in the
graphs, with lines for the models (averaged across the 500 subjects?) and filled circles to
“represent data from empirical studies” (but the form of this data is not clear at all – are these
circles the average probability of choosing the correct across the trials for that experiment? In
which case it would be clearer to have a figure that illustrates a similar metric for the model.) The
difficulty in interpreting these graphs is perhaps illustrated by the fact that I cannot work out
how it can be that the lower panel in figure one is showing that the model “overpredicts”
performance in the ravens tool task (as claimed in the results section), as it looks very much like it
underpredicts it. It is also confusing to have 3 factors for the model (tool, small reward,
distractor) but only one for the empirical studies… these graphs need to be significantly clarified.
Relatedly, Mulcahy and call give the exact trials in which the apes selected the correct tool in their
paper, so why is this detail not represented?
In general it remains not entirely clear what question the study is addressing. I think the ms
would benefit from approaching this data with two distinct hypotheses:
1. These tasks can be solved with a chaining model (i.e. this model will choose the functional
object significantly more often than expected by chance
1a… and can do so in a time frame (number of trials) equivalent to that of apes/ravens
2. This model can match the behavioural performance of apes and ravens in these tasks.
This differentiates between the two messages one might be able to take form these results – i.e. 1.
That the task is solvable with chaining, and 2. That those animals in that experiment likely DID
4
solve these tasks using chaining.
If 500 subjects were run, then surely there should be some standard deviations and error terms
representing the variance in the different subjects, which can then be statistically assessed with
reference to these hypotheses? ie:
1. Did the model subjects choose the functional tool significantly more often than expected by
chance (when given the same number of trials as apes and ravens)
2. Does the performance of the animals subjects differ significantly from that of the models (for
example, a one-sample t-test with the data from the model being the “population”).
You can then enter into the more qualitative exploration of the pattern of performance across the
different experiments (as the results currently contain).
When it comes to looking that the results for experiment 4 in the ape experiment, it should be
noted that the whole point of that experiment was that it was a control - the authors argued that
if the task was being solved through associative learning, then you’d expect the apes to do just as
well on this task as the others (like the model did), but if it was being solved through
understanding of future functionality, then they wouldn’t (as what’s the point of choosing a tool
when the apparatus isn’t there). So in theory, the fact that the model significantly outperformed
the apes on that task indicates that the apes *weren’t* using chaining (or at least, one could make
that argument). In practice, however, if you look at the data of the four apes that took part in that
experiment (in the table in Mulcahy & Call), then 2 of them never happened to pick the right tool
(and therefore could not have learned the association) and the other two performed comparably
to the apes in the other experiments (see Cheke & Clayton, 2010 for this critique). As such, the
argument could be made that the model *does* fit with the data of the apes that had the
opportunity to learn (by choosing the rewarded behaviour in early trials).
Why is the comparison between Rescorla-Wagner and chaining only shown for the raven data? It
is also misleading to refer to Rescorla-Wagner style learning as “stimulus-response” as this (to
my mind) implies that the learning would be inflexible and habitual rather than goal-oriented.
Again, the figure legend for figure 2 is not sufficient and the graph is very difficult to understand
without proper explanation.
Discussion
Some spelling and punctuation errors (e.g. “bothe” – line 41)
Again, its important to distinguish between showing the that tasks are solvable with associative
processes (comment on the *task*) and showing that the animals used associative processes
(comment on the *animal*) as I’ve suggested above, these hypotheses can be addressed and
interpreted separately.
label_author_2
Review form: Reviewer 2 (Ralph Miller)
Is the manuscript scientifically sound in its present form?
Yes
Are the interpretations and conclusions justified by the results?
Yes
Is the language acceptable?
No
5
Is it clear how to access all supporting data?
Yes
Do you have any ethical concerns with this paper?
No
Have you any concerns about statistical analyses in this paper?
No
Recommendation?
label_recommendation_2
Accept with minor revision (please list in comments)
Comments to the Author(s)
label_comment_2
Royal Society Open Science (Manuscript ID RSOS-180778)
This paper addresses the paradox that many cognitive psychologists believe that associative
learning is unable to explain flexible planning in animals, whereas there are ever-increasing
demonstrations that AI networks, which are based on association-like connections, that are able
to simulate the flexible planning observed in animals. Lind uses a conventional associative
model to simulate flexible planning as observed in two previously published experiments with
nonhuman animals and concludes that the power of associative models to yield complex
cognitive processes is underappreciated. One frequently cited alternative to associative learning
accounts of flexible planning is that animals have evolved many task-specific skills to address
ecological challenges in their evolutionary history, with these skills often involving higher-order
cognitive abilities supposedly well beyond the capabilities of associative networks. If an
associative model proved capable of simulating successful planning by animals in some of these
situations, researchers might spend less effort trying to model higher-order cognition is each of
these tasks, and instead spend more time trying to identify a particular associative set of
principles that works across these diverse tasks, Certainly the latter approach, if reasonably
successful, would be far more parsimonious than the present emphasis on many task-specific
capabilities that usually depend on higher-order cognition. Lind’s present demonstration of the
power of associative models in several well-known complex tasks that at least some species of
animals (two species of great apes and ravens) are able to solve is compelling evidence of the
power of associative models. The paper should receive a good bit of attention based on its clear
presentation, and consequently is apt to have considerable influence on future studies of animal
cognition. In summary, I recommend publication of this manuscript in Open Science. However,
I saw a number of places where I thought that some revision could make this excellent paper
even stronger. Below I list them, without meaning to suggest that the present paper is faulty in
any major way.
Lind might want to note that bottom-up associative models are potentially more illuminating
than top-down higher-order cognitive models in that the top-down models often leave us
wondering exactly how the animal actually performed the cognitive process. That is, top-down
models often ‘leave the animal lost in thought’ rather than illuminating the critical underlying
processes by which the animal solves the task.
On p. 3, Lind writes “A brief summary of the logic of our model is included in the Methods
section below.” I think that the architecture and rules of the associative models used here are too
central to the paper to put off any sort of description of it until the Methods section (even when
the Methods section starts in the next paragraph). Something should be said about it in the
Introduction because there are so many different associative models that generic reference to
6
them is almost meaningless. Related to this, the paragraph containing Eq. 1 is rather opaque and
will challenge most readers. The material here has to be elaborated and clarified.
The paper is cast as ‘associative models of broad generality’ vs. ‘cognitive models that posit
animals have an assortment of higher-order cognitive skills.’ An alternative to this viewpoint
would be to consider associative models as a different level of analysis than cognitive models.
That is, instead of simple associative learning and higher-order cognitive processing of
information being fundamentally different, it is possible that ‘automatically’ acquired
associations could be the basis of higher-order cognitive processing. Thus, they might simply be
different levels of analysis as are physics, chemistry, and biology. Associations could be the
building blocks of propositions, and hence propositions and automatic associations may simply
reflect different levels of analysis similar to the relationship of physics to chemistry and chemistry
to biology. ... Obviously dissociations such as the Perruchet effect challenge such a relationship
just as they challenge purely propositional accounts. But in my [biased] opinion, such
dissociations are not grounds for categorical rejection just as the absence of missing links is not
grounds for rejection of the theory of evolution; further research may well fill in the void. My
central concern with the propositional position is that it does not adequately specify underlying
mechanisms, thereby leaving too much of the heavy lifting to some sort of executive function (aka
the homunculus). A further challenge to explaining learning entirely in terms of associations
arises from papers concluding that associative learning does not occur without conscious
awareness. However, although less often cited, there is a parallel literature asserting that
associative learning does sometimes occur without awareness... and I do not think that the nay
sayers have managed to discredit this entire literature. Thus, I think that it is premature to
dismiss the possibility that associations are the basis of all learning. Obviously, this is a minority
view. Hence, I am only suggesting that the author entertain it.
More trivially:
The paper considers S-B (operant, so-called habits) and S=S’ (Pavlovian) learning, but does not
consider R-S’ (so-called goal-directed behavior) learning. Perhaps including this would make the
paper unduly long and complex, but the author might at least mention its omission.
The paper would have been far easier for me to critique if Lind had double spaced it.
More commas would make the paper easier to absorb... okay, English is not Lind’s native
language, but the paper still needs more commas (e.g., after dependent clauses that precede
independent clauses). Other grammatical issues include use of the wrong preposition, spelling
errors that use of a spell checker would have caught, and missing words.
Lind often allowed the simulations’ procedural parameters to differ from that of the behavioral
experiments. For example, on p. 8, he writes “the number of trials did not perfectly match the
empirical studies was due to the probabilistic nature of the decision making equation.” I
understand what he has written, i.e., the rationale for his doing what he did , but I still find it less
than satisfying. There are other ways that Lind could have addressed this problem (e.g.,
averaging the results of more runs).
Other likely deviations between the simulations and the behavioral data arose from information
that Lind asserts was omitted in the behavioral publications. For example, on p. 8, “Exact
information about the amount of extinction was lacking from the raven study, therefore it was
assumed that the ravens had have extinction experiences with the distractors.” It would have
been more appropriate for Lind to contact Kabadayi & Osvath to obtain this information.
On p. 4, “If is large behavior” perplexed me.\
7
On p. 10, Lind writes “Increasing the cost of a behavior that does not lead to a reward will
increase the rate at which the behavior is chosen.” This sentence clearly needs to be reworded.
label_end_comment
Decision letter (RSOS-180778.R0)
06-Aug-2018
Dear Dr Lind,
The editors assigned to your paper ("What can associative learning do for planning?") have now
received comments from reviewers. We would like you to revise your paper in accordance with
the referee and Associate Editor suggestions which can be found below (not including
confidential reports to the Editor). Please note this decision does not guarantee eventual
acceptance.
Please submit a copy of your revised paper before 29-Aug-2018. Please note that the revision
deadline will expire at 00.00am on this date. If we do not hear from you within this time then it
will be assumed that the paper has been withdrawn. In exceptional circumstances, extensions
may be possible if agreed with the Editorial Office in advance. We do not allow multiple rounds
of revision so we urge you to make every effort to fully address all of the comments at this stage.
If deemed necessary by the Editors, your manuscript will be sent back to one or more of the
original reviewers for assessment. If the original reviewers are not available, we may invite new
reviewers.
To revise your manuscript, log into http://mc.manuscriptcentral.com/rsos and enter your
Author Centre, where you will find your manuscript title listed under "Manuscripts with
Decisions." Under "Actions," click on "Create a Revision." Your manuscript number has been
appended to denote a revision. Revise your manuscript and upload a new version through your
Author Centre.
When submitting your revised manuscript, you must respond to the comments made by the
referees and upload a file "Response to Referees" in "Section 6 - File Upload". Please use this to
document how you have responded to the comments, and the adjustments you have made. In
order to expedite the processing of the revised manuscript, please be as specific as possible in
your response.
In addition to addressing all of the reviewers' and editor's comments please also ensure that your
revised manuscript contains the following sections as appropriate before the reference list:
• Ethics statement (if applicable)
If your study uses humans or animals please include details of the ethical approval received,
including the name of the committee that granted approval. For human studies please also detail
whether informed consent was obtained. For field studies on animals please include details of all
permissions, licences and/or approvals granted to carry out the fieldwork.
• Data accessibility
It is a condition of publication that all supporting data are made available either as
supplementary information or preferably in a suitable permanent repository. The data
accessibility section should state where the article's supporting data can be accessed. This section
8
should also include details, where possible of where to access other relevant research materials
such as statistical tools, protocols, software etc can be accessed. If the data have been deposited in
an external repository this section should list the database, accession number and link to the DOI
for all data from the article that have been made publicly available. Data sets that have been
deposited in an external repository and have a DOI should also be appropriately cited in the
manuscript and included in the reference list.
If you wish to submit your supporting data or code to Dryad (http://datadryad.org/), or modify
your current submission to dryad, please use the following link:
http://datadryad.org/submit?journalID=RSOS&manu=RSOS-180778
• Competing interests
Please declare any financial or non-financial competing interests, or state that you have no
competing interests.
• Authors’ contributions
All submissions, other than those with a single author, must include an Authors’ Contributions
section which individually lists the specific contribution of each author. The list of Authors
should meet all of the following criteria; 1) substantial contributions to conception and design, or
acquisition of data, or analysis and interpretation of data; 2) drafting the article or revising it
critically for important intellectual content; and 3) final approval of the version to be published.
All contributors who do not meet all of these criteria should be included in the
acknowledgements.
We suggest the following format:
AB carried out the molecular lab work, participated in data analysis, carried out sequence
alignments, participated in the design of the study and drafted the manuscript; CD carried out
the statistical analyses; EF collected field data; GH conceived of the study, designed the study,
coordinated the study and helped draft the manuscript. All authors gave final approval for
publication.
• Acknowledgements
Please acknowledge anyone who contributed to the study but did not meet the authorship
criteria.
• Funding statement
Please list the source of funding for each author.
Please note that Royal Society Open Science charge article processing charges for all new
submissions that are accepted for publication. Charges will also apply to papers transferred to
Royal Society Open Science from other Royal Society Publishing journals, as well as papers
submitted as part of our collaboration with the Royal Society of Chemistry
(http://rsos.royalsocietypublishing.org/chemistry). If your manuscript is newly submitted and
subsequently accepted for publication, you will be asked to pay the article processing charge,
unless you request a waiver and this is approved by Royal Society Publishing. You can find out
more about the charges at http://rsos.royalsocietypublishing.org/page/charges. Should you
have any queries, please contact openscience@royalsociety.org.
Once again, thank you for submitting your manuscript to Royal Society Open Science and I look
forward to receiving your revision. If you have any questions at all, please do not hesitate to get
in touch.
9
Kind regards,
Royal Society Open Science Editorial Office
Royal Society Open Science
openscience@royalsociety.org
on behalf of Dr Alecia Carter (Associate Editor) and Prof. Kevin Padian (Subject Editor)
openscience@royalsociety.org
Associate Editor's comments (Dr Alecia Carter):
Associate Editor: 1
Comments to the Author:
I have now received two constructive reviews of your manuscript. Both reviewers agree that the
manuscript asks an interesting question but both point out several places where the manuscript
could be improved. These encompass alternative interpretations that should be considered,
clarifications that will improve the readability of the manuscript, and literature that should be
included to present a more impartial view of the field. I found the comments to be very helpful
and I encourage the author to fully consider these in a resubmission.
Comments to Author:
Reviewers' Comments to Author:
Reviewer: 1
Comments to the Author(s)
The authors present an attempt to model the performance of apes and corvids on two tests
putatively assessing foresight and planning using associative learning models that rely on
secondary reinforcement. I am extremely enthusiastic about such an attempt and am in general
very keen on this study. There are, however, a number of issues that I think need to be addressed
before this manuscript is suitable for publication.
My critiques are detailed below. In general, I believe that there needs to be greater clarity and
detail in both the review of the animal cognition literature in this field and of the model being
used. In particular, the authors have omitted some key studies (osvath & Osvath 2008 in
particular) which may speak to their results, and alternative methodologies for assessing
planning (e.g. Correia et al., 2007; Cheke et al., 2012) which may be less easily modelled. The
studies being modelled need to be more clearly explained and in particular it should be noted
which experiments were performed as “controls”. The results section needs to be significantly
clarified, in particular the graphs are extremely difficult to interpret, and the overall reporting of
results appears to be very “gist-based” when, unless I’ve completely misunderstood (which is
possible!) it would be possible to conduct some statistical tests to address the hypotheses directly.
Detailed comments
Abstract
Some spelling errors (“exible”)
Introduction
Beating human players on a specific game does not equate to providing a “attractive account for
human behaviour” or an example of “human-like” behaviour. These games are not reflections of
any particular cognitive skill in humans, and indeed most humans find them very difficult, while
modern computers find some of them (e.g. chess) relatively simple. In general, associative
10
learning as an explanation for human behaviour is even more dismissed than it is in animal
behaviour. The paradox might thus be more accurately described as one of great achievement in
producing complex flexible behaviour in the field of AI, while dismissed as a model for flexible
behaviour in biological systems (both human and animal).
I would suggest that the introduction needs to be significantly expanded to allow the authors to
go into more detail on a number of issues.
- Firstly, its important to recognise the degree of discussion regarding associative learning in
flexible animal behaviour. These do not fall entirely into the boxes of “ignored” or “dismissed” –
there are plenty of associative controls out there. The very valid criticism of these is that they
usually assume the simplest possible form of associative learning and often don’t take account of
the more complex factors that the authors’ model relies upon, such as secondary reinforcement
(infact, this distinction would be a good one to make, since the authors explicitly assess the
different prediction of a “simplistic” associative learning model relying on primary reinforcement
vs. a chaining one). However, some DO take a more nuanced approach (see Osvath & Osvath,
2008, for a secondary reinforcement control within a very similar task to Mulcahy & Call – i.e. the
two-choice task in which the subjects should choose the tool, but once they have the tool (because
they do not need more than one) should then choose the small reward – the authors argued that
had the functional tool acquired secondary reinforcement properties, then the apes would choose
the tool in both choices).
- Secondly the associative learning literature itself (and the theories regarding secondary
reinforcement) seems to be completely ignored. I recognise that this is explored in depth in their
previous work (Enquist et al., 2016), but both the background literature and the model itself need
more introduction here, and not to assume that the readers have read that paper (which I had to
do to follow this ms).
- It is important to distinguish between conclusions about animal cognition and conclusions
about tests of animal cognition. While in the introduction the aim of this study in modelling
particular animal cognition tasks is mostly clear, in places (particularly the discussion) it reads
much more like they are modelling animal planning itself. The methods of the modelled studies
have been criticised in the animal cognition literature for years as solvable through associative
learning (see Cheke & Clayton, 2010). But that doesn’t tell us anything about animal planning –
just that these tasks are unsuitable for testing it. This is in contrast to the “planning for a future
motivational state” style studies, which are less vulnerable to this critique (although there are
plenty of different critiques of those!). For example, while the studies with scrubjays and
Eurasian jays may indeed be using a “specialised memory repertoire” (what kind of repertoire
this might be has never been defined, so it is not at present a valid reason to dismiss them), there
are two studies (Correia et al., 2007 and Cheke et el., 2012) that specifically modulate the current
and future value of certain rewards. It would be interesting to have a perspective on these types
of planning experiments, rather than avoiding mentioning them altogether.
Methods
The available actions doesn’t include moving between rooms, which was a key feature in the
original study. Why was this?
The details of the original empirical studies needs to be made clearer. For example, the
significance of experiment 4 (in mulcahy & call) where the tool is “rewarded” but not
“functional”.
Results
The results as presented are not at all easily interpretable. They are presented entirely in the
graphs, with lines for the models (averaged across the 500 subjects?) and filled circles to
“represent data from empirical studies” (but the form of this data is not clear at all – are these
circles the average probability of choosing the correct across the trials for that experiment? In
which case it would be clearer to have a figure that illustrates a similar metric for the model.) The
11
difficulty in interpreting these graphs is perhaps illustrated by the fact that I cannot work out
how it can be that the lower panel in figure one is showing that the model “overpredicts”
performance in the ravens tool task (as claimed in the results section), as it looks very much like it
underpredicts it. It is also confusing to have 3 factors for the model (tool, small reward,
distractor) but only one for the empirical studies… these graphs need to be significantly clarified.
Relatedly, Mulcahy and call give the exact trials in which the apes selected the correct tool in their
paper, so why is this detail not represented?
In general it remains not entirely clear what question the study is addressing. I think the ms
would benefit from approaching this data with two distinct hypotheses:
1. These tasks can be solved with a chaining model (i.e. this model will choose the functional
object significantly more often than expected by chance
1a… and can do so in a time frame (number of trials) equivalent to that of apes/ravens
2. This model can match the behavioural performance of apes and ravens in these tasks.
This differentiates between the two messages one might be able to take form these results – i.e. 1.
That the task is solvable with chaining, and 2. That those animals in that experiment likely DID
solve these tasks using chaining.
If 500 subjects were run, then surely there should be some standard deviations and error terms
representing the variance in the different subjects, which can then be statistically assessed with
reference to these hypotheses? ie:
1. Did the model subjects choose the functional tool significantly more often than expected by
chance (when given the same number of trials as apes and ravens)
2. Does the performance of the animals subjects differ significantly from that of the models (for
example, a one-sample t-test with the data from the model being the “population”).
You can then enter into the more qualitative exploration of the pattern of performance across the
different experiments (as the results currently contain).
When it comes to looking that the results for experiment 4 in the ape experiment, it should be
noted that the whole point of that experiment was that it was a control - the authors argued that
if the task was being solved through associative learning, then you’d expect the apes to do just as
well on this task as the others (like the model did), but if it was being solved through
understanding of future functionality, then they wouldn’t (as what’s the point of choosing a tool
when the apparatus isn’t there). So in theory, the fact that the model significantly outperformed
the apes on that task indicates that the apes *weren’t* using chaining (or at least, one could make
that argument). In practice, however, if you look at the data of the four apes that took part in that
experiment (in the table in Mulcahy & Call), then 2 of them never happened to pick the right tool
(and therefore could not have learned the association) and the other two performed comparably
to the apes in the other experiments (see Cheke & Clayton, 2010 for this critique). As such, the
argument could be made that the model *does* fit with the data of the apes that had the
opportunity to learn (by choosing the rewarded behaviour in early trials).
Why is the comparison between Rescorla-Wagner and chaining only shown for the raven data? It
is also misleading to refer to Rescorla-Wagner style learning as “stimulus-response” as this (to
my mind) implies that the learning would be inflexible and habitual rather than goal-oriented.
Again, the figure legend for figure 2 is not sufficient and the graph is very difficult to understand
without proper explanation.
12
Discussion
Some spelling and punctuation errors (e.g. “bothe” – line 41)
Again, its important to distinguish between showing the that tasks are solvable with associative
processes (comment on the *task*) and showing that the animals used associative processes
(comment on the *animal*) as I’ve suggested above, these hypotheses can be addressed and
interpreted separately.
Reviewer: 2
Comments to the Author(s)
Royal Society Open Science (Manuscript ID RSOS-180778)
This paper addresses the paradox that many cognitive psychologists believe that associative
learning is unable to explain flexible planning in animals, whereas there are ever-increasing
demonstrations that AI networks, which are based on association-like connections, that are able
to simulate the flexible planning observed in animals. Lind uses a conventional associative
model to simulate flexible planning as observed in two previously published experiments with
nonhuman animals and concludes that the power of associative models to yield complex
cognitive processes is underappreciated. One frequently cited alternative to associative learning
accounts of flexible planning is that animals have evolved many task-specific skills to address
ecological challenges in their evolutionary history, with these skills often involving higher-order
cognitive abilities supposedly well beyond the capabilities of associative networks. If an
associative model proved capable of simulating successful planning by animals in some of these
situations, researchers might spend less effort trying to model higher-order cognition is each of
these tasks, and instead spend more time trying to identify a particular associative set of
principles that works across these diverse tasks, Certainly the latter approach, if reasonably
successful, would be far more parsimonious than the present emphasis on many task-specific
capabilities that usually depend on higher-order cognition. Lind’s present demonstration of the
power of associative models in several well-known complex tasks that at least some species of
animals (two species of great apes and ravens) are able to solve is compelling evidence of the
power of associative models. The paper should receive a good bit of attention based on its clear
presentation, and consequently is apt to have considerable influence on future studies of animal
cognition. In summary, I recommend publication of this manuscript in Open Science. However,
I saw a number of places where I thought that some revision could make this excellent paper
even stronger. Below I list them, without meaning to suggest that the present paper is faulty in
any major way.
Lind might want to note that bottom-up associative models are potentially more illuminating
than top-down higher-order cognitive models in that the top-down models often leave us
wondering exactly how the animal actually performed the cognitive process. That is, top-down
models often ‘leave the animal lost in thought’ rather than illuminating the critical underlying
processes by which the animal solves the task.
On p. 3, Lind writes “A brief summary of the logic of our model is included in the Methods
section below.” I think that the architecture and rules of the associative models used here are too
central to the paper to put off any sort of description of it until the Methods section (even when
the Methods section starts in the next paragraph). Something should be said about it in the
Introduction because there are so many different associative models that generic reference to
them is almost meaningless. Related to this, the paragraph containing Eq. 1 is rather opaque and
will challenge most readers. The material here has to be elaborated and clarified.
13
The paper is cast as ‘associative models of broad generality’ vs. ‘cognitive models that posit
animals have an assortment of higher-order cognitive skills.’ An alternative to this viewpoint
would be to consider associative models as a different level of analysis than cognitive models.
That is, instead of simple associative learning and higher-order cognitive processing of
information being fundamentally different, it is possible that ‘automatically’ acquired
associations could be the basis of higher-order cognitive processing. Thus, they might simply be
different levels of analysis as are physics, chemistry, and biology. Associations could be the
building blocks of propositions, and hence propositions and automatic associations may simply
reflect different levels of analysis similar to the relationship of physics to chemistry and chemistry
to biology. ... Obviously dissociations such as the Perruchet effect challenge such a relationship
just as they challenge purely propositional accounts. But in my [biased] opinion, such
dissociations are not grounds for categorical rejection just as the absence of missing links is not
grounds for rejection of the theory of evolution; further research may well fill in the void. My
central concern with the propositional position is that it does not adequately specify underlying
mechanisms, thereby leaving too much of the heavy lifting to some sort of executive function (aka
the homunculus). A further challenge to explaining learning entirely in terms of associations
arises from papers concluding that associative learning does not occur without conscious
awareness. However, although less often cited, there is a parallel literature asserting that
associative learning does sometimes occur without awareness... and I do not think that the nay
sayers have managed to discredit this entire literature. Thus, I think that it is premature to
dismiss the possibility that associations are the basis of all learning. Obviously, this is a minority
view. Hence, I am only suggesting that the author entertain it.
More trivially:
The paper considers S-B (operant, so-called habits) and S=S’ (Pavlovian) learning, but does not
consider R-S’ (so-called goal-directed behavior) learning. Perhaps including this would make the
paper unduly long and complex, but the author might at least mention its omission.
The paper would have been far easier for me to critique if Lind had double spaced it.
More commas would make the paper easier to absorb... okay, English is not Lind’s native
language, but the paper still needs more commas (e.g., after dependent clauses that precede
independent clauses). Other grammatical issues include use of the wrong preposition, spelling
errors that use of a spell checker would have caught, and missing words.
Lind often allowed the simulations’ procedural parameters to differ from that of the behavioral
experiments. For example, on p. 8, he writes “the number of trials did not perfectly match the
empirical studies was due to the probabilistic nature of the decision making equation.” I
understand what he has written, i.e., the rationale for his doing what he did , but I still find it less
than satisfying. There are other ways that Lind could have addressed this problem (e.g.,
averaging the results of more runs).
Other likely deviations between the simulations and the behavioral data arose from information
that Lind asserts was omitted in the behavioral publications. For example, on p. 8, “Exact
information about the amount of extinction was lacking from the raven study, therefore it was
assumed that the ravens had have extinction experiences with the distractors.” It would have
been more appropriate for Lind to contact Kabadayi & Osvath to obtain this information.
On p. 4, “If is large behavior” perplexed me.\
14
On p. 10, Lind writes “Increasing the cost of a behavior that does not lead to a reward will
increase the rate at which the behavior is chosen.” This sentence clearly needs to be reworded.
Author's Response to Decision Letter for (RSOS-180778.R0)
See Appendix A.
label_version_2
RSOS-180778.R1 (Revision)
label_author_3
Review form: Reviewer 1 (Lucy Cheke)
Is the manuscript scientifically sound in its present form?
Yes
Are the interpretations and conclusions justified by the results?
No
Is the language acceptable?
Yes
Is it clear how to access all supporting data?
Yes
Do you have any ethical concerns with this paper?
No
Have you any concerns about statistical analyses in this paper?
No
Recommendation?
label_recommendation_3
Accept with minor revision (please list in comments)
Comments to the Author(s)
label_comment_3
In general, I am happy with the responses to my comments and revisions to the manuscript.
However, I’m afraid I’m going to have to double down on a particular point.
I profoundly disagree that further discussion of experiment 4 is unhelpful – this is because often
in animal cognition we are unable to rule out associative learning in the critical behaviour, but
instead include controls that, were associative learning being used, should be equally well solved,
but were another system being used, should be failed. In practice, Mulcahy and Call’s RESULTS
were hard to interpret, as they imply that this control task was not solved well, but as we’ve
already discussed, this is because two animals never had the chance to learn. But your model
nicely shows the point that this control task, if approached with associative learning, would have
been solved just as well as the other tasks. Thus if M&C’s results HAD shown that the apes
legitimately failed that task, then the difference between the model and the behavioural data
15
might be expected to be seen ONLY in this task and that would be highly meaningful. I realise
that this is a somewhat hypothetical distinction, but it is an important point when it comes to
differentiating results in animal cognition studies from associative accounts – for example
Osvath & Osvath’s (2008) control condition speaks to exactly the account your model uses (i.e.
that the behaviour of taking a tool becomes itself reinforcing). If you were to run your model with
the data from that study, you’d expect the model agent to match the apes in most tasks (i.e. you
wouldn't see a difference and could potentially conclude that the behaviour was the result of
associative learning), but in the control task choose the tool twice, not choose the tool then the
small reward as the apes did. Thus you’d expect a difference ONLY in the control task, which is
why it’s important.
This discussion of the control task needs to be included in the discussion. At face value (if you
don’t delve into the details of the individual apes performance on exp 4) your data actually
support M&C’s conclusions – that is, if associative leaning were being used, you’d expect good
performance on everything, if planning, then you’d expect good performance on everything
except exp 4. This links back to the highlighted difference between whether you’re assessing the
animals themselves or the task. In terms of the animals, the results of exp 4 are inconclusive and
we can’t make a good comparison, in terms of the design of the control task as an associative
learning control, your model provides support for it – it is well solved by associative learning, so
if animals fail that while passing the others, that does provide a differentiation between the
accounts.
Detailed comments
Abstract
• “One phenomenon in which associative learning has 13 been ruled out as an explanation for
animal behavior is <U+FB02>exible planning”
This makes it sound like associative learning has been generally “ruled out” as a possible
explanation, which I don’t think is true at all.
Introduction
• Page 2, paragraph starting line 68 –
This is totally up to the authors to judge, but I mentioned Osvath and Osvath in the initial review
because they included a well thought-through associative control which would actually be able to
differentiate between behaviours and was based on a specified associative account. This is in
contrast to most associative controls which involve different behaviours and feedback to the main
tasks and are generally not useful comparisons (see the “search” controls in this paper for
example (https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0026887). Thus I
don’t think its really fair to use this example here, especially since Suddendorf’s critique is
incredibly vague and doesn’t really speak to associative learning.
Methods
• “This last part was of little importance here for reasons mentioned in the Result section.” …
“experiment 4 in [26] is of the least importance for comparative purposes.”
I have explained why this is not the case. You cannot ignore control tasks when making these
kinds of comparisons between animals and computational models. The contrast in performance
between the control tasks and the experimental tasks is the whole point.
• Page 10 line 397 - “That animals remember stimulus-response associations and stimulus values
for a very long time was not mentioned in neither of the simulated studies [26, 30].”
Two points – the first grammatical (either say “not mentioned in either” or “mentioned in
16
neither”) (see also page 13, line 530) the second is relevance – you should cite evidence that these
are maintained in long term memory, not that this was not mentioned in those previous studies.
Discussion
• “figure ??” – page 13, lines 527 and 529; page 14 line 557
label_author_4
Review form: Reviewer 2 (Ralph Miller)
Is the manuscript scientifically sound in its present form?
Yes
Are the interpretations and conclusions justified by the results?
Yes
Is the language acceptable?
Yes
Is it clear how to access all supporting data?
Yes
Do you have any ethical concerns with this paper?
No
Have you any concerns about statistical analyses in this paper?
No
Recommendation?
label_recommendation_4
Accept as is
Comments to the Author(s)
label_comment_4
The author has extensively revised the original submission largely in accord with the suggestions
of the two reviewers, and as a result this is a significantly better paper. The paper has been
clarified, albeit sometimes not quite to the degree that I would have liked. Importantly, clearer
‘qualifiers’ with respect to what the present model cannot do have been added (e.g., differential
motivational states as cues). I was particularly pleased to see that the author now makes clear
that he is modeling behavior, as opposed to modeling planning about behavior. More generally,
the larger question being posed concerning the power of associative models to account for
complex behavior is a monumental one. As a result, the answer to it is necessarily going to be
exceedingly complex and surely will not ever be comprehensively addressed in any single paper.
The present paper might best be viewed as a ‘proof of concept.’ That said, although I can think of
many additional aspects of matching the simulations to the actual experiments that I would like
to see addressed, the present manuscript does a good job in addressing the major facets of this
complex issue. Yet more detail concerning modeling details of the focal data could be requested,
but elaborating on them would likely obscure the basic point of the paper (i.e., the forest would
get lost among the trees). As is stands, the present revision moves the discussion forward in a
constructive manner. Consequently, I strongly support it being published in its current form.
17
label_end_comment
Decision letter (RSOS-180778.R1)
19-Oct-2018
Dear Dr Lind:
On behalf of the Editors, I am pleased to inform you that your Manuscript RSOS-180778.R1
entitled "What can associative learning do for planning?" has been accepted for publication in
Royal Society Open Science subject to minor revision in accordance with the referee suggestions.
Please find the referees' comments at the end of this email.
The reviewers and Subject Editor have recommended publication, but also suggest some minor
revisions to your manuscript. Therefore, I invite you to respond to the comments and revise your
manuscript.
• Ethics statement
If your study uses humans or animals please include details of the ethical approval received,
including the name of the committee that granted approval. For human studies please also detail
whether informed consent was obtained. For field studies on animals please include details of all
permissions, licences and/or approvals granted to carry out the fieldwork.
• Data accessibility
It is a condition of publication that all supporting data are made available either as
supplementary information or preferably in a suitable permanent repository. The data
accessibility section should state where the article's supporting data can be accessed. This section
should also include details, where possible of where to access other relevant research materials
such as statistical tools, protocols, software etc can be accessed. If the data has been deposited in
an external repository this section should list the database, accession number and link to the DOI
for all data from the article that has been made publicly available. Data sets that have been
deposited in an external repository and have a DOI should also be appropriately cited in the
manuscript and included in the reference list.
If you wish to submit your supporting data or code to Dryad (http://datadryad.org/), or modify
your current submission to dryad, please use the following link:
http://datadryad.org/submit?journalID=RSOS&manu=RSOS-180778.R1
• Competing interests
Please declare any financial or non-financial competing interests, or state that you have no
competing interests.
• Authors’ contributions
All submissions, other than those with a single author, must include an Authors’ Contributions
section which individually lists the specific contribution of each author. The list of Authors
should meet all of the following criteria; 1) substantial contributions to conception and design, or
acquisition of data, or analysis and interpretation of data; 2) drafting the article or revising it
critically for important intellectual content; and 3) final approval of the version to be published.
All contributors who do not meet all of these criteria should be included in the
acknowledgements.
We suggest the following format:
AB carried out the molecular lab work, participated in data analysis, carried out sequence
alignments, participated in the design of the study and drafted the manuscript; CD carried out
18
the statistical analyses; EF collected field data; GH conceived of the study, designed the study,
coordinated the study and helped draft the manuscript. All authors gave final approval for
publication.
• Acknowledgements
Please acknowledge anyone who contributed to the study but did not meet the authorship
criteria.
• Funding statement
Please list the source of funding for each author.
Please note that we cannot publish your manuscript without these end statements included. We
have included a screenshot example of the end statements for reference. If you feel that a given
heading is not relevant to your paper, please nevertheless include the heading and explicitly state
that it is not relevant to your work.
Because the schedule for publication is very tight, it is a condition of publication that you submit
the revised version of your manuscript before 28-Oct-2018. Please note that the revision deadline
will expire at 00.00am on this date. If you do not think you will be able to meet this date please let
me know immediately.
To revise your manuscript, log into https://mc.manuscriptcentral.com/rsos and enter your
Author Centre, where you will find your manuscript title listed under "Manuscripts with
Decisions". Under "Actions," click on "Create a Revision." You will be unable to make your
revisions on the originally submitted version of the manuscript. Instead, revise your manuscript
and upload a new version through your Author Centre.
When submitting your revised manuscript, you will be able to respond to the comments made by
the referees and upload a file "Response to Referees" in "Section 6 - File Upload". You can use this
to document any changes you make to the original manuscript. In order to expedite the
processing of the revised manuscript, please be as specific as possible in your response to the
referees.
When uploading your revised files please make sure that you have:
1) A text file of the manuscript (tex, txt, rtf, docx or doc), references, tables (including captions)
and figure captions. Do not upload a PDF as your "Main Document".
2) A separate electronic file of each figure (EPS or print-quality PDF preferred (either format
should be produced directly from original creation package), or original software format)
3) Included a 100 word media summary of your paper when requested at submission. Please
ensure you have entered correct contact details (email, institution and telephone) in your user
account
4) Included the raw data to support the claims made in your paper. You can either include your
data as electronic supplementary material or upload to a repository and include the relevant doi
within your manuscript
5) All supplementary materials accompanying an accepted article will be treated as in their final
form. Note that the Royal Society will neither edit nor typeset supplementary material and it will
be hosted as provided. Please ensure that the supplementary material includes the paper details
where possible (authors, article title, journal name).
Supplementary files will be published alongside the paper on the journal website and posted on
the online figshare repository (https://figshare.com). The heading and legend provided for each
supplementary file during the submission process will be used to create the figshare page, so
19
please ensure these are accurate and informative so that your files can be found in searches. Files
on figshare will be made available approximately one week before the accompanying article so
that the supplementary material can be attributed a unique DOI.
Please note that Royal Society Open Science charge article processing charges for all new
submissions that are accepted for publication. Charges will also apply to papers transferred to
Royal Society Open Science from other Royal Society Publishing journals, as well as papers
submitted as part of our collaboration with the Royal Society of Chemistry
(http://rsos.royalsocietypublishing.org/chemistry). If your manuscript is newly submitted and
subsequently accepted for publication, you will be asked to pay the article processing charge,
unless you request a waiver and this is approved by Royal Society Publishing. You can find out
more about the charges at http://rsos.royalsocietypublishing.org/page/charges. Should you
have any queries, please contact openscience@royalsociety.org.
Once again, thank you for submitting your manuscript to Royal Society Open Science and I look
forward to receiving your revision. If you have any questions at all, please do not hesitate to get
in touch.
Kind regards,
Royal Society Open Science Editorial Office
Royal Society Open Science
openscience@royalsociety.org
on behalf of Dr Alecia Carter (Associate Editor) and Prof. Kevin Padian (Subject Editor)
openscience@royalsociety.org
Associate Editor Comments to Author (Dr Alecia Carter):
I have now received two review of your resubmission from the original reviewers. Both
reviewers and myself find the manuscript to be greatly improved. I agree with reviewer 1's final
point for the reasons highlighted by the reviewer, but, as pointed out, this could easily be
addressed by some additional text in the discussion.
Reviewer comments to Author:
Reviewer: 1
Comments to the Author(s)
In general, I am happy with the responses to my comments and revisions to the manuscript.
However, I’m afraid I’m going to have to double down on a particular point.
I profoundly disagree that further discussion of experiment 4 is unhelpful – this is because often
in animal cognition we are unable to rule out associative learning in the critical behaviour, but
instead include controls that, were associative learning being used, should be equally well solved,
but were another system being used, should be failed. In practice, Mulcahy and Call’s RESULTS
were hard to interpret, as they imply that this control task was not solved well, but as we’ve
already discussed, this is because two animals never had the chance to learn. But your model
nicely shows the point that this control task, if approached with associative learning, would have
been solved just as well as the other tasks. Thus if M&C’s results HAD shown that the apes
legitimately failed that task, then the difference between the model and the behavioural data
might be expected to be seen ONLY in this task and that would be highly meaningful. I realise
that this is a somewhat hypothetical distinction, but it is an important point when it comes to
20
differentiating results in animal cognition studies from associative accounts – for example
Osvath & Osvath’s (2008) control condition speaks to exactly the account your model uses (i.e.
that the behaviour of taking a tool becomes itself reinforcing). If you were to run your model with
the data from that study, you’d expect the model agent to match the apes in most tasks (i.e. you
wouldn't see a difference and could potentially conclude that the behaviour was the result of
associative learning), but in the control task choose the tool twice, not choose the tool then the
small reward as the apes did. Thus you’d expect a difference ONLY in the control task, which is
why it’s important.
This discussion of the control task needs to be included in the discussion. At face value (if you
don’t delve into the details of the individual apes performance on exp 4) your data actually
support M&C’s conclusions – that is, if associative leaning were being used, you’d expect good
performance on everything, if planning, then you’d expect good performance on everything
except exp 4. This links back to the highlighted difference between whether you’re assessing the
animals themselves or the task. In terms of the animals, the results of exp 4 are inconclusive and
we can’t make a good comparison, in terms of the design of the control task as an associative
learning control, your model provides support for it – it is well solved by associative learning, so
if animals fail that while passing the others, that does provide a differentiation between the
accounts.
Detailed comments
Abstract
• “One phenomenon in which associative learning has 13 been ruled out as an explanation for
animal behavior is <U+FB02>exible planning”
This makes it sound like associative learning has been generally “ruled out” as a possible
explanation, which I don’t think is true at all.
Introduction
• Page 2, paragraph starting line 68 –
This is totally up to the authors to judge, but I mentioned Osvath and Osvath in the initial review
because they included a well thought-through associative control which would actually be able to
differentiate between behaviours and was based on a specified associative account. This is in
contrast to most associative controls which involve different behaviours and feedback to the main
tasks and are generally not useful comparisons (see the “search” controls in this paper for
example (https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0026887). Thus I
don’t think its really fair to use this example here, especially since Suddendorf’s critique is
incredibly vague and doesn’t really speak to associative learning.
Methods
• “This last part was of little importance here for reasons mentioned in the Result section.” …
“experiment 4 in [26] is of the least importance for comparative purposes.”
I have explained why this is not the case. You cannot ignore control tasks when making these
kinds of comparisons between animals and computational models. The contrast in performance
between the control tasks and the experimental tasks is the whole point.
• Page 10 line 397 - “That animals remember stimulus-response associations and stimulus values
for a very long time was not mentioned in neither of the simulated studies [26, 30].”
Two points – the first grammatical (either say “not mentioned in either” or “mentioned in
21
neither”) (see also page 13, line 530) the second is relevance – you should cite evidence that these
are maintained in long term memory, not that this was not mentioned in those previous studies.
Discussion
• “figure ??” – page 13, lines 527 and 529; page 14 line 557
Reviewer: 2
Comments to the Author(s)
The author has extensively revised the original submission largely in accord with the suggestions
of the two reviewers, and as a result this is a significantly better paper. The paper has been
clarified, albeit sometimes not quite to the degree that I would have liked. Importantly, clearer
‘qualifiers’ with respect to what the present model cannot do have been added (e.g., differential
motivational states as cues). I was particularly pleased to see that the author now makes clear
that he is modeling behavior, as opposed to modeling planning about behavior. More generally,
the larger question being posed concerning the power of associative models to account for
complex behavior is a monumental one. As a result, the answer to it is necessarily going to be
exceedingly complex and surely will not ever be comprehensively addressed in any single paper.
The present paper might best be viewed as a ‘proof of concept.’ That said, although I can think of
many additional aspects of matching the simulations to the actual experiments that I would like
to see addressed, the present manuscript does a good job in addressing the major facets of this
complex issue. Yet more detail concerning modeling details of the focal data could be requested,
but elaborating on them would likely obscure the basic point of the paper (i.e., the forest would
get lost among the trees). As is stands, the present revision moves the discussion forward in a
constructive manner. Consequently, I strongly support it being published in its current form.
Author's Response to Decision Letter for (RSOS-180778.R1)
See Appendix B.
label_end_comment
Decision letter (RSOS-180778.R2)
29-Oct-2018
Dear Dr Lind,
I am pleased to inform you that your manuscript entitled "What can associative learning do for
planning?" is now accepted for publication in Royal Society Open Science.
You can expect to receive a proof of your article in the near future. Please contact the editorial
office (openscience_proofs@royalsociety.org and openscience@royalsociety.org) to let us know if
you are likely to be away from e-mail contact. Due to rapid publication and an extremely tight
schedule, if comments are not received, your paper may experience a delay in publication.
Royal Society Open Science operates under a continuous publication model
(http://bit.ly/cpFAQ). Your article will be published straight into the next open issue and this
22
will be the final version of the paper. As such, it can be cited immediately by other researchers.
As the issue version of your paper will be the only version to be published I would advise you to
check your proofs thoroughly as changes cannot be made once the paper is published.
On behalf of the Editors of Royal Society Open Science, we look forward to your continued
contributions to the Journal.
Kind regards,
Royal Society Open Science Editorial Office
Royal Society Open Science
openscience@royalsociety.org
on behalf of Dr Alecia Carter (Associate Editor) and Kevin Padian (Subject Editor)
openscience@royalsociety.org
Follow Royal Society Publishing on Twitter: @RSocPublishing
Follow Royal Society Publishing on Facebook:
https://www.facebook.com/RoyalSocietyPublishing.FanPage/
Read Royal Society Publishing's blog: https://blogs.royalsociety.org/publishing/
Appendix A
Reply to reviewers
Johan Lind
August 24, 2018
I would like to thank reviewer 1 and 2 for making many good points that I
think have improved the manuscript. Please nd comments and replies below
(comments in italics and replies in normal font). Comments about typos are
excluded and typos have been corrected.
1 Editorial comments
• I have now received two constructive reviews of your manuscript. Both
reviewers agree that the manuscript asks an interesting question but both
point out several places where the manuscript could be improved. These
encompass alternative interpretations that should be considered, clarica-
tions that will improve the readability of the manuscript, and literature
that should be included to present a more impartial view of the eld. I
found the comments to be very helpful and I encourage the author to fully
consider these in a resubmission.
Please nd my responses to the reviewers suggestions below. I would like
to thank the reviewers for many good and constructive suggestions. I have
considered all comments and suggestions and edited the manuscript and
gures in line with almost all of them. I did not add statistical analyses
which was put forward by reviewer 1. Please see detailed comments below
for why I did not consider that adding statistical analysis would improve
this manuscript. This was the only major suggestion that I did not agree
with.
2 Reviewer 1
2.1 Major points
• My critiques are detailed below. In general, I believe that there needs to
be greater clarity and detail in both the review of the animal cognition
literature in this eld and of the model being used. In particular, the au-
thors have omitted some key studies (Osvath & Osvath 2008 in particular)
which may speak to their results, and alternative methodologies for assess-
ing planning (e.g. Correia et al., 2007; Cheke et al., 2012) which may be
1
less easily modelled. The studies being modelled need to be more clearly
explained and in particular it should be noted which experiments were per-
formed as controls. The results section needs to be signicantly claried,
in particular the graphs are extremely dicult to interpret, and the overall
reporting of results appears to be very gist-based when, unless I've com-
pletely misunderstood (which is possible!) it would be possible to conduct
some statistical tests to address the hypotheses directly.
• These general points are all covered by the detailed comments and there-
fore answered in detail below.
2.2 Detailed comments
• Beating human players on a specic game does not equate to providing a
attractive account for human behaviour or an example of human-like
behaviour. These games are not reections of any particular cognitive
skill in humans, and indeed most humans nd them very dicult, while
modern computers nd some of them (e.g. chess) relatively simple. In
general, associative learning as an explanation for human behaviour is
even more dismissed than it is in animal behaviour. The paradox might
thus be more accurately described as one of great achievement in producing
complex exible behaviour in the eld of AI, while dismissed as a model
for exible behaviour in biological systems (both human and animal).
I would like to thank reviewer 1 for making this point. I agree and changed
the paradox sentence in the rst paragraph of the introduction to: It is
an intriguing paradox that associative learning is acknowledged for pro-
ducing complex exible behavior within AI research, but is often dismissed
and neglected as a model for exible behavior in biological systems (both
humans and non-human animals).
• I would suggest that the introduction needs to be signicantly expanded to
allow the authors to go into more detail on a number of issues. - Firstly,
its important to recognise the degree of discussion regarding associative
learning in exible animal behaviour. These do not fall entirely into the
boxes of ignored or dismissed  there are plenty of associative controls
out there. The very valid criticism of these is that they usually assume the
simplest possible form of associative learning and often don't take account
of the more complex factors that the authors' model relies upon, such as
secondary reinforcement (infact, this distinction would be a good one to
make, since the authors explicitly assess the dierent prediction of a sim-
plistic associative learning model relying on primary reinforcement vs. a
chaining one). However, some DO take a more nuanced approach (see Os-
vath & Osvath, 2008, for a secondary reinforcement control within a very
similar task to Mulcahy & Call  i.e. the two-choice task in which the
2
subjects should choose the tool, but once they have the tool (because they
do not need more than one) should then choose the small reward  the au-
thors argued that had the functional tool acquired secondary reinforcement
properties, then the apes would choose the tool in both choices).
Thanks again, I think these comments were really helpful. I have expanded
the introduction as suggested. First I added a paragraph (third paragraph
in Introduction) to better represent the literature and acknowledge that
there are plenty of associative controls out there. it does not fall into two
boxes of ignored and dismissed. I agree with the reviewer that a large
problem is that when associative learning is acknowledged it is assumed
to be the simplest form of associative learning. I have also added the
suggested studies. But, as the suggested study (Osvath & Osvath 2008)
has been criticized I added a reference to one of the critics of that study.
This new paragraph now reads:
Not all studies fall into these two categories and many studies test al-
ternative explanations and control for associative learning. However, it is
common that such studies assume only the simplest forms of associative
learning. This will likely result in false rejections of associative learning
hypotheses. This is because most vertebrates and invertebrates exhibit
capacities for both operant and Pavlovian learning (Carew and Sahley,
1986; Bouton, 2007), that together with specialized memories (Lind et al.,
2015) make most animals capable of more complex learning than what the
simplest forms of associative learning allow. For example, Osvath and Os-
vath (Osvath and Osvath, 2008) set out to control for associative learning
when studying planning in apes. However, their conclusion that apes plan
for the future seem premature as the dynamic nature of memory updates
and decision making involved in associative learning was not taken into
account (Suddendorf et al., 2009).
• Secondly the associative learning literature itself (and the theories regard-
ing secondary reinforcement) seems to be completely ignored. I recognise
that this is explored in depth in their previous work (Enquist et al., 2016),
but both the background literature and the model itself need more introduc-
tion here, and not to assume that the readers have read that paper (which
I had to do to follow this ms).
Yet another good point. To rely less upon our own previous work and make
this manuscript more comprehensible on its own I added two paragraphs in
the introduction that specically introduces aspects of associative learning
that are relevant for planning behavior and a brief introduction to our
model. Here are the two new paragraphs (paragraph ve and sixe in the
Introduction):
Why would an associative learning model be useful for understanding
future oriented behavior? Associative learning is well known for causing
3
anticipatory behaviors, behaviors that can predict later meaningful events
without immediate benets (Bouton, 2007; Pearce, 2008). Furthermore,
self control, often mentioned as important for planning (Shettleworth,
2010; Kabadayi and Osvath, 2017), can arise through associative learning
(Enquist et al., 2016). It might be assumed that self-control is not possi-
ble through associative learning because immediately rewarded behavior
should always be preferred to non-rewarding behavior. But, for many an-
imals 'wait' or 'stalk' are behaviors that can be reinforced when followed
by later possibilities for rewards. For example, predators learn stalking
and waiting skills when they are young (Fox, 1969; Eaton, 1970).
The model used here is an associative learning model capable of learning
optimal behavior in a complex world (Enquist et al., 2016). The model
includes two dierent memories and a decision making mechanism. One
memory stores the associative strength of performing behavior B towards
stimulus S, and the other memory stores the estimated value of stimulus
S. The model can learn behavior sequences by linking single behaviors to-
gether through conditioned reinforcement (secondary reinforcement). This
way, initially neutral stimuli that precedes primary reinforcers can them-
selves become reinforcers, thereby modifying previously unrewarded be-
havior (Kelleher and Gollub, 1962; Mackintosh, 1974; Williams, 1994). For
example, a clicker trained rabbit has heard clicks repeatedly prior to food
rewards. For this rabbit a click becomes rewarding in itself and the rabbit
will learn to perform behaviors that only result in the rabbit hearing a
click (McGreevy and Boakes, 2011). The model is further explained in
the Methods section below.
• It is important to distinguish between conclusions about animal cognition
and conclusions about tests of animal cognition. While in the introduction
the aim of this study in modelling particular animal cognition tasks is
mostly clear, in places (particularly the discussion) it reads much more
like they are modelling animal planning itself. The methods of the modelled
studies have been criticised in the animal cognition literature for years as
solvable through associative learning (see Cheke & Clayton, 2010). But
that doesn't tell us anything about animal planning  just that these tasks
are unsuitable for testing it. This is in contrast to the planning for a
future motivational state style studies, which are less vulnerable to this
critique (although there are plenty of dierent critiques of those!). For
example, while the studies with scrubjays and Eurasian jays may indeed
be using a specialised memory repertoire (what kind of repertoire this
might be has never been dened, so it is not at present a valid reason to
dismiss them), there are two studies (Correia et al., 2007 and Cheke et
el., 2012) that specically modulate the current and future value of certain
rewards. It would be interesting to have a perspective on these types of
planning experiments, rather than avoiding mentioning them altogether.
4
I would like to thank the reviewer for pointing out the dierence between
modelling particular tasks and modelling cognitive mechanisms per se. I
am not aware of a Cheke et al. 2012 planning study, I hope the reviewer
meant Cheke and Clayton 2012. In line with the suggestion I have edited
the discussion so that it is clear now that the simulations address animal
cognition tasks, and not how planning takes place in the brain of an an-
imal. Also, I added Cheke and Clayton 2010 to the introduction as yet
another source that points out methodological problems with Mulcahy
and Call 2006. Thanks for reminding me about that reference.
I agree that it is important to mention the caching studies. For this reason
I added a paragraph in the discussion where I discuss these clever studies
and I mention suggestions for how an associative learning model can be
expanded to also be applicable to planning in caching contexts. This new
paragraph reads:
The simulations show that the ape- (Mulcahy and Call, 2006) and raven-
study (Kabadayi and Osvath, 2017) can be understood through associa-
tive learning. However, results from experiments with caching special-
ists (Correia et al., 2007; Cheke and Clayton, 2012), probably dependent
upon genetic specializations (Premack, 2007; Suddendorf and Corballis,
2010; Bourjade et al., 2014), are currently beyond the scope of our learn-
ing model. Caching- and feeding behavior involve dierent motivational
states in animals (Clayton and Dickinson, 1999). Motivational states can
be regarded as internal stimuli and readily integrated in an associative
learning model, which would result in increased exibility in terms of
making foraging and caching decisions. Our model does not include dif-
ferent motivational states in its current state, but we have given examples
of how genetic predispositions can be integrated with the model (Table
2 in Enquist et al., 2016). One possible solution would be to introduce
context-dependence, so that exploration is dierent for dierent external
stimuli and/or for dierent internal states. Importantly, when making
assumptions about more exible mental mechanisms, the higher costs of
exploration that are incurred by increased exibility needs to be taken into
account (see 3.3. in (Enquist et al., 2016)). We expect that evolution has
ne-tuned genetic predispositions that together with associative learning
generate productive and species-specic behaviors.
• The available actions doesn't include moving between rooms, which was a
key feature in the original study. Why was this?
Thanks for pointing this out. One could include more details in the sim-
ulations of the behavior sequences. When it comes to moving between
rooms, Mulcahy and Call wrote that the subjects were ushered outside
the test room into a waiting room (suppl. material page 1-2). And, af-
ter the waiting time Subjects were allowed to return to the test room.
5
I think no important details are left out when assuming that the apes
were fully capable of being ushered outside the test room and that they
were capable of returning to the test room. However, as the simulations
start from scratch whereas the animals were experienced adults, the sim-
ulations would need more trials to learn the specied everyday behaviors.
After everyday behaviors had been learnt, there is no reason to suspect
that model results would dier from the ones obtained now. To make
this clearer, that such everyday behaviors were excluded from the simula-
tions, I emphasized it more clearly in the method section. I modied this
sentence in the second paragraph under the heading Simulating planning
studies on great apes and ravens:
To both make the simulations possible and realistic, it was assumed that
the animals entered these studies with some necessary everyday skills. It
was assumed that the animals had, for example, previously learned to hold
objects, how to move between rooms and compartments, where dierent
things were located, and some basic skills regarding how to interact with
the experimenters. The apes were for instance ushered out of the test room
after choices to later be allowed back into the test room. By ignoring such
everyday skills, the simulations and the behavior descriptions were focused
on the unique behavior sequences the animals had to learn as part of the
experiments.
• The details of the original empirical studies needs to be made clearer. For
example, the signicance of experiment 4 (in mulcahy & call) where the
tool is rewarded but not functional.
To make the descriptions clearer I have changed Materials and methods.
After introductory paragraphs Simulating planning studies on great apes
and ravens I added more complete verbal descriptions of both Mulc-
ahy and Call and Kabadayi and Osvath. These new verbal descriptions
should, together with the formal descriptions of procedures and behavior
sequences, make it easier to follow the logic of these two studies. I also
added more information about the tool in experiment 4 in Mulcahy and
Call. To show how formal behavioral descriptions can be integrated with
the model I also added a short section with an Illustration of memory
updates during pretraining.
Experiment 4 in Mulcahy and Call is now mentioned in both the method
section and the results. The third paragraph of the result section mentions
this aspect in detail. This experiment is dicult to interpret. Importantly,
the conclusions of the manuscript do not depend on this experiment.
• The results as presented are not at all easily interpretable. They are pre-
sented entirely in the graphs, with lines for the models (averaged across the
6
500 subjects?) and lled circles to represent data from empirical studies
(but the form of this data is not clear at all  are these circles the average
probability of choosing the correct across the trials for that experiment?
In which case it would be clearer to have a gure that illustrates a similar
metric for the model.) The diculty in interpreting these graphs is per-
haps illustrated by the fact that I cannot work out how it can be that the
lower panel in gure one is showing that the model overpredicts perfor-
mance in the ravens tool task (as claimed in the results section), as it looks
very much like it underpredicts it. It is also confusing to have 3 factors for
the model (tool, small reward, distractor) but only one for the empirical
studies. . . these graphs need to be signicantly claried.
I would like to thank reviewer 1 for pointing these things out. To make the
results easier to understand I have made several changes. First, I added a
short paragraph in the method section to clarify what empirical data was
extracted from the two empirical studies and used in the graphs. This
has its own subheading Data from the empirical studies and it reads like
this:
To compare the simulation results with the empirical data from the two
studies (Mulcahy and Call, 2006; Kabadayi and Osvath, 2017), averages
were calculated from the available data in the two respective studies (see
gures in Results). This resulted in the average proportion of correct and
incorrect choices in the forced choice tests. Note that experiment 4 in
the ape study did not involve any correct behavior using the tool upon
returning to the apparatus after the delay, making this experiment dicult
to interpret. In addition, data for experiment 4 was not available in the
text, therefore data from Figure S2 (Mulcahy and Call, 2006) was used for
that data point. It is unfortunate to mix data this way, but experiment
4 in (Mulcahy and Call, 2006) is of the least importance for comparative
purposes.
Secondly, thanks to the reviewer I have made the graphs clearer. I un-
derstand that there was a lot of information in each graph, so to improve
readability of the graphs I have changed both gure 1 and 2 and the re-
spective gure captions. The new gure 1 only represents proportion of
correct responses comparing simulation data with empirical data. I re-
moved model simulations of incorrect choices to distractor items because
this is of little importance and practically given by the correct choices. I
changed the label on the y-axis to Proportion of responses towards stim-
uli as this is correct for both simulations and empirical data points. For
comparative purposes I also added what chance levels, as horizontal lines,
were expected for correct choices in the empirical studies. This shows that
simulations are well above chance levels. Responses towards small rewards
were kept in the graph because the authors of the raven study consider
that of key importance.
The new gure 2 includes the same data for both the raven- and the ape
study. In addition, in this gure information about small reward was
7
excluded from the raven study for two reasons. First the gure is now
more clearly focused on the comparison between our chaining model and
stimulus-response learning, and the gure becomes clearer and easier to
interpret.
Thanks also for pointing out an important typo, yes, the simulation un-
derpredicts, not the other way around. This has been xed.
• Relatedly, Mulcahy and call give the exact trials in which the apes selected
the correct tool in their paper, so why is this detail not represented?
The data that is simulated and extracted from the two empirical studies
are the forced choice decisions, these are given as averages in the main text
of the Mulcahy and Call paper and are not presented in the table with
exact trials (not even in the supplementary information). Regardless,
comparing averages suces for these purposes. There is a lot of variation
in the Mulcahy and Call study that makes a more detailed comparison
dicult. For example, of the 10 apes that participated, six individuals
did experiment 1, two individuals did experiment 2, four individuals did
experiment 3 and 4. With such methodological variation, I think compar-
isons of averages per experiment suces. Better experimental control is
needed to enable comparisons of individual learning trajectories.
• In general it remains not entirely clear what question the study is address-
ing. I think the ms would benet from approaching this data with two
distinct hypotheses: 1. These tasks can be solved with a chaining model
(i.e. this model will choose the functional object signicantly more often
than expected by chance 1a. . . and can do so in a time frame (number
of trials) equivalent to that of apes/ravens 2. This model can match the
behavioural performance of apes and ravens in these tasks.
This dierentiates between the two messages one might be able to take
form these results  i.e. 1. That the task is solvable with chaining, and 2.
That those animals in that experiment likely DID solve these tasks using
chaining. If 500 subjects were run, then surely there should be some stan-
dard deviations and error terms representing the variance in the dierent
subjects, which can then be statistically assessed with reference to these
hypotheses? ie:
1. Did the model subjects choose the functional tool signicantly more
often than expected by chance (when given the same number of trials as
apes and ravens) 2. Does the performance of the animals subjects dier
signicantly from that of the models (for example, a one-sample t-test with
the data from the model being the population).
8
You can then enter into the more qualitative exploration of the pattern
of performance across the dierent experiments (as the results currently
contain).
These are interesting suggestions. I made the hypothesis explicit in the
rst sentence of the last paragraph in the Introduction. Here I test the hy-
pothesis that an associative learning model can account for results found in
these nonhuman planning studies I also start the Discussion by relating
to this hypothesis more explicitly. Now the Discussion starts: Simula-
tions of the two planning studies on ravens and great apes suggest that
behavior previously claimed to have been generated by exible planning
(Kabadayi and Osvath, 2017; Mulcahy and Call, 2006) can be accounted
for by associative learning.
To address the question regarding more detailed comparisons between the
simulations and the empirical studies I want to point out that I wish the
researchers would've performed experiments with known initial values.
For instance by using methods that were more novel to the animals. The
ravens in Lund (used in the Kabadayi and Osvath study) are well trained
on various tasks and have been used in many previous studies. For under-
standing behavior, it is very dierent when these well trained ravens enter
an experiment with a lot of knowledge whereas the simulations start from
scratch. I mention this in the Result section. Initial values are critical for
evaluating behaviors in tests, and neither of the studies provide in depth
descriptions of the subjects' prior experiences. I therefore think that little
would be gained from performing statistical analyses. Furthermore, to
be conservative all simulations started from scratch, assuming that there
was no knowledge about the setups and all associative strengths between
behaviors and stimuli were equal for all combinations. Despite this, consid-
erable learning took place and simulations show that associative learning
will yield similar results as the empirical studies with choices well above
chance levels. But I hope future studies will be performed that allow de-
tailed tests. To make this point, I added three sentences to the Methods
section where initial values of the simulations are specied:
The lack of information of initial values of the animals make exact quan-
titative comparisons dicult.
Although both the ravens and the apes had rich backgrounds, previously
learned behavior were ignored and initial values were assumed to be the
same for distractor objects and functional objects. To be conservative all
associative strengths between behaviors and stimuli were assumed to be
equal at the start of the simulations.
• When it comes to looking that the results for experiment 4 in the ape
experiment, it should be noted that the whole point of that experiment
was that it was a control - the authors argued that if the task was being
9
solved through associative learning, then you'd expect the apes to do just
as well on this task as the others (like the model did), but if it was being
solved through understanding of future functionality, then they wouldn't
(as what's the point of choosing a tool when the apparatus isn't there).
So in theory, the fact that the model signicantly outperformed the apes
on that task indicates that the apes *weren't* using chaining (or at least,
one could make that argument). In practice, however, if you look at the
data of the four apes that took part in that experiment (in the table in
Mulcahy & Call), then 2 of them never happened to pick the right tool
(and therefore could not have learned the association) and the other two
performed comparably to the apes in the other experiments (see Cheke &
Clayton, 2010 for this critique). As such, the argument could be made that
the model *does* t with the data of the apes that had the opportunity to
learn (by choosing the rewarded behaviour in early trials).
I agree with the reviewer (and the selected references) that experiment 4 is
not clear cut to interpret. I want to stress, however, that the overall results
show that learning occurs in the simulations and that simulations match
overall patterns in the empirical data. I think that the manuscript will
not benet from more discussions about what experiment 4 in Mulcahy
and Call means. I want to thank the reviewer for reminding me of Cheke
and Claytons discussion about this paper, I added this reference to the
Result section where this issue is mentioned.
• Why is the comparison between Rescorla-Wagner and chaining only shown
for the raven data?
This has been xed. I have now included the ape date into gure 2.
• It is also misleading to refer to Rescorla-Wagner style learning as stimulus-
response as this (to my mind) implies that the learning would be inexible
and habitual rather than goal-oriented.
It is technically correct to call this operant version of the Rescorla-Wagner
stimulus-response learning (as dened in Enquist, Lind & Ghirlanda
2016). But because this version includes a decision making mechanism
and involves updates between stimuli and responses instead of between
a CS and a US I have changed in the manuscript. Now I refer to our
instrumental version of the Rescorla-Wagner model as one version of
stimulus-response learning. I think this also helps clarifying that the
stimulus-response learning model used in the manuscript is is dierent in
that it concerns operant learning rather than classical conditioning.
• Again, the gure legend for gure 2 is not sucient and the graph is very
dicult to understand without proper explanation.
The gure caption has been edited.
10
3 Reviewer 2
• Lind might want to note that bottom-up associative models are potentially
more illuminating than top-down higher-order cognitive models in that the
top-down models often leave us wondering exactly how the animal actually
performed the cognitive process. That is, top-down models often `leave
the animal lost in thought' rather than illuminating the critical underlying
processes by which the animal solves the task.
I think this is a good point, and I added a sentence in the last paragraph
of the Discussion to make this point. Thank you.
• On p. 3, Lind writes A brief summary of the logic of our model is included
in the Methods section below. I think that the architecture and rules of
the associative models used here are too central to the paper to put o any
sort of description of it until the Methods section (even when the Methods
section starts in the next paragraph). Something should be said about it
in the Introduction because there are so many dierent associative models
that generic reference to them is almost meaningless. Related to this,
the paragraph containing Eq. 1 is rather opaque and will challenge most
readers. The material here has to be elaborated and claried.
Here, reviewer 2, just like reviewer 1, want a more elaborate introduction
to the model and associative learning. As mentioned above I have now
included more information about this in the Introduction (paragraph 5
and 6 in the edited manuscript). To clarify the description of the model I
included a practical example of a dog learning to lift its paw when hearing
the command `shake'. The dog example is now at the start and at the
end of the description of the model, which I hope should make the model
description easier to follow.
• The paper is cast as `associative models of broad generality' vs. `cognitive
models that posit animals have an assortment of higher-order cognitive
skills.' An alternative to this viewpoint would be to consider associative
models as a dierent level of analysis than cognitive models. That is,
instead of simple associative learning and higher-order cognitive process-
ing of information being fundamentally dierent, it is possible that `au-
tomatically' acquired associations could be the basis of higher-order cog-
nitive processing. Thus, they might simply be dierent levels of analysis
as are physics, chemistry, and biology. Associations could be the building
blocks of propositions, and hence propositions and automatic associations
may simply reect dierent levels of analysis similar to the relationship
of physics to chemistry and chemistry to biology. ... Obviously dissoci-
ations such as the Perruchet eect challenge such a relationship just as
11
they challenge purely propositional accounts. But in my [biased] opin-
ion, such dissociations are not grounds for categorical rejection just as the
absence of missing links is not grounds for rejection of the theory of evo-
lution; further research may well ll in the void. My central concern with
the propositional position is that it does not adequately specify underlying
mechanisms, thereby leaving too much of the heavy lifting to some sort of
executive function (aka the homunculus). A further challenge to explain-
ing learning entirely in terms of associations arises from papers conclud-
ing that associative learning does not occur without conscious awareness.
However, although less often cited, there is a parallel literature asserting
that associative learning does sometimes occur without awareness... and
I do not think that the nay sayers have managed to discredit this entire
literature. Thus, I think that it is premature to dismiss the possibility that
associations are the basis of all learning. Obviously, this is a minority
view. Hence, I am only suggesting that the author entertain it.
I would like to thank the reviewer for pointing this out. There is some
important synthetic work to be done to harmonize the view of dierent
elds. This comment also relates to the rst comment of reviewer 2 about
top down vs. bottom up, and to make that point I added a sentence to
the nal paragraph of the Discussion.
• The paper considers S-B (operant, so-called habits) and S=S' (Pavlo-
vian) learning, but does not consider R-S' (so-called goal-directed behavior)
learning. Perhaps including this would make the paper unduly long and
complex, but the author might at least mention its omission.
I included this in the rst part of the model description where I added a
reference to Pearce's discussion of this in his book Animal Learning and
Cognition and wrote:
The model can thus generate goal-directed behavior (see (Pearce, 2008,
p. 32) for another discussion of goal-directed behavior and learning).
• Lind often allowed the simulations' procedural parameters to dier from
that of the behavioral experiments. For example, on p. 8, he writes the
number of trials did not perfectly match the empirical studies was due
to the probabilistic nature of the decision making equation. I understand
what he has written, i.e., the rationale for his doing what he did , but I
still nd it less than satisfying. There are other ways that Lind could have
addressed this problem (e.g., averaging the results of more runs).
This is valid point and I have given it a lot of thought as to how one
would best compare the empirical data with the results from the simula-
tions. However, the mismatch in the number of trials is small, and making
12
averages would not change the conclusions. Averaging the results of more
runs would skew the learning curves a little bit towards the right, making
the t better for some results (e.g. the Token condition and the start of
the tool condition in the raven study), but slightly worse for some other
results (e.g. the end of the tool condition in the raven study). But, as I
pointed out in response to reviewer 1 (and claried in the edited Discus-
sion), exact quantitative comparisons between empirical and simulation
data are dicult to make. Most importantly because the ravens and the
apes enter the experiment with lots of knowledge whereas the simulations
start from scratch.
• Other likely deviations between the simulations and the behavioral data
arose from information that Lind asserts was omitted in the behavioral
publications. For example, on p. 8, Exact information about the amount
of extinction was lacking from the raven study, therefore it was assumed
that the ravens had have extinction experiences with the distractors. It
would have been more appropriate for Lind to contact Kabadayi & Osvath
to obtain this information.
I have contacted the corresponding author of that paper to get the details,
but have not yet received a reply. I think the assumptions are reasonable.
And given the rich backgrounds these birds have, in combination with
the fact that simulations start our with no prior discriminative knowledge
about the dierent objects, the simulations are likely to be conservative.
References
Bourjade, M., Call, J., Pelé, M., Maumy, M., Dufour, V., 2014. Bonobos and
orangutans, but not chimpanzees, exibly plan for the future in a token-
exchange task. Animal cognition 17 (6), 13291340.
Bouton, M. E., 2007. Learning and behavior: A modern synthesis. Sinauer,
Sunderland, MA.
Carew, T. J., Sahley, C. L., 1986. Invertebrate learning and memory: from
behavior to molecules. Annual review of neuroscience 9 (1), 435487.
Cheke, L. G., Clayton, N. S., 2012. Eurasian jays (garrulus glandarius) overcome
their current desires to anticipate two distinct future needs and plan for them
appropriately. Biology letters 8 (2), 171175.
Clayton, N. S., Dickinson, A., 1999. Motivational control of caching behaviour
in the scrub jay, aphelocoma coerulescens. Animal Behaviour 57 (2), 435444.
Correia, S. P., Dickinson, A., Clayton, N. S., 2007. Western scrub-jays antici-
pate future needs independently of their current motivational state. Current
Biology 17 (10), 856861.
13
Eaton, R. L., 1970. The predatory sequence, with emphasis on killing behavior
and its ontogeny, in the cheetah (acinonyx jubatus schreber). Zeitschrift für
Tierpsychologie 27 (4), 492504.
Enquist, M., Lind, J., Ghirlanda, S., 2016. The power of associative learning
and the ontogeny of optimal behaviour. Royal Society Open Science 3 (11),
160734.
Fox, M., 1969. Ontogeny of prey-killing behavior in canidae. Behaviour 35 (3-4),
259272.
Kabadayi, C., Osvath, M., 2017. Ravens parallel great apes in exible planning
for tool-use and bartering. Science 357 (6347), 202204.
Kelleher, R. T., Gollub, L. R., 1962. A review of positive conditioned reinforce-
ment. Journal of the Experimental Analysis of Behavior 5 (4 Suppl), 543597.
Lind, J., Enquist, M., Ghirlanda, S., 2015. Animal memory: A review of delayed
matching-to-sample data. Behavioural processes 117, 5258.
Mackintosh, N. J., 1974. The psychology of animal learning. Academic Press,
London.
McGreevy, P., Boakes, R., 2011. Carrots and sticks: Principles of animal train-
ing. Darlington Press.
Mulcahy, N. J., Call, J., 2006. Apes save tools for future use. Science 312 (5776),
10381040.
Osvath, M., Osvath, H., 2008. Chimpanzee (Pan troglodytes ) and orangutan
(Pongo abelii ) forethought: self-control and pre-experience in the face of fu-
ture tool use. Animal cognition 11 (4), 661674.
Pearce, J. M., 2008. Animal learning and cognition, 3rd Edition. Psychology
Press, Hove, East Sussex.
Premack, D., 2007. Human and animal cognition: Continuity and discontinuity.
Proceedings of the National Academy of Sciences 104 (35), 1386113867.
Shettleworth, S., 2010. Cognition, evolution, and behavior. Oxford University
Press.
Suddendorf, T., Corballis, M. C., 2010. Behavioural evidence for mental time
travel in nonhuman animals. Behavioural brain research 215 (2), 292298.
Suddendorf, T., Corballis, M. C., Collier-Baker, E., 2009. How great is great
ape foresight? Animal cognition 12 (5), 751754.
Williams, B. A., 1994. Conditioned reinforcement: Experimental and theoretical
issues. Behavior Analyst 2, 261285.
14
Appendix B
Reply to reviewers
Johan Lind
October 23, 2018
Thanks again to the reviewers for reading the ms carefully. Please nd
comments and replies below (comments in italics and replies in upright font).
1 Editorial comments
• I have now received two review of your resubmission from the original
reviewers. Both reviewers and myself nd the manuscript to be greatly
improved. I agree with reviewer 1's nal point for the reasons highlighted
by the reviewer, but, as pointed out, this could easily be addressed by some
additional text in the discussion.
Thanks for the comments. I have now modied the manuscript in line
with the comments made by the reviewers. See replies to reviewers for
details.
Another thing is that I have changed operant to instrumental through-
out. As instrumental conditioning and operant conditioning are synony-
mous, and because I normally use instrumental conditioning in other
manuscripts, I have changed from operant to instrumental throughout
the manuscript. (The problem of not writing in one's mothertongue has
diverse manifestations.)
When replying to reviewer 1's comments about Mulcahy and Call's Exper-
iment 4 I noticed a minor error in the gures, I have changed the gures
accordingly. I ran all scripts again and conrmed that all simulations are
correct.
To make this study easier to replicate and the supplementary information
more transparent, I have improved the supplementary material. For this
reason I have submitted a supplementary information le that contains
a description of the learning simulator, the simulations and all necessary
code. It contains all information needed to perform the simulations, make
the gures and export data. I have therefore updated the section Data
Availability and now point to supplementary information instead of a data
repository.
1
2 Reviewer 1
2.1 Major points
• In general, I am happy with the responses to my comments and revisions
to the manuscript. However, I'm afraid I'm going to have to double down
on a particular point.
• I profoundly disagree that further discussion of experiment 4 is unhelp-
ful  this is because often in animal cognition we are unable to rule out
associative learning in the critical behaviour, but instead include controls
that, were associative learning being used, should be equally well solved,
but were another system being used, should be failed. In practice, Mulcahy
and Call's RESULTS were hard to interpret, as they imply that this control
task was not solved well, but as we've already discussed, this is because two
animals never had the chance to learn. But your model nicely shows the
point that this control task, if approached with associative learning, would
have been solved just as well as the other tasks. Thus if M&C's results
HAD shown that the apes legitimately failed that task, then the dierence
between the model and the behavioural data might be expected to be seen
ONLY in this task and that would be highly meaningful. I realise that this
is a somewhat hypothetical distinction, but it is an important point when
it comes to dierentiating results in animal cognition studies from asso-
ciative accounts  for example Osvath & Osvath's (2008) control condition
speaks to exactly the account your model uses (i.e. that the behaviour of
taking a tool becomes itself reinforcing). If you were to run your model
with the data from that study, you'd expect the model agent to match the
apes in most tasks (i.e. you wouldn't see a dierence and could potentially
conclude that the behaviour was the result of associative learning), but in
the control task choose the tool twice, not choose the tool then the small re-
ward as the apes did. Thus you'd expect a dierence ONLY in the control
task, which is why it's important.
This discussion of the control task needs to be included in the discussion.
At face value (if you don't delve into the details of the individual apes per-
formance on exp 4) your data actually support M&C's conclusions  that
is, if associative leaning were being used, you'd expect good performance
on everything, if planning, then you'd expect good performance on every-
thing except exp 4. This links back to the highlighted dierence between
whether you're assessing the animals themselves or the task. In terms of
the animals, the results of exp 4 are inconclusive and we can't make a
good comparison, in terms of the design of the control task as an associa-
tive learning control, your model provides support for it  it is well solved
by associative learning, so if animals fail that while passing the others,
that does provide a dierentiation between the accounts.
As suggested by reviewer 1 and the editor, I have included a new paragraph
in the Discussion where I address this issue and also present arguments
2
from Mulcahy and Call. In the paragraph I present the arguments given by
Mulcahy and Call that this could separate between associative and more
cognitive explanations. I also put forward out one of their own arguments
for the idea that their control cannot distinguish between the dierent
explanations. This paragraph reads:
Mulcahy and Call (Mulcahy and Call, 2006) attempted to rule out in-
strumental conditioning as an explanation for the behavior of the apes by
performing Experiment 4. This phase was similar to Experiment 3, but
the apes were not rewarded for using the functional tool. Instead of an
ape entering the room with a functional tool that could be used to get a
reward (as in Exp. 3), an ape entered the room and found a reward if it
had carried the functional tool to the test room from the waiting room.
It was argued that if the apes performed better in the other experiments
than in this one, it would suggest that the apes planned exibly. Mulcahy
and Call concluded their results represent a genuine case of future plan-
ning. A devil's advocate could identify dierences between experiment 3
and 4 rendering learning a more likely explanation. In Experiment 3 the
apes were explicitly rewarded for using the tool. This results in a high
conditioned reinforcement value for the tool and a high stimulus-response
value for using the tool on the apparatus. In Experiment 4 however, Mulc-
ahy and Call point out that there was a longer time between picking the
tool up in the waiting room, carrying the tool to the test room, to subse-
quently get a reward without using the tool. Perhaps the low performance
in Experiment 4 was caused by the unclear connection between the tool
and the reward, as the delay inhibits the acquisition of picking up the
tool to later receive a reward. Proper control conditions are important to
enable the rejection of hypotheses unambiguously (see e.g. recent discus-
sions in (Ghirlanda and Lind, 2017) and (Henneeld et al., 2018)). Our
learning model can be used in future research to analyze such behavioral
dierences caused by variation in learning contingencies.
2.2 Detailed comments
• Abstract: One phenomenon in which associative learning has 13 been
ruled out as an explanation for animal behavior is exible planning
This makes it sound like associative learning has been generally ruled out
as a possible explanation, which I don't think is true at all.
Thanks for spotting this. I have improved the wording so that it now
reads: One phenomenon in which associative learning often is ruled out
as an explanation for animal behavior is exible planning. 
3
• Introduction: Page 2, paragraph starting line 68  This is totally up to
the authors to judge, but I mentioned Osvath and Osvath in the initial re-
view because they included a well thought-through associative control which
would actually be able to dierentiate between behaviours and was based on
a specied associative account. This is in contrast to most associative con-
trols which involve dierent behaviours and feedback to the main tasks and
are generally not useful comparisons (see the search controls in this paper
for example (https://journals.plos.org/plosone/article?id=10.1371 /jour-
nal.pone.0026887). Thus I don't think its really fair to use this example
here, especially since Suddendorf 's critique is incredibly vague and doesn't
really speak to associative learning.
I omitted the use of Osvath and Osvath in this example as suggested.
• Methods: This last part was of little importance here for reasons men-
tioned in the Result section. . . . experiment 4 in [26] is of the least
importance for comparative purposes. I have explained why this is not
the case. You cannot ignore control tasks when making these kinds of
comparisons between animals and computational models. The contrast in
performance between the control tasks and the experimental tasks is the
whole point.
I omitted the sentence in line with this comment.
• Methods: Page 10 line 397 - That animals remember stimulus-response
associations and stimulus values for a very long time was not mentioned
in neither of the simulated studies [26, 30]. Two points  the rst gram-
matical (either say not mentioned in either or mentioned in neither)
(see also page 13, line 530) the second is relevance  you should cite ev-
idence that these are maintained in long term memory, not that this was
not mentioned in those previous studies.
Thanks for pointing this out as it of course makes sense.
• Discussion: gure ??  page 13, lines 527 and 529; page 14 line 557
Sorry for broken references, this has been xed.
4
3 Reviewer 2
3.1 Major points
• The author has extensively revised the original submission largely in ac-
cord with the suggestions of the two reviewers, and as a result this is a
signicantly better paper. The paper has been claried, albeit sometimes
not quite to the degree that I would have liked. Importantly, clearer `qual-
iers' with respect to what the present model cannot do have been added
(e.g., dierential motivational states as cues). I was particularly pleased
to see that the author now makes clear that he is modeling behavior, as
opposed to modeling planning about behavior. More generally, the larger
question being posed concerning the power of associative models to account
for complex behavior is a monumental one. As a result, the answer to it is
necessarily going to be exceedingly complex and surely will not ever be com-
prehensively addressed in any single paper. The present paper might best
be viewed as a `proof of concept.' That said, although I can think of many
additional aspects of matching the simulations to the actual experiments
that I would like to see addressed, the present manuscript does a good
job in addressing the major facets of this complex issue. Yet more detail
concerning modeling details of the focal data could be requested, but elab-
orating on them would likely obscure the basic point of the paper (i.e., the
forest would get lost among the trees). As is stands, the present revision
moves the discussion forward in a constructive manner. Consequently, I
strongly support it being published in its current formn this information.
I thank reviewer 2 for this comment.
References
Ghirlanda, S., Lind, J., 2017. 'Aesop's fable' experiments demonstrate trial-and-
error learning in birds, but no causal understanding. Animal Behaviour 123,
239247.
Henneeld, L., Hwang, H. G., Weston, S. J., Povinelli, D. J., 2018. Meta-analytic
techniques reveal that corvid causal reasoning in theÂ aesop's fable paradigm
is driven by trial-and-error learning. Animal Cognition.
Mulcahy, N. J., Call, J., 2006. Apes save tools for future use. Science 312 (5776),
10381040.
5
Society Open
