The missing metric: quantifying contributions of reviewers
Maurício Cantor, Shane Gero
Article citation details
R. Soc. open sci. 2: 140540.
http://dx.doi.org/10.1098/rsos.140540
Review timeline
Original submission: 28 September 2014 Note: Reports are unedited and appear as
Revised submission: 23 December 2014 submitted by the referee. The review history
Final acceptance: 13 January 2015 appears in chronological order.
Review History
label_version_1
RSOS-140341.R0 (Original submission)
label_author_1
Review form: Reviewer 1 (Jelte Wicherts)
Is the manuscript scientifically sound in its present form?
Yes
Are the interpretations and conclusions justified by the results?
Yes
Is the language acceptable?
Yes
Is it clear how to access all supporting data?
Yes. Data were there!
Do you have any ethical concerns with this paper?
No
Have you any concerns about statistical analyses in this paper?
No
© 2015 The Authors. Published by the Royal Society under the terms of the Creative Commons
Attribution License http://creativecommons.org/licenses/by/4.0/, which permits unrestricted use,
provided the original author and source are credited
2
Recommendation?
label_recommendation_1
Major revision is needed (please make suggestions in comments)
Comments to the Author(s)
label_comment_1
Review of “The missing metric: Quantifying contributions of reviewers” by Jelte M. Wicherts
In this manuscript the authors present a novel metric to assess the contribution of peer reviewers
over time. I wholeheartedly agree with their statement that currently most journals do
insufficiently incentivize the writing of high quality reviews, and that this may affect the quality
of the peer review system. Many journals acknowledge reviewers by listing them at the end of
each year, but most of the rewarding of reviewers is informal, with little effort to standardize. For
instance, a regular reviewer with subjectively assessed reviews may be promoted to become a
member of the editorial board or may become (action/associate/chief) editor him/herself over
time. Such a reward system may benefit from having an index of peer reviewer’s contributions
that is (1) is intuitive, (2) comparable across journals, (3) fair to early-career scientists, and (4)
contains ingredients about which most scientists (and/or editors?) agree. The current authors
propose such an index, which they denoted R. I liked their idea in general and welcome debate
about such an index. But for any index to become useful, it is vital that these four goals (and
possibly others) are met. I focus my review on the debate whether R meets these requirements.
Let me compare, for the sake of discussion, R to the H-index that has so rapidly grown in
popularity and use. One might criticize the H-index on several grounds. Specifically, H is
incomparable across fields that differ in size, pace of publication, number of authors per article,
and citation culture. Similarly, it is clearly unfair to early-career scientists and could be viewed as
a measure of seniority and of a scientist’s ability to acquire co-authorship. Yet, technically, H has
some nice features. It is attempts to describe the distribution of highly skewed citation data. It is
impossible to “game” by simply having a few highly-cited articles or simply by publishing a lot
of papers that few are willing to cite. H has what psychometricians would call face validity,
because it rings an intuitive bell with many users. The question is whether R has some of the
same nice features as H does. I am not sure. The index R does not appear strong in terms of face
validity and I was unable to arrive at an easy interpretation and its intellectual parents failed to
provide it in the current manuscript. It has no clear “scaled value” that provides values that are
readily interpretable (e.g. what does R of 50 mean?). In my view, this renders R less convincing to
potential users, thereby lowering its chance of being adopted widely.
Surely, the former point does not take away other potential qualities of R, like its potential to
contribute to the rewarding of high quality reviews. So we are left with points 2-4 above. The
authors take into account journals’ Impact Factor (IF) and want to use a measure of the quality of
the individual reviews that is standardized, but it is not entirely clear how they want to achieve
this. Also, it is not immediately clear how R deal with the fact that higher IFs are associated with
lower word counts in some journals (Science and Nature have clear word limits, whereas some
substantive journals with lower IFs have none). As the authors argue, R does appear to deal with
point (3), which is probably a good thing given the research showing that early and mid-career
scientists write the best reviews and are often (at least in my experience at PLOS ONE) more
willing to review. Perhaps the most essential question is (4): would all (or at least most)
stakeholders agree with the inclusion of w, s, and IF in the index? Here I have some doubts. I
could envision a lot of my colleagues arguing vigorously against the adoption of the IF (although
I am not as critical as they are regarding IF), and there may also be issues with w. For instance, I
also review for technical journals in my field and these reviews typically require much more time
(e.g., because I need to check the formulas) despite the fact that the number of words in the
manuscripts is limited.
3
The core question about support for the inclusion of these ingredients is probably mostly an
empirical one. Hence, I feel that the current work would be considerably strengthened by adding
data on the support for the R index (and its ingredients) in various fields. Similarly, one would
need to have more data in order to get a sense of the weighting that the current R index uses for
IF, w, and s. The current rationale for the use of the current weights did not entirely convince me
and needs more work.
label_author_2
Review form: Reviewer 2 (David Duffy)
Is the manuscript scientifically sound in its present form?
No
Are the interpretations and conclusions justified by the results?
No
Is the language acceptable?
No
Is it clear how to access all supporting data?
The data seem to be there but my Mac was quite hostile about opening the files. There does;t
seem to be a metadata or index of the data, but then again it may be the Mac OS being snotty.
Do you have any ethical concerns with this paper?
No
Have you any concerns about statistical analyses in this paper?
I do not feel qualified to assess the statistics
Recommendation?
label_recommendation_2
Major revision is needed (please make suggestions in comments)
Comments to the Author(s)
label_comment_2
The English needs work and the discussion needs to be tightened up.
I think the model can be better explained, with more details. More specifically, is the number of
words in a paper really a proxy for time spent? And is this the published number? A reviewer
may put a lot of time into helping shorten a paper but their zeal would in fact be penalized. Also
some fields are wordier than others. For the s-score how does one standardize across journals
(and across editors) and how does one weigh the various aspects of a review? Only whether the
reviewer met the deadline is objective; the rest are not.
l. 76 early career reviewing "many". But until they are known and published, they will not be
asked to be reviewers. This and what follows assumes a Peer of Science arrangement where
reviewers know what manuscripts are out there and can chose to do lots of reviews. For many
journals, reviewing is by invitation only.
l. 80-85 opportunist and specialist--what is the basis for this? Many might pursue a 'mixed'
strategy. For example, a senior person might be both opportunistic, if offered the chance to
review for Nature or Science, but willing to review lots of papers within their own field.
94 "tendency" within the model's conditions
4
98 how did you adjust for editor's feedback? This needs bait more explanation, as in l. 100 why
would opportunistic reviewers provide poor reviews to top journals?
l. 110 "standing in the field" not clear how it does this
I guess I feel uneasy at the mixed results/discussion as it is hard to tell where results end and
discussion begins, i.e. l. 111 "unheralded pillars", l. 113 :"in our experience", "we assume", l. 118
"hard working", l. 119 "typically", ;. 121 "daunting", l. 126 "inevitably" l. 144-145 results appear,
after a long run of discussion. I'd suggest separating them and elaborating on the results and
shortening the discussion.
l. 147. judging journals by metrics. Editors will game this, as they will score their own reviewers
higher to elevate R (and to avoid annoying their reviewers who will avoid reviewing for journals
with tough-scoring editors), unless there are quantitative standards.
l. 156 "practical aid in the publication process". One complaint of reviewers and editors is that
sometimes authors expect them to do the work of honing the manuscript for publication. In other
words, an author is seen as "just sending it in to see how the reviewers respond", rather than
having colleagues vet the paper before submission.
Given the gaming that is going on with less reputable on-line journals, the authors should look
into how such journals might be used to game R, as they are used to game H and other indices.
Indeed they might tinker with the model and see what vulnerabilities it has.
I do wish the authors would have actually cited my paper rather than referring to it in the
acknowledgements, but I am happy in any event that they are advancing the idea.
David Cameron Duffy, University of Hawaii Manoa.
label_end_comment
Decision letter (RSOS-140341)
02-Dec-2014
Dear Mr Cantor:
Manuscript ID RSOS-140341 entitled "The missing metric: Quantifying contributions of
reviewers" which you submitted to Royal Society Open Science, has been reviewed. The
comments from reviewers are included at the bottom of this letter.
In view of the criticisms of the reviewers, the manuscript has been rejected in its current form.
However, a new manuscript may be submitted which takes into consideration these comments.
Please note that resubmitting your manuscript does not guarantee eventual acceptance, and that
your resubmission will be subject to peer review before a decision is made.
You will be unable to make your revisions on the originally submitted version of your
manuscript. Instead, revise your manuscript and upload the files via your author centre.
Once you have revised your manuscript, go to https://mc.manuscriptcentral.com/rsos and login
to your Author Center. Click on "Manuscripts with Decisions," and then click on "Create a
5
Resubmission" located next to the manuscript number. Then, follow the steps for resubmitting
your manuscript.
Your resubmitted manuscript should be submitted by 01-Jun-2015. If you are unable to submit
by this date please contact the Editorial Office.
I look forward to a resubmission.
Sincerely,
Emilie Aime
Senior Publishing Editor, Royal Society Open Science
openscience@royalsociety.org
Author's Response to Decision Letter for (RSOS-140341)
See Appendix A.
label_version_2
RSOS-140540 (Revision)
label_author_3
Review form: Reviewer 1 (Jelte Wicherts)
Is the manuscript scientifically sound in its present form?
Yes
Are the interpretations and conclusions justified by the results?
Yes
Is the language acceptable?
Yes
Is it clear how to access all supporting data?
Yes
Do you have any ethical concerns with this paper?
No
Have you any concerns about statistical analyses in this paper?
No
Recommendation?
label_recommendation_3
Accept as is
Comments to the Author(s)
label_comment_3
The authors have revised their manuscript well and I have no further comments.
label_author_4
Review form: Reviewer 2 (David Duffy)
Is the manuscript scientifically sound in its present form?
Yes
6
Are the interpretations and conclusions justified by the results?
Yes
Is the language acceptable?
Yes
Is it clear how to access all supporting data?
Yes
Do you have any ethical concerns with this paper?
No
Have you any concerns about statistical analyses in this paper?
No
Recommendation?
label_recommendation_4
Accept as is
Comments to the Author(s)
label_comment_4
The authors have addressed the major issues. There are other items that inherently haven't as
clear answers. We could tweak the formula but its effectiveness is best measured by how widely
it is adopted rather than by continuing review. The authors have done the community a service
with this effort.
label_end_comment
Decision letter (RSOS-140540)
09-Jan-2015
Dear Mr Cantor
On behalf of the Editor, I am pleased to inform you that your Manuscript RSOS-140540 entitled
"The missing metric: Quantifying contributions of reviewers" has been accepted for publication in
Royal Society Open Science
The reviewers and Subject Editor have recommended publication, therefore please proofread
your manuscript carefully and ensure that the following editorial points are addressed.
• Ethics statement
If your study uses humans or animals please include details of the ethical approval received,
including the name of the committee that granted approval. For human studies please also detail
whether informed consent was obtained. For field studies on animals please include details of all
permissions, licences and/or approvals granted to carry out the fieldwork.
• Data accessibility
It is a condition of publication that all supporting data are made available either as
supplementary information or preferably in a suitable permanent repository. The data
accessibility section should state where the article's supporting data can be accessed. This section
should also include details, where possible of where to access other relevant research materials
such as statistical tools, protocols, software etc can be accessed. If the data has been deposited in
an external repository this section should list the database, accession number and link to the DOI
for all data from the article that has been made publicly available. Data sets that have been
deposited in an external repository and have a DOI should also be appropriately cited in the
manuscript and included in the reference list.
7
• Competing interests
Please declare any financial or non-financial competing interests, or state that you have no
competing interests.
• Authors’ contributions
All submissions, other than those with a single author, must include an Authors’ Contributions
section which individually lists the specific contribution of each author. The list of Authors
should meet all of the following criteria; 1) substantial contributions to conception and design, or
acquisition of data, or analysis and interpretation of data; 2) drafting the article or revising it
critically for important intellectual content; and 3) final approval of the version to be published.
All contributors who do not meet all of these criteria should be included in the
acknowledgements.
We suggest the following format:
AB carried out the molecular lab work, participated in data analysis, carried out sequence
alignments, participated in the design of the study and drafted the manuscript; CD carried out
the statistical analyses; EF collected field data; GH conceived of the study, designed the study,
coordinated the study and helped draft the manuscript. All authors gave final approval for
publication.
• Acknowledgements
Please acknowledge anyone who contributed to the study but did not meet the authorship
criteria.
• Funding statement
Please list the source of funding for each author.
Because the schedule for publication is very tight, it is a condition of publication that you submit
the revised version of your manuscript within 7 days (i.e. by the 18-Jan-2015). If you do not think
you will be able to meet this date please let me know immediately.
To revise your manuscript, log into https://mc.manuscriptcentral.com/rsos and enter your
Author Centre, where you will find your manuscript title listed under "Manuscripts with
Decisions". Under "Actions," click on "Create a Revision." You will be unable to make your
revisions on the originally submitted version of the manuscript. Instead, revise your manuscript
and upload a new version through your Author Centre.
When submitting your revised manuscript, you will be able to respond to the comments made by
the referees and upload a file "Response to Referees" in "Section 6 - File Upload". You can use this
to document any changes you make to the original manuscript. In order to expedite the
processing of the revised manuscript, please be as specific as possible in your response to the
referees.
When uploading your revised files please make sure that you have:
1) A text file of the manuscript (tex, txt, rtf, docx or doc), references, tables (including captions)
and figure captions. Do not upload a PDF as your "Main Document".
2) A separate electronic file of each figure (EPS or print-quality PDF preferred (either format
should be produced directly from original creation package), or original software format)
3) Included a 100 word media summary of your paper when requested at submission. Please
ensure you have entered correct contact details (email, institution and telephone) in your user
account
8
4) Included the raw data to support the claims made in your paper. You can either include your
data as electronic supplementary material or upload to a repository and include the relevant doi
within your manuscript
5) Included your supplementary files in a format you are happy with (no line numbers,
vancouver referencing, track changes removed etc) as these files will NOT be edited in
production
Once again, thank you for submitting your manuscript to Royal Society Open Science and I look
forward to receiving your revision. If you have any questions at all, please do not hesitate to get
in touch.
Best wishes
Emilie Aime
Senior Publishing Editor
openscience@royalsociety.org
Author's Response to Decision Letter for (RSOS-140540)
To the editorial board at the Royal Society Open Science Emilie Aime, Senior Publishing Editor
Dear Dr. Aime, We are very happy with the editorial decision on the manuscript RSOS-140540.
We thank the editor and the two reviewers for the final positive feedback on our work. Please
find attached the final manuscript, its two figures in high-quality pdf format and the electronic
supplementary materials (simulation details and the R package). We double-checked the
manuscript to make sure it includes all the required sections: Author's contributions,
Acknowledgements, Funding statement, Competing Interests and Data accessibility. We are
making the original data and the simulated data available with the statistical package for R as an
electronic supplementary material. The data is necessary to run the simulations in the package
thus we found more straightforward to provide the whole material together. In addition, the
same material is available in our online repository described in the "Data accessibility" section.
Finally, the Ethics statement section does not apply to our manuscript. Once again, we thank you
very much for managing our manuscript. Sincerely, Mauricio Cantor & Shane Gero
pendix A
nuscript ID RSOS-140341
missing metric: Quantifying contributions of reviewers
ricio Cantor & Shane Gero
To the editorial board at Royal Society Open Science
December 22, 2014
r Dr. Emilie Aime,
or Publishing Editor
We are very grateful for the opportunity of reviewing and resubmitting our manuscript RSOS-
341. Both reviewers strongly supported the need for quantifying reviewers’ contributions as a way to
rove the peer-review system, while also raising valid points we needed to address. Please, see below a
t-by-point response letter describing how we addressed each of their comments, both rephrasing and
uding new analyses, in the new version of the manuscript.
We believe strongly that any version of an index for such purpose is unlikely to completely satisfy
entire scientific community. This is made evident among these two reviewers, who welcome the index
disagree with each other about the parameters of the index. While they have questioned some of the
ies we have used to quantify reviewers’ contributions, they have not proposed any alternatives.
eover, the reviewers have not requested specific changes on the index, such as pointing out which
icular parameters they would like to add or remove from the equation. Therefore we chose to keep the
inal formulation, but have greatly improved the justification for each parameter, both in the manuscript
in the response letter. To support the final formulation, we have reanalysed the original data and
uded additional analyses in the supplementary material (ESM3). We modelled 8 alternative, reduced
ions of our index to show that removing parameters from the index might change its absolute scale, but
ould not improve its validity, utility, or applicability. The additional parameters weight the number of
ewed manuscripts with other contributions of reviewers (time invested, standing in the field, quality of
review). This way, our proposal is more conservative and appropriate to capture the essentials of the
ributions through peer-reviewing. Most critically from the standpoint of publication in Royal Society
n Science, our metric delivers on the first three key integrative components suggested by Dr. Wicherts.
or the last component, the debate upon if the metric will be accepted widely, it can only genuinely take
e after it is formally proposed in press.
Encouraged by their positive feedback and enthusiasm for the need of evaluate and reward
ewers, we are resubmitting a new version of the manuscript RSOS-140341 for your consideration for
lication in the Royal Society Open Science. This review process was not only a great opportunity to
rove our work, but an proof that focus on measuring reviewers’ efforts can indeed stimulate high-quality
thoughtful reviews. We hope that you will find our revised manuscript suitable for publication.
our very best,
Mauricio Cantor
Department of Biology
Dalhousie University, Canada
&
Shane Gero
Department of Biosciences
Aarhus University, Denmark
mments from the Reviewer 1, Dr. Jelte M. Wicherts
r Dr. Wicherts,
very much appreciate the thoughtful signed review highlighting the strong points of our manuscript, as
as many important issues that begged more explanation. Please find below how each of your comments
e addressed in the new version of the manuscript.
iewer 1:
is manuscript the authors present a novel metric to assess the contribution of peer reviewers over time.
holeheartedly agree with their statement that currently most journals do insufficiently incentivize the
ing of high quality reviews, and that this may affect the quality of the peer review system. Many journals
nowledge reviewers by listing them at the end of each year, but most of the rewarding of reviewers is
rmal, with little effort to standardize. For instance, a regular reviewer with subjectively assessed reviews
be promoted to become a member of the editorial board or may become (action/associate/chief) editor
/herself over time. Such a reward system may benefit from having an index of peer reviewer’s
ributions that is (1) is intuitive, (2) comparable across journals, (3) fair to early-career scientists, and
contains ingredients about which most scientists (and/or editors?) agree. The current authors propose
an index, which they denoted R. I liked their idea in general and welcome debate about such an index.
for any index to become useful, it is vital that these four goals (and possibly others) are met. I focus
review on the debate whether R meets these requirements.
ponse by the Authors: We thank the reviewer for clearly laying out the criteria for an ideal metric. Dr.
herts supports the need of such a metric and we feel that our metric can or has the ability to deliver on
our of the goals outlined. Furthermore, we have refocused part of our discussion to highlight how R-
x fits into your outline of the ideal metric, by dedicating an entire subsection to it (starting on L229).
will highlight our specific comments below in relations to these goals.
l 1: Intuitive
iewer #1: Let me compare, for the sake of discussion, R to the H-index that has so rapidly grown in
ularity and use. One might criticize the H-index on several grounds. Specifically, H is incomparable
ss fields that differ in size, pace of publication, number of authors per article, and citation culture.
ilarly, it is clearly unfair to early-career scientists and could be viewed as a measure of seniority and of
ientist’s ability to acquire co-authorship. Yet, technically, H has some nice features. It is attempts to
ribe the distribution of highly skewed citation data. It is impossible to “game” by simply having a few
ly-cited articles or simply by publishing a lot of papers that few are willing to cite. H has what
chometricians would call face validity, because it rings an intuitive bell with many users. The question
hether R has some of the same nice features as H does. I am not sure. The index R does not appear
ng in terms of face validity and I was unable to arrive at an easy interpretation and its intellectual parents
d to provide it in the current manuscript. It has no clear “scaled value” that provides values that are
ily interpretable (e.g. what does R of 50 mean?). In my view, this renders R less convincing to potential
s, thereby lowering its chance of being adopted widely.
hors: We appreciate the insightful exercise of comparing the R-index with the well-stablished H-index.
goal was to operationalize an index that is simple to calculate and yet contains multiple parameters to
ure the reality of an individual’s time and efforts as a reviewer. By doing so, the index gains in broader
grity but yields numerical outputs that could be immediately less intuitive—but not less informative—
rectly compared to the H-index. Put simply, the R-index quantifies reviewers proportionally to their
ributions. It is a more relative comparator than the absolute face validity of H. The R-index increases
ost linearly with the number of reviews, while it is weighted by other relevant aspects (time and effort
sted; standing in the field) that prevent it from being gamed.
le an H-index of 50 means an author published 50 articles that were cited at least 50 times, an R-index
0 means a solid contribution to the review system, which can be achieved through different routes. For
ance, a R-index could result from a) 50 excellent reviews of long manuscripts to low rank journals
50, s=1, IF=1, w=10000); b) 100 excellent reviews of short manuscripts to low rank journals (n=100,
IF=1, w=5000), c) 100 good reviews of short manuscripts to mid rank journals (n=100, s=0.5, IF=4,
000), d) 25 very good reviews of short manuscripts to top journals (n=25, s=0.8, IF=25, w=5000). The
-review system is equally benefited from different individual contributions. Thus, as opposed to H-
x, the R-index is more diversely applicable and egalitarian since it levels off different reviewers’ styles,
er stages and disciplines. As Dr. Wicherts point out, we failed to clearly interpret this in text and it now
ears in the current revised manuscript (see the new section “The ideal of an ideal metric”, L229. What
eemed by academia to be a “good” R-index is as controversial as what is deemed to be an impressive
The publication of this proposed metric is partly meant to encourage this debate. In particular, as we
light in the manuscript (L187-189), the ratio of H to R will be particularly revealing of a researchers
ributions to the community. This ratio yields the number of constructive reviews produced for each
quality publication.
would also like to highlight that, as Dr. Wicherts points out, the H-index suffers in several of the ideal
ria for a metric in so much as it delivers strongly on (1) but is weak on both (2) and (3). Although it is
to interpret, it is not comparable across fields and is challenging to early career scientists. Nonetheless,
n with its weaknesses, it has risen to virtual ubiquity and has spawned a number of variants which
ress directly some of its shortfalls. The H-index has created the substrate for discussion and change,
ably negative or positive, in our community in relation to researcher assessment. We believe that the
lication of the index quantifying reviewers, and the discussion which follows, will do the same for our
-review system. And it appears that on this point, Dr. Wicherts agrees with us.
l 2: Comparable across journals
iewer 1: Surely, the former point does not take away other potential qualities of R, like its potential to
ribute to the rewarding of high quality reviews. So we are left with points 2-4 above. The authors take
account journals’ Impact Factor (IF) and want to use a measure of the quality of the individual reviews
is standardized, but it is not entirely clear how they want to achieve this. Also, it is not immediately
r how R deals with the fact that higher IFs are associated with lower word counts in some journals
ence and Nature have clear word limits, whereas some substantive journals with lower IFs have none).
hors: We agree that the original description of the R-index parameters was insufficient. To make clearer
the R-index will be comparable across journals, we now improved this excerpt by 1) justifying the IF
manuscript length as valid working proxies (L61-69), and 2) detailing the definition of the s-score to
e it more objective and standardized across disciplines (L70-86). Usually highly productive researchers
given area are invited to review for higher rank journals, so we see the IF as a proxy for one’s prestige
heir field as a reviewer. IFs are inherently different across disciplines and we propose that its square-
ed value can alleviate the differences. There is no single measure for time spent in a review; manuscript
th is a very intuitive measure of it. Clearly, word count is a common feature of all manuscripts, despite
rent variations among reviewers’ abilities to review longer or more methodological papers, and
ations across disciplines. The rescaled word count (w/10^4) used in the formula reduces the weight of
disparity in the index calculation. Yet, we argue that because journals with higher impact factors
ally require shorter manuscripts, we could expect IF and w balancing each other out in the final index
ulation. Finally, our proposed measure of the quality of reviews, the s-score, is intended to rank the
ews in a standardized way (from 0 to 1) based on criteria that we are intrinsic in any review and so
ul tools for editors of any journal: punctuality, utility to authors, utility to editors and impact of the
ew report. We now delineate these criteria better in the L70-86, further suggesting a way to make it
e objective and readily comparable across journals of different disciplines.
l 3: Fairness across career stages
iewer 1: As the authors argue, R does appear to deal with point (3), which is probably a good thing
n the research showing that early and mid-career scientists write the best reviews and are often (at least
y experience at PLOS ONE) more willing to review.
hors: We appreciate you highlighting the qualities of the R-index. As early career scientist ourselves,
elt it necessary to deal with what appears to be an unspoken truth about review – as a result a key part
eveloping R was to ensure that it was fair across career stages. Given reviewer#2’s comments about the
s of career stages and proportions of reviews, we are curious, if Dr. Wicherts would be willing to pers.
m. his experience at PLOS One to further support our model; and if so, we have temporarily included
in L116 and L191 which can be removed if not consented to
l 4: Community agreement on parameters
iewer 1: Perhaps the most essential question is (4): would all (or at least most) stakeholders agree with
inclusion of w, s, and IF in the index? Here I have some doubts. I could envision a lot of my colleagues
ing vigorously against the adoption of the IF (although I am not as critical as they are regarding IF),
there may also be issues with w. For instance, I also review for technical journals in my field and these
ews typically require much more time (e.g., because I need to check the formulas) despite the fact that
number of words in the manuscripts is limited.
core question about support for the inclusion of these ingredients is probably mostly an empirical one.
ce, I feel that the current work would be considerably strengthened by adding data on the support for
R index (and its ingredients) in various fields. Similarly, one would need to have more data in order to
a sense of the weighting that the current R index uses for IF, w, and s. The current rationale for the use
he current weights did not entirely convince me and needs more work.
hors: Dr. Wicherts review highlights a key truth about quantifying science: it has the right combination
ppeal and controversy and no one ever agrees with the metrics entirely. There is dissent about the H-
x just as there is varying agreement with Impact Factor, and the extent of their usage. Accordingly, this
greement will exist for any attempt to quantify the reviewers’ contributions. As is evident in the reviews
his manuscript in which the two reviewers suggests different improvements for the proposed index. In
airness, it is unlikely that any metric would completely satisfy the entire scientific community. Despite
general agreement on a reward that is proportional to the number of reviewed manuscripts, we would
ect arguments in favour and against differing additional parameters of the index equation. There is no
le recipe for such an index. We attempted to formulate one that is as fair as possible, captures as many
cts as possible of reviewers’ contribution and yet still uses a minimum number of working proxies. To
port the proposed index formulation in the revised manuscript, we have improved the description and
fication each parameter (L56-86) and also provide supplementary analyses modelling the performance
reduced versions of the index (see the updated SM3 and the new figure S4). We suggest that removing
meters only changes the absolute scale of the index, and the original formulation weights the number
eviewed papers with other aspects of contributions through peer-reviewing (L95, L135-138, ESM3).
etheless, we absolutely agree with the reviewer on the need of empirical data to test the index
ormance beforehand. However, this is not possible at this stage simply because there are no such data
lable—either because journals do not yet keep track of these parameters we need, or more likely, they
not willing to share such information at this stage. We truly had a hard time gathering the empirical data
the journals and, ultimately we got a database from a single journal. This is why we decided to model
istically-generated data to predict R-index outputs. The greatest advantage of the simulations is the
ity of generating large amounts of realistic data. Our first simulations were performed with a very large
ple size and a high number of reviewed papers per reviewer (50,000 reviewers and 2,875,000
uscripts: 27,000 early-career researchers reviewing 75 manuscripts each, 16,000 mid-careers reviewing
manuscripts each, and 3,500 opportunist and 3,500 specialist lead-researchers reviewing 30 manuscripts
). To double-check if the sample size was sufficient, during this review process we re-ran all the
ulations with 10 times more data (500,000 reviewers and 28,750,000 manuscripts), using the empirical
istribution with 7,514 journals. This given us the very high average of 3,826 manuscripts per journal.
results were exactly the same shown in the Figures 1 and 2— except, obviously, that the clouds of
ts in the Figure 1 had 10 times more points and the frequencies of the histograms (y-axis of the Figure
were tenfold. This shows the index is reliable and that we have found a consistent pattern. More
ortantly here, this shows our original sample size is enough to evaluate the weights of R-index
meters and likely to be a realistic sample. For instance, a top journal such as Nature receives about 200
uscripts per week (9600 per year); our original sample of almost 3 million manuscripts would represent
bulk of submissions to at least 300 journals of the same caliber. Given that lower rank journals inherently
ive less than 9600 manuscripts per year, we are likely covering a realistic number of manuscripts being
ewed per year.
believe our metric can be easily implemented but more importantly it will stimulate the debate on the
d for quantifying reviewers’ contributions, and so provide the impetus for the collection of the empirical
needed. We are actually very excited to test the index as soon as this data is made available. Just like
dex, Impact Factor and other metrics, we expected and hope that R-index evolves as we learn from the
data.
mments from the Reviewer 2, Dr. David Cameron Duffy
r Dr. Duffy,
are grateful for the thorough and signed review that helped us improve the quality of this work. Please
below how we addressed each of your comments in the new version of the manuscript.
ment # 1 by Reviewer 2:
English needs work and the discussion needs to be tightened up.
ponse by the Authors: The manuscript was authored by a native English speaker, and reviewed prior
ubmission by another. While we would be happy to address any grammatical, structural, or spelling
akes, in addition to any stylistic points brought forward by the reviewer; the vagueness of this comment
challenge to address specifically. In an attempt to broadly address this point, we have thoroughly revised
final version of the manuscript after improved and reformatted the discussion into a separate section
comment #9).
ment # 2 by Reviewer 2:
nk the model can be better explained, with more details.
ponse by the Authors: We followed the reviewer’s suggestion and expanded the description of all
meters of the index (L56-86).
ment # 3 by Reviewer 2:
e specifically, is the number of words in a paper really a proxy for time spent? And is this the published
ber? A reviewer may put a lot of time into helping shorten a paper but their zeal would in fact be
alized. Also some fields are wordier than others.
ponse by the Authors: The main idea of our index is to quantify the different facets of contributing as
ndividual reviewer. Given the time trade-off between publishing and reviewing, we wanted a proxy for
spent in the review. We concluded that the fairest proxy was the manuscript length, which is also a
mon feature of any manuscript of any discipline. In reality, there is no single measure for time invested,
manuscript lengths vary across disciplines and reviewers vary in their ability to review longer
uscripts or with more mathematical formulae. Although not impeccable, word count is still an effective
intuitive working proxy for the time taken to complete a review that we feel will be accepted widely.
or the distinction been submitted length and final length, we intended on the use of the word count of
submitted manuscript, given that this is the length of the manuscript when reviewed by the reviewer.
hermore, this is often data collected during the online submission process, thereby making it simple to
ss for the journals and editors. Furthermore, w is rescaled by 10^4 thereby minimizing the difference
een these two values (in the same way it reduces disparities among disciplines). Consider an extreme
mple: a long manuscript (say 10,000 words) and a reviewer who did the great job of reducing 3 pages
t (300 words/page for a standard Word manuscript with 12pt and double-sized spacing). For the
uscript, the parameter is w=1 and for the paper it is w=0.91 ([10,000 – 900] /10^4), which would
esent a reduction of only 9% of the contribution of that single reviewed paper for the reviewer’s overall
dex. Nonetheless, we have made it clear in text that the word length is derived from the submitted
uscript (L61-65).
ment # 4 by Reviewer 2:
the s-score how does one standardize across journals (and across editors) and how does one weigh the
ous aspects of a review? Only whether the reviewer met the deadline is objective; the rest are not.
ponse by the Authors: We agree with the reviewer: the s-score is a subjective measure to a certain
ree. But subjective measures are useful when a standardized objective one is not immediately available
ven possible. Our intention was suggesting that the quality of a review could be ranked and evaluated
standardized way. For that we propose a score ranging from 0 to 1 that will weight each review based
intuitive proxies (impact, usefulness to the editors, thoroughness and time taken). The actual
onsibility of ranking and scoring a paper, however, it is entirely the editor’s. We understand that the
ors’ job is to evaluate the quality of a paper, using the reviewers reports as the most appropriate tools;
it seem logical that editors would be the only and the best ones to evaluate the quality of their tools.
inally, we have suggested a minimum of proxies that should be taken into consideration, some of which
e objective, others were subjective. In the revised version, we greatly improved the description of our
ies and proposed a Multi-step Likert-type scales as a new way of standardizing s-score across
iplines (L70-86). Defining a single way of weighting the aspects of a review across editors of different
iplines would be, quite honestly, almost arrogant of us, and certainly inaccurate. Our suggestions give
amework for the editors, but it is still flexible, as we believe that one should trust the editors’ ability to
uate each review as an individual case.
ment # 5 by Reviewer 2:
early career reviewing “many”. But until they are known and published, they will not be asked to be
ewers. This and what follows assumes a Peer of Science arrangement where reviewers know what
uscripts are out there and can chose to do lots of reviews. For many journals, reviewing is by invitation
.
ponse by the Authors: To be clear, our index is aimed to the traditional peer-review system that works
er invitation only. We concur that it is likely that the opportunities for review are driven by publications:
more one publishes, the more likely you will be invited to review—although there is no available
irical data that confirms this argument either. Our own experience as early-career researchers, as well
hat of reviewer 1’s experience as an editor at PLOS One, matches with the data available by the online
ey we cited in this paragraph: there are a larger population of early-careers and these tend to review
e. Furthermore, offers to review are often deflected by senior researchers onto their graduate students
fellows through suggestions of alternative reviewers. Our simulations attempt to mimic the real world
reating career stage categories that reflect the proportions given by the real data. We now clarify this
e adding the proportions to this paragraph (L112-117).
ment # 6 by Reviewer 2:
0-85 opportunist and specialist--what is the basis for this? Many might pursue a 'mixed' strategy. For
mple, a senior person might be both opportunistic, if offered the chance to review for Nature or Science,
willing to review lots of papers within their own field.
ponse by the Authors: We absolutely agree with the reviewer that other possible strategies are possible,
indeed such mixed strategy is very likely to occur. Empirical data on researchers reviewing habits
ld be of great help here. But such data is just not available, as surveys on this type of behaviour do not
t. R-index, and particular its ratio with H-index, would begin to elucidate the reality within our
munity, and differing patterns between fields. For now, we overcame the limitations of real-world data
simulation experiments. Our point when simulating reviewer strategies was to portray extremist habits,
ch could cover the broad range of possibilities in between and provide insights on the spectrum of R-
x’s performance. By doing so, we considered that mixed strategies (such as the referred case in which
viduals are both opportunistic and specialist) would lay within the range of possibilities considered by
extreme strategies. To follow specifically the reviewer’s suggestion, we now modelled two new mixed
egies that basically differ on the number of reviews performed, being either from low- or high rank
nals. We have shown that the R-indices of reviewers following either mixed strategy lay within the R-
x of opportunist and specialist reviewers, ultimately cross-validating our original approach. The new
lts (Figure S2) are available in the new supplementary material SM2 (as well as the simulations are in
R package) and we make reference to it in the main text (L110-111).
ment # 7 by Reviewer 2:
tendency" within the model's conditions
ponse by the Authors: The reviewer is correct. However, such a tendency was only included within the
els conditions because of the empirical evidence (given by the reference 12). This sentence now makes
very clear (L143).
ment # 8 by Reviewer 2:
ow did you adjust for editor's feedback?
ponse by the Authors: We first model the R-index outcomes for all reviewer strategies drawing the
or’s feedback (i.e. the s-score) from an empirical distribution made available to use from a real journal’s
or. The results are in the Figure 1A. We then adjusted the editor’s feedback by assigning s-scores for
reviewer strategy drawing from different ranges of the empirical distribution, i.e. using a stratified
pling of the empirical s-score distribution. We agree with the reviewer that the previous version lack
ity and we now make these details available in the Methods section (L118-126), as well as in the new
plementary Material SM2.
ment # 9 by Reviewer 2:
needs bait more explanation, as in l. 100 why would opportunistic reviewers provide poor reviews to
journals?
ponse by the Authors: We agree with the reviewer that the excerpt was unclear. As detailed in the
ment#6, our simulations aimed to portray a broad range of possibilities of reviewing habits. While there
no empirical evidences that reviewers for top journals do a poor job, with the implementation of the R-
x such strategy could arise as an attempt to game the system by providing quick, poor reviews to boost
index. Our modelling aimed to evaluate how this strategy would perform, in order to assess how easily
could game the R-index. We now made our modelling goals more clear in text L127-130.
ment # 10 by Reviewer 2:
0 "standing in the field" not clear how it does this
ponse by the Authors: We used “standing in the field” in this sentence to mean the inclusion of the
nal’s impact factor on the index. This is now clearly stated in the index formulation, L67-68. We argue
usually renowned researchers in a given area (i.e. highly productive in that area) are the ones that are
ted to higher rank journals. With our own experience as early-career researchers, and of our renowned
eagues’, it is less likely that early-career or researchers who publish few papers will get invitations to
ew for top-journals. Your previous comment suggests that you agree with this statement, in that you
gested that being “known” increases the likelihood of invitations. Similarly, in your The Scientist article
stated that, “I believe being asked to referee reflects one’s true standing in a field” and so suggested
g IF as a multiplier to reflect standing in the field. Therefore, we considered impact factor as a proxy
one’s prestige in their field as a reviewer. To remove any possible ambiguities, we now better explain
impact factor in the index formulation (L65-70).
ment # 11 by Reviewer 2:
ess I feel uneasy at the mixed results/discussion as it is hard to tell where results end and discussion
ns, i.e. l. 111 "unheralded pillars", l. 113 :"in our experience", "we assume", l. 118 "hard working", l.
"typically", ;. 121 "daunting", l. 126 "inevitably" l. 144-145 results appear, after a long run of
ussion. I'd suggest separating them and elaborating on the results and shortening the discussion.
ponse by the Authors: We followed the reviewer’s suggestion and attempted to delineate the results
the discussion. All of the quoted text appears now in the discussion.
ment # 12 by Reviewer 2:
7. judging journals by metrics. Editors will game this, as they will score their own reviewers higher to
ate R (and to avoid annoying their reviewers who will avoid reviewing for journals with tough-scoring
ors), unless there are quantitative standards.
ponse by the Authors: In truth, this practice would only result in extra work for the editors and a
ease in the quality and utility of reviews overall. If editors score poor reviews highly in an attempt to
e the journal’s R-index, and editors (within or between journals) select reviewers based on high
vidual R scores, then it will lead to more poor reviewers and more work for the editors. There would
negative feedback loop which would prevent the practice. As for reviewers refusing reviews from
h-scoring editors, this can be viewed in the opposite framing as well. One could only know that an
or gives consistent low s-scores if they have reviewed for that editor before (unless the journals also
lically publish individual editors mean s-scores – which should be encouraged for transparency, but
kely initially). It’s unlikely that editors would invite the reviewer again if they gave them poor scores
iously, and therefore the reviewer would not be able to avoid them.
ment # 13 by Reviewer 2:
6 "practical aid in the publication process". One complaint of reviewers and editors is that sometimes
ors expect them to do the work of honing the manuscript for publication. In other words, an author is
as "just sending it in to see how the reviewers respond", rather than having colleagues vet the paper
re submission.
ponse by the Authors: That is, in our experience, very true. Using the peer-review as sheer step in the
ess of publication is one way authors can game the peer-review system. It is, however, a problem with
system itself rather than the proposed index. Given the large amount of manuscripts produced, the
ction probability is already high just because journals cannot accommodate all manuscripts rather than
tly for the quality of the work. So using reviewers from prestigious journal to improve the manuscript
another journal became a fairly common strategy. What we meant by “practical aid in the publication
ess” is not quite this game. We understand that the key contributions of reviewers’ are to judge the
it, and to help improving the quality of the manuscript. Thus, competent reviewers should be able to
nguish between a final product and a work in progress. While in the former case reviewers can indeed
rove the work, in the latter, we think, the manuscript should be rejected right away.
ment # 14 by Reviewer 2:
en the gaming that is going on with less reputable on-line journals, the authors should look into how
journals might be used to game R, as they are used to game H and other indices. Indeed they might
er with the model and see what vulnerabilities it has.
ponse by the Authors: That is a very valid point. Even as a debatable evaluation of a journal’s
tation, the IF would control for attempts to boost R-index, because the referred “less reputable” online
nals have very low, if any, impact factor. (We understand “less reputable online journals” as the bloom
nals that rise and fall each month somewhere in the world with tricky similar names to renowned
nals). On the reviewer’s side, reviewing for such journals would not weight much in their individual
x. On the journal’s side, the only way editors could try to game R-index would be by overprizing their
ewers with a disproportionally high s-score to advertise the journal’s review quality. Still, high s-scores
very low IFs would not considerably lift the journal’s average R-index. We tinkered with the
ulation by varying the other parameters but fixing IF to a very low number, say 0.001, to show that the
x would still be proportionally driven by the number of reviews performed and mainly weighted by the
ity of the review (s). These new simulations suggesting that R-index is not vulnerable to such potential
e are now available in the electronic supplementary material SM4 and referred in the main results
ion (L97, L156-158). Please note that while the R-index outputs behave similarly with empirical IF and
very low and fixed IFs, the range of the R-index is largely affected. We believe R-index is robust to
game since the journal’s Impact Factor is accounted for.
ment # 15 by Reviewer 2:
o wish the authors would have actually cited my paper rather than referring to it in the
nowledgements, but I am happy in any event that they are advancing the idea.
ponse by the Authors: We found that clearly acknowledging how your article has inspired our work
ld be more appropriate than just citing it, almost anonymously, in the numbered reference list. In the
ent version, we unmistakably recognize the influence of your work by doing both (L41, L273-274,
3).
Society Open
