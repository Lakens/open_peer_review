Data availability, reusability, and analytic reproducibility:
evaluating the impact of a mandatory open data policy at
the journal Cognition
Tom E. Hardwicke, Maya B. Mathur, Kyle MacDonald, Gustav Nilsonne, George C. Banks,
Mallory C. Kidwell, Alicia Hofelich Mohr, Elizabeth Clayton, Erica J. Yoon, Michael Henry
Tessler, Richie L. Lenne, Sara Altman, Bria Long and Michael C. Frank
Article citation details
R. Soc. open sci. 5: 180448.
http://dx.doi.org/10.1098/rsos.180448
Review timeline
Original submission: 19 March 2018 Note: Reports are unedited and appear as
1st revised submission: 21 May 2018 submitted by the referee. The review history
2nd revised submission: 20 June 2018 appears in chronological order.
Final acceptance: 25 June 2018
Review History
label_version_1
RSOS-180448.R0 (Original submission)
label_author_1
Review form: Reviewer 1 (Heather Urry)
Is the manuscript scientifically sound in its present form?
Yes
Are the interpretations and conclusions justified by the results?
Yes
Is the language acceptable?
Yes
Is it clear how to access all supporting data?
Yes
© 2018 The Authors. Published by the Royal Society under the terms of the Creative Commons
Attribution License http://creativecommons.org/licenses/by/4.0/, which permits unrestricted use,
provided the original author and source are credited
2
Do you have any ethical concerns with this paper?
No
Have you any concerns about statistical analyses in this paper?
No
Recommendation?
label_recommendation_1
Accept with minor revision (please list in comments)
Comments to the Author(s)
label_comment_1
Review of RSOS-180448, Data availability, reusability, and analytic reproducibility: Evaluating
the impact of a mandatory open data policy at the journal Cognition
Summary: In this article, the authors are interested in determining to what extent a mandatory
open data policy at a specific journal, Cognition, impacts whether the data are available, whether
they’re curated in a way that makes them useable, and whether the primary results of studies
reported in the journal are reproducible. They adopted one-group interrupted time-series design
to examine these things both before and after the new open data policy was adopted in March
2015.
Overall, this is an interesting and important effort, one that is relevant to the readership of Royal
Society Open Science. This clearly was a Herculean effort given the large number of empirical
articles assessed, the number of coders and analysis pilots/co-pilots necessary to wrangle the
data and reproduce analyses, and the careful attention the authors paid to transparency in this
endeavor. These positive aspects of the work are offset a bit by a methodological limitation,
namely the absence of a comparison journal similar to Cognition but without introduction of an
open data policy, and questions about coding reliability. Ultimately, however, I believe the paper
could make a useful contribution to the literature despite these issues.
Below I provide specific questions and comments about this manuscript; I hope they are helpful
to the editor and authors. Please note that I have written this review in two phases. In the first
phase, I read only the introduction and method sections so that my evaluation of the premise of
the work and its methodological rigor would be unaffected by the pattern of results and the
authors’ interpretation therein.
Phase 1: Introduction and Method (without reading Abstract, Results, or Discussion)
1. The introduction nicely frames the relevant theoretical issues. By the end, the reader
understands that the data on which empirical studies are based are rarely made publicly
accessible and, even when the data are made publicly accessible, they’re often curated in a way
that makes them essentially useless to those who may wish to reproduce reported results.
2. Study 1 Method:
a. The authors clearly acknowledge potential limitations, including the notion that the open data
policy coincided with introduction of a new editor-in-chief (EIC) at Cognition, and the notion that
announcement of the new EIC may have led to anticipatory changes in data sharing prior to the
March 1, 2015 start date of the open data policy.
b. To what extent does an imbalance in available articles from before (n = 417) to after (n = 174)
instituting the open data policy affect conclusions that can be drawn? The smaller of the two
numbers is still rather large, thus perhaps it doesn’t.
c. Coders demonstrated 90% agreement with a gold standard coding of 5 articles prior to coding
3
the articles to be included in the analyses for this paper. Percent agreement doesn’t, of course,
take into account agreement by chance. The authors might consider using a different method of
assessing interrater reliability that takes into account chance agreement.
d. With a one-group interrupted time-series design, changes in instrumentation (in this case,
coders) represent a threat to internal validity. Can the authors please clarify the procedures they
used to prevent or evaluate coder drift? Among the batch of 20 articles received by coders each
time, were there roughly even numbers of articles from the pre- and post- policy periods?
e. It would strengthen the study’s internal validity if the authors had a comparison journal of
some kind, one that has similar features to Cognition but did not adopt an open data policy on
March 1, 2015.
3. Study 2 Method:
a. The authors describe a reasonable set of variables and procedures to evaluate analytic
reproducibility of 35 articles for which the data were available. It’s possible I missed it, but I
didn’t see reported whether the articles in question were published before or after the
introduction of the open data policy at Cognition. A reader might reasonably wonder whether
analytic reproducibility was higher after.
b. Edited after reading results and discussion sections: A reader may wonder whether the 35
articles selected for evaluation of analytic reproducibility are representative of the larger group of
articles from which they were selected (35+61 = 96); a selection bias could influence outcome
estimates. For example, if the 35 articles selected for evaluation in Study 2 are more
“straightforward” than the remaining 61, estimates of reproducibility may be inflated.
Phase 2: Results, Discussion, and Abstract
Results
4. The reporting of results in both studies is both complete and clear.
5. I love the figures.
6. One question is whether the 50-day binning is necessary in Figure 1, Figure C1, and Figure D1
for “ease of presentation”. I found myself wanting to see the actual individual-article
observations.
7. Figure 3 seems superfluous.
Discussion
8. I love this analogy: “Conducting an analytic reproducibility check without an analysis script is
rather like assembling flat pack furniture without an instruction booklet: one is given some
materials (categorised and labelled with varying degrees of helpfulness), and a diagram of the
final product, but is missing the step-by-step instructions required to convert one into the other.”
9. The discussion section provides a good summary of the work and makes reasonable
conclusions based on the results. It also addresses (and dispels) some alternative explanations for
the results, and considers interesting implications of the findings.
Abstract
10. The abstract provides a reasonable summary of the work.
Thank you for the opportunity to review this work. I am signing this review for the sake of
transparency.
Heather Urry
Tufts University
4
label_author_2
Review form: Reviewer 2
Is the manuscript scientifically sound in its present form?
No
Are the interpretations and conclusions justified by the results?
Yes
Is the language acceptable?
Yes
Is it clear how to access all supporting data?
Yes
Do you have any ethical concerns with this paper?
No
Have you any concerns about statistical analyses in this paper?
No
Recommendation?
label_recommendation_2
Major revision is needed (please make suggestions in comments)
Comments to the Author(s)
label_comment_2
This is an interesting, and costly effort, but it is not as easy to read or process as it should.
I struggled and did not read it as closely as it would like given how costly it was to parse the
information. If I complain about something that you already address, assume you are not
addressing it in an easy enough to understand way.
My most important comment is that in its present form the paper is alarmist. To a casual reader
(e.g., a journalist, or the 90% of readers that will just skim it), it tells that a large share of scientific
findings are not reproducible. But the evidence is much weaker than that, it is merely that a large
share of scientific papers contain at least one secondary results that appear to have an error.
Specific comments.
1) Abstract
My guess, partially because of base rates, partially because the paper is hard to read, is that few
people will get through the paper. My forecast is that upon publication readers will get through
the abstract and just use it as a justification to argue there is a reproducibility crisis, saying things
like "37% of articles are not reproducible even with author assistance" (Hardwicke et al).
There is a distinction between inconsequential errors that can be introduced at copy-editing say,
or the result of a minor act of sloppiness, and errors in execution of research that truly invalidate
a paper.
The abstract, and the figures in the papers, what readers will consult without reading the rest,
should be unambiguous. The authors should actively revise their paper keeping in mind the
sensationalist journalist or careless readers who will put words in their mouths.
The authors do have this line in the abstract "Importantly, original conclusions did not appear to
be seriously impacted." But it does not sufficiently, for my taste, downplay the alarmist message
of the previous line. Something like this seems more accurate a summary "While none of the
5
articles had errors that seriously impact the conclusions, we were unable to account for
inconsequential differences in results, even with authors assistance, in 37% of articles"
That’s less punchy a claim, but I think a more accurate one.
2) Too many procedural details
The paper feels too detailed in things that are not that key, e.g., exactly how the studies were
selected, the co-piloting rules, etc. Most readers won't worry about those things. You can provide
general reassurances in the text that allows readers to evaluate if the study selection was fair and
unbiased, and delegate the details of execution to the supplement. e.g., Study 1 Sampling frame
could be shorten to a single line of text
3) Study 1 is needed, but not super important.
The idea that when a journal requires something, it happens more often, is not that interesting, I
do think the authors should report Study 1, but there is no need to get into the weeds, discuss
possible confounding factors, etc. Nobody really doubts that requiring something makes it more
prevalent, and the exact point estimate is probably context specific and not particularly
consequential (e.g., who the editor was). We don't want to read quotes from the Cognition
editorial, etc etc. I would say 1-2 pages for Study 1 are more than enough. We just want to know
"so did more people do it? Yes, about 30-50% increase" ok. Done. So I would suggest the authors
get more promptly to Study 2, the one that's much more interesting and policy relevant. You can
say, "we deployed various analytical approaches and our estimates of the policy ranged from x%
to y%, see Supplement X for details. Readers will be happier, and less likely to put the article
down before they get to more valuable Study 2.
Similarly, the discussion in page 15-18 would make more sense in a setting where we did not
obviously expect requiring something to make it more prevalent, or where a point estimate was
intrinsically important. I would drop almost if not all of it, though obviously it is the authors' call.
Nobody doubts some causal effect, nothing really hinges on it being 17% vs 35% , say.
4) Say more about IPR
The "IPR" is a very important construct, there are many ways to construct it, and reading the
paper I don't get a very good sense of how it was done, how subjective it was, etc. I would
expand this to a full page, and tabulate more details.
5) Results for Study 2 are hard to parse
I had a really hard time processing the results from Study 2. In part because I am not used to
having magnitude be represented by the size of an X, perhaps an actual number will be better.
Also, one does not get a sense of how important the errors were, and to what extent they are
independent from one another. If the original authors dropped one observation they shouldn't
have, they will have the wrong mean, confidence interval, p-value, effect size, and fill the tables
for Study 2 with red crosses. Maybe worth counting actions that cause errors rather than
consequences of such actions (if this is reasonably easy to do), and simply tabulate how many
independent origin of errors are present in each paper. Then maybe say something about those
origins and how they can be avoided
6) Figure 3 provides no information beyond reporting three %s. It feels too "People magazine" to
me. It almost guarantees an alarmist unjustified read of the evidence presented in this paper. It
almost guarantees a journalist will claim this paper found that 1/3 studies failed to be
reproduced (which is technically true, but readers will assume that what failed to be reproduced
is something that mattered rather than a detail that may be irrelevant).
7) Say more about what it meant to require author assistance, for instance, did the authors explain
6
something you should have known? Or did they indicate something that is critical and nobody
could possibly figure out without contacting them?
8) The writing is tedious also in terms of reporting everything in a very formulaic way, for
instance, we don't need to convert to % and report a confidence interval every single time a
frequency is reported. We should be able to say simply "We encountered errors in 24 of the 35
statements", without then converting that to a % and putting the 95% confidence interval. Also,
you can probably just say once all confidence interval are 95%, and not print "95%" 53 times in the
paper.
The algorithmic way of reporting results makes it hard for humans to get through the results,
consider for example this paragraph: "
519 The errors broken down by outcome type are displayed in Figure 4. Of the major
520 errors, 17 (27%, 95% CI [16, 39]) were standard deviations or standard errors, 17 (27%, 95%
521 p-values, 10 (16%, 95% CI [5, 28]) were test statistics, such as t and F
522 values, 8 (12%, 95% CI [2, 25]) were effect sizes such as Cohen’s d or Pearson’s r values, 4
523 (6%, 95% CI [0, 19]) were means or medians, 4 (6%, 95% CI [0, 19]) were degrees of freedom,
524 1 (2%, 95% CI [0, 14]) was a count or proportion, 1 (2%, 95% CI [0, 14]) was a confidence
525 interval, and 2 (3%, 95% CI [0, 16]) were other miscellaneous values."
You could also do a bar chart, but if you keep it in text, I would just report the frequencies.
9) The 5-6 pages of discussion at the end are a lot. A good figure is worth 1000 discussion words. I
personally think papers should be mostly about establishing facts; this is not an editorial.
Moreover, much of the discussion qualifies factual matters, such as the type of errors you
uncovered and how consequential they were. That type of analysis belongs in the meat of the
paper, belongs in the tables and figures. One should avoid presenting factual information that is
possibly misleading, and then address the possibly misleading facts in the discussion. The tables
and figures themselves should be created in ways that the most natural interpretation of the facts
there presented is correct. In this paper, the most natural interpretation of the summary stats and
figures is alarmist ("Jesus, it's all irreproducible") and then the discussion calms you down a bit
('well, but not the stuff that matters').
10) Appendix E contains important information. The authors can probably create a table that
summarizes it.
label_author_3
Review form: Reviewer 3 (Chin-Yuan Fan)
Is the manuscript scientifically sound in its present form?
Yes
Are the interpretations and conclusions justified by the results?
Yes
Is the language acceptable?
Yes
Is it clear how to access all supporting data?
Yes
7
Do you have any ethical concerns with this paper?
No
Have you any concerns about statistical analyses in this paper?
No
Recommendation?
label_recommendation_3
Accept with minor revision (please list in comments)
Comments to the Author(s)
label_comment_3
This paper shows very interesting results in open data, and also concerned different kinds of data
base, I think this paper can be acceppt in this version.
However, Authors should compare with other methods and other situation,and also use other
method tp stress this situation, that will help your paper more powerful.
label_end_comment
Decision letter (RSOS-180448.R0)
02-May-2018
Dear Dr Hardwicke,
The editors assigned to your paper ("Data availability, reusability, and analytic reproducibility:
Evaluating the impact of a mandatory open data policy at the journal Cognition") have now
received comments from reviewers. We would like you to revise your paper in accordance with
the referee and Associate Editor suggestions which can be found below (not including
confidential reports to the Editor). Please note this decision does not guarantee eventual
acceptance.
Please submit a copy of your revised paper within three weeks (i.e. by the 25-May-2018). If we do
not hear from you within this time then it will be assumed that the paper has been withdrawn. In
exceptional circumstances, extensions may be possible if agreed with the Editorial Office in
advance.We do not allow multiple rounds of revision so we urge you to make every effort to
fully address all of the comments at this stage. If deemed necessary by the Editors, your
manuscript will be sent back to one or more of the original reviewers for assessment. If the
original reviewers are not available, we may invite new reviewers.
To revise your manuscript, log into http://mc.manuscriptcentral.com/rsos and enter your
Author Centre, where you will find your manuscript title listed under "Manuscripts with
Decisions." Under "Actions," click on "Create a Revision." Your manuscript number has been
appended to denote a revision. Revise your manuscript and upload a new version through your
Author Centre.
When submitting your revised manuscript, you must respond to the comments made by the
referees and upload a file "Response to Referees" in "Section 6 - File Upload". Please use this to
document how you have responded to the comments, and the adjustments you have made. In
order to expedite the processing of the revised manuscript, please be as specific as possible in
your response.
In addition to addressing all of the reviewers' and editor's comments please also ensure that your
revised manuscript contains the following sections as appropriate before the reference list:
8
• Ethics statement (if applicable)
If your study uses humans or animals please include details of the ethical approval received,
including the name of the committee that granted approval. For human studies please also detail
whether informed consent was obtained. For field studies on animals please include details of all
permissions, licences and/or approvals granted to carry out the fieldwork.
• Data accessibility
It is a condition of publication that all supporting data are made available either as
supplementary information or preferably in a suitable permanent repository. The data
accessibility section should state where the article's supporting data can be accessed. This section
should also include details, where possible of where to access other relevant research materials
such as statistical tools, protocols, software etc can be accessed. If the data have been deposited in
an external repository this section should list the database, accession number and link to the DOI
for all data from the article that have been made publicly available. Data sets that have been
deposited in an external repository and have a DOI should also be appropriately cited in the
manuscript and included in the reference list.
If you wish to submit your supporting data or code to Dryad (http://datadryad.org/), or modify
your current submission to dryad, please use the following link:
http://datadryad.org/submit?journalID=RSOS&manu=RSOS-180448
• Competing interests
Please declare any financial or non-financial competing interests, or state that you have no
competing interests.
• Authors’ contributions
All submissions, other than those with a single author, must include an Authors’ Contributions
section which individually lists the specific contribution of each author. The list of Authors
should meet all of the following criteria; 1) substantial contributions to conception and design, or
acquisition of data, or analysis and interpretation of data; 2) drafting the article or revising it
critically for important intellectual content; and 3) final approval of the version to be published.
All contributors who do not meet all of these criteria should be included in the
acknowledgements.
We suggest the following format:
AB carried out the molecular lab work, participated in data analysis, carried out sequence
alignments, participated in the design of the study and drafted the manuscript; CD carried out
the statistical analyses; EF collected field data; GH conceived of the study, designed the study,
coordinated the study and helped draft the manuscript. All authors gave final approval for
publication.
• Acknowledgements
Please acknowledge anyone who contributed to the study but did not meet the authorship
criteria.
• Funding statement
Please list the source of funding for each author.
Please note that Royal Society Open Science will introduce article processing charges for all new
submissions received from 1 January 2018. Charges will also apply to papers transferred to Royal
Society Open Science from other Royal Society Publishing journals, as well as papers submitted
9
as part of our collaboration with the Royal Society of Chemistry
(http://rsos.royalsocietypublishing.org/chemistry). If your manuscript is submitted and
accepted for publication after 1 Jan 2018, you will be asked to pay the article processing charge,
unless you request a waiver and this is approved by Royal Society Publishing. You can find out
more about the charges at http://rsos.royalsocietypublishing.org/page/charges. Should you
have any queries, please contact openscience@royalsociety.org.
Once again, thank you for submitting your manuscript to Royal Society Open Science and I look
forward to receiving your revision. If you have any questions at all, please do not hesitate to get
in touch.
Kind regards,
Andrew Dunn
Royal Society Open Science
openscience@royalsociety.org
on behalf of Chris Chambers (Subject Editor)
openscience@royalsociety.org
Editor's comments (Chris Chambers):
Comments to the Author:
label_comment_4
Three reviewers have now assessed the manuscript. All judge the project to be of importance,
however some substantial revisions are necessary to ensure accuracy and clarity. Reviewer 1 is
overall positive and asks for a number of clarifications mainly to the methodological procedures.
Reviewer 2 is more critical, highlighting two chief concerns: that the main message of the paper is
obscured in amongst the reporting of detail (which the reviewer criticizes as repetitive and
"tedious" in places), and that the framing is overly alarmist.
As editor I am always reluctant to recommend that authors remove key methodological details
from a paper, and I am happy to entertain a rebuttal on these issues - but I do think Reviewer 2
makes a number of astute points. There is risk that readers will lose their way as they navigate
the paper. I would therefore encourage the authors to consider ways of streamlining the key
messages without reducing information that is necessary to evaluate quality or replicability. The
reviewer recommends the use of supplementary information, especially for Study 1. An
alternative may be clear interim summaries and highlighting of important points.
Reviewer 2's main concern is with the risk of an alarmist intepretation of the findings, and this
will need to be thoroughly addressed in a revision.
Reviewers 1 and 2 reach opposite conclusions about the validity of the Discussion, with Reviewer
1 praising the completeness of the summary and coverage of alternative explanations, while
Reviewer 2 favours a tighter presentation and suggests that steps to prevent alarmist
interpretations of the results should come sooner. My steer here is that the Discussion is generally
acceptable in length but I agree with Reviewer 2 that it would better to head off some of the
interpretative concerns earlier in the manuscript (perhaps reiterating them in the Discussion, but
in slightly streamlined form).
I am including Reviewer 3's assessment for completeness; however the review is perfunctory and
of limited use, therefore the authors need not respond to these comments.
Comments to Author:
Reviewers' Comments to Author:
10
Reviewer: 1
Comments to the Author(s)
label_comment_5
Please see attached PDF. (Also pasted below without formatting)
Review of RSOS-180448, Data availability, reusability, and analytic reproducibility: Evaluating
the impact of a mandatory open data policy at the journal Cognition
Summary: In this article, the authors are interested in determining to what extent a mandatory
open data policy at a specific journal, Cognition, impacts whether the data are available, whether
they’re curated in a way that makes them useable, and whether the primary results of studies
reported in the journal are reproducible. They adopted one-group interrupted time-series design
to examine these things both before and after the new open data policy was adopted in March
2015.
Overall, this is an interesting and important effort, one that is relevant to the readership of Royal
Society Open Science. This clearly was a Herculean effort given the large number of empirical
articles assessed, the number of coders and analysis pilots/co-pilots necessary to wrangle the
data and reproduce analyses, and the careful attention the authors paid to transparency in this
endeavor. These positive aspects of the work are offset a bit by a methodological limitation,
namely the absence of a comparison journal similar to Cognition but without introduction of an
open data policy, and questions about coding reliability. Ultimately, however, I believe the paper
could make a useful contribution to the literature despite these issues.
Below I provide specific questions and comments about this manuscript; I hope they are helpful
to the editor and authors. Please note that I have written this review in two phases. In the first
phase, I read only the introduction and method sections so that my evaluation of the premise of
the work and its methodological rigor would be unaffected by the pattern of results and the
authors’ interpretation therein.
Phase 1: Introduction and Method (without reading Abstract, Results, or Discussion)
1. The introduction nicely frames the relevant theoretical issues. By the end, the reader
understands that the data on which empirical studies are based are rarely made publicly
accessible and, even when the data are made publicly accessible, they’re often curated in a way
that makes them essentially useless to those who may wish to reproduce reported results.
2. Study 1 Method:
a. The authors clearly acknowledge potential limitations, including the notion that the open data
policy coincided with introduction of a new editor-in-chief (EIC) at Cognition, and the notion that
announcement of the new EIC may have led to anticipatory changes in data sharing prior to the
March 1, 2015 start date of the open data policy.
b. To what extent does an imbalance in available articles from before (n = 417) to after (n = 174)
instituting the open data policy affect conclusions that can be drawn? The smaller of the two
numbers is still rather large, thus perhaps it doesn’t.
c. Coders demonstrated 90% agreement with a gold standard coding of 5 articles prior to coding
the articles to be included in the analyses for this paper. Percent agreement doesn’t, of course,
take into account agreement by chance. The authors might consider using a different method of
assessing interrater reliability that takes into account chance agreement.
d. With a one-group interrupted time-series design, changes in instrumentation (in this case,
coders) represent a threat to internal validity. Can the authors please clarify the procedures they
used to prevent or evaluate coder drift? Among the batch of 20 articles received by coders each
time, were there roughly even numbers of articles from the pre- and post- policy periods?
11
e. It would strengthen the study’s internal validity if the authors had a comparison journal of
some kind, one that has similar features to Cognition but did not adopt an open data policy on
March 1, 2015.
3. Study 2 Method:
a. The authors describe a reasonable set of variables and procedures to evaluate analytic
reproducibility of 35 articles for which the data were available. It’s possible I missed it, but I
didn’t see reported whether the articles in question were published before or after the
introduction of the open data policy at Cognition. A reader might reasonably wonder whether
analytic reproducibility was higher after.
b. Edited after reading results and discussion sections: A reader may wonder whether the 35
articles selected for evaluation of analytic reproducibility are representative of the larger group of
articles from which they were selected (35+61 = 96); a selection bias could influence outcome
estimates. For example, if the 35 articles selected for evaluation in Study 2 are more
“straightforward” than the remaining 61, estimates of reproducibility may be inflated.
Phase 2: Results, Discussion, and Abstract
Results
4. The reporting of results in both studies is both complete and clear.
5. I love the figures.
6. One question is whether the 50-day binning is necessary in Figure 1, Figure C1, and Figure D1
for “ease of presentation”. I found myself wanting to see the actual individual-article
observations.
7. Figure 3 seems superfluous.
Discussion
8. I love this analogy: “Conducting an analytic reproducibility check without an analysis script is
rather like assembling flat pack furniture without an instruction booklet: one is given some
materials (categorised and labelled with varying degrees of helpfulness), and a diagram of the
final product, but is missing the step-by-step instructions required to convert one into the other.”
9. The discussion section provides a good summary of the work and makes reasonable
conclusions based on the results. It also addresses (and dispels) some alternative explanations for
the results, and considers interesting implications of the findings.
Abstract
10. The abstract provides a reasonable summary of the work.
Thank you for the opportunity to review this work. I am signing this review for the sake of
transparency.
Heather Urry
Tufts University
Reviewer: 2
Comments to the Author(s)
label_comment_6
This is an interesting, and costly effort, but it is not as easy to read or process as it should.
I struggled and did not read it as closely as it would like given how costly it was to parse the
information. If I complain about something that you already address, assume you are not
addressing it in an easy enough to understand way.
12
My most important comment is that in its present form the paper is alarmist. To a casual reader
(e.g., a journalist, or the 90% of readers that will just skim it), it tells that a large share of scientific
findings are not reproducible. But the evidence is much weaker than that, it is merely that a large
share of scientific papers contain at least one secondary results that appear to have an error.
Specific comments.
1) Abstract
My guess, partially because of base rates, partially because the paper is hard to read, is that few
people will get through the paper. My forecast is that upon publication readers will get through
the abstract and just use it as a justification to argue there is a reproducibility crisis, saying things
like "37% of articles are not reproducible even with author assistance" (Hardwicke et al).
There is a distinction between inconsequential errors that can be introduced at copy-editing say,
or the result of a minor act of sloppiness, and errors in execution of research that truly invalidate
a paper.
The abstract, and the figures in the papers, what readers will consult without reading the rest,
should be unambiguous. The authors should actively revise their paper keeping in mind the
sensationalist journalist or careless readers who will put words in their mouths.
The authors do have this line in the abstract "Importantly, original conclusions did not appear to
be seriously impacted." But it does not sufficiently, for my taste, downplay the alarmist message
of the previous line. Something like this seems more accurate a summary "While none of the
articles had errors that seriously impact the conclusions, we were unable to account for
inconsequential differences in results, even with authors assistance, in 37% of articles"
That’s less punchy a claim, but I think a more accurate one.
2) Too many procedural details
The paper feels too detailed in things that are not that key, e.g., exactly how the studies were
selected, the co-piloting rules, etc. Most readers won't worry about those things. You can provide
general reassurances in the text that allows readers to evaluate if the study selection was fair and
unbiased, and delegate the details of execution to the supplement. e.g., Study 1 Sampling frame
could be shorten to a single line of text
3) Study 1 is needed, but not super important.
The idea that when a journal requires something, it happens more often, is not that interesting, I
do think the authors should report Study 1, but there is no need to get into the weeds, discuss
possible confounding factors, etc. Nobody really doubts that requiring something makes it more
prevalent, and the exact point estimate is probably context specific and not particularly
consequential (e.g., who the editor was). We don't want to read quotes from the Cognition
editorial, etc etc. I would say 1-2 pages for Study 1 are more than enough. We just want to know
"so did more people do it? Yes, about 30-50% increase" ok. Done. So I would suggest the authors
get more promptly to Study 2, the one that's much more interesting and policy relevant. You can
say, "we deployed various analytical approaches and our estimates of the policy ranged from x%
to y%, see Supplement X for details. Readers will be happier, and less likely to put the article
down before they get to more valuable Study 2.
Similarly, the discussion in page 15-18 would make more sense in a setting where we did not
obviously expect requiring something to make it more prevalent, or where a point estimate was
intrinsically important. I would drop almost if not all of it, though obviously it is the authors' call.
Nobody doubts some causal effect, nothing really hinges on it being 17% vs 35% , say.
13
4) Say more about IPR
The "IPR" is a very important construct, there are many ways to construct it, and reading the
paper I don't get a very good sense of how it was done, how subjective it was, etc. I would
expand this to a full page, and tabulate more details.
5) Results for Study 2 are hard to parse
I had a really hard time processing the results from Study 2. In part because I am not used to
having magnitude be represented by the size of an X, perhaps an actual number will be better.
Also, one does not get a sense of how important the errors were, and to what extent they are
independent from one another. If the original authors dropped one observation they shouldn't
have, they will have the wrong mean, confidence interval, p-value, effect size, and fill the tables
for Study 2 with red crosses. Maybe worth counting actions that cause errors rather than
consequences of such actions (if this is reasonably easy to do), and simply tabulate how many
independent origin of errors are present in each paper. Then maybe say something about those
origins and how they can be avoided
6) Figure 3 provides no information beyond reporting three %s. It feels too "People magazine" to
me. It almost guarantees an alarmist unjustified read of the evidence presented in this paper. It
almost guarantees a journalist will claim this paper found that 1/3 studies failed to be
reproduced (which is technically true, but readers will assume that what failed to be reproduced
is something that mattered rather than a detail that may be irrelevant).
7) Say more about what it meant to require author assistance, for instance, did the authors explain
something you should have known? Or did they indicate something that is critical and nobody
could possibly figure out without contacting them?
8) The writing is tedious also in terms of reporting everything in a very formulaic way, for
instance, we don't need to convert to % and report a confidence interval every single time a
frequency is reported. We should be able to say simply "We encountered errors in 24 of the 35
statements", without then converting that to a % and putting the 95% confidence interval. Also,
you can probably just say once all confidence interval are 95%, and not print "95%" 53 times in the
paper.
The algorithmic way of reporting results makes it hard for humans to get through the results,
consider for example this paragraph: "
519 The errors broken down by outcome type are displayed in Figure 4. Of the major
520 errors, 17 (27%, 95% CI [16, 39]) were standard deviations or standard errors, 17 (27%, 95%
521 p-values, 10 (16%, 95% CI [5, 28]) were test statistics, such as t and F
522 values, 8 (12%, 95% CI [2, 25]) were effect sizes such as Cohen’s d or Pearson’s r values, 4
523 (6%, 95% CI [0, 19]) were means or medians, 4 (6%, 95% CI [0, 19]) were degrees of freedom,
524 1 (2%, 95% CI [0, 14]) was a count or proportion, 1 (2%, 95% CI [0, 14]) was a confidence
525 interval, and 2 (3%, 95% CI [0, 16]) were other miscellaneous values."
You could also do a bar chart, but if you keep it in text, I would just report the frequencies.
9) The 5-6 pages of discussion at the end are a lot. A good figure is worth 1000 discussion words. I
personally think papers should be mostly about establishing facts; this is not an editorial.
Moreover, much of the discussion qualifies factual matters, such as the type of errors you
uncovered and how consequential they were. That type of analysis belongs in the meat of the
paper, belongs in the tables and figures. One should avoid presenting factual information that is
possibly misleading, and then address the possibly misleading facts in the discussion. The tables
and figures themselves should be created in ways that the most natural interpretation of the facts
14
there presented is correct. In this paper, the most natural interpretation of the summary stats and
figures is alarmist ("Jesus, it's all irreproducible") and then the discussion calms you down a bit
('well, but not the stuff that matters').
10) Appendix E contains important information. The authors can probably create a table that
summarizes it.
Reviewer: 3
Comments to the Author(s)
label_comment_7
This paper shows very interesting results in open data, and also concerned different kinds of data
base, I think this paper can be acceppt in this version.
However, Authors should compare with other methods and other situation,and also use other
method tp stress this situation, that will help your paper more powerful.
Author's Response to Decision Letter for (RSOS-180448.R0)
See Appendix A.
label_version_2
RSOS-180448.R1 (Revision)
label_author_4
Review form: Reviewer 1 (Heather Urry)
Is the manuscript scientifically sound in its present form?
Yes
Are the interpretations and conclusions justified by the results?
Yes
Is the language acceptable?
Yes
Is it clear how to access all supporting data?
Yes
Do you have any ethical concerns with this paper?
No
Have you any concerns about statistical analyses in this paper?
No
Recommendation?
label_recommendation_4
Accept with minor revision (please list in comments)
15
Comments to the Author(s)
label_comment_8
I continue to think this paper is well-conceived and will make a nice contribution to the literature.
Moreover, the authors have revised this paper to my satisfaction, at least for the most part. A
holdover issue for me is about coder drift. The response letter notes that random assignment of
articles to coders assuages concerns about systematic errors abetting the conclusion that the
policy led to more data available statements and in principle reusable data. I agree that this is
true to the extent that each batch of 20 of the randomly-assigned articles ended up containing
roughly even numbers of pre and post policy articles for all coders.In that case, drift would affect
pre and post periods to the same degree. A statement addressing this in the manuscript would be
welcome.
A few minor points about things I did not catch first time around (sorry):
1. Figure 1 shows nonlinear prediction/estimate lines but the interrupted time series analysis
does not appear to have modeled anything but linear effects. The curved lines are fine in
principle but it is probably worth noting in the figure caption that the depiction deviates from the
analysis it parallels. (Alternatively, perhaps I'm misunderstanding the logistic regression models;
in that case, consider clarifying in text how nonlinearities were possible.)
2. I don't believe the paper indicates whether Study One coders were blind to hypotheses. As
such, a reader may wonder whether DAS and IPR rates could be affected by coder expectancies.
This bears noting in the method section (lines 166-171). If coders were not blind, this also bears
comment in the "threats to validity" section of the discussion (lines 279-296).
3. Line 365: "reproducible" should be "reproducibility"
4. Line 688: Should "underestimate" be "overestimate"? Given the context of the paragraph, I
believe so but perhaps I'm misunderstanding the referent. In that case, it would be good to clarify
wording.
Thank you for the opportunity to review this interesting work. I'm signing this review for the
sake of transparency.
Heather Urry
Tufts University
label_author_5
Review form: Reviewer 2
Is the manuscript scientifically sound in its present form?
Yes
Are the interpretations and conclusions justified by the results?
Yes
Is the language acceptable?
Yes
Is it clear how to access all supporting data?
Yes
16
Do you have any ethical concerns with this paper?
No
Have you any concerns about statistical analyses in this paper?
No
Recommendation?
label_recommendation_5
Accept with minor revision (please list in comments)
Comments to the Author(s)
label_comment_9
Overall the paper got better in the directions proposed by the review team. In most instances
when the authors deviated from what was suggested they provide a justification, the editor can
adjudicate those disagreements which are largely stylistic. Lots of great improvements i should
say.
A few additional comments
1) I think the abstract could use further edits to avoid misunderstandings.
Maybe the abstract could read something like: "In a sample of 35 articles we attempted to
reproduce a total of XXX reported statistical results (means, d.f., t-tests, etc). For 11 articles we
successfully reproduced all XXX results we attempted to reproduce, for another 11 we required
some assistance to do so for at least one, and for the remaining 13 articles there was at least one
reported result we were unable to reproduce even with assistance. None of the discrepancies
reverse the studies' conclusions."
So making it clear what is being compared, how many are being compared, and that if one is not
reproduced, the entire paper counts as non-reproduced.
1.1) In the statement "there were no clear indications that original conclusions were seriously
impacted" has two qualifiers "clear" and "serious". Are they necessary, don't you have all the
information needed to say something more concrete like "no conclusions were affected". That's an
honest question, if you don't have the goods, i understand the need for hedged language. But i
would have thought that you did have the goods.
1.2) I would prefer it if the abstract also included examples of the discrepancy, so maybe the
abstract could follow: "For example, one article did a heteroskedasticity correction which was not
reported and another incompletely specified a regression model" (ideally for the latter the authors
will explain just how it is that it was incompletely specific, missing a covariate?)
I am using actual examples reported in the paper page 31, lines 550-566.
When i hear non-reproducible what i have in mind is someone reports b=2.99, p=.001, but when
you run the code, you get b=.07, p=.37, i am trying to make sure readers don't come up with the
wrong impression i do.
2) i would turn the %s reported in page 31 into a figure, so that readers quickly have access to
information on the types of discrepancies we are talking about. I find interesting that there were 8
cases of incomplete or corrupt data files.
3) perhaps the authors could also quantify the magnitude of the discrepancies, something like
17
"the median deviation between reported and reproduced results, for results that were not fully
reproducible, was xx%")
label_end_comment
Decision letter (RSOS-180448.R1)
15-Jun-2018
Dear Dr Hardwicke:
On behalf of the Editors, I am pleased to inform you that your Manuscript RSOS-180448.R1
entitled "Data availability, reusability, and analytic reproducibility: Evaluating the impact of a
mandatory open data policy at the journal Cognition" has been accepted for publication in Royal
Society Open Science subject to minor revision in accordance with the referee suggestions. Please
find the referees' comments at the end of this email.
The reviewers and Subject Editor have recommended publication, but also suggest some minor
revisions to your manuscript. Therefore, I invite you to respond to the comments and revise your
manuscript.
• Ethics statement
If your study uses humans or animals please include details of the ethical approval received,
including the name of the committee that granted approval. For human studies please also detail
whether informed consent was obtained. For field studies on animals please include details of all
permissions, licences and/or approvals granted to carry out the fieldwork.
• Data accessibility
It is a condition of publication that all supporting data are made available either as
supplementary information or preferably in a suitable permanent repository. The data
accessibility section should state where the article's supporting data can be accessed. This section
should also include details, where possible of where to access other relevant research materials
such as statistical tools, protocols, software etc can be accessed. If the data has been deposited in
an external repository this section should list the database, accession number and link to the DOI
for all data from the article that has been made publicly available. Data sets that have been
deposited in an external repository and have a DOI should also be appropriately cited in the
manuscript and included in the reference list.
If you wish to submit your supporting data or code to Dryad (http://datadryad.org/), or modify
your current submission to dryad, please use the following link:
http://datadryad.org/submit?journalID=RSOS&manu=RSOS-180448.R1
• Competing interests
Please declare any financial or non-financial competing interests, or state that you have no
competing interests.
• Authors’ contributions
All submissions, other than those with a single author, must include an Authors’ Contributions
section which individually lists the specific contribution of each author. The list of Authors
should meet all of the following criteria; 1) substantial contributions to conception and design, or
acquisition of data, or analysis and interpretation of data; 2) drafting the article or revising it
critically for important intellectual content; and 3) final approval of the version to be published.
18
All contributors who do not meet all of these criteria should be included in the
acknowledgements.
We suggest the following format:
AB carried out the molecular lab work, participated in data analysis, carried out sequence
alignments, participated in the design of the study and drafted the manuscript; CD carried out
the statistical analyses; EF collected field data; GH conceived of the study, designed the study,
coordinated the study and helped draft the manuscript. All authors gave final approval for
publication.
• Acknowledgements
Please acknowledge anyone who contributed to the study but did not meet the authorship
criteria.
• Funding statement
Please list the source of funding for each author.
Please note that we cannot publish your manuscript without these end statements included. We
have included a screenshot example of the end statements for reference. If you feel that a given
heading is not relevant to your paper, please nevertheless include the heading and explicitly state
that it is not relevant to your work.
Because the schedule for publication is very tight, it is a condition of publication that you submit
the revised version of your manuscript within 7 days (i.e. by the 24-Jun-2018). If you do not think
you will be able to meet this date please let me know immediately.
To revise your manuscript, log into https://mc.manuscriptcentral.com/rsos and enter your
Author Centre, where you will find your manuscript title listed under "Manuscripts with
Decisions". Under "Actions," click on "Create a Revision." You will be unable to make your
revisions on the originally submitted version of the manuscript. Instead, revise your manuscript
and upload a new version through your Author Centre.
When submitting your revised manuscript, you will be able to respond to the comments made by
the referees and upload a file "Response to Referees" in "Section 6 - File Upload". You can use this
to document any changes you make to the original manuscript. In order to expedite the
processing of the revised manuscript, please be as specific as possible in your response to the
referees.
When uploading your revised files please make sure that you have:
1) A text file of the manuscript (tex, txt, rtf, docx or doc), references, tables (including captions)
and figure captions. Do not upload a PDF as your "Main Document".
2) A separate electronic file of each figure (EPS or print-quality PDF preferred (either format
should be produced directly from original creation package), or original software format)
3) Included a 100 word media summary of your paper when requested at submission. Please
ensure you have entered correct contact details (email, institution and telephone) in your user
account
4) Included the raw data to support the claims made in your paper. You can either include your
data as electronic supplementary material or upload to a repository and include the relevant doi
within your manuscript
5) All supplementary materials accompanying an accepted article will be treated as in their final
form. Note that the Royal Society will neither edit nor typeset supplementary material and it will
19
be hosted as provided. Please ensure that the supplementary material includes the paper details
where possible (authors, article title, journal name).
Supplementary files will be published alongside the paper on the journal website and posted on
the online figshare repository (https://figshare.com). The heading and legend provided for each
supplementary file during the submission process will be used to create the figshare page, so
please ensure these are accurate and informative so that your files can be found in searches. Files
on figshare will be made available approximately one week before the accompanying article so
that the supplementary material can be attributed a unique DOI.
Please note that Royal Society Open Science will introduce article processing charges for all new
submissions received from 1 January 2018. Charges will also apply to papers transferred to Royal
Society Open Science from other Royal Society Publishing journals, as well as papers submitted
as part of our collaboration with the Royal Society of Chemistry
(http://rsos.royalsocietypublishing.org/chemistry). If your manuscript is submitted and
accepted for publication after 1 Jan 2018, you will be asked to pay the article processing charge,
unless you request a waiver and this is approved by Royal Society Publishing. You can find out
more about the charges at http://rsos.royalsocietypublishing.org/page/charges. Should you
have any queries, please contact openscience@royalsociety.org.
Once again, thank you for submitting your manuscript to Royal Society Open Science and I look
forward to receiving your revision. If you have any questions at all, please do not hesitate to get
in touch.
Kind regards,
Thadcha Retneswaran
Royal Society Open Science
openscience@royalsociety.org
on behalf of Professor Chris Chambers (Subject Editor)
openscience@royalsociety.org
Associate Editor Comments to Author (Professor Chris Chambers):
Associate Editor: 1
Comments to the Author:
label_comment_10
The manuscript was returned to two of the original reviewers, who have assessed it favourably.
Some final minor clarifications are required (the most substnative being the issue of potential
coder bias raised by Reviewer 1), but provided these issues are addressed thoroughly in a final
revision, acceptance should be forthcoming without requiring further review.
Reviewer comments to Author:
Reviewer: 2
Comments to the Author(s)
label_comment_11
Overall the paper got better in the directions proposed by the review team. In most instances
when the authors deviated from what was suggested they provide a justification, the editor can
adjudicate those disagreements which are largely stylistic. Lots of great improvements i should
say.
A few additional comments
1) I think the abstract could use further edits to avoid misunderstandings.
20
Maybe the abstract could read something like: "In a sample of 35 articles we attempted to
reproduce a total of XXX reported statistical results (means, d.f., t-tests, etc). For 11 articles we
successfully reproduced all XXX results we attempted to reproduce, for another 11 we required
some assistance to do so for at least one, and for the remaining 13 articles there was at least one
reported result we were unable to reproduce even with assistance. None of the discrepancies
reverse the studies' conclusions."
So making it clear what is being compared, how many are being compared, and that if one is not
reproduced, the entire paper counts as non-reproduced.
1.1) In the statement "there were no clear indications that original conclusions were seriously
impacted" has two qualifiers "clear" and "serious". Are they necessary, don't you have all the
information needed to say something more concrete like "no conclusions were affected". That's an
honest question, if you don't have the goods, i understand the need for hedged language. But i
would have thought that you did have the goods.
1.2) I would prefer it if the abstract also included examples of the discrepancy, so maybe the
abstract could follow: "For example, one article did a heteroskedasticity correction which was not
reported and another incompletely specified a regression model" (ideally for the latter the authors
will explain just how it is that it was incompletely specific, missing a covariate?)
I am using actual examples reported in the paper page 31, lines 550-566.
When i hear non-reproducible what i have in mind is someone reports b=2.99, p=.001, but when
you run the code, you get b=.07, p=.37, i am trying to make sure readers don't come up with the
wrong impression i do.
2) i would turn the %s reported in page 31 into a figure, so that readers quickly have access to
information on the types of discrepancies we are talking about. I find interesting that there were 8
cases of incomplete or corrupt data files.
3) perhaps the authors could also quantify the magnitude of the discrepancies, something like
"the median deviation between reported and reproduced results, for results that were not fully
reproducible, was xx%")
Reviewer: 1
Comments to the Author(s)
label_comment_12
I continue to think this paper is well-conceived and will make a nice contribution to the literature.
Moreover, the authors have revised this paper to my satisfaction, at least for the most part. A
holdover issue for me is about coder drift. The response letter notes that random assignment of
articles to coders assuages concerns about systematic errors abetting the conclusion that the
policy led to more data available statements and in principle reusable data. I agree that this is
true to the extent that each batch of 20 of the randomly-assigned articles ended up containing
roughly even numbers of pre and post policy articles for all coders.In that case, drift would affect
pre and post periods to the same degree. A statement addressing this in the manuscript would be
welcome.
A few minor points about things I did not catch first time around (sorry):
1. Figure 1 shows nonlinear prediction/estimate lines but the interrupted time series analysis
does not appear to have modeled anything but linear effects. The curved lines are fine in
21
principle but it is probably worth noting in the figure caption that the depiction deviates from the
analysis it parallels. (Alternatively, perhaps I'm misunderstanding the logistic regression models;
in that case, consider clarifying in text how nonlinearities were possible.)
2. I don't believe the paper indicates whether Study One coders were blind to hypotheses. As
such, a reader may wonder whether DAS and IPR rates could be affected by coder expectancies.
This bears noting in the method section (lines 166-171). If coders were not blind, this also bears
comment in the "threats to validity" section of the discussion (lines 279-296).
3. Line 365: "reproducible" should be "reproducibility"
4. Line 688: Should "underestimate" be "overestimate"? Given the context of the paragraph, I
believe so but perhaps I'm misunderstanding the referent. In that case, it would be good to clarify
wording.
Thank you for the opportunity to review this interesting work. I'm signing this review for the
sake of transparency.
Heather Urry
Tufts University
Author's Response to Decision Letter for (RSOS-180448.R1)
See Appendix B.
label_end_comment
Decision letter (RSOS-180448.R2)
25-Jun-2018
Dear Dr Hardwicke,
I am pleased to inform you that your manuscript entitled "Data availability, reusability, and
analytic reproducibility: Evaluating the impact of a mandatory open data policy at the journal
Cognition" is now accepted for publication in Royal Society Open Science.
You can expect to receive a proof of your article in the near future. Please contact the editorial
office (openscience_proofs@royalsociety.org and openscience@royalsociety.org) to let us know if
you are likely to be away from e-mail contact. Due to rapid publication and an extremely tight
schedule, if comments are not received, your paper may experience a delay in publication.
Royal Society Open Science operates under a continuous publication model
(http://bit.ly/cpFAQ). Your article will be published straight into the next open issue and this
will be the final version of the paper. As such, it can be cited immediately by other researchers.
As the issue version of your paper will be the only version to be published I would advise you to
check your proofs thoroughly as changes cannot be made once the paper is published.
In order to raise the profile of your paper once it is published, we can send through a PDF of your
22
paper to selected colleagues. If you wish to take advantage of this, please reply to this email with
the name and email addresses of up to 10 people who you feel would wish to read your article.
Please note that Royal Society Open Science will introduce article processing charges for all new
submissions received from 1 January 2018. Charges will also apply to papers transferred to Royal
Society Open Science from other Royal Society Publishing journals, as well as papers submitted
as part of our collaboration with the Royal Society of Chemistry
(http://rsos.royalsocietypublishing.org/chemistry). If your manuscript is submitted and
accepted for publication after 1 Jan 2018, you will be asked to pay the article processing charge,
unless you request a waiver and this is approved by Royal Society Publishing. You can find out
more about the charges at http://rsos.royalsocietypublishing.org/page/charges. Should you
have any queries, please contact openscience@royalsociety.org.
On behalf of the Editors of Royal Society Open Science, we look forward to your continued
contributions to the Journal.
Kind regards,
Thadcha Retneswaran
Royal Society Open Science
openscience@royalsociety.org
on behalf of Professor Chris Chambers (Subject Editor)
openscience@royalsociety.org
pendix A
sponse to reviewers
nuscript title: Data availability, reusability, and analytic reproducibility: Evaluating the
act of a mandatory open data policy at the journal Cognition
nuscript id: RSOS-180448
hors: Tom E. Hardwicke, Maya B. Mathur, Kyle MacDonald, Gustav Nilsonne, George C.
ks, Mallory C. Kidwell, Alicia Hofelich Mohr, Elizabeth Clayton, Erica J. Yoon,
hael Henry Tessler, Richie L. Lenne, Sara Altman, Bria Long, & Michael C. Frank
respond to each reviewer comment in turn below.
viewer Reviewer comment Author response
Summary: In this article, the We thank the reviewer for their feedback – we
authors are interested in will address the comments about a control
determining to what extent a journal and coder reliability below.
mandatory open data policy at a
specific journal, Cognition,
impacts whether the data are
available, whether they’re
curated in a way that makes them
useable, and whether the primary
results of studies reported in the
journal are reproducible. They
adopted one-group interrupted
1 time-series design to examine
these things both before and after
the new open data policy was
adopted in March 2015.
Overall, this is an interesting and
important effort, one that is
relevant to the readership of
Royal Society Open Science.
This clearly was a Herculean
effort given the large number of
empirical articles assessed, the
number of coders and analysis
pilots/co-pilots necessary to
wrangle the data and reproduce
analyses, and the careful
attention the authors paid to
transparency in this endeavor.
These positive aspects of the
work are offset a bit by a
methodological limitation,
namely the absence of a
comparison journal similar to
Cognition but without
introduction of an open data
policy, and questions about
coding reliability. Ultimately,
however, I believe the paper
could make a useful contribution
to the literature despite these
issues.
Below I provide specific
questions and comments about
this manuscript; I hope they are
helpful to the editor and authors.
Please note that I have written
this review in two phases. In the
first phase, I read only the
introduction and method sections
so that my evaluation of the
premise of the work and its
methodological rigor would be
unaffected by the pattern of
results and the authors’
interpretation therein.
Phase 1: Introduction and N/A
Method (without reading
Abstract, Results, or Discussion)
1
1. The introduction nicely
frames the relevant theoretical
issues. By the end, the reader
understands that the data on
which empirical studies are based
are rarely made publicly
accessible and, even when the
data are made publicly
accessible, they’re often curated
in a way that makes them
essentially useless to those who
may wish to reproduce reported
results.
2. Study 1 Method:
a. The authors clearly
acknowledge potential
limitations, including the notion
that the open data policy
coincided with introduction of a
new editor-in-chief (EIC) at
Cognition, and the notion that
announcement of the new EIC
may have led to anticipatory
changes in data sharing prior to
the March 1, 2015 start date of
the open data policy.
b. To what extent does an We believe there are a sufficient number of
imbalance in available articles articles in both the pre- and post- policy
from before (n = 417) to after (n periods: the confidence intervals reflect
reasonable certainty in the point estimates. An
= 174) instituting the open data imbalance in the number of articles still leads
policy affect conclusions that can to unbiased point estimation; it’s just a bit less
1 be drawn? The smaller of the two efficient than having equal numbers in each
numbers is still rather large, thus group. Extending data collection too long after
perhaps it doesn’t. policy introduction would dilute the extent to
which we can infer the observed trend is
caused by the policy, rather than other events
that we are unaware of or cannot control.
c. Coders demonstrated 90% We recognize the limitations of inter-rater
1 agreement, however, they are of little concern
agreement with a gold standard
coding of 5 articles prior to here (see McHugh, 2012;
coding the articles to be included https://bit.ly/2I8mYTz). Firstly, the data
in the analyses for this paper. extraction form (https://osf.io/qr6e4/) is
relatively straightforward and generally leaves
Percent agreement doesn’t, of little room for subjective interpretation
course, take into account (except perhaps for the determination of data
agreement by chance. The ‘understandability’). Secondly, options such
authors might consider using a as “other” and “unclear” were provided to
different method of assessing limit any guess-work. Thirdly, inter-rater
agreement exceeded 90% for the training
interrater reliability that takes articles for all coders. Thus, any correction for
into account chance agreement. guesswork is unlikely to serious diminish our
confidence in coder reliability.
Note that only training articles were double-
coded. The actual data were single coded. We
believe this is reasonable given the
straightforwardness of data extraction (above).
d. With a one-group Thank you for flagging this issue. Although
interrupted time-series design, drift might introduce some errors, we are not
changes in instrumentation (in concerned about systematic errors from drift
that would bias our conclusions because all
this case, coders) represent a articles from across the assessment period
threat to internal validity. Can the were randomly assigned to coders.
authors please clarify the
procedures they used to prevent
1
or evaluate coder drift? Among
the batch of 20 articles received
by coders each time, were there
roughly even numbers of articles
from the pre- and post- policy
periods?
e. It would strengthen the We agree that a comparison journal would add
study’s internal validity if the additional protection against some threats to
authors had a comparison journal validity. However, it would take considerable
time to collect this additional data, so it is
of some kind, one that has similar worth weighing the added value of such an
1 features to Cognition but did not analysis. We determined it would not be
adopt an open data policy on worthwhile and provide our rationale below.
March 1, 2015.
We describe potential threats to validity and
how a single-group interrupted time-series
design can address them in the Study One
discussion (pages 16-17). For example, it
could be that the observed trend is influenced
by something intrinsic changing about the
population of authors submitting to Cognition
(a ‘maturation’ threat). However, this would
not explain the abrupt change at the point of
intervention.
Similarly, the observed trend could be due to
some external event, such as an influential
academic society endorsing data sharing.
However, again, a candidate event would have
to be time-locked to the point of intervention
to explain the abrupt change. We are not
aware of such an event, although it remains
possible that one occurred.
The addition of a control journal could help to
rule this out more definitively. However, some
relevant data can be acquired from the
Kidwell et al. study
(https://doi.org/10.1371/journal.pbio.1002456)
which measured data availability at several
control (psychology) journals up to May 2015.
As the Cognition data policy was introduced
in March 2015, there is some overlap here of
the critical post-policy period. No discernable
increase in data availability was observed in
the control journals. This observation
reinforces the conclusion that it was the
introduction of Cognition’s open data policy
rather than some external event that drove the
observed changes.
3. Study 2 Method: It was not our intention to do a pre-/post-
a. The authors describe a comparison for Study Two (we report that this
reasonable set of variables and was a “non-comparative case-study design”).
procedures to evaluate analytic Articles were pseudo-randomly sampled from
reproducibility of 35 articles for those that were deemed to have in-principle
1
which the data were available. reusable data in Study One. Of the eligible
It’s possible I missed it, but I articles, 5 were from the pre-policy period and
didn’t see reported whether the 30 were from the post-policy period. We now
state this in the “sample” section of study two
articles in question were (page 21), but the numbers are too small to
published before or after the make an informative pre-/post- comparison.
introduction of the open data
policy at Cognition. A reader
might reasonably wonder
whether analytic reproducibility
was higher after.
b. Edited after reading results Details of article selection for Study Two are
and discussion sections: A reader reported in the “Sample” section of the Study
may wonder whether the 35 Two Methods. However, the reviewer’s
comment highlights that we had not explained
articles selected for evaluation of this process with sufficient clarity. We have
analytic reproducibility are made adjustments and the section (page 21)
representative of the larger group now reads as follows:
of articles from which they were
selected (35+61 = 96); a “A sample size was not pre-registered: we
continued running reproducibility checks on
selection bias could influence an ad-hoc basis until we reached the limits of
outcome estimates. For example, our personnel resources. Ultimately 108
if the 35 articles selected for articles had in-principle reusable data and
evaluation in Study 2 are more entered the triage pool (see Study One). 47
“straightforward” than the articles were assessed for eligibility, 12 of
which were rejected because a finding that
remaining 61, estimates of was both “straightforward” and “substantive”
reproducibility may be inflated. could not be readily identified (according to
the above criteria). 35 articles were deemed
1 eligible for reproducibility checks, of which 5
were from the pre-policy period and 30 were
from the post-policy period.”
We agree with the reviewer that this process
could introduce selection bias and explicitly
address the issue (and other factors that are
likely to introduce selection bias) at length in
the Study Two Discussion (page 35-36),
starting “It is important to note that Study
Two was based on a sample that is likely to
provide an optimistic estimate of analytic
reproducibility…”. In brief, we do not believe
selection bias undermines our conclusions, but
it does places an important limitation on the
generalizability of our findings – which we
acknowledge in the manuscript. Ideally, future
studies should also seek to examine a broader
range of analyses, including complex
analyses.
Phase 2: Results, Discussion, and Great!
Abstract
Results
1
4. The reporting of results in
both studies is both complete and
clear.
5. I love the figures.
6. One question is whether the As the data are binary (data available or not
50-day binning is necessary in available) plotting each data point would lead
Figure 1, Figure C1, and Figure to massing of the data at either 1 or 0, causing
D1 for “ease of presentation”. I over-plotting and obscuring the trend. The 50-
1 found myself wanting to see the day binning is one solution to enable
actual individual-article straightforward visualization of the trend; we
observations. are open to others but have not been able to
identify a better one.
7. Figure 3 seems Reviewer 2 also found this Figure superfluous
superfluous. and we have decided to remove it. There was
however, one important piece of information
in Figure 3 regarding the proportion of articles
that were reproducible. We retained this
1
information by making a small addition to
what was previously Figure 4 (now Figure 3)
– the article id codes on the x-axis are now
colored depending on the overall
reproducibility outcome for that article.
Discussion Thank you. We appreciate you taking the time
8. I love this analogy: to provide this helpful feedback.
“Conducting an analytic
reproducibility check without an
analysis script is rather like
1 assembling flat pack furniture
without an instruction booklet:
one is given some materials
(categorised and labelled with
varying degrees of helpfulness),
and a diagram of the final
product, but is missing the step-
by-step instructions required to
convert one into the other.”
9. The discussion section
provides a good summary of the
work and makes reasonable
conclusions based on the results.
It also addresses (and dispels)
some alternative explanations for
the results, and considers
interesting implications of the
findings.
Abstract
10. The abstract provides a
reasonable summary of the work.
Thank you for the opportunity to
review this work. I am signing
this review for the sake of
transparency.
Heather Urry
Tufts University
Reviewer: 2 We regret that the reviewer found the paper
difficult to read. We have made efforts to
Comments to the Author(s) increase the clarity of the manuscript (detailed
label_comment_13
below) while retaining what we are feel are
This is an interesting, and costly important methodological details that allow
effort, but it is not as easy to read for comprehensive evaluation and exact
2 or process as it should. replication of the work. In particular, we have
I struggled and did not read it as added several brief summary sentences at the
closely as it would like given start of particularly lengthy sections to
how costly it was to parse the highlight the most relevant information for the
information. If I complain about reader.
something that you already
address, assume you are not
addressing it in an easy enough
to understand way.
My most important comment is Thank you for bringing up this important
that in its present form the paper issue. While drafting the manuscript, we
is alarmist. To a casual reader shared the concern that our findings could
potentially be misinterpreted, and we made
(e.g., a journalist, or the 90% of efforts to address it in the abstract,
readers that will just skim it), it introduction, and discussion sections.
tells that a large share of
scientific findings are not In our revision, we have taken the following
reproducible. But the evidence is additional steps to reduce the likelihood of the
findings being misinterpreted:
much weaker than that, it is
merely that a large share of (1) Rewording of key findings in the abstract
scientific papers contain at least (2) Removal of Figure 3
one secondary results that appear (3) Repeating in the general discussion the
to have an error. key caveat that there was no clear indication
that original conclusions were affected.
Some other caveats given in the text are
below:
After reporting the reproducibility findings in
2 the abstract, we were sure to immediately add
the caveat that “Importantly, original
conclusions did not appear to be seriously
impacted”. We have now combined the caveat
with the result into the same sentence, so it is
less likely a reader would miss it: “In 13
(37%) articles we could not reproduce some
target outcomes despite author assistance, but
importantly there were no clear indications
that original conclusions were seriously
impacted.”
Similarly, in the Study Two Discussion: “In
37% of articles reproducibility issues could
not be entirely resolved. Crucially, however,
there were no clear indications that original
conclusions based on the target outcomes
were seriously undermined by the
reproducibility issues we encountered. We
arrived at this determination through careful
consideration of multiple factors…” (page 32)
We also discuss the nuances of interpreting
the findings: “Similarly, it was not always
clear whether the non-reproducible target
outcomes are actually erroneous (i.e.,
originally calculated/reported incorrectly) or
are accurate and cannot be reproduced (i.e.,
originally calculated/reported correctly but the
analysis pipeline to recreate them cannot be
reconstructed)” (page 33)
We reinforce this again: “Assessing the
implications of our findings is complex and
they could be viewed positively or negatively
from different perspectives. The fact that
almost two-thirds of the cases were eventually
reproduced successfully, and none of the
original conclusions based on the target
outcomes appears to have been seriously
undermined, suggests a sub-optimal but
reasonably healthy situation with regards to
the credibility of the original findings.” (page
36)
Specific comments. We appreciate the distinction between
1) Abstract inconsequential errors and errors that clearly
My guess, partially because of undermine the original conclusions, and, as
mentioned above, have taken active steps to
base rates, partially because the avoid misinterpretation by readers.
paper is hard to read, is that few
people will get through the paper. To avoid readers missing the key caveat in the
My forecast is that upon conclusion – we have combined the results
publication readers will get and caveat sentences into one:
“In 13 (37%) articles we could not reproduce
through the abstract and just use some target outcomes despite author
2 it as a justification to argue there assistance, but importantly there were no clear
is a reproducibility crisis, saying indications that original conclusions were
things like "37% of articles are seriously impacted.”
not reproducible even with
The key caveat (as above) is mentioned in all
author assistance" (Hardwicke et places that we report the reproducibility
al). outcomes.
There is a distinction between Additionally, we have removed Figure 3,
inconsequential errors that can be which as the reviewer suggests, could be
misinterpreted if the accompanying text was
introduced at copy-editing say, or not read.
the result of a minor act of
sloppiness, and errors in
execution of research that truly
invalidate a paper.
The abstract, and the figures in
the papers, what readers will
consult without reading the rest,
should be unambiguous. The
authors should actively revise
their paper keeping in mind the
sensationalist journalist or
careless readers who will put
words in their mouths.
The authors do have this line in
the abstract "Importantly,
original conclusions did not
appear to be seriously impacted."
But it does not sufficiently, for
my taste, downplay the alarmist
message of the previous line.
Something like this seems more
accurate a summary "While none
of the articles had errors that
seriously impact the conclusions,
we were unable to account for
inconsequential differences in
results, even with authors
assistance, in 37% of articles"
That’s less punchy a claim, but I
think a more accurate one.
2) Too many procedural We believe these procedural details are
details important so that readers can evaluate our
The paper feels too detailed in methods, and other researchers can replicate
2 our study if they wish. Reviewer One for
things that are not that key, e.g., example, specifically mentions the issue of
exactly how the studies were how studies were selected.
selected, the co-piloting rules,
etc. Most readers won't worry One of the key findings of our study is that
about those things. You can authors provide insufficient details in their
provide general reassurances in published articles, we do not wish to be part of
the problem.
the text that allows readers to
evaluate if the study selection To aid readers who are not interested in detail,
was fair and unbiased, and we have either added a summary sentence or
delegate the details of execution two to the start of particularly lengthy sections
to the supplement. e.g., Study 1 or moved especially pertinent information to
the start of the section.
Sampling frame could be shorten
to a single line of text For Study 2 Sample (page 21), we have
added:
“We assessed the analytic reproducibility of a
subset of descriptive and inferential statistics
reported in 35 articles. Details of the selection
process are outlined in detail below.”
For Study 2 Procedure (page 25), we have
added:
“At least two members of our team attempt to
reproduce the target outcomes for each article
by repeating the reported analysis. When any
reproducibility issues were encountered, we
attempted to resolve them through contact
with the original authors. Further procedural
details are outlined below.”
For Study 1 Procedure (page 9), we have
moved the paragraph starting “For each
article, coders extracted information related to
the measured variables” to the start of this
section, as it contains the most pertinent
information.
3) Study 1 is needed, but not We were interested in the extent to which
super important. authors complied with the policy, not simply
The idea that when a journal whether more did or not. Study 1 also makes
an important contribution by examining the
requires something, it happens quality of data sharing. We are only aware of
more often, is not that interesting, one previous study that has addressed this
2
issue, and the sample assessed in that study
was small (Kidwell et al. 2016).
More generally, we disagree with the
reviewer’s assessment of the impact of
policies on behavior (even requirements). It is
not obvious that when a journal requires
something all authors will comply, as our
findings demonstrate. The fact that not all
articles in the post-policy period shared data,
and that, among those that did, they often did
not share reusable data, shows that the policy
was only partially successful. This
information can inform refinement of the
policy to improve its effectiveness, and also
highlight issues that other journals should be
aware of when implementing similar
initiatives.
More broadly, our findings suggest that open
data alone is not enough – attention to the
curation of data files is necessary to enable
their re-use.
I do think the authors should We disagree with the reviewer that it is not
report Study 1, but there is no important to discuss potential confounding
need to get into the weeds, factors when interpreting study findings. As
this was an observational study, causal
discuss possible confounding conclusions are not straightforward. We
factors, etc. Nobody really outlined several potential confounds in the
doubts that requiring something discussion of Study One (pages 16-17). Other
makes it more prevalent, and the studies of journal open data policies have been
exact point estimate is probably criticized for over-stating their conclusions on
2 precisely these grounds, and rightly so (e.g.,
context specific and not http://blogs.plos.org/absolutely-
particularly consequential (e.g., maybe/2017/08/29/bias-in-open-science-
who the editor was). advocacy-the-case-of-article-badges-for-data-
sharing/). We addressed these potential
confounds through the use of an interrupted-
time series design, which offers considerable
protection against them. However, we cannot
exclude the possibility that alternative factors
contributed to the observed trend (as also
pointed out by Reviewer 1).
We don't want to read quotes Thank you for the suggestion. We have
from the Cognition editorial, etc removed the verbatim quotes from the main
2 text and refer readers to the appendix for this
etc.
additional information.
I would say 1-2 pages for Study 1 We thank the reviewer for sharing this
are more than enough. We just perspective. There is always a difficult
2 balance to be struck between brevity and
want to know "so did more
detail. We have chosen to report all relevant
people do it? Yes, about 30-50%
increase" ok. Done. So I would details that we believe readers need to fully
suggest the authors get more evaluate and understand the methods and
promptly to Study 2, the one results.
that's much more interesting and
policy relevant. You can say, "we
deployed various analytical
approaches and our estimates of
the policy ranged from x% to
y%, see Supplement X for
details. Readers will be happier,
and less likely to put the article
down before they get to more
valuable Study 2.
Similarly, the discussion in page We have commented on this issue above.
15-18 would make more sense in
a setting where we did not
obviously expect requiring
something to make it more
prevalent, or where a point
estimate was intrinsically
2
important. I would drop almost if
not all of it, though obviously it
is the authors' call. Nobody
doubts some causal effect,
nothing really hinges on it being
17% vs 35% , say.
4) Say more about IPR Thank you for this comment which highlights
The "IPR" is a very important the need to be clearer about in-principle
construct, there are many ways to reproducibility (IPR).
construct it, and reading the A detailed description of how we determined
paper I don't get a very good IPR is provided in the Study One Procedure
2 sense of how it was done, how (page 9). We have now made this information
subjective it was, etc. I would more prominent by positioning it at the start of
expand this to a full page, and this section. We have also added an additional
note to the Study One Measures section (page
tabulate more details. 8) that for some analyses we combined
accessibility, completeness, understandability
into the composite measure of IPR.
I had a really hard time With a heterogeneous dataset like the one we
processing the results from Study created, it can be a challenge to create a
2. In part because I am not used visualization that allows for full recovery of
the underlying data. The two figures in
2 to having magnitude be question are intended to provide a visual
represented by the size of an X, illustration of the results. The exact numbers
perhaps an actual number will be are present in the main text.
better.
5) Results for Study 2 are hard Thank you flagging this important issue. We
to parse were aware that a single reproducibility issue
Also, one does not get a sense of could cause multiple errors, so wherever
possible we attempted to determine the
how important the errors were, number of discrete reproducibility issues
and to what extent they are found for each article. We classified them as
independent from one another. If analysis issues, typographical issues, data
the original authors dropped one issues, specification issues, or unidentified
observation they shouldn't have, issues. This information is displayed in Figure
5 (now Figure 4).
they will have the wrong mean,
confidence interval, p-value, In the Study Two Discussion (page 34), we
effect size, and fill the tables for make several suggestions for how these issues
Study 2 with red crosses. Maybe could be avoided. For example, we suggest
worth counting actions that cause that providing analysis scripts and clear,
detailed descriptions of analytic methods in
errors rather than consequences the main text could help to avoid specification
of such actions (if this is issues.
2 reasonably easy to do), and
simply tabulate how many We have now provided some additional
independent origin of errors are comments on how authors can avoid many of
the issues we encountered by adopting a gold-
present in each paper. Then standard reproducible workflow (pages 34-
maybe say something about those 35):
origins and how they can be
avoided “Authors can greatly improve the traceability
of reported outcomes, and reduce the
likelihood of typographical errors, by taking
advantage of technologies like R Markdown
which interleave analysis code with regular
prose to generate reproducible documents [55,
56]. These documents can also be shared in
online software ‘containers’ that handle
software dependency issues and enable
reported outcomes to be re-generated with a
single click of the mouse [57]. The present
manuscript provides a working example (see
Data Accessibility).”
6) Figure 3 provides no Thanks for this feedback. We have now
information beyond reporting removed Figure 3 (see comment to Reviewer
three %s. It feels too "People 1 above).
magazine" to me. It almost
guarantees an alarmist unjustified
read of the evidence presented in
this paper. It almost guarantees a
journalist will claim this paper
2
found that 1/3 studies failed to be
reproduced (which is technically
true, but readers will assume that
what failed to be reproduced is
something that mattered rather
than a detail that may be
irrelevant).
7) Say more about what it To address this issue, we have added the
meant to require author following to the Study 2 results: “We only
assistance, for instance, did the consulted authors after extensive efforts by at
least two team members to reproduce the
authors explain something you target outcomes. There were no cases where
2 should have known? Or did they we retrospectively felt we should have already
indicate something that is critical known something and author contact was
and nobody could possibly figure unjustified.” (page 31)
out without contacting them?
8) The writing is tedious also Thank you for the suggestion. We agree that
in terms of reporting everything some of the confidence intervals are
in a very formulaic way, for superfluous and have reduced their number
throughout.
instance, we don't need to
convert to % and report a
confidence interval every single
time a frequency is reported. We
2
should be able to say simply "We
encountered errors in 24 of the
35 statements", without then
converting that to a % and
putting the 95% confidence
interval. Also, you can probably
just say once all confidence
interval are 95%, and not print
"95%" 53 times in the paper.
The algorithmic way of reporting See above.
results makes it hard for humans
to get through the results,
consider for example this
paragraph: "
519 The errors broken down by
outcome type are displayed in
Figure 4. Of the major
520 errors, 17 (27%, 95% CI [16,
39]) were standard deviations or
standard errors, 17 (27%, 95%
521 p-values, 10 (16%, 95% CI
[5, 28]) were test statistics, such
as t and F
522 values, 8 (12%, 95% CI [2,
25]) were effect sizes such as
2 Cohen’s d or Pearson’s r values,
4
523 (6%, 95% CI [0, 19]) were
means or medians, 4 (6%, 95%
CI [0, 19]) were degrees of
freedom,
524 1 (2%, 95% CI [0, 14]) was a
count or proportion, 1 (2%, 95%
CI [0, 14]) was a confidence
525 interval, and 2 (3%, 95% CI
[0, 16]) were other miscellaneous
values."
You could also do a bar chart,
but if you keep it in text, I would
just report the frequencies.
9) The 5-6 pages of discussion We have presented the ‘raw facts’ in the
2 at the end are a lot. A good figure results section and then provided
is worth 1000 discussion words. I interpretation of those findings in the
personally think papers should be discussion section. We hope the removal of
mostly about establishing facts; Figure 3 and prominent placement of caveats
this is not an editorial. Moreover, (see above) reduces the likelihood of an
unwarranted interpretation of the results.
much of the discussion qualifies
factual matters, such as the type We have also repeated the key caveat again in
of errors you uncovered and how the general discussion: “However,
consequential they were. That importantly, there were no clear indications
type of analysis belongs in the that any original conclusions were seriously
undermined by the reproducibility issues we
meat of the paper, belongs in the encountered.” (page 38).
tables and figures. One should
avoid presenting factual
information that is possibly
misleading, and then address the
possibly misleading facts in the
discussion. The tables and figures
themselves should be created in
ways that the most natural
interpretation of the facts there
presented is correct. In this paper,
the most natural interpretation of
the summary stats and figures is
alarmist ("Jesus, it's all
irreproducible") and then the
discussion calms you down a bit
('well, but not the stuff that
matters').
10) Appendix E contains The dense information in Appendix E (the
important information. The reproducibility vignettes) is summarized
authors can probably create a throughout the results section and Figures. In
particular, Figure 5 (now Figure 4)
2 table that summarizes it. summarizes the potential causes of non-
reproducibility that we mention on a case-by-
case basis in the appendix. Several illustrative
examples are provided in the main text (page
28).
2 Thank you very much; we are grateful for the
feedback.
Appendix B
Response to reviewers round two
Manuscript title: Data availability, reusability, and analytic reproducibility:
Evaluating the impact of a mandatory open data policy at the journal
Cognition
Manuscript id: RSOS-180448.R1
Authors: Tom E. Hardwicke, Maya B. Mathur, Kyle MacDonald, Gustav
Nilsonne, George C. Banks, Mallory C. Kidwell, Alicia Hofelich Mohr,
Elizabeth Clayton, Erica J. Yoon,
Michael Henry Tessler, Richie L. Lenne, Sara Altman, Bria Long, & Michael
C. Frank
We respond to each reviewer comment in turn below.
Reviewer 2
R2_1: Comments to the Author(s)
Overall the paper got better in the directions proposed by the review team. In
most instances when the authors deviated from what was suggested they
provide a justification, the editor can adjudicate those disagreements which
are largely stylistic. Lots of great improvements i should say.
A few additional comments
1) I think the abstract could use further edits to avoid misunderstandings.
Maybe the abstract could read something like: "In a sample of 35 articles we
attempted to reproduce a total of XXX reported statistical results (means, d.f.,
t-tests, etc). For 11 articles we successfully reproduced all XXX results we
attempted to reproduce, for another 11 we required some assistance to do so
for at least one, and for the remaining 13 articles there was at least one
reported result we were unable to reproduce even with assistance. None of the
discrepancies reverse the studies' conclusions."
So making it clear what is being compared, how many are being compared,
and that if one is not reproduced, the entire paper counts as non-reproduced.
Authors: We appreciate the reviewer’s attention to this important issue and
have made the following adjustments to the relevant section of the abstract:
For 35 of the articles determined to have reusable data, we attempted to
reproduce 1324 target values. Ultimately, 64 values could not be reproduced
within a 10% margin of error. For 22 articles all target values were
reproduced, but 11 of these required author assistance. For 13 articles at least
one value could not be reproduced despite author assistance.
R2_2: 1.1) In the statement "there were no clear indications that original
conclusions were seriously impacted" has two qualifiers "clear" and
"serious". Are they necessary, don't you have all the information needed to
say something more concrete like "no conclusions were affected". That's an
honest question, if you don't have the goods, i understand the need for hedged
language. But i would have thought that you did have the goods.
Authors: We think these qualifiers are necessary. Firstly, there were three
cases where we could not complete the analysis. This creates uncertainty
about whether the original conclusions were affected. Secondly, there were
cases where effect sizes were lower than those reported. This should have
some impact on the conclusions, although given the size of the differences, we
think it is reasonable to say that this impact is not serious.
R2_3: 1.2) I would prefer it if the abstract also included examples of the
discrepancy, so maybe the abstract could follow: "For example, one article
did a heteroskedasticity correction which was not reported and another
incompletely specified a regression model" (ideally for the latter the authors
will explain just how it is that it was incompletely specific, missing a
covariate?)
I am using actual examples reported in the paper page 31, lines 550-566.
When i hear non-reproducible what i have in mind is someone reports
b=2.99, p=.001, but when you run the code, you get b=.07, p=.37, i am trying
to make sure readers don't come up with the wrong impression i do.
Authors: We appreciate the reviewer’s suggestion, however the word limit
for the abstract prevents us including additional content. We have now
indicated in the abstract (see above, R2_1) that we used a 10% margin of error
to define non-reproducible values which hopefully makes our approach more
explicit to the reader.
R2_4: 2) i would turn the %s reported in page 31 into a figure, so that readers
quickly have access to information on the types of discrepancies we are
talking about. I find interesting that there were 8 cases of incomplete or
corrupt data files.
Authors: These data (causes of non-reproducibility) are displayed in Figure 4
to quickly convey the overall pattern of results. Counts and percentages are
provided in the text for readers interested in more detail.
R2_5: 3) perhaps the authors could also quantify the magnitude of the
discrepancies, something like "the median deviation between reported and
reproduced results, for results that were not fully reproducible, was xx%")
Authors: Thank you for this suggestion. For the 1324 target values we
evaluated, we assessed the percentage error (PE) and recorded whether there
was a minor or major error. However, we did not actively record the PE. It
would technically be possible to return to our reproducibility reports and
manually extract the PE for each value, but this would take a considerable
amount of time. Furthermore, we do not think such an analysis would be
straightforward to interpret given the range of PE across different value types:
p-values for example, typically had very large percentage error. Overall, we
do not think it is worthwhile to pursue this data collection and analysis given
the time investment relative to information yield.
Reviewer 1
R1_1: Comments to the Author(s)
I continue to think this paper is well-conceived and will make a nice
contribution to the literature. Moreover, the authors have revised this paper
to my satisfaction, at least for the most part. A holdover issue for me is about
coder drift. The response letter notes that random assignment of articles to
coders assuages concerns about systematic errors abetting the conclusion that
the policy led to more data available statements and in principle reusable
data. I agree that this is true to the extent that each batch of 20 of the
randomly-assigned articles ended up containing roughly even numbers of pre
and post policy articles for all coders.In that case, drift would affect pre and
post periods to the same degree. A statement addressing this in the manuscript
would be welcome.
Authors: We have now modified the following sentence in the methods
section: ‘Articles were randomly selected and assigned to coders in order to
ameliorate selection bias and ensure that any coder drift would affect pre-
policy and post-policy articles evenly.’ (Lines 174-176)
R1_2: A few minor points about things I did not catch first time around
(sorry):
1. Figure 1 shows nonlinear prediction/estimate lines but the interrupted time
series analysis does not appear to have modeled anything but linear effects.
The curved lines are fine in principle but it is probably worth noting in the
figure caption that the depiction deviates from the analysis it parallels.
(Alternatively, perhaps I'm misunderstanding the logistic regression models;
in that case, consider clarifying in text how nonlinearities were possible.)
Authors: Thank you for flagging this potential source of confusion. The
model and the fitted line are the same. The model is linear on the logit scale
whereas the Y-axis of the figure is on the probability scale, which is a
nonlinear transformation of the logit. So linear on the logit implies nonlinear
on the probability scale. We have added a note to the figure 1 caption to make
this explicit.
R1_3: 2. I don't believe the paper indicates whether Study One coders were
blind to hypotheses. As such, a reader may wonder whether DAS and IPR
rates could be affected by coder expectancies. This bears noting in the method
section (lines 166-171). If coders were not blind, this also bears comment in
the "threats to validity" section of the discussion (lines 279-296).
Authors: Coders were not blind to the hypotheses. We agree with the
reviewer that this is an important issue, but we believe the potential for bias is
low because the coding scheme mostly involves questions that have objective
answers and leave little room for subjective interpretation to creep in – for
example, either the data is available or it is not available.
We now comment on this in both the method and discussion section.
Specifically:
“Coders were not blind to the study hypotheses, however the data extraction
protocol mostly left little room for subjective interpretation (see
https://osf.io/qr6e4/).” (lines 173-174)
“An additional source of bias worth noting is that coders were not blind to the
study hypotheses. However, because the data extraction protocol mostly left
little room for subjective interpretation, we consider the potential for bias to
be low.” (line 305-307)
R1_4: 3. Line 365: "reproducible" should be "reproducibility"
Authors: Changed, thank you.
R1_5: 4. Line 688: Should "underestimate" be "overestimate"? Given the
context of the paragraph, I believe so but perhaps I'm misunderstanding the
referent. In that case, it would be good to clarify wording.
Authors: Yes, we meant overestimate and have clarified the wording. Thanks
for spotting.
Society Open
