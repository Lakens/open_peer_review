Estimating statistical power, posterior probability and
publication bias of psychological research using the
observed replication rate
Michael Ingre and Gustav Nilsonne
Article citation details
R. Soc. open sci. 5: 181190.
http://dx.doi.org/10.1098/rsos.181190
Review timeline
Original submission: 12 December 2017 Note: Reports are unedited and appear as
Revised submission: 25 July 2018 submitted by the referee. The review history
Final acceptance: 3 August 2018 appears in chronological order.
Review History
label_version_1
RSOS-172173.R0 (Original submission)
label_author_1
Review form: Reviewer 1 (Marian-Gabriel Hancean)
Is the manuscript scientifically sound in its present form?
Yes
Are the interpretations and conclusions justified by the results?
Yes
Is the language acceptable?
Yes
Is it clear how to access all supporting data?
Yes
Do you have any ethical concerns with this paper?
No
© 2018 The Authors. Published by the Royal Society under the terms of the Creative Commons
Attribution License http://creativecommons.org/licenses/by/4.0/, which permits unrestricted use,
provided the original author and source are credited
2
Have you any concerns about statistical analyses in this paper?
No
Recommendation?
label_recommendation_1
Accept as is
Comments to the Author(s)
label_comment_1
In the paper "Estimating statistical power, posterior probability and publication bias of
psychological research using the observed replication rate", the authors illustrate how Bayes’
theorem can be used to better understand the implications of the 36% reproducibility rate of
published psychological findings that was reported by the Open Science Collaboration.
The manuscript addresses a hot topic not only for the psychology field, but also, via its
implications, for other related disciplines (i.e. the reproducibility of published findings, the
assessment of publication bias, statistical power and the posterior probability of findings).
I find this work to be very interesting and illustrative, as it considers, in a general perspective /
framework, through its implications, but not limited to that, some of the diseases that threaten
nowadays science, such as: a disorderly focus on statistically significant results, the excessive
orientation to novelty etc.
The manuscript is very easy to read, while it clearly indicates how the results support the
outlined claims and conclusions. The manuscript is scientifically sound and successfully
describes the properties of research that lead to 90% positive findings and the expected
reproducibility of research with 90% positive findings.
One my view, I do believe the authors nicely perform a demonstration wherein a method to
assess publication bias is plugged in as to test the consistency of observed reproducibility rate
vis-a-vis unbiased literature.
From another perspective, I think the paper, in case of being published, will target a wide general
audience, as the topic of publication bias is of interest not only for psychologists, but also for
science in general.
I recommended this manuscript for being accepted as it is, by the Royal Society Open Science. In
spite of the fact that, perhaps, authors should have briefly made some remarks to the possible
implications of their results for the general field of psychology - but perhaps I am subjective on
this and it would move the paper a little bit in the area of speculations.
label_author_2
Review form: Reviewer 2
Is the manuscript scientifically sound in its present form?
No
Are the interpretations and conclusions justified by the results?
No
Is the language acceptable?
No
3
Is it clear how to access all supporting data?
Yes
Do you have any ethical concerns with this paper?
No
Have you any concerns about statistical analyses in this paper?
Yes
Recommendation?
label_recommendation_2
Major revision is needed (please make suggestions in comments)
Comments to the Author(s)
label_comment_2
See attached file (Appendix A).
label_end_comment
Decision letter (RSOS-172173.R0)
19-Jan-2018
Dear Dr Ingre:
Manuscript ID RSOS-172173 entitled "Estimating statistical power, posterior probability and
publication bias of psychological research using the observed replication rate" which you
submitted to Royal Society Open Science, has been reviewed. The comments from reviewers are
included at the bottom of this letter.
In view of the criticisms of the reviewers, the manuscript has been rejected in its current form.
However, a new manuscript may be submitted which takes into consideration these comments.
Please note that resubmitting your manuscript does not guarantee eventual acceptance, and that
your resubmission will be subject to peer review before a decision is made.
You will be unable to make your revisions on the originally submitted version of your
manuscript. Instead, revise your manuscript and upload the files via your author centre.
Once you have revised your manuscript, go to https://mc.manuscriptcentral.com/rsos and login
to your Author Center. Click on "Manuscripts with Decisions," and then click on "Create a
Resubmission" located next to the manuscript number. Then, follow the steps for resubmitting
your manuscript.
Your resubmitted manuscript should be submitted by 19-Jul-2018. If you are unable to submit by
this date please contact the Editorial Office.
Please note that Royal Society Open Science will introduce article processing charges for all new
submissions received from 1 January 2018. Charges will also apply to papers transferred to Royal
Society Open Science from other Royal Society Publishing journals, as well as papers submitted
as part of our collaboration with the Royal Society of Chemistry
(http://rsos.royalsocietypublishing.org/chemistry). If your manuscript is submitted and
accepted for publication after 1 Jan 2018, you will be asked to pay the article processing charge,
4
unless you request a waiver and this is approved by Royal Society Publishing. You can find out
more about the charges at http://rsos.royalsocietypublishing.org/page/charges. Should you
have any queries, please contact openscience@royalsociety.org.
We look forward to receiving your resubmission.
Kind regards,
Alice Power
Editorial Coordinator
Royal Society Open Science
openscience@royalsociety.org
on behalf of Dr Matjaz Perc (Associate Editor) and Antonia Hamilton (Subject Editor)
openscience@royalsociety.org
Reviewers' Comments to Author:
Reviewer: 1
Comments to the Author(s)
In the paper "Estimating statistical power, posterior probability and publication bias of
psychological research using the observed replication rate", the authors illustrate how Bayes’
theorem can be used to better understand the implications of the 36% reproducibility rate of
published psychological findings that was reported by the Open Science Collaboration.
The manuscript addresses a hot topic not only for the psychology field, but also, via its
implications, for other related disciplines (i.e. the reproducibility of published findings, the
assessment of publication bias, statistical power and the posterior probability of findings).
I find this work to be very interesting and illustrative, as it considers, in a general perspective /
framework, through its implications, but not limited to that, some of the diseases that threaten
nowadays science, such as: a disorderly focus on statistically significant results, the excessive
orientation to novelty etc.
The manuscript is very easy to read, while it clearly indicates how the results support the
outlined claims and conclusions. The manuscript is scientifically sound and successfully
describes the properties of research that lead to 90% positive findings and the expected
reproducibility of research with 90% positive findings.
One my view, I do believe the authors nicely perform a demonstration wherein a method to
assess publication bias is plugged in as to test the consistency of observed reproducibility rate
vis-a-vis unbiased literature.
From another perspective, I think the paper, in case of being published, will target a wide general
audience, as the topic of publication bias is of interest not only for psychologists, but also for
science in general.
I recommended this manuscript for being accepted as it is, by the Royal Society Open Science. In
spite of the fact that, perhaps, authors should have briefly made some remarks to the possible
implications of their results for the general field of psychology - but perhaps I am subjective on
this and it would move the paper a little bit in the area of speculations.
5
Reviewer: 2
Comments to the Author(s)
See attached file.
Author's Response to Decision Letter for (RSOS-172173.R0)
See Appendix B.
label_end_comment
Decision letter (RSOS-181190.R0)
03-Aug-2018
Dear Dr Ingre,
I am pleased to inform you that your manuscript entitled "Estimating statistical power, posterior
probability and publication bias of psychological research using the observed replication rate" is
now accepted for publication in Royal Society Open Science.
You can expect to receive a proof of your article in the near future. Please contact the editorial
office (openscience_proofs@royalsociety.org and openscience@royalsociety.org) to let us know if
you are likely to be away from e-mail contact. Due to rapid publication and an extremely tight
schedule, if comments are not received, your paper may experience a delay in publication.
Royal Society Open Science operates under a continuous publication model
(http://bit.ly/cpFAQ). Your article will be published straight into the next open issue and this
will be the final version of the paper. As such, it can be cited immediately by other researchers.
As the issue version of your paper will be the only version to be published I would advise you to
check your proofs thoroughly as changes cannot be made once the paper is published.
You have the opportunity to archive your accepted, unbranded manuscript, but access to the full
text must be embargoed until publication.
Articles are normally press released. For this to be effective we set an embargo on news coverage
corresponding to the publication date of the article. We request that news media and the authors
do not publish stories ahead of this embargo (when final version of the article is available).
In order to raise the profile of your paper once it is published, we can send through a PDF of your
paper to selected colleagues. If you wish to take advantage of this, please reply to this email with
the name and email addresses of up to 10 people who you feel would wish to read your article.
Please note that Royal Society Open Science will introduce article processing charges for all new
submissions received from 1 January 2018. Charges will also apply to papers transferred to Royal
Society Open Science from other Royal Society Publishing journals, as well as papers submitted
as part of our collaboration with the Royal Society of Chemistry
(http://rsos.royalsocietypublishing.org/chemistry).
6
If your manuscript is newly submitted and subsequently accepted for publication after 1 Jan 2018,
you will be asked to pay the article processing charge, unless you request a waiver and this is
approved by Royal Society Publishing. Manuscripts originally submitted prior to 1 Jan 2018 will
not subject to a charge, even if they are accepted in 2018. You can find out more about the charges
at http://rsos.royalsocietypublishing.org/page/charges. Should you have any queries, please
contact openscience@royalsociety.org.
On behalf of the Editors of Royal Society Open Science, we look forward to your continued
contributions to the Journal.
Kind regards,
Royal Society Open Science Editorial Office
Royal Society Open Science
openscience@royalsociety.org
on behalf of Dr Matjaz Perc (Associate Editor) and Antonia Hamilton (Subject Editor)
openscience@royalsociety.org
Appendix A
Review of “Estimating statistical power, posterior probability
and publication bias of psychological research using the
observed replication rate”
January 12, 2018
1 General comments
This manuscript provides an interesting, informal back-of-an-envelope type analysis to
estimate the prior probability that a hypothesis tested in the psychological literature is
true (i.e., a non-zero effect exists) based on the OSC data. The numbers produced by
theses analysis are consistent with a previous fully Bayesian analysis presented in [16].
The general approach taken in the paper can be summarized as follows. Let Hi de-
note the hypothesis tested in study i, and suppose that the original study i resulted in
a significant finding (i.e., p < 0.05). Then the “prior” probability that Hi represents a
true positive <U+03B8>^ is approximated as the true discovery rate. The true discovery rate is in
turn expressed in terms of type 1 (a) and type 2 (ß) errors and the prior probability that
any hypothesis tested in the psychology literature is true (<U+03B8>). The theoretical, marginal
replication rate R is expressed in terms of the type 1 (ar ) and type 2 error (ßr ) rates in
the replicated studies. The authors then estimate the replication rate R from data and
impose reasonable constraints on the relations between the other variables. They draw
psuedo-inferences regarding the relationships between these variables by varying some of
the parameters across heuristically defined constraints.
A very serious flaw of the manuscript is the claim that a Bayesian analysis of the OSC
data has been conducted and forms the basis for inference. In fact, the manuscript does
not describe any statistical model for the OSC data. Instead, analyses are restricted to
an examination of the ranges over which several parameters of interest might be varied
in such a way as to produce summary statistics that are consistent with the summary
statistics observed for the OSC data. To conduct this analysis, the manuscript (i) imposes
a range of constraints on the relations between some of the parameters of interest, and
(ii) sets other parameters of interest equal to sample estimates. Sampling distributions
are not defined for quantities observed in the OSC studies or for summary statistics, and
prior distributions appearing for parameters in these distributions are not defined. One
1
might argue that Bayesian inference could be based on treating the summary statistics
as the raw data, but the authors do not define sampling distributions for these quantities
either. (Modeling only summary statistics would entail a significant loss of information,
and would also require the specification of prior distributions on the parameters appearing
in the sampling distributions of the summary statistics). Unfortunately, the manuscript
ignores sampling variability altogether.
Aside from the lack of formal statistical analysis of the OSC data, the analyses presented
in the manuscript also ignore two of the most important sources of information from the
OSC experiment. First, by having two independent replications of each study, the OSC
data can be used to obtain estimates of treatment effects for each study. Second, the OSC
data contains data from 100 studies, making it is possible to estimate the distribution
of effect sizes across studies. These features of the studies mean, for example, that the
posterior probability that Hi is true does not need to be treated as if it was the same
across all studies, but can instead by adapted to account for the particular data collected
for each study.
Despite its rather serious shortcomings, the fact that the analyses in the manuscript
produce “reasonable” estimates of important parameters of interest that are generally
consistent with more formal (but much more complicated) Bayesian analyses and other
historical estimates might warrant publication of a heavily revised manuscript. In my
view, any such revision should be very explicit in emphasizing the informal nature of the
calculations performed, and to the extent possible should highlight the connection between
the estimates so obtained and estimates reported previously in the literature.
2 Specific comments
1. page 5, line 10. The expectation notation used in equations (1-3) is non-standard.
Expectations refer to the expectation of a random variable. For example, if X is a
random variable that represents a finding of positive evidence for a trial (1 if yes, 0
if no), then equation (1) might be written
Etrue X = <U+03B8>(1 - ß) or E(X&true) = <U+03B8>(1 - ß).
The use of the expectation symbol without an argument is somewhat nonsensical.
2. page 5, line 10-13. The authors should emphasize that these are marginal expecta-
tions.
3. page 5, line 16-32. This passage is critical to later development despite its rather
innocuous wording. The authors correctly point out that there are many ways to
achieve an average of 90% positive findings, but then arbitrarily settle on an assump-
tion of 0.95 true hypotheses with 95% statistical power.
2
4. page 5, line 45. Contrary to the authors’ claim, equations 1-3 do not provide the
information needed to compute the “posterior probability.” While it is true that the
expected value of an indicator variable is the probability that the indicator equals 1
(see comment 1), it is not true that
 
X E(X)
E = ,
Y E(Y )
as the authors imply. Also, referring to the posterior probability that a hypothesis
that produced a significant finding in an original study as simply the “posterior
probability” is extremely confusing. In its normal usage, “posterior probability” is
a probability assigned to a specified, arbitrary event. Also, using <U+03B8>^ for the posterior
probability is confusing if you mean that it is a population parameter. The hat
suggests that you are estimating it from data, which implies it cannot be expected
to exactly equal any function of population parameters (i.e, there shouldn’t be a =
sign in this equation).
At this point the manuscript is defining a “posterior probability.” How can this be
done? No sampling distributions have been defined, and no prior densities have been
defined. Where does the posterior probability come from? Equation 4 is not Bayes
theorem and does not follow from Bayes theorem. Bayes theorem states
p(x|<U+03B8>)p(<U+03B8>)
p(<U+03B8>|x) =
p(x)
where <U+03B8> is an unknown parameter, x represents data, and p(·) represents a generic
density function. The article never describes sampling densities, likelihood functions,
prior distributions, or posterior distributions. Put simply, there is nothing Bayesian
about this estimator. It might be a useful back-of-the-envelope point estimator, but it
does not yield a Bayesian posterior distribution and cannot be used to assess posterior
uncertainties.
It is also an error to assume that there is a single posterior probability that a positive
finding is true. The posterior probability that a positive finding is true depends on
the specific outcome of an experiment. For example, studies with very small p values
are more likely to represent positive findings than are studies with p-values close to
0.05, and studies that replicate are more likely to represent positive findings than are
studies that don’t replicate.
Equation 4 also does not reflect that fact that quantities estimated from data are
random. To the extent that equation 4 can be considered an approximation to
the probability that an originally significant hypothesis is true, the absence of an
error term or distributional assumptions regarding quantities in this equation means
both that equality will in general not hold and that the authors’ framework cannot
3
be used for inference (i.e. uncertainty quantification). Evaluating the accuracy of
the approximation from a Bayesian perspective (to get a Bayesian posterior) would
require the specification of a full probability model for the data.
As stated above, the authors might propose equation 4 as a back-of-an-envelope type
estimator of the marginal posterior probability that a hypothesis that generated a
significant result in an original experiment is true, but they should then make it clear
what quantities are estimated and what quantities are assumed known. Again, a full
probability model specification would be needed in order to evaluate the posterior
distribution of the estimate.
5. page 6, line 11, equation (5) and surrounding text. The authors state an equality
that states either that the true or observed replication rate R (again not clear from
context which) equals a function of an estimated posterior probability and type 1
and type 2 error rates. The observed replication rate and the estimated posterior
probability are subject to sampling error, so there can only be a probabilistic relation
between these variables. Again, evaluating the posterior uncertainty in this relation
requires specification of a probability model for individual data values. To see the
importance of distinguishing between known population parameters and estimates
of these parameters, note that a 95% frequentist confidence interval for R based on
the OSC data and a simple binomial model extends from (0.266,0.462); treating this
value as a known parameter and equal to 36% ignores substantial uncertainty.
6. page 6, line 18. The selection of a “middle ground” on the prior probability that a
tested hypothesis is true and statistical power is completely arbitrary, as the authors
have pointed out on page 5, lines 17-28. There are an infinite number of choices for
these parameters that lead to 90% positive findings. There is no clear rationale for
why their choice is “middle ground”.
7. page 6, line 26. There’s no question that the literature is biased. But the author’s
argument to “show” that it’s biased depends on an arbitrary choice of a “repro-
ducibility rate.” The p-value (< 10-15 ) cited could change dramatically if different
“reproducibility rates” were assumed.
8. page 7, line 15 - 19. Until this point, the manuscript has attempted (unsuccessfully)
to pose the discussion in terms of Bayesian posterior probabilities. In equation (6) it
switches direction and now chooses to estimate unknown quantities by specifying a
linear system of equations. The system has 5 unknown parameters, (<U+03B8>, ß0 , <U+03B8>^, R, ßr ).
To get bounds on any two of the variables, one has to estimate or constrain three of
the other variables. R is handled by setting it to its observed value in the OSC data,
i.e., R = 0.36. As noted above, there is significant variation in this value and simply
inserting a point estimate ignores this uncertainty.
4
<U+03B8>^ can be eliminated by substituting the right-hand side (RHS) of the first equation
for <U+03B8>^ in the RHS of the second equation.
Reasonable but completely heuristic arguments are used to constrain ßr to be in the
range (ß0 + 0.6, ß0 + 0.10). That leaves just <U+03B8> and ß0 , which can be examined as a
function of the other parameters. Again, it is important to note (i) that the random
variation is ignored in the heuristic arguments used to derive these estimates and
the (ii) ranges for parameter values indicated in the figures are highly dependent
on these somewhat arbitrary choices. Most importantly, some readers may interpret
the parameter ranges cited in the text and figures to represent some type of formally
derived probabilistic bounds (e.g., posterior credible intervals or confidence intervals).
They don’t, and this should be made very clear in the text and figures.
9. page 10, line 6. The authors again claim to have used Bayes theorem as the basis
for their studies. This is simply not true. The authors fail to define sampling dis-
tributions for any of the data or summary statistics observed in the OSC study; nor
do they specify prior distributions on the parameters that define these distributions.
Bayes theorem is never invoked. As a consequence, it is impossible to obtain any
summary of the uncertainties in the parameters that are estimated. The “plausible”
ranges for parameters are based on heuristic arguments regarding ranges of param-
eters that have not been formally incorporated into either a Bayesian or frequentist
analysis.
10. page 10, line 33. The analysis in [16] used a0 = 0.052 because one of the significant
tests in the OSC study was only significant with at this level. The numbers in the
current manuscript are based on the same assumption. [16] did not assume 75%
power; this was the marginal posterior mean of the power estimated from the data
and probability model. It was obtained from the posterior distribution on the effect
sizes.
11. page 10, line 45. The analysis in [16] formally used the sample sizes specific to each
experiment (original and replication) to determine the power of each experiment.
Such information was only used heuristically in the current manuscript, and all studies
were assumed to have the same power parameters. This passage suggests the reverse.
12. page 11, line 22. The manuscript states “for completeness we presented results for
the full range of priors....”. Again, it must be emphasized that the authors have not
performed a Bayesian analysis. This statement is particularly alarming because it
seems to suggest that the authors think they are applying (uniform?) prior distribu-
tions on parameters simply by assessing changes in a system of equations that occur
when these parameters are varied over a range of values. This is, of course, not the
case.
5
Appendix B
Dear Editor,
Attached, please find our revised manuscript.
Reviewer #1 recommended our manuscript to be published as is. Reviewer #2 also
ecommended publication, but was more critical and expressed a major concern: <U+200B>A very
erious flaw of the manuscript is the claim that a Bayesian analysis of the OSC data has
een conducted and forms the basis for inference<U+200B>.
We want to point out that there are different ways to apply Bayes’ theorem, depending on
he specific problem at hand. As the reviewer notes, we do not estimate parameters from
aw data; instead, we present a mathematical analysis where we use certain reasonable
ssumptions based on previously reported summary estimates. This approach has
imitations, which are discussed in the paper, but also important advantages; for example, it
llows us to illustrate relations between important meta-properties of research, in a way that
we hope will be accessible to a broad audience.
<U+200B>
n our analysis, we used an approach similar to Puga et al.<U+200B> [1]<U+200B> , and in the revised manuscript
we show more clearly how the equations we present were derived from Bayes’ theorem.
However, we agree with the reviewer that in our ambition to present simple mathematics, we
versimplified the problem by ignoring variance in statistical power. In the revised
manuscript we integrate over a distribution of statistical power and have updated our findings
ccordingly. We have also adjusted our language throughout the text and have clarified the
omewhat informal nature of our approach.
Assuming variance in statistical power increased expected power and reduced the posterior
robability of the original findings somewhat, compared to naive estimates; but our main
esults on publication bias were only marginally affected. The odds of suppressing negative
indings were previously estimated in the range 60—100 and are now estimated in the range
5—98. As we discuss, this is explained by the fact that observing a high proportion of
ositive evidence depends on both high statistical power and a large prior, which in turn
mplies a very high reproducibility, far from the 36% reported by the OSC. We also
nvestigated implications of uncertainty in the reproducibility estimate, showing that expected
ower and posterior depend on the assumed true reproducibility rate; but once again, that
stimates of publication bias were only marginally affected.
Below, please find our response to specific concerns raised by reviewer #2.
Sincerely,
Michael Ingre
References
. <U+200B>Puga JL, Krzywinski M, Altman N. 2015 Bayes’ theorem. <U+200B>Nature Methods<U+200B> <U+200B>12<U+200B>, 277.
esponses to specific comments by reviewer #2 below:
1. We agree with the reviewer that it was a mistake to use capital E to signify the
probability of positive evidence. It has been changed to capital P.
2. From a frequentist perspective, all probabilities are marginal expectations. However,
these equations do not exclude other types of probabilities, such as “subjective
beliefs” that the hypothesis is true, based on expert knowledge. In such a paradigm,
the prior probability is updated in subsequent studies from the posterior of a previous
observation. Thus, when applied to a single hypothesis, that may or may not be
tested a large number of times in subsequent studies, we cannot say that the prior (or
the posterior) is a marginal expectation, but rather, that it reflects our current state of
knowledge.
3. We have adjusted our approach to pick the most conservative solution instead.
4. We have addressed these concerns in multiple ways:
a. We show more clearly how our equations are derived from Bayes’ theorem
(equations 4 & 5).
b. We stress that it assumes binary summary data from NHST.
c. We point out the limitations of assuming zero variance in statistical power
(equations 1-7).
d. We integrate over a distribution of statistical power to produce the expected
posterior probability (equation 8 & 9).
e. We use the hat on theta ( ) to indicate that the posterior probability of the
hypothesis is estimated from data. In this equation the data is binary, and
describe a (single) positive result from NHST, but the equation can easily be
expanded to also include negative results from NHST. While using binary
data from NHST may not be optimal in many situations, it is arguably one of
the most important quantities for studying publication bias, and it is generally
used in the literature as a basis for inference.
5. We have clarified the text to show that this assumes all quantities to be known.
6. We have adjusted our approach to the most conservative assumption.
7. We use the most conservative assumption instead, which still produces <U+200B>p<U+200B><10<U+200B>-15<U+200B>.
8. We have changed the way we present and discuss these results and also report the
results for other assumed true reproducibility rates in figure 3. We have clarified that
we present expected estimates and discuss the limitations of this approach.
9. We have clarified how we used Bayes theorem in our calculations, and clearly state
the limitations of our approach in the beginning of the discussion.
10. We have adjusted the text so it is more representative of the work by Johnson et al.
11. This text has been rewritten.
12. This should be clearer in the revised manuscript.
Society Open
