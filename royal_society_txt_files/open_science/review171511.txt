Modelling science trustworthiness under publish or perish
pressure
David Robert Grimes, Chris T. Bauch and John P. A. Ioannidis
Article citation details
R. Soc. open sci. 4: 171511.
http://dx.doi.org/10.1098/rsos.171511
Review timeline
Original submission: 29 September 2017 Note: Reports are unedited and appear as
Revised submission: 27 November 2017 submitted by the referee. The review history
Final acceptance: 1 December 2017 appears in chronological order.
Review History
label_version_1
RSOS-171511.R0 (Original submission)
label_author_1
Review form: Reviewer 1
Is the manuscript scientifically sound in its present form?
Yes
Are the interpretations and conclusions justified by the results?
No
Is the language acceptable?
No
Is it clear how to access all supporting data?
No
Do you have any ethical concerns with this paper?
No
Have you any concerns about statistical analyses in this paper?
No
© 2017 The Authors. Published by the Royal Society under the terms of the Creative Commons
Attribution License http://creativecommons.org/licenses/by/4.0/, which permits unrestricted use,
provided the original author and source are credited
2
Recommendation?
label_recommendation_1
Major revision is needed (please make suggestions in comments)
Comments to the Author(s)
label_comment_1
The manuscript “Modelling science trustworthiness under publish or perish pressure” by Grimes
et al. develops a stylized model of science in which there are 3 types of scientists (diligent,
careless, and fraudulent) who compete for funding which is awarded proportional to their prior
productivity.
The governing equations proposed here are deterministic, and so the results are somewhat
tautological in that they arise merely from the construction of the model. For example, it is
written on page 6 that when there is no funding penalization due to fraud detection (\eta=0) that
the results of the model are independent of J - the publication capacity of the top-tier journals -
but this means that the entire paper boils down to a single subtractive term in Eq. 1.10.
I do understand that the model has been supplied with a few parameters derived from empirical
investigation, but there must be more effort to match the dynamics/predictions of the model with
real data - i.e. there must be more effort in demonstrating that this is a realistic model that
captures the essential aspects of this particular problem. One blatant shortcoming is the fact that
scientists, and thus their outputs, are categorized by just 3 absolute types - but most research
teams/outputs involve a mixture of these types - i.e. scientific fraud could arise from just one
fraudster in a team of diligents.
As such, the tone of the manuscript is much too definitive - for a purely theoretical model that
relies on many generalizing assumptions, too many statements could be taken out of context and
converted into bad press concerning the state of fraud in science, etc.
The model also completely glosses over the fact that the reward system in science is oriented
around scientific impact (e.g. citations, intellectual property, prestigious awards, PhDs advised,
etc) rather than the number of publications produced. The model somewhat accounts for this by
substituting publications in “top journals” (measured as a proportion of the total J) as the means
for connecting prior performance to future funding.
In the discussion section, the authors use the example of emerging open access journals as an
example to support their observation that “[the] best outcome was obtained by simply paying no
heed to whether a result was significant or not” - this is confounding in 2 ways. First, the model is
built around high-impact journals, for which the carrying capacity is not growing significantly,
which is certainly not the case for PLOS One. Second, the authors are confounding the idea of
“trustworthiness of published science” with “paying no heed to whether a result was significant
or not” - while this may be the model of PLOS One and Scientific Reports, this is not the model
nor will it ever be of top-tier journals whose entire market is built around prestige. In other
words, model doesn’t account for this important difference - targeting top-tier vs rapid open
access - and thus, it is outside the scope of the model to make such statements or extensions in
this regard.
And finally, statements such as “The analysis here suggests that science trustworthiness is
affected too by changes in funding resources, and that when an increase of funding improves the
over-all trustworthiness of science, as depicted in figure 2. “ should be more carefully phrased
because as is, this sounds like more funding means more trustworthiness but there is no
mechanism in the model by which fraudsters convert to diligent scientists if relatively speaking
there is higher R(t). Empirically, the authors could test whether fraud rates decreased after the
budget doubling of the NIH from 1998-2003.
3
There are other minor details that should also be addressed:
- The discussion of the assumptions and the sources of error should take place in the paper, not in
the supplement 1.
- In many of the figure panels the legend covers up a significant portion of the data being
reported
- There are several font conversion typos, e.g. Ref 41, 47, and page 6 “deficienciesain10âA <U+0306>S ¸20%”
- Many small typos: Page 2 weighed>weighted; Page 4 help>held and \beta_{UD} > \beta{D};
Page 6 J_F = J/100 > J_F = 1/100
label_author_2
Review form: Reviewer 2 (Paul Smaldino)
Is the manuscript scientifically sound in its present form?
Yes
Are the interpretations and conclusions justified by the results?
Yes
Is the language acceptable?
Yes
Is it clear how to access all supporting data?
Not Applicable
Do you have any ethical concerns with this paper?
No
Have you any concerns about statistical analyses in this paper?
No
Recommendation?
label_recommendation_2
Accept with minor revision (please list in comments)
Comments to the Author(s)
label_comment_2
This paper analyzes a mathematical model of scientific “trustworthiness” (defined as the
proportion of published research that is true) and considers how incentives to publish as well as
some other sociological and institutional factors can contribute to trustworthiness in a dynamic
manner. The model is well motivated and well constructed, and analyzed in a sufficiently
thorough manner. I think this model also provides a nice framework for future extensions such as
those the authors themselves discuss. Some of their findings are redundant with other modeling
work (such as Ioannidis 2005 and the cited papers by Smaldino and McElreath), but if anything
this simply provides additional support for those findings with a new set of modeling
assumptions. I do worry that their most novel result, that decreasing available funding has
negative consequences for resulting trustworthiness, is actually an artifact of the model’s initial
conditions, though I could be mistaken (see below). Even if that is true, it does not damage the
model’s basic structure, and I think this is a useful paper on an important and timely problem,
with a simple and elegant model. I have a few concerns about the paper, but I think the authors
should be able to address them in a minor revision.
SUBSTANTIAL COMMENTS____________________________
It would be useful to define “trustworthiness” when the term is introduced. I presume the
4
authors mean “correct,” in that the claim in the paper is well supported by the evidence rallied by
the paper. Other definitions are possible, such as when results represent a valid attempt to
minimize error (and therefore are not fraudulent. An accidental false positive is impossible in the
former definition but not in the latter. Eq. 1.13 makes it clear that it is the former, but this isn’t
made clear until p. 7.
p. 3, lines 40-42: “Even when research groups are very diligent, they may reasonably submit a
fraction e of their false positives.”
It’s unclear how the researchers would know which of their false positives are actually false. If
they know it’s false, then it’s not a false positive, but a true negative. If they don’t know it’s false,
then they should submit 100% of these. It seems like the parameters were chosen for some
mathematical convenience, but I think this warrants more justification.
p. 4, lines 18-20: “The respective rates of submission...”
The assumption here is that output is a linear function of the funding received. The extent to
which this holds will vary, but will probably never be totally accurate. For example, wet-lab
science requires continuous funding, but there will likely be diminishing marginal returns (but
also possible punctuated leaps as new paradigms become possible). Meanwhile, fields like
computational neuroscience may require little funding beyond ensuring access to computing
resources. The authors should address the appropriateness of their assumption here.
p. 5, Eqs 1.8 & 1.9
In my reading, G isn’t really about how funding changes over time. Rather, it reflects to which
funds are simply allocated by current frequency of strategy. In other words, G can be seen as the
inverse of the strength of selection on the various phenotypes. This characterization might be
noted, as well as acknowledging other ways in which funding might change over time, at least
verbally.
p. 5, lines 52-57: “We might envision a situation where scientific works are audited for
reproducibility...”
It’s unclear to me how this could be assessed. How would an auditor be able to tell hwo
reproducible a lab’s work is? Also, this *in theory* (not in practice) could only be achieved by the
diligent cohort, but it may be possible to give the appearance of diligence without actually being
diligent.
p. 9, lines 43-44: “reducing funding increases the publication pressure and results in increases
selection of suspect works and a fall in scientific reproducibility.”
I worry that this result may occur only be because you assume that initially the majority of
researchers are diligent. If diligent researchers start out as a minority, my reading of the model
dynamics suggests that extra funding would make it harder for them to increase in number. If
this is correct, it weakens one of the central conclusions of the paper, especially since the goal is
often to *increase* the number of diligent researchers.
p. 12, line 54: “For B-independence...”
Two questions. First, why not just say B = 0.5, rather than B-independence? Second, it’s unclear
why researchers would update their submission of null results so cruelly, submitting only half
when 35% of articles published are null but all when 50% of articles published are null.
p. 15, lines 11-12: “It would be useful to know precisely how much is never submitted, and to
gauge the extent of the file-drawer problem.”
Some good estimations of this have been published. See, for example:
Franco et al. (2014) Publication bias in the social sciences: Unlocking the file drawer. Science 345:
1502-1505.
And other papers by the same authors.
p. 15, lines 57-58: “We need a better understanding of the factors driving publication and
5
productivity-related behaviors.”
I’m not convinced that this is really what we need. It seems like we have a pretty good
understanding of this. Also, this model takes it as given. It seems to me that we need ways of
changing incentives and evaluation.
MINOR COMMENTS____________________________
P. 2, line 51: It might be useful to define “questionable research practices,” since it will affect
readers’ interpretation of the fact that 1/3 of all researchers exhibit these. A couple of examples
might suffice.
p. 3, line 55: typo: should be “in that THEY may”
p. 4, line 45: “These probabilities are given by”
Should note that the assumption here must be that the number of papers of each type that
journals will publish must be greater than the total number of submissions. Which is reasonable,
but should be made explicit.
p. 4, line 60: “The average rate of publications per unit of funding per unit time is thus”
It is important to note that is about “top-tier” journals only. What these are is debatable
depending on field. An argument that is regularly made is to put less weight on publishing
venue.
p. 7, line 46: Typo, should be “10-20%”
p. 7, lines 41-42: “Simulation results suggests that the trustworthiness of published science in any
given field is strongly dependent on the false positive rate in that field under a publish or perish
paradigm.”
OK. But, it is also clear from Figure 1 that the number of diligent researchers is still declining and
the number of trustworthy results is declining even when FPR is low with publish-or-perish
incentives, as suggested by Smaldino & McElreath 2016.
Results section more generally: Most of the subsections starting on p. 10 talk about an analysis
focusing on a key parameter. Because there are a lot of parameters in the model, a line to remind
the reader as to the interpretation of the parameter (e.g., \eta on p. 10) would be very helpful.
p. 14, line 51: typo, should be *figure* 5(a).
p. 15, lines 56-57: “Publishing is not intrinsically flawed, and conversely complete, unbiased
publication is essential for scientific progress.”
Awkward phrasing, and also not converse. The logical converse would be something like “Not
everything intrinsically flawed is publishing.
P. 15, lines 33-35: “while publications are indeed one measure of productivity, they are not
necessarily the sole measure. While a much harder aspect to gauge, trustworthiness is more
fundamentally important.”
Even more than this, the model in this paper considered only papers in “top journals.” Some have
suggested that these journals are in fact more likely than most to publish false positives (e.g.
http://bjoern.brembs.net/2016/01/even-without-retractions-top-journals-publish-the-least-
reliable-science/)
p. 16, Section 4: Conclusion.
That should probably go.
Missing journal info for citation Ref 32.
6
-Paul Smaldino, UC Merced
label_author_3
Review form: Reviewer 3 (Horvatic Davor)
Is the manuscript scientifically sound in its present form?
Yes
Are the interpretations and conclusions justified by the results?
Yes
Is the language acceptable?
Yes
Is it clear how to access all supporting data?
Yes
Do you have any ethical concerns with this paper?
No
Have you any concerns about statistical analyses in this paper?
No
Recommendation?
label_recommendation_3
Accept with minor revision (please list in comments)
Comments to the Author(s)
label_comment_3
In this well written and presented paper authors model science trustworthiness under publish or
perish pressure. Research is more than timely and I can only recommend this paper for
acceptance. The only reason I recommended that this paper should be accepted with minor
correction is due to the section 4. Conclusion that states: The conclusion text goes here. I view this
as a typo since section 3. nicely wraps up the paper. As far as I'm concerned there is no need for
additional iteration, as removing this section is a technical issue.
label_end_comment
Decision letter (RSOS-171511)
02-Nov-2017
Dear Dr Grimes,
The editors assigned to your paper ("Modelling science trustworthiness under publish or perish
pressure") have now received comments from reviewers. We would like you to revise your
paper in accordance with the referee and Associate Editor suggestions which can be found below
(not including confidential reports to the Editor). Please note this decision does not guarantee
eventual acceptance.
Please submit a copy of your revised paper within three weeks (i.e. by the 25-Nov-2017). If we do
not hear from you within this time then it will be assumed that the paper has been withdrawn. In
7
exceptional circumstances, extensions may be possible if agreed with the Editorial Office in
advance.We do not allow multiple rounds of revision so we urge you to make every effort to
fully address all of the comments at this stage. If deemed necessary by the Editors, your
manuscript will be sent back to one or more of the original reviewers for assessment. If the
original reviewers are not available we may invite new reviewers.
To revise your manuscript, log into http://mc.manuscriptcentral.com/rsos and enter your
Author Centre, where you will find your manuscript title listed under "Manuscripts with
Decisions." Under "Actions," click on "Create a Revision." Your manuscript number has been
appended to denote a revision. Revise your manuscript and upload a new version through your
Author Centre.
When submitting your revised manuscript, you must respond to the comments made by the
referees and upload a file "Response to Referees" in "Section 6 - File Upload". Please use this to
document how you have responded to the comments, and the adjustments you have made. In
order to expedite the processing of the revised manuscript, please be as specific as possible in
your response.
In addition to addressing all of the reviewers' and editor's comments please also ensure that your
revised manuscript contains the following sections as appropriate before the reference list:
• Ethics statement (if applicable)
If your study uses humans or animals please include details of the ethical approval received,
including the name of the committee that granted approval. For human studies please also detail
whether informed consent was obtained. For field studies on animals please include details of all
permissions, licences and/or approvals granted to carry out the fieldwork.
• Data accessibility
It is a condition of publication that all supporting data are made available either as
supplementary information or preferably in a suitable permanent repository. The data
accessibility section should state where the article's supporting data can be accessed. This section
should also include details, where possible of where to access other relevant research materials
such as statistical tools, protocols, software etc can be accessed. If the data have been deposited in
an external repository this section should list the database, accession number and link to the DOI
for all data from the article that have been made publicly available. Data sets that have been
deposited in an external repository and have a DOI should also be appropriately cited in the
manuscript and included in the reference list.
If you wish to submit your supporting data or code to Dryad (http://datadryad.org/), or modify
your current submission to dryad, please use the following link:
http://datadryad.org/submit?journalID=RSOS&manu=RSOS-171511
• Competing interests
Please declare any financial or non-financial competing interests, or state that you have no
competing interests.
• Authors’ contributions
All submissions, other than those with a single author, must include an Authors’ Contributions
section which individually lists the specific contribution of each author. The list of Authors
should meet all of the following criteria; 1) substantial contributions to conception and design, or
acquisition of data, or analysis and interpretation of data; 2) drafting the article or revising it
critically for important intellectual content; and 3) final approval of the version to be published.
All contributors who do not meet all of these criteria should be included in the
acknowledgements.
8
We suggest the following format:
AB carried out the molecular lab work, participated in data analysis, carried out sequence
alignments, participated in the design of the study and drafted the manuscript; CD carried out
the statistical analyses; EF collected field data; GH conceived of the study, designed the study,
coordinated the study and helped draft the manuscript. All authors gave final approval for
publication.
• Acknowledgements
Please acknowledge anyone who contributed to the study but did not meet the authorship
criteria.
• Funding statement
Please list the source of funding for each author.
Once again, thank you for submitting your manuscript to Royal Society Open Science and I look
forward to receiving your revision. If you have any questions at all, please do not hesitate to get
in touch.
Yours sincerely,
Alice Power
Editorial Coordinator
Royal Society Open Science
on behalf of Mark Chaplain
Subject Editor, Royal Society Open Science
openscience@royalsociety.org
Comments to Author:
Reviewers' Comments to Author:
Reviewer: 1
Comments to the Author(s)
The manuscript “Modelling science trustworthiness under publish or perish pressure” by Grimes
et al. develops a stylized model of science in which there are 3 types of scientists (diligent,
careless, and fraudulent) who compete for funding which is awarded proportional to their prior
productivity.
The governing equations proposed here are deterministic, and so the results are somewhat
tautological in that they arise merely from the construction of the model. For example, it is
written on page 6 that when there is no funding penalization due to fraud detection (\eta=0) that
the results of the model are independent of J - the publication capacity of the top-tier journals -
but this means that the entire paper boils down to a single subtractive term in Eq. 1.10.
I do understand that the model has been supplied with a few parameters derived from empirical
investigation, but there must be more effort to match the dynamics/predictions of the model with
real data - i.e. there must be more effort in demonstrating that this is a realistic model that
captures the essential aspects of this particular problem. One blatant shortcoming is the fact that
scientists, and thus their outputs, are categorized by just 3 absolute types - but most research
teams/outputs involve a mixture of these types - i.e. scientific fraud could arise from just one
fraudster in a team of diligents.
As such, the tone of the manuscript is much too definitive - for a purely theoretical model that
9
relies on many generalizing assumptions, too many statements could be taken out of context and
converted into bad press concerning the state of fraud in science, etc.
The model also completely glosses over the fact that the reward system in science is oriented
around scientific impact (e.g. citations, intellectual property, prestigious awards, PhDs advised,
etc) rather than the number of publications produced. The model somewhat accounts for this by
substituting publications in “top journals” (measured as a proportion of the total J) as the means
for connecting prior performance to future funding.
In the discussion section, the authors use the example of emerging open access journals as an
example to support their observation that “[the] best outcome was obtained by simply paying no
heed to whether a result was significant or not” - this is confounding in 2 ways. First, the model is
built around high-impact journals, for which the carrying capacity is not growing significantly,
which is certainly not the case for PLOS One. Second, the authors are confounding the idea of
“trustworthiness of published science” with “paying no heed to whether a result was significant
or not” - while this may be the model of PLOS One and Scientific Reports, this is not the model
nor will it ever be of top-tier journals whose entire market is built around prestige. In other
words, model doesn’t account for this important difference - targeting top-tier vs rapid open
access - and thus, it is outside the scope of the model to make such statements or extensions in
this regard.
And finally, statements such as “The analysis here suggests that science trustworthiness is
affected too by changes in funding resources, and that when an increase of funding improves the
over-all trustworthiness of science, as depicted in figure 2. “ should be more carefully phrased
because as is, this sounds like more funding means more trustworthiness but there is no
mechanism in the model by which fraudsters convert to diligent scientists if relatively speaking
there is higher R(t). Empirically, the authors could test whether fraud rates decreased after the
budget doubling of the NIH from 1998-2003.
There are other minor details that should also be addressed:
- The discussion of the assumptions and the sources of error should take place in the paper, not in
the supplement 1.
- In many of the figure panels the legend covers up a significant portion of the data being
reported
- There are several font conversion typos, e.g. Ref 41, 47, and page 6 “deficienciesain10âA <U+0306>S ¸20%”
- Many small typos: Page 2 weighed>weighted; Page 4 help>held and \beta_{UD} > \beta{D};
Page 6 J_F = J/100 > J_F = 1/100
Reviewer: 2
Comments to the Author(s)
This paper analyzes a mathematical model of scientific “trustworthiness” (defined as the
proportion of published research that is true) and considers how incentives to publish as well as
some other sociological and institutional factors can contribute to trustworthiness in a dynamic
manner. The model is well motivated and well constructed, and analyzed in a sufficiently
thorough manner. I think this model also provides a nice framework for future extensions such as
those the authors themselves discuss. Some of their findings are redundant with other modeling
work (such as Ioannidis 2005 and the cited papers by Smaldino and McElreath), but if anything
this simply provides additional support for those findings with a new set of modeling
assumptions. I do worry that their most novel result, that decreasing available funding has
negative consequences for resulting trustworthiness, is actually an artifact of the model’s initial
conditions, though I could be mistaken (see below). Even if that is true, it does not damage the
10
model’s basic structure, and I think this is a useful paper on an important and timely problem,
with a simple and elegant model. I have a few concerns about the paper, but I think the authors
should be able to address them in a minor revision.
SUBSTANTIAL COMMENTS____________________________
It would be useful to define “trustworthiness” when the term is introduced. I presume the
authors mean “correct,” in that the claim in the paper is well supported by the evidence rallied by
the paper. Other definitions are possible, such as when results represent a valid attempt to
minimize error (and therefore are not fraudulent. An accidental false positive is impossible in the
former definition but not in the latter. Eq. 1.13 makes it clear that it is the former, but this isn’t
made clear until p. 7.
p. 3, lines 40-42: “Even when research groups are very diligent, they may reasonably submit a
fraction e of their false positives.”
It’s unclear how the researchers would know which of their false positives are actually false. If
they know it’s false, then it’s not a false positive, but a true negative. If they don’t know it’s false,
then they should submit 100% of these. It seems like the parameters were chosen for some
mathematical convenience, but I think this warrants more justification.
p. 4, lines 18-20: “The respective rates of submission...”
The assumption here is that output is a linear function of the funding received. The extent to
which this holds will vary, but will probably never be totally accurate. For example, wet-lab
science requires continuous funding, but there will likely be diminishing marginal returns (but
also possible punctuated leaps as new paradigms become possible). Meanwhile, fields like
computational neuroscience may require little funding beyond ensuring access to computing
resources. The authors should address the appropriateness of their assumption here.
p. 5, Eqs 1.8 & 1.9
In my reading, G isn’t really about how funding changes over time. Rather, it reflects to which
funds are simply allocated by current frequency of strategy. In other words, G can be seen as the
inverse of the strength of selection on the various phenotypes. This characterization might be
noted, as well as acknowledging other ways in which funding might change over time, at least
verbally.
p. 5, lines 52-57: “We might envision a situation where scientific works are audited for
reproducibility...”
It’s unclear to me how this could be assessed. How would an auditor be able to tell hwo
reproducible a lab’s work is? Also, this *in theory* (not in practice) could only be achieved by the
diligent cohort, but it may be possible to give the appearance of diligence without actually being
diligent.
p. 9, lines 43-44: “reducing funding increases the publication pressure and results in increases
selection of suspect works and a fall in scientific reproducibility.”
I worry that this result may occur only be because you assume that initially the majority of
researchers are diligent. If diligent researchers start out as a minority, my reading of the model
dynamics suggests that extra funding would make it harder for them to increase in number. If
this is correct, it weakens one of the central conclusions of the paper, especially since the goal is
often to *increase* the number of diligent researchers.
p. 12, line 54: “For B-independence...”
Two questions. First, why not just say B = 0.5, rather than B-independence? Second, it’s unclear
why researchers would update their submission of null results so cruelly, submitting only half
when 35% of articles published are null but all when 50% of articles published are null.
11
p. 15, lines 11-12: “It would be useful to know precisely how much is never submitted, and to
gauge the extent of the file-drawer problem.”
Some good estimations of this have been published. See, for example:
Franco et al. (2014) Publication bias in the social sciences: Unlocking the file drawer. Science 345:
1502-1505.
And other papers by the same authors.
p. 15, lines 57-58: “We need a better understanding of the factors driving publication and
productivity-related behaviors.”
I’m not convinced that this is really what we need. It seems like we have a pretty good
understanding of this. Also, this model takes it as given. It seems to me that we need ways of
changing incentives and evaluation.
MINOR COMMENTS____________________________
P. 2, line 51: It might be useful to define “questionable research practices,” since it will affect
readers’ interpretation of the fact that 1/3 of all researchers exhibit these. A couple of examples
might suffice.
p. 3, line 55: typo: should be “in that THEY may”
p. 4, line 45: “These probabilities are given by”
Should note that the assumption here must be that the number of papers of each type that
journals will publish must be greater than the total number of submissions. Which is reasonable,
but should be made explicit.
p. 4, line 60: “The average rate of publications per unit of funding per unit time is thus”
It is important to note that is about “top-tier” journals only. What these are is debatable
depending on field. An argument that is regularly made is to put less weight on publishing
venue.
p. 7, line 46: Typo, should be “10-20%”
p. 7, lines 41-42: “Simulation results suggests that the trustworthiness of published science in any
given field is strongly dependent on the false positive rate in that field under a publish or perish
paradigm.”
OK. But, it is also clear from Figure 1 that the number of diligent researchers is still declining and
the number of trustworthy results is declining even when FPR is low with publish-or-perish
incentives, as suggested by Smaldino & McElreath 2016.
Results section more generally: Most of the subsections starting on p. 10 talk about an analysis
focusing on a key parameter. Because there are a lot of parameters in the model, a line to remind
the reader as to the interpretation of the parameter (e.g., \eta on p. 10) would be very helpful.
p. 14, line 51: typo, should be *figure* 5(a).
p. 15, lines 56-57: “Publishing is not intrinsically flawed, and conversely complete, unbiased
publication is essential for scientific progress.”
Awkward phrasing, and also not converse. The logical converse would be something like “Not
everything intrinsically flawed is publishing.
P. 15, lines 33-35: “while publications are indeed one measure of productivity, they are not
necessarily the sole measure. While a much harder aspect to gauge, trustworthiness is more
fundamentally important.”
12
Even more than this, the model in this paper considered only papers in “top journals.” Some have
suggested that these journals are in fact more likely than most to publish false positives (e.g.
http://bjoern.brembs.net/2016/01/even-without-retractions-top-journals-publish-the-least-
reliable-science/)
p. 16, Section 4: Conclusion.
That should probably go.
Missing journal info for citation Ref 32.
-Paul Smaldino, UC Merced
Reviewer: 3
Comments to the Author(s)
In this well written and presented paper authors model science trustworthiness under publish or
perish pressure. Research is more than timely and I can only recommend this paper for
acceptance. The only reason I recommended that this paper should be accepted with minor
correction is due to the section 4. Conclusion that states: The conclusion text goes here. I view this
as a typo since section 3. nicely wraps up the paper. As far as I'm concerned there is no need for
additional iteration, as removing this section is a technical issue.
Author's Response to Decision Letter for (RSOS-171511)
See Appendix A.
label_end_comment
Decision letter (RSOS-171511.R1)
01-Dec-2017
Dear Dr Grimes,
I am pleased to inform you that your manuscript entitled "Modelling science trustworthiness
under publish or perish pressure" is now accepted for publication in Royal Society Open Science.
You can expect to receive a proof of your article in the near future. Please contact the editorial
office (openscience_proofs@royalsociety.org and openscience@royalsociety.org) to let us know if
you are likely to be away from e-mail contact. Due to rapid publication and an extremely tight
schedule, if comments are not received, your paper may experience a delay in publication.
Royal Society Open Science operates under a continuous publication model
(http://bit.ly/cpFAQ). Your article will be published straight into the next open issue and this
will be the final version of the paper. As such, it can be cited immediately by other researchers.
As the issue version of your paper will be the only version to be published I would advise you to
check your proofs thoroughly as changes cannot be made once the paper is published.
In order to raise the profile of your paper once it is published, we can send through a PDF of your
13
paper to selected colleagues. If you wish to take advantage of this, please reply to this email with
the name and email addresses of up to 10 people who you feel would wish to read your article.
Please note that Royal Society Open Science will introduce article processing charges for all new
submissions received from 1 January 2018. Charges will also apply to papers transferred to Royal
Society Open Science from other Royal Society Publishing journals, as well as papers submitted
as part of our collaboration with the Royal Society of Chemistry
(http://rsos.royalsocietypublishing.org/chemistry). If your manuscript is submitted and
accepted for publication after 1 Jan 2018, you will be asked to pay the article processing charge,
unless you request a waiver and this is approved by Royal Society Publishing. You can find out
more about the charges at http://rsos.royalsocietypublishing.org/page/charges. Should you
have any queries, please contact openscience@royalsociety.org.
On behalf of the Editors of Royal Society Open Science, we look forward to your continued
contributions to the Journal.
Kind regards,
Alice Power
Editorial Coordinator
Royal Society Open Science
openscience@royalsociety.org
on behalf of Dr Matjaz Perc (Associate Editor) and Mark Chaplain (Subject Editor)
openscience@royalsociety.org
Associate Editor Comments to Author:
Thank you for the comprehensive revision of your manuscript, which we are happy to accept for
publication in Royal Society Open Science.
Appendix A
Response to reviewers and editors
21st November 2017
Re: RSOS-171511 - Modelling science trustworthiness under publish or perish pressure.
Dear Reviewers and journal staff,
Thank you for these productive and prompt comments. I apologise for the time taken in getting to
you, but we thought it worthwhile to craft a response which took the many useful comments on
board. These are enumerated below – thank you again for your hard work on this, and I hope these
comments and changes address any lingering concerns. Please do not hesitate to contact us if
anything further is required. We also feel the reviewers have made suggestions which have
improved the manuscript and deserve acknowledgement. Currently this has been added to the
acknowledgements, and if the reviewers consent to sharing their names, we’d be happy to include
them specifically.
Yours, on behalf of the authors,
David Robert Grimes
Response to reviewer 1
1. I do understand that the model has been supplied with a few parameters derived from
empirical investigation, but there must be more effort to match the dynamics/predictions of
the model with real data - i.e. there must be more effort in demonstrating that this is a
realistic model that captures the essential aspects of this particular problem. One blatant
shortcoming is the fact that scientists, and thus their outputs, are categorized by just 3
absolute types - but most research teams/outputs involve a mixture of these types - i.e.
scientific fraud could arise from just one fraudster in a team of diligents. As such, the tone of
the manuscript is much too definitive - for a purely theoretical model that relies on many
generalizing assumptions, too many statements could be taken out of context and converted
into bad press concerning the state of fraud in science, etc.
This is a very fair point – definitive predictions about the behaviour of researchers are inherently
complex, and we don’t wish to claim our model does this. Rather, we intend to present the model as
a tool for allowing us to formulate fraud dynamics in a population of scientists in a formal and
precise way subject to certain caveats, rather than as a predictive tool per se. The distinction
between directly predictive versus informative models in a subtle one (May, Uses and Abuses of
Mathematics in Biology, Mathematics in Biology, 2017) and one we should elucidate. While
predictive models are the end goal, the process of writing down a model helps illuminate what kind
of questions can be answered with the models, and what kind of data you might need to gather to
test a more predictive model. Certainly, it is not our intention to create a panic over science quality,
but rather to ask whether our current system might require some reform. To reflect this, we have
markedly changed the tone and added a number of important caveats, outlined below.
Modified Introduction Text:
“To better understand the impact of publish or perish on scientific research, and to garner insight
into what practices drives the trustworthiness of published science is of paramount importance if
we are to counter-act any detrimental impacts of such practices. In this work, we present a simple
but instructive model of scientific publishing trustworthiness under the assumption that
researchers are rewarded for their published output, taking account of field-specific differences
and the proportion of resources allocated with funding cycle. The factors that influence resultant
trustworthiness in simulation are quantified and discussed, as well as implications for improving
the trustworthiness of scientific publishing. It is important to note that the motivations of
scientists and ecosystem of scientific publishing are inherently complex, and we do not expect the
model discussed to be deterministically predictive - rather, the model is presented as a tool for
allowing us to formulate publication dynamics in a population of scientists in a formal and precise
way, subject to certain caveats.”
2. The model also completely glosses over the fact that the reward system in science is oriented
around scientific impact (e.g. citations, intellectual property, prestigious awards, PhDs
advised, etc) rather than the number of publications produced. The model somewhat
accounts for this by substituting publications in “top journals” (measured as a proportion of
the total J) as the means for connecting prior performance to future funding. In the
discussion section, the authors use the example of emerging open access journals as an
example to support their observation that “*the+ best outcome was obtained by simply
paying no heed to whether a result was significant or not” - this is confounding in 2 ways.
First, the model is built around high-impact journals, for which the carrying capacity is not
growing significantly, which is certainly not the case for PLOS One. Second, the authors are
confounding the idea of “trustworthiness of published science” with “paying no heed to
whether a result was significant or not” - while this may be the model of PLOS One and
Scientific Reports, this is not the model nor will it ever be of top-tier journals whose entire
market is built around prestige. In other words, model doesn’t account for this important
difference - targeting top-tier vs rapid open access - and thus, it is outside the scope of the
model to make such statements or extensions in this regard.
There are two major points to address here – the first of which is to which extent number of
publications truly matters – in a study of the tenure success rates of 25,604 scientists, number of
publications was a major deciding factor in whether a scientist made PI, and that scientists who
published in low impact factor journals and made PI compensated with additional publications (van
Dijk et al 2014). This is part might explain the rise of predatory journals, and the willingness of
scientists to publish in them. While scientific work on this topic is in its infancy, it is certainly an
alarming trend and has recently been discussed in Nature, the New York Times and on the
retractionwatch website quite recently. The reviewers points certainly require some extended
discussion, and the following has been added to the discussion:
On prestige journals:
“… However, there is an important point to consider in the form of the parameter J ( the
publication carrying-capacity). This can be highly field specific, comprising the top-tier journals in
that specific field. In general, these publications are focused on prestige rather than rapid
dissemination of science and it is unlikely these journals would move to replicate the approach of
rapid open-access publishers. Accordingly, the suggestion that top-tier journals might aspire to
treat all studies, regardless of their results, as equally worthy of publication is likely to be an
unworkable ideal.”
On publication as a vital, game-able metric for success:
“The model presented pivots on the assumption of a scenario that publication is the dominant
metric upon which scientists are rewarded, and elucidates the potential consequences of such a
situation. It is important to note this is a substantial simplification, and there are other metrics by
which scientists are assessed, including other measures of impact, awards and citations. However,
the number of publications attributed to a scientist has a marked effect on their career success,
with more publications associated with principle investigator status, and acquisition of funding
(van Dijk et al 2014). The average number of authors per paper is increasing over time, and this is
not just due to more interdisciplinary work, but also due to a greater demand for having more
papers in one’s CV (ref: Inflated numbers of authors over time have not been just due to
increasing research complexity. Papatheodorou SI, Trikalinos TA, Ioannidis JP. J Clin Epidemiol.
2008 Jun;61(6):546-51). “
3. And finally, statements such as “The analysis here suggests that science trustworthiness is
affected too by changes in funding resources, and that when an increase of funding improves
the over-all trustworthiness of science, as depicted in figure 2. “ should be more carefully
phrased because as is, this sounds like more funding means more trustworthiness but there is
no mechanism in the model by which fraudsters convert to diligent scientists if relatively
speaking there is higher R(t). Empirically, the authors could test whether fraud rates
decreased after the budget doubling of the NIH from 1998-2003.
This is an excellent point –while such an empirical investigation might be beyond the scope of this
piece, it is a very clever idea that should be mentioned. We also agree that as phrased, the paper
could give the misleading impression that there is a mechanism for conversion of unethical to
diligent researchers. We have changed the language in the results and the discussion section to
counteract this, and to make it clear.
New Results text:
“The implications of this require some elaboration, and are considered in the discussion section. “
New discussion text:
“The analysis here suggests that science trustworthiness is affected too by changes in funding
resources, and that when an increase of funding improves the over-all trustworthiness of science,
as depicted in figure 2. Conversely when this is diminished, the increased competition on
scientists appears to create conditions when false positives and dubious results are more likely to
be selected for and rewarded. This is a natural consequence of the model, but requires careful
interpretation. Crucially, it is important to note that there is no mechanism in the model for
unethical or careless researchers to transition into diligent scientists. Rather, decreasing funding
increases competition, and amplifies the career advantages of questionable findings. Conversely, if
global funding rates are increased, then competition for resources decreases and the advantage of
suspect findings is somewhat mitigated. While beyond the scope of this work, such a prediction
could be empirically tested by analysing situations when research budgets change markedly, such
as the doubling of the NIH budget from 1998 to 2003.“
4. There are other minor details that should also be addressed:The discussion of the assumptions and
the sources of error should take place in the paper, not in the supplement 1 / - In many of the figure
panels the legend covers up a significant portion of the data being reported / - There are several font
conversion typos, e.g. ef 4 , 47, and page 6 “deficienciesain A 2 ” - Many small typos: Page 2
weighed>weighted; Page 4 help>held and \beta_{UD} > \beta{D}; Page 6 J_F = J/100 > J_F = 1/100
These issues should now be corrected; we thank the reviewer for spotting them.
Response to reviewer 2
1. It would be useful to define “trustworthiness” when the term is introduced. I presume the
authors mean “correct,” in that the claim in the paper is well supported by the evidence
rallied by the paper. Other definitions are possible, such as when results represent a valid
attempt to minimize error (and therefore are not fraudulent. An accidental false positive is
impossible in the former definition but not in the latter. Eq. 1.13 makes it clear that it is the
former, but this isn’t made clear until p. 7.
This was a daft oversight on our behalf – we had added the following line to the initial paragraph;
“As science pivots on replicable findings, for the purposes of this work we define a trustworthy
finding as one that can be replicated independently, corresponding to a true positive or true
negative.”
2. p. 3, lines 40-42: “Even when research groups are very diligent, they may reasonably submit
a fraction e of their false positives.” It’s unclear how the researchers would know which of
their false positives are actually false. If they know it’s false, then it’s not a false positive, but
a true negative. If they don’t know it’s false, then they should submit of these. It seems
like the parameters were chosen for some mathematical convenience, but I think this
warrants more justification.
This requires some clarification – the principle here was to impress upon the reader that even very
good scientists will occasionally find a false positive which for some reason is hard to eliminate, and
which is not due to carelessness but rather due to the experimental or theoretical difficulty. This
section has been reworded to read
“Even when research groups are very diligent, they may occasionally happen upon a false or
misleading result which is hard to eliminate and due to experimental or theoretical difficultly
rather than carelessness. For the diligent cohort, this will be as low as can reasonably be achieved
and so we state they submit a small fraction e of their false positives for publication”
3. p. 4, lines 18-2 : “The respective rates of submission...” The assumption here is that output is
a linear function of the funding received. The extent to which this holds will vary, but will
probably never be totally accurate. For example, wet-lab science requires continuous
funding, but there will likely be diminishing marginal returns (but also possible punctuated
leaps as new paradigms become possible). Meanwhile, fields like computational
neuroscience may require little funding beyond ensuring access to computing resources. The
authors should address the appropriateness of their assumption here.
This is a fantastic point – the following text has been added to the discussion to address it;
“The model also implicitly assumes that output is an approximately linear function of funding in a
given-field. The exact applicability of this assumption may vary across fields. For example, wet-lab
sciences have require a certain threshold of continuous funding just to operate, whereas
computational or theoretical sciences may be able to operate with comparatively little funding.
Presuming direct comparison of researchers and their teams across a given field, however, the
assumption of direct correspondence between resources and outputs is reasonable, although
outliers are to be expected. “
4. p. 5, Eqs 1.8 & 1.9: In my reading, G isn’t really about how funding changes over time.
Rather, it reflects to which funds are simply allocated by current frequency of strategy. In
other words, G can be seen as the inverse of the strength of selection on the various
phenotypes. This characterization might be noted, as well as acknowledging other ways in
which funding might change over time, at least verbally.
This is indeed a fair point – please see reply to reviewer 1, point 3.
5. p. 5, lines 52-57: “We might envision a situation where scientific works are audited for
reproducibility...” It’s unclear to me how this could be assessed. How would an auditor be
able to tell hwo reproducible a lab’s work is? Also, this *in theory* (not in practice) could only
be achieved by the diligent cohort, but it may be possible to give the appearance of diligence
without actually being diligent.
This is a good question – in reality, it might require dedicated and independent auditing body, which
currently is not something available in most of science. There is some forthcoming worth in PLOS
One on scientific auditing and some auditing collaborations, but for now it is entirely in the realms of
the ideal. We have updated the text to clarify this.
6. p. 9, lines 43-44: “reducing funding increases the publication pressure and results in increases
selection of suspect works and a fall in scientific reproducibility.” I worry that this result may
occur only be because you assume that initially the majority of researchers are diligent. If
diligent researchers start out as a minority, my reading of the model dynamics suggests that
extra funding would make it harder for them to increase in number. If this is correct, it
weakens one of the central conclusions of the paper, especially since the goal is often to
*increase* the number of diligent researchers.
There is certainly an element of truth to this; as noted by the reviewer in point 4, and elucidated in
our reply to reviewer 1 (point 3), there is no mechanism in the model for funding to directly convert
researchers from one cohort to another. This is now reflected in the text. However, increasing
available funding tends to reduce the relative advantage of dubious publications, and makes fraud or
carelessness less of a boon.
7. p. 2, line 54: “For B-independence...” Two questions. First, why not just say B = 0.5, rather
than B-independence? econd, it’s unclear why researchers would update their submission of
null results so cruelly, submitting only half when 35% of articles published are null but all
when 50% of articles published are null.
B-independence has now been rephrased as it was potentially misleading. In B independent scenario
(eq 1.12), there is no discrimination between null and positive results, rather than B = 0.5. In a
situation where B >> 0.5 (positive discrimination), null results have reduced odds of publication and
there is less desire to submit them, hence the small fraction of null results submitted (0.4 here is the
fraction of total nulls submitted, not total results). However, when B is not a factor (as in equation
1.12), then it makes sense to submit all results, both null and positive. This has been clarified in the
text to;
“In this simulation, $\beta_{D} = \beta_{C} = \beta_{D} = 0.5$ when publications were $B >> 0.5$
(skewed towards positive publication). For the scenario where null and positive results were
equally likely to be published null results were as likely to be published so all were submitted and
thus the fraction of null results submitted respectively were $\beta_{D} = \beta_{C} = \beta_{D} =
1$. “
8. p. 15, lines 11-12: “It would be useful to know precisely how much is never submitted, and to
gauge the extent of the file-drawer problem.” Some good estimations of this have been
published. See, for example: Franco et al. (2014) Publication bias in the social sciences:
Unlocking the file drawer. Science 345: 1502-1505. And other papers by the same authors.
These are both fascinating and alarming – thank you again, this has been added to the manuscript at
this juncture. Curiously, the Franco paper says on 35% of null results were written up, broadly in
agreement with our beta figures in table 1. The text now reads
“It would be useful to know precisely how much is never submitted, and to gauge the extent of
the file-drawer problem. Certainly estimates have been made in some fields, notably by Franco
and colleagues (Franco 2014) who determined that in one study of publications in social sciences,
only 35% of the null results were ever written up (in good agreement with our estimates for beta
in table 1) and ultimately, just over 20.8% of these findings were published. Also for NIH-funded
clinical trials, 32% remained unpublished a median of 51 months after their completion (ref: BMJ.
2012 Jan 3;344:d7292. doi: 10.1136/bmj.d7292. Publication of NIH funded trials registered in
ClinicalTrials.gov: cross sectional analysis. Ross JS1, Tse T, Zarin DA, Xu H, Zhou L, Krumholz HM).
Whether these patterns apply also in other fields remains to be seen. One approach might be to
consider the issue from an energy-expenditure point of view or game-theory approach which
could be coupled with the model to estimate how much vital science never reaches the public
domain, though this is beyond the scope of this investigation.”
9. p. 15, lines 57-58: “We need a better understanding of the factors driving publication and
productivity-related behaviors.”I’m not convinced that this is really what we need. It seems
like we have a pretty good understanding of this. Also, this model takes it as given. It seems
to me that we need ways of changing incentives and evaluation.
On reflection, this is a much more pertinent point than what we initially wrote – the text has
now changed to
“This work illuminates potential consequences of a system where publication is the
dominating measure of academic success, and strongly suggests we should consider the
consequences of our incentives, and look at changing how academics are evaluated.”
MINOR COMMENTS
1. P. 2, line 5 : It might be useful to define “questionable research practices,” since it will affect
readers’ interpretation of the fact that /3 of all researchers exhibit these. A couple of
examples might suffice.
This is an excellent point – it also allows us to include a reference to quantifying some of these
behaviours. The next text reads:
“…may be far less important than more subtle questionable research practices, which might
include selective reporting of (dependent) variables, failure to disclose experimental conditions
and unreported data exclusions (John et al 2012).”
2. p. 3, line 55: typo: should be “in that THEY may” – typo corrected.
3. p. 4, line 45: “These probabilities are given by” Should note that the assumption here must be that
the number of papers of each type that journals will publish must be greater than the total number of
submissions. Which is reasonable, but should be made explicit. - This has now been added.
4. p. 4, line 6 : “The average rate of publications per unit of funding per unit time is thus” It is
important to note that is about “top-tier” journals only. What these are is debatable depending on
field. An argument that is regularly made is to put less weight on publishing venue. – Now clarified
5. p. 7, line 46: Typo, should be “ -2 ” – typo corrected.
6. p. 7, lines 41-42: “ imulation results suggests that the trustworthiness of published science in any
given field is strongly dependent on the false positive rate in that field under a publish or perish
paradigm.” OK. But, it is also clear from Figure 1 that the number of diligent researchers is still
declining and the number of trustworthy results is declining even when FPR is low with publish-or-
perish incentives, as suggested by Smaldino & McElreath 2016.
Rephrased to
“Simulation results suggests that the trustworthiness of published science in any given field is
strongly dependent on the false positive rate in that field under a publish or perish paradigm. As
evidenced by the figure, outputs from diligent research and the number of trustworthy results still
decline even when FPR is low with publish-or-perish incentives, as suggested by Smaldino and
McElreath (2016).”
7. Results section more generally: Most of the subsections starting on p. 10 talk about an analysis
focusing on a key parameter. Because there are a lot of parameters in the model, a line to remind the
reader as to the interpretation of the parameter (e.g., \eta on p. 10) would be very helpful.
Now added at each point.
8. p. 14, line 51: typo, should be *figure* 5(a). – Typo corrected
9. p. 15, lines 56-57: “Publishing is not intrinsically flawed, and conversely complete, unbiased
publication is essential for scientific progress.” Awkward phrasing, and also not converse. The logical
converse would be something like “Not everything intrinsically flawed is publishing.
Agreed – this sentence is fairly monstrous! Rephrased as “Scientific publishing is not intrinsically
flawed, and complete, unbiased publication is essential for scientific progress.”
10. P. 15, lines 33-35: “while publications are indeed one measure of productivity, they are not
necessarily the sole measure. While a much harder aspect to gauge, trustworthiness is more
fundamentally important.” Even more than this, the model in this paper considered only papers in
“top journals.” ome have suggested that these journals are in fact more likely than most to publish
false positives (e.g.http://bjoern.brembs.net/2016/01/even-without-retractions-top-journals-publish-
the-least-reliable-science/)
Certainly agree with this!
11. p. 16, Section 4: Conclusion. That should probably go. – Agreed.
12. Missing journal info for citation Ref 32. – Fixed.
Response to reviewer 3
1. In this well written and presented paper authors model science trustworthiness under publish
or perish pressure. Research is more than timely and I can only recommend this paper for
acceptance. The only reason I recommended that this paper should be accepted with minor
correction is due to the section 4. Conclusion that states: The conclusion text goes here. I
view this as a typo since section 3. nicely wraps up the paper. As far as I'm concerned there is
no need for additional iteration, as removing this section is a technical issue.
We thank the reviewer for their kind words, and have removed the inadvertent section.
Society Open
