Searching for the bottom of the ego well: failure to uncover
ego depletion in Many Labs 3
Miguel A. Vadillo, Natalie Gold and Magda Osman
Article citation details
R. Soc. open sci. 5: 180390.
http://dx.doi.org/10.1098/rsos.180390
Review timeline
Original submission: 9 March 2018 Note: Reports are unedited and appear as
Revised submission: 31 May 2018 submitted by the referee. The review history
Final acceptance: 3 July 2018 appears in chronological order.
Review History
label_version_1
RSOS-180390.R0 (Original submission)
label_author_1
Review form: Reviewer 1 (Charles Ebersole)
Is the manuscript scientifically sound in its present form?
Yes
Are the interpretations and conclusions justified by the results?
Yes
Is the language acceptable?
Yes
Is it clear how to access all supporting data?
Yes
Do you have any ethical concerns with this paper?
No
© 2018 The Authors. Published by the Royal Society under the terms of the Creative Commons
Attribution License http://creativecommons.org/licenses/by/4.0/, which permits unrestricted use,
provided the original author and source are credited
2
Have you any concerns about statistical analyses in this paper?
Yes
Recommendation?
label_recommendation_1
Accept with minor revision (please list in comments)
Comments to the Author(s)
label_comment_1
Dr. Vadillo and colleagues:
I recently had the pleasure of reviewing your manuscript “Searching for the bottom of
the ego well: Failure to uncover ego depletion in Many Labs 3.” As the paper notes, I gave
comments on an earlier draft of this paper and, at that time, mainly focused on the description of
Many Labs 3 in this paper. I gave the paper a much more thorough reading this time. I’m not an
expert in the field of ego depletion, so my comments are mainly focused on the design of the
tasks in Many Labs 3 and the handling of the data.
Overall, I like how you have described and discussed the Stroop task that was used in
Many Labs 3. Although d = .91 is a rather large effect, it is small by some Stroop comparisons.
You rightly note that this could be due to the smaller number of trials. Another factor that was
pointed out to us during the review of Many Labs 3 is that the balance of congruent to
incongruent trials might have also dampened the effect. In our study, 2/3 of trials are
incongruent. When incongruent trials are rarer, they can be harder to detect for participants.
Since incongruent trials were more common in our study, it’s possible that participants were
more “on guard” for these trials and thus showed a smaller Stroop effect. I don’t believe that this
is a problem for the conclusions of this paper, but it may be worth noting alongside the relatively
brief length as a reason for a relatively small Stroop effect.
My main concern with this paper is with the selection of participants for analysis. At
present, you select participants who completed the critical tasks between slots 6 and 22. The slots
that you use to indicate task order correspond to the page numbers of the Many Labs 3
procedure. The replication studies (where both the anagrams task and Stroop task were
administered) fell between slots 2 and 26. Some studies took up multiple pages. For instance, the
Stroop task occurred over 4 pages. Therefore, as the authors point out, if participants completed
the anagram task in slot 2, 3, 4, or 5, we know that the Stroop task came after. However, since task
order was randomly assigned, all participants had just as much chance to have one of the critical
tasks before the other, even if that involved the anagram task being one of the earliest or latest
pages. I think it would help to explain the logic of these exclusions more in the paper or consider
using all participants in the analyses (e.g., not only selecting participants who completed the
anagrams between slots 6 and 22). Running the analyses without this exclusion criteria produces
the same results, but with greater precision (it adds about 1200 participants by my calculations;
you can see my reanalysis at osf.io/f62b7).
In considering the exclusion criteria, I thought of other ways to operationalize task order
in the study. For Many Labs 3, we looked at order effects by slot (from 2-26) but also by order of
study (from 1-10, the number of replications we conducted). Interestingly, when rerunning your
models using study order, instead of slot order, you do find a small effect of order of the Stroop
task with participants persisting a little longer when doing the anagram task first, p = .015, <U+03B7>p2 =
.0018 (you can see these analyses in my script as well). It’s important to note that this effect is
pretty tiny, with a relatively high p value given the sample size, and the effect is not detected in
the meta-analysis when using study order instead of slot order. I also think that slot order is a
better operationalization of task order, given that it takes into account the fact that some studies
took up more slots than others (in fact, study order is a much worse predictor of persistence
compared to slot order). Regardless, I think it would strengthen the paper to speak more about
your rationale for using slot order, as compared to something like study order, as the
operationalization of task order, given that there are multiple ways to operationalize this
variable.
As a more minor point, I like that you provide power analyses for various possible
3
values of depletion effects using both the Stroop and unsolvable anagrams paradigms. Certainly,
the current sample size should give you the power to detect the effects reported in the previous
meta-analyses. However, as you also point out, it looks like studies using the anagrams task
might have some evidence of selective reporting and publication bias. This makes the power
analyses you ran potentially optimistic (even considering the fact that you used the lower bounds
of the confidence intervals for those effects). Given that you make both of these claims, I think it
would help to link the two in your discussion; that if this literature is suffering from publication
and reporting biases, it is possible that there are true effects that are just much smaller than we’ve
estimated, making even samples in the thousands underpowered to detect them. I think this
would say much more about those effects than your current investigation, but this seems a
plausible interpretation of what you found given your assessment of this field.
Finally, I just wanted to say thank you for making your analysis scripts available. They
were very easy to follow and helped immensely with my review.
Warm Regards,
Charlie Ebersole
University of Virginia
label_author_2
Review form: Reviewer 2 (Paul Christiansen)
Is the manuscript scientifically sound in its present form?
Yes
Are the interpretations and conclusions justified by the results?
Yes
Is the language acceptable?
Yes
Is it clear how to access all supporting data?
No
Do you have any ethical concerns with this paper?
No
Have you any concerns about statistical analyses in this paper?
Yes
Recommendation?
label_recommendation_2
Accept with minor revision (please list in comments)
Comments to the Author(s)
label_comment_2
This is an interesting study looking at ED in a large sample. The method for exploring it is (as the
authors acknowledge) not ideal due to the lack of control in timing of tasks etc. The authors are
however open with these issues and attempt to control for them in the analyses. They are indeed
very open with the limitations and despite these issues I think this paper makes a contribution to
the literature. I have some broad comments on the study as written
Page three author state - Seemingly minor feature “of investing more effort into a task”. Within
the framework of the model is not a minor feature, rather a critical feature. If increased effort is
put into a task surely this would be more depleting (according to the model). A participant that
puts no effort into the task would not show a depleting effect. I think this comment conflates task
4
related-issues (whether a dominant response was developed or not) and whether effort mediates
the effect of the manipulation. Take an individual who did not have the dominant response
trained but still found the task very hard and worked their hardest to complete the task (by no
means and impossibility)- according to the model they would show effects of the manipulation
(whether by the proposed mechanism or simply because they cannot be bothered trying
anymore).
Re statistics: Was the data used in the regressions normally distributed it seems from the
description of the tasks (with a timeout) it could be skewed or bimodal), it would be good to see a
statement from the authors about this is inappropriate regression analyses are commonplace.
Was collinearity an issue with the data at all? (VIFs>3)
-Another limitation with the study and indeed much of the ego deletion research is the
assumption that the participants are, in some way, motivated to try hard on the task used as the
DV. Unless there is a good reason for them doing so e.g. monetary reward, sticking with a diet etc
I cannot see a reason why there would be a motivation to try. In the context of this study after a
large battery of tests this could be a problem, lack of effort in a no-consequence task is a plausible
reason for null findings. I think this represents a limitation of the current study and indeed a
plausible explanation for null in many studies findings.
label_end_comment
Decision letter (RSOS-180390.R0)
15-May-2018
Dear Dr Vadillo
On behalf of the Editors, I am pleased to inform you that your Manuscript RSOS-180390 entitled
"Searching for the bottom of the ego well: Failure to uncover ego depletion in Many Labs 3" has
been accepted for publication in Royal Society Open Science subject to minor revision in
accordance with the referee suggestions. Please find the referees' comments at the end of this
email.
The reviewers and handling editors have recommended publication, but also suggest some minor
revisions to your manuscript. Therefore, I invite you to respond to the comments and revise your
manuscript.
• Ethics statement
If your study uses humans or animals please include details of the ethical approval received,
including the name of the committee that granted approval. For human studies please also detail
whether informed consent was obtained. For field studies on animals please include details of all
permissions, licences and/or approvals granted to carry out the fieldwork.
• Data accessibility
It is a condition of publication that all supporting data are made available either as
supplementary information or preferably in a suitable permanent repository. The data
accessibility section should state where the article's supporting data can be accessed. This section
should also include details, where possible of where to access other relevant research materials
such as statistical tools, protocols, software etc can be accessed. If the data has been deposited in
an external repository this section should list the database, accession number and link to the DOI
for all data from the article that has been made publicly available. Data sets that have been
5
deposited in an external repository and have a DOI should also be appropriately cited in the
manuscript and included in the reference list.
If you wish to submit your supporting data or code to Dryad (http://datadryad.org/), or modify
your current submission to dryad, please use the following link:
http://datadryad.org/submit?journalID=RSOS&manu=RSOS-180390
• Competing interests
Please declare any financial or non-financial competing interests, or state that you have no
competing interests.
• Authors’ contributions
All submissions, other than those with a single author, must include an Authors’ Contributions
section which individually lists the specific contribution of each author. The list of Authors
should meet all of the following criteria; 1) substantial contributions to conception and design, or
acquisition of data, or analysis and interpretation of data; 2) drafting the article or revising it
critically for important intellectual content; and 3) final approval of the version to be published.
All contributors who do not meet all of these criteria should be included in the
acknowledgements.
We suggest the following format:
AB carried out the molecular lab work, participated in data analysis, carried out sequence
alignments, participated in the design of the study and drafted the manuscript; CD carried out
the statistical analyses; EF collected field data; GH conceived of the study, designed the study,
coordinated the study and helped draft the manuscript. All authors gave final approval for
publication.
• Acknowledgements
Please acknowledge anyone who contributed to the study but did not meet the authorship
criteria.
• Funding statement
Please list the source of funding for each author.
Please note that we cannot publish your manuscript without these end statements included. We
have included a screenshot example of the end statements for reference. If you feel that a given
heading is not relevant to your paper, please nevertheless include the heading and explicitly state
that it is not relevant to your work.
Because the schedule for publication is very tight, it is a condition of publication that you submit
the revised version of your manuscript within 7 days (i.e. by the 24-May-2018). If you do not
think you will be able to meet this date please let me know immediately.
To revise your manuscript, log into https://mc.manuscriptcentral.com/rsos and enter your
Author Centre, where you will find your manuscript title listed under "Manuscripts with
Decisions". Under "Actions," click on "Create a Revision." You will be unable to make your
revisions on the originally submitted version of the manuscript. Instead, revise your manuscript
and upload a new version through your Author Centre.
When submitting your revised manuscript, you will be able to respond to the comments made by
the referees and upload a file "Response to Referees" in "Section 6 - File Upload". You can use this
to document any changes you make to the original manuscript. In order to expedite the
6
processing of the revised manuscript, please be as specific as possible in your response to the
referees. We strongly recommend uploading two versions of your revised manuscript:
1) Identifying all the changes that have been made (for instance, in coloured highlight, in bold
text, or tracked changes);
2) A 'clean' version of the new manuscript that incorporates the changes made, but does not
highlight them.
When uploading your revised files please make sure that you have:
1) A text file of the manuscript (tex, txt, rtf, docx or doc), references, tables (including captions)
and figure captions. Do not upload a PDF as your "Main Document";
2) A separate electronic file of each figure (EPS or print-quality PDF preferred (either format
should be produced directly from original creation package), or original software format);
3) Included a 100 word media summary of your paper when requested at submission. Please
ensure you have entered correct contact details (email, institution and telephone) in your user
account;
4) Included the raw data to support the claims made in your paper. You can either include your
data as electronic supplementary material or upload to a repository and include the relevant doi
within your manuscript. Make sure it is clear in your data accessibility statement how the data
can be accessed;
5) All supplementary materials accompanying an accepted article will be treated as in their final
form. Note that the Royal Society will neither edit nor typeset supplementary material and it will
be hosted as provided. Please ensure that the supplementary material includes the paper details
where possible (authors, article title, journal name).
Supplementary files will be published alongside the paper on the journal website and posted on
the online figshare repository (https://rs.figshare.com/). The heading and legend provided for
each supplementary file during the submission process will be used to create the figshare page,
so please ensure these are accurate and informative so that your files can be found in searches.
Files on figshare will be made available approximately one week before the accompanying article
so that the supplementary material can be attributed a unique DOI.
Please note that Royal Society Open Science will introduce article processing charges for all new
submissions received from 1 January 2018. Charges will also apply to papers transferred to Royal
Society Open Science from other Royal Society Publishing journals, as well as papers submitted
as part of our collaboration with the Royal Society of Chemistry
(http://rsos.royalsocietypublishing.org/chemistry).
If your manuscript is newly submitted and subsequently accepted for publication after 1 Jan 2018,
you will be asked to pay the article processing charge, unless you request a waiver and this is
approved by Royal Society Publishing. Manuscripts originally submitted prior to 1 Jan 2018 will
not subject to a charge, even if they are accepted in 2018. You can find out more about the charges
at http://rsos.royalsocietypublishing.org/page/charges. Should you have any queries, please
contact openscience@royalsociety.org.
Once again, thank you for submitting your manuscript to Royal Society Open Science and I look
forward to receiving your revision. If you have any questions at all, please do not hesitate to get
in touch.
Kind regards,
Thadcha Retneswaran
7
Royal Society Open Science
openscience@royalsociety.org
on behalf of Dr Joshua Buckholtz (Associate Editor) and Prof. Antonia Hamilton (Subject Editor)
openscience@royalsociety.org
Associate Editor Comments to Author (Dr Joshua Buckholtz):
Associate Editor: 1
Comments to the Author:
label_comment_3
Thank you for submitting this work to RSOS. I've now received reviews from two expert referees,
and have read the manuscript myself both prior to and after receiving their comments. There is
broad enthusiasm for this work, as it addresses a timely and important question, namely, the
replicability of so-called ego-depletion effects on measures of self-control. The primary concern
voiced among the reviewers relates to the suitability of the methods employed here for providing
a strong test of ego-depletion. While these issue do limit the impact of the work somewhat, you
are quite candid about these limitations and, for the most part, appropriately consider how the
might affect the strength of inferences drawn from these data. On the whole, I am comfortable
proceeding after the remaining, relatively minor, concerns raised in review are addressed. Please
respond to both reviewers' points in detail, with additional data, analyses or textual revisions as
appropriate.
Associate Editor: 2
Comments to the Author:
label_comment_4
It appears that the references in this manuscript are corrupted. Please resubmit a revised version
of this manuscript with corrected references.
Reviewer comments to Author:
Reviewer: 1
Comments to the Author(s)
label_comment_5
Dr. Vadillo and colleagues:
I recently had the pleasure of reviewing your manuscript “Searching for the bottom of
the ego well: Failure to uncover ego depletion in Many Labs 3.” As the paper notes, I gave
comments on an earlier draft of this paper and, at that time, mainly focused on the description of
Many Labs 3 in this paper. I gave the paper a much more thorough reading this time. I’m not an
expert in the field of ego depletion, so my comments are mainly focused on the design of the
tasks in Many Labs 3 and the handling of the data.
Overall, I like how you have described and discussed the Stroop task that was used in
Many Labs 3. Although d = .91 is a rather large effect, it is small by some Stroop comparisons.
You rightly note that this could be due to the smaller number of trials. Another factor that was
pointed out to us during the review of Many Labs 3 is that the balance of congruent to
incongruent trials might have also dampened the effect. In our study, 2/3 of trials are
incongruent. When incongruent trials are rarer, they can be harder to detect for participants.
Since incongruent trials were more common in our study, it’s possible that participants were
more “on guard” for these trials and thus showed a smaller Stroop effect. I don’t believe that this
is a problem for the conclusions of this paper, but it may be worth noting alongside the relatively
brief length as a reason for a relatively small Stroop effect.
My main concern with this paper is with the selection of participants for analysis. At
present, you select participants who completed the critical tasks between slots 6 and 22. The slots
that you use to indicate task order correspond to the page numbers of the Many Labs 3
procedure. The replication studies (where both the anagrams task and Stroop task were
administered) fell between slots 2 and 26. Some studies took up multiple pages. For instance, the
8
Stroop task occurred over 4 pages. Therefore, as the authors point out, if participants completed
the anagram task in slot 2, 3, 4, or 5, we know that the Stroop task came after. However, since task
order was randomly assigned, all participants had just as much chance to have one of the critical
tasks before the other, even if that involved the anagram task being one of the earliest or latest
pages. I think it would help to explain the logic of these exclusions more in the paper or consider
using all participants in the analyses (e.g., not only selecting participants who completed the
anagrams between slots 6 and 22). Running the analyses without this exclusion criteria produces
the same results, but with greater precision (it adds about 1200 participants by my calculations;
you can see my reanalysis at osf.io/f62b7).
In considering the exclusion criteria, I thought of other ways to operationalize task order
in the study. For Many Labs 3, we looked at order effects by slot (from 2-26) but also by order of
study (from 1-10, the number of replications we conducted). Interestingly, when rerunning your
models using study order, instead of slot order, you do find a small effect of order of the Stroop
task with participants persisting a little longer when doing the anagram task first, p = .015, <U+03B7>p2 =
.0018 (you can see these analyses in my script as well). It’s important to note that this effect is
pretty tiny, with a relatively high p value given the sample size, and the effect is not detected in
the meta-analysis when using study order instead of slot order. I also think that slot order is a
better operationalization of task order, given that it takes into account the fact that some studies
took up more slots than others (in fact, study order is a much worse predictor of persistence
compared to slot order). Regardless, I think it would strengthen the paper to speak more about
your rationale for using slot order, as compared to something like study order, as the
operationalization of task order, given that there are multiple ways to operationalize this
variable.
As a more minor point, I like that you provide power analyses for various possible
values of depletion effects using both the Stroop and unsolvable anagrams paradigms. Certainly,
the current sample size should give you the power to detect the effects reported in the previous
meta-analyses. However, as you also point out, it looks like studies using the anagrams task
might have some evidence of selective reporting and publication bias. This makes the power
analyses you ran potentially optimistic (even considering the fact that you used the lower bounds
of the confidence intervals for those effects). Given that you make both of these claims, I think it
would help to link the two in your discussion; that if this literature is suffering from publication
and reporting biases, it is possible that there are true effects that are just much smaller than we’ve
estimated, making even samples in the thousands underpowered to detect them. I think this
would say much more about those effects than your current investigation, but this seems a
plausible interpretation of what you found given your assessment of this field.
Finally, I just wanted to say thank you for making your analysis scripts available. They
were very easy to follow and helped immensely with my review.
Warm Regards,
Charlie Ebersole
University of Virginia
Reviewer: 2
Comments to the Author(s)
label_comment_6
This is an interesting study looking at ED in a large sample. The method for exploring it is (as the
authors acknowledge) not ideal due to the lack of control in timing of tasks etc. The authors are
however open with these issues and attempt to control for them in the analyses. They are indeed
very open with the limitations and despite these issues I think this paper makes a contribution to
the literature. I have some broad comments on the study as written
Page three author state - Seemingly minor feature “of investing more effort into a task”. Within
the framework of the model is not a minor feature, rather a critical feature. If increased effort is
put into a task surely this would be more depleting (according to the model). A participant that
9
puts no effort into the task would not show a depleting effect. I think this comment conflates task
related-issues (whether a dominant response was developed or not) and whether effort mediates
the effect of the manipulation. Take an individual who did not have the dominant response
trained but still found the task very hard and worked their hardest to complete the task (by no
means and impossibility)- according to the model they would show effects of the manipulation
(whether by the proposed mechanism or simply because they cannot be bothered trying
anymore).
Re statistics: Was the data used in the regressions normally distributed it seems from the
description of the tasks (with a timeout) it could be skewed or bimodal), it would be good to see a
statement from the authors about this is inappropriate regression analyses are commonplace.
Was collinearity an issue with the data at all? (VIFs>3)
-Another limitation with the study and indeed much of the ego deletion research is the
assumption that the participants are, in some way, motivated to try hard on the task used as the
DV. Unless there is a good reason for them doing so e.g. monetary reward, sticking with a diet etc
I cannot see a reason why there would be a motivation to try. In the context of this study after a
large battery of tests this could be a problem, lack of effort in a no-consequence task is a plausible
reason for null findings. I think this represents a limitation of the current study and indeed a
plausible explanation for null in many studies findings.
Author's Response to Decision Letter for (RSOS-180390.R0)
See Appendix A.
label_end_comment
Decision letter (RSOS-180390.R1)
03-Jul-2018
Dear Dr Vadillo,
I am pleased to inform you that your manuscript entitled "Searching for the bottom of the ego
well: Failure to uncover ego depletion in Many Labs 3" is now accepted for publication in Royal
Society Open Science.
You can expect to receive a proof of your article in the near future. Please contact the editorial
office (openscience_proofs@royalsociety.org and openscience@royalsociety.org) to let us know if
you are likely to be away from e-mail contact. Due to rapid publication and an extremely tight
schedule, if comments are not received, your paper may experience a delay in publication.
Royal Society Open Science operates under a continuous publication model
(http://bit.ly/cpFAQ). Your article will be published straight into the next open issue and this
will be the final version of the paper. As such, it can be cited immediately by other researchers.
As the issue version of your paper will be the only version to be published I would advise you to
check your proofs thoroughly as changes cannot be made once the paper is published.
10
In order to raise the profile of your paper once it is published, we can send through a PDF of your
paper to selected colleagues. If you wish to take advantage of this, please reply to this email with
the name and email addresses of up to 10 people who you feel would wish to read your article.
Please note that Royal Society Open Science will introduce article processing charges for all new
submissions received from 1 January 2018. Charges will also apply to papers transferred to Royal
Society Open Science from other Royal Society Publishing journals, as well as papers submitted
as part of our collaboration with the Royal Society of Chemistry
(http://rsos.royalsocietypublishing.org/chemistry).
If your manuscript is newly submitted and subsequently accepted for publication after 1 Jan 2018,
you will be asked to pay the article processing charge, unless you request a waiver and this is
approved by Royal Society Publishing. Manuscripts originally submitted prior to 1 Jan 2018 will
not subject to a charge, even if they are accepted in 2018. You can find out more about the charges
at http://rsos.royalsocietypublishing.org/page/charges. Should you have any queries, please
contact openscience@royalsociety.org.
On behalf of the Editors of Royal Society Open Science, we look forward to your continued
contributions to the Journal.
Kind regards,
Royal Society Open Science Editorial Office
Royal Society Open Science
openscience@royalsociety.org
on behalf of Dr Joshua Buckholtz (Associate Editor) and Prof. Antonia Hamilton (Subject Editor)
openscience@royalsociety.org
Appendix A
May 29th, 2018
Dr Joshua Buckholtz
Royal Society Open Science
Dear Dr Buckholtz,
Please find attached a revised version of our manuscript “Searching for the
bottom of the ego well: Failure to uncover ego depletion in Many Labs 3”
(RSOS-180390), which we are submitting for consideration for publication in
Royal Society Open Science. We are extremely grateful for the earlier round of
insightful reviews which have enabled us, we believe, to substantially improve
the manuscript.
The present version of the manuscript attempts to address all of the concerns
raised by the reviewers. On the following pages, we provide detailed
information regarding the main changes we have made to accommodate the
reviewers’ suggestions. We hope that you will find our revision to be suitable
for publication in Royal Society Open Science.
Thank you very much for your time and attention in considering our revision.
Sincerely,
Miguel A. Vadillo
Departamento de Psicología Básica
Universidad Autónoma de Madrid
e-mail: miguel.vadillo@uam.es
Comments made by Reviewer 1 Changes to the manuscript
Overall, I like how you have described and discussed the We would like to thank Reviewer 1 for alerting us about this
Stroop task that was used in Many Labs 3. Although d = .91 is a potential explanation for the smaller-than-usual size of the
rather large effect, it is small by some Stroop comparisons. You Stroop effect in ML3. This is now explicitly mentioned in the ms.
rightly note that this could be due to the smaller number of
trials. Another factor that was pointed out to us during the
review of Many Labs 3 is that the balance of congruent to
incongruent trials might have also dampened the effect. In our
study, 2/3 of trials are incongruent. When incongruent trials are
rarer, they can be harder to detect for participants. Since
incongruent trials were more common in our study, it’s possible
that participants were more “on guard” for these trials and thus
showed a smaller Stroop effect. I don’t believe that this is a
problem for the conclusions of this paper, but it may be worth
noting alongside the relatively brief length as a reason for a
relatively small Stroop effect.
My main concern with this paper is with the selection of This is an excellent point. Note, however, that because
participants for analysis. At present, you select participants who Condition and Anagram order are confounded, our main
completed the critical tasks between slots 6 and 22. The slots regression has a slight problem of collinearity (noted by
that you use to indicate task order correspond to the page Reviewer 2) that becomes even worse when all participants are
numbers of the Many Labs 3 procedure. The replication studies included (because this makes the confound even stronger).
(where both the anagrams task and Stroop task were With our selection criterion, the slope of the anagrams_order ~
administered) fell between slots 2 and 26. Some studies took up AFvsSF regression is -1.82, while without the selection criterion
multiple pages. For instance, the Stroop task occurred over 4 the slope becomes -5.20. Consequently, VIFs increase from
pages. Therefore, as the authors point out, if participants approx. 1.16 to 1.80. Because of this, and also given that the
completed the anagram task in slot 2, 3, 4, or 5, we know that exclusion criterion was part of our original analytic plan (we
the Stroop task came after. However, since task order was don’t want to be blamed of anti-p-hacking), if possible we would
randomly assigned, all participants had just as much chance to prefer to keep the original analysis, although we note in the
have one of the critical tasks before the other, even if that main text that the results remain unchanged with the whole
involved the anagram task being one of the earliest or latest sample of participants.
pages. I think it would help to explain the logic of these
exclusions more in the paper or consider using all participants
in the analyses (e.g., not only selecting participants who
completed the anagrams between slots 6 and 22). Running the
analyses without this exclusion criteria produces the same
results, but with greater precision (it adds about 1200
participants by my calculations; you can see my reanalysis at
osf.io/f62b7).
In considering the exclusion criteria, I thought of other ways to It is true that we could have analysed the data with alternative
operationalize task order in the study. For Many Labs 3, we operationalizations of task order. We felt that it didn’t make
looked at order effects by slot (from 2-26) but also by order of sense to give the same weight to difficult tasks comprising
study (from 1-10, the number of replications we conducted). several slots as to simple tasks with just a single slot.
Interestingly, when rerunning your models using study order, Therefore, slot seemed a better index. Furthermore, slot was
instead of slot order, you do find a small effect of order of the available in the main datafile (“ML3 PPool and MTurk
Stroop task with participants persisting a little longer when Data.csv”), while study order needs to be imported from a
doing the anagram task first, p = .015, <U+03B7>p2 = .0018 (you can different data file, which we thought made the analyses a bit
see these analyses in my script as well). It’s important to note more obscure, although logically this is absolutely irrelevant
that this effect is pretty tiny, with a relatively high p value given from a methodological point of view.
the sample size, and the effect is not detected in the meta-
analysis when using study order instead of slot order. I also
think that slot order is a better operationalization of task order,
given that it takes into account the fact that some studies took
up more slots than others (in fact, study order is a much worse
predictor of persistence compared to slot order). Regardless, I
think it would strengthen the paper to speak more about your
rationale for using slot order, as compared to something like
study order, as the operationalization of task order, given that
there are multiple ways to operationalize this variable.
As a more minor point, I like that you provide power analyses True, our power calculations may be too optimistic if the effect
for various possible values of depletion effects using both the size estimations from previous meta-analyses are biased. We
Stroop and unsolvable anagrams paradigms. Certainly, the now say this explicitly at several points of the Discussion.
current sample size should give you the power to detect the
effects reported in the previous meta-analyses. However, as
you also point out, it looks like studies using the anagrams task
might have some evidence of selective reporting and
publication bias. This makes the power analyses you ran
potentially optimistic (even considering the fact that you used
the lower bounds of the confidence intervals for those effects).
Given that you make both of these claims, I think it would help
to link the two in your discussion; that if this literature is
suffering from publication and reporting biases, it is possible
that there are true effects that are just much smaller than we’ve
estimated, making even samples in the thousands
underpowered to detect them. I think this would say much more
about those effects than your current investigation, but this
seems a plausible interpretation of what you found given your
assessment of this field.
Comments made by Reviewer 2 Changes to the manuscript
Page three author state - Seemingly minor feature “of investing Reviewer 2 is absolutely correct. We have amended this
more effort into a task”. Within the framework of the model is sentence to avoid any misunderstanding.
not a minor feature, rather a critical feature. If increased effort is
put into a task surely this would be more depleting (according to
the model). A participant that puts no effort into the task would
not show a depleting effect. I think this comment conflates task
related-issues (whether a dominant response was developed or
not) and whether effort mediates the effect of the manipulation.
Take an individual who did not have the dominant response
trained but still found the task very hard and worked their
hardest to complete the task (by no means and impossibility)-
according to the model they would show effects of the
manipulation (whether by the proposed mechanism or simply
because they cannot be bothered trying anymore).
Was the data used in the regressions normally distributed it The distribution of the dependent variable was indeed bimodal if
seems from the description of the tasks (with a timeout) it could all participants were included (this is now explicitly stated in the
be skewed or bimodal), it would be good to see a statement Methods section). Because of this, we also repeated the
from the authors about this is inappropriate regression analyses analyses without the participants who used all the available time
are commonplace. (who gave rise to this anomaly in the distribution of the
dependent variable). The results did not change including or
excluding these participants.
Was collinearity an issue with the data at all? (VIFs>3) VIFs in the main analyses were below 1.16 (although this
increases substantially (to 1.80) if in response to Reviewer 1 we
include all participants in the analysis).
Another limitation with the study and indeed much of the ego We don’t think that our data support this interpretation, as it
deletion research is the assumption that the participants are, in follows from it that ego depletion should have been observed at
some way, motivated to try hard on the task used as the DV. early stages of the experiment (presumably when participants
Unless there is a good reason for them doing so e.g. monetary are still motivated) and then vanish. We don’t find this trend in
reward, sticking with a diet etc I cannot see a reason why there the data. But we do mention this possibility explicitly in the new
would be a motivation to try. In the context of this study after a Discussion.
large battery of tests this could be a problem, lack of effort in a
no-consequence task is a plausible reason for null findings. I
think this represents a limitation of the current study and indeed
a plausible explanation for null in many studies findings.
Society Open
