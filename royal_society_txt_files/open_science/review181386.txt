Generalizability is not optional: insights from a cross-
cultural study of social discounting
Leonid Tiokhin, Joseph Hackman, Shirajum Munira, Khaleda Jesmin and Daniel Hruschka
Article citation details
R. Soc. open sci. 6: 181386.
http://dx.doi.org/10.1098/rsos.181386
Review timeline
Original submission: 21 August 2018 Note: Reports are unedited and appear as
Revised submission: 4 February 2019 submitted by the referee. The review history
Final acceptance: 5 February 2019 appears in chronological order.
Review History
label_version_1
RSOS-181386.R0 (Original submission)
label_author_1
Review form: Reviewer 1 (Cristine Legare)
Is the manuscript scientifically sound in its present form?
Yes
Are the interpretations and conclusions justified by the results?
Yes
Is the language acceptable?
Yes
Is it clear how to access all supporting data?
Yes
Do you have any ethical concerns with this paper?
No
Reports © 2019 The Reviewers; Decision Letters © 2019 The Reviewers and Editors;
Responses © 2019 The Reviewers, Editors and Authors. Published by the Royal Society under the
terms of the Creative Commons Attribution License http://creativecommons.org/licenses/by/4.0/,
which permits unrestricted use, provided the original author and source are credited
2
Have you any concerns about statistical analyses in this paper?
No
Recommendation?
label_recommendation_1
Accept with minor revision (please list in comments)
Comments to the Author(s)
label_comment_1
I rarely review papers this well-written, compelling, and consequential for improving
psychological research. This is truly outstanding work, and will make a major contribution to
increasing the state of the art in cross-cultural research.
I have a few minor comments and speculations.
1) I was intrigued by the authors' speculation that there is cultural variation in norms
surrounding generosity (that giving to those without need is seen as a sign of superiority for
example). I think this is worth elaborating on further, for each of the populations. One thing that
occurred to me re the results for the U.S. sample, for example, is that it is common for middle
class U.S. adults (including college students) to live away from those closest to them. And thus
gift giving/generosity may serve a different function (to solidify social bonds in the absence of
physical contact).
I think it is also worth commenting (in the final reflections) that research of this kind is
tremendously time and labor intensive, and takes longer to publish (utterly worth doing, but this
is one of the primary reasons it isn't getting done). The expectations for high volume publication
in psychology is actively undermining doing the kind of work this research requires, and the
result is that the field is producing a large volume of low quality papers. Why has replication
gotten more traction that the "diversity problem"? The field is using the same methods with the
same convenience samples to tackle replication.
Lack of training in conducting research outside of WEIRD samples within psychology is a major
obstacle as well. Note that the authors are unusual in that they are trained in multiple disciplines
(including psychology).
The need for convergent methods is worth mentioning I think. Asking participants to explain is a
good example of the benefits of multiple measures, but I think developing multiple methods to
study a construct/process is the gold standard (and rarely done in psychological research).
label_author_2
Review form: Reviewer 2 (Chris Chartier)
Is the manuscript scientifically sound in its present form?
Yes
Are the interpretations and conclusions justified by the results?
Yes
Is the language acceptable?
Yes
Is it clear how to access all supporting data?
Yes
3
Do you have any ethical concerns with this paper?
No
Have you any concerns about statistical analyses in this paper?
No
Recommendation?
label_recommendation_2
Accept with minor revision (please list in comments)
Comments to the Author(s)
label_comment_2
My overall assessment is that this is a strong article worthy of publication. It articulates
generalizability concerns clearly and provides a compelling specific example of the non-
universality of a well-established finding. I include here a set of questions, critiques, and
suggestions for the authors to consider, should a revision of this manuscript be requested.
The authors seem to suggest generalizability should always be a primary concern on par with
replicability. I enjoy the “not optional” assertion, but wonder if there is a reasonable ordering of
these two research goals, even if just a temporal order. Isn’t it appropriate that replicability be
established prior to generalizability? Why attempt to generalize a finding that very well may be
spurious even in the original samples tested? Some discussion of this issue may strengthen the
paper.
On page 4, starting on line 45. The authors ask why generalizability hasn’t captured the same
attention as replicability concerns in the field. I think it’s quite obvious. BEM (discovery of the
metaphysically impossible) and Stapel (blatant and widespread fraud) are such glaring and
powerful evidence of a sick system. I would argue that generalizability concerns likely can’t have
similarly stark demonstrations of fundamental problems (much to the detriment of attention
focused on them). This doesn’t make generalizability any less important in my mind, but I think
it’s perfectly understandable that replicability has captured more attention given the direct and
mind-blowing demonstrations of problems in that sphere. I suggest removing this section.
The authors seem to occasionally conflate (or at least write with a lack of precision about)
convenience samples vs. deeper community samples and WEIRD/nonWEIRD cultures. As you
argue in the discussion, simply moving into nonWEIRD countries and then collecting
convenience samples of college students is only one step in the right direction. I suggest making
the distinction more clearly in the introduction and giving each concern to get a bit of
independent coverage (highlight problems with convenience samples and WEIRD samples as
distinct issues).
I think the authors should discuss further the possible sources of inconsistency in Bangladesh and
Indonesia responding? This seems the most problematic element of the empirical portions of the
paper, and I was a bit dissatisfied with the lack of attempted explanation. I want to be clear,
though: I’m not asking you to HARK! If it’s a real head-scratcher, fine!
The sample size planning is inconsistent the rationale is weak (mirroring earlier studies was
given as one justification). This is simply a criticism. Not much can be done now and this isn’t a
“fatal” flaw in my mind.
Consider toning down the terminology such as “inconceivable” sample sizes with the PSA.
Others have run massive studies. These sample sizes will just become more common now with
the PSA.
4
As I mentioned above, I really love to point about PSA not being a panacea. We need “deep”
samples moving away from college students to complement our “broad” samples collected in
many countries. No suggestion, just a comment :)
I’m glad you included coverage of COGs. The discussion could highlight how consistent use of
COGs could be an excellent mechanism for promoting tests of generalizability. By explicitly
specifying COGs, researchers essentially welcome others to follow-up on their work with tests of
phenomena in other samples and settings.
Given the focus on making tests of generalizability with non-convenience samples in non-WEIRD
countries, I have a nagging concern and general complaint. As the authors themselves point out,
collecting these data is very time and resource intensive. One of the best ways to incentivize more
of this work is to provide academic incentives for the researchers who help conduct such studies
in the local communities. The authors acknowledge the support they received collecting data in
Indonesia and mention that the Bangladeshi team provided “valuable contributions at all phases
of the Bangladesh study.” I find it odd that none of these individuals appear as authors on this
submission. It sounds like some of these induvial contributed across multiple CRediT taxonomy
categories, and could be considered for authorship. If we hope to increase “deep” work on
generalizability and move beyond convenience samples in nonWEIRD countries, incentivizing
the work of our local collaborators should be prioritized.
Summary statement: this article is worth publishing. It is a renewed call for more work on
generalizability of psychological science accompanied by an open empirical puzzle, and could
serve to spur on further cross-cultural work on this and other phenomena. I suggest acceptance
following minor revisions.
Christopher R. Chartier
label_end_comment
Decision letter (RSOS-181386.R0)
15-Jan-2019
Dear Dr Tiokhin
On behalf of the Editors, I am pleased to inform you that your Manuscript RSOS-181386 entitled
"Generalizability is not optional: Insights from a cross-cultural study of social discounting" has
been accepted for publication in Royal Society Open Science subject to minor revision in
accordance with the referee suggestions. Please find the referees' comments at the end of this
email.
The reviewers and handling editors have recommended publication, but also suggest some minor
revisions to your manuscript. Therefore, I invite you to respond to the comments and revise your
manuscript.
• Ethics statement
If your study uses humans or animals please include details of the ethical approval received,
including the name of the committee that granted approval. For human studies please also detail
whether informed consent was obtained. For field studies on animals please include details of all
permissions, licences and/or approvals granted to carry out the fieldwork.
5
• Data accessibility
It is a condition of publication that all supporting data are made available either as
supplementary information or preferably in a suitable permanent repository. The data
accessibility section should state where the article's supporting data can be accessed. This section
should also include details, where possible of where to access other relevant research materials
such as statistical tools, protocols, software etc can be accessed. If the data has been deposited in
an external repository this section should list the database, accession number and link to the DOI
for all data from the article that has been made publicly available. Data sets that have been
deposited in an external repository and have a DOI should also be appropriately cited in the
manuscript and included in the reference list.
If you wish to submit your supporting data or code to Dryad (http://datadryad.org/), or modify
your current submission to dryad, please use the following link:
http://datadryad.org/submit?journalID=RSOS&manu=RSOS-181386
• Competing interests
Please declare any financial or non-financial competing interests, or state that you have no
competing interests.
• Authors’ contributions
All submissions, other than those with a single author, must include an Authors’ Contributions
section which individually lists the specific contribution of each author. The list of Authors
should meet all of the following criteria; 1) substantial contributions to conception and design, or
acquisition of data, or analysis and interpretation of data; 2) drafting the article or revising it
critically for important intellectual content; and 3) final approval of the version to be published.
All contributors who do not meet all of these criteria should be included in the
acknowledgements.
We suggest the following format:
AB carried out the molecular lab work, participated in data analysis, carried out sequence
alignments, participated in the design of the study and drafted the manuscript; CD carried out
the statistical analyses; EF collected field data; GH conceived of the study, designed the study,
coordinated the study and helped draft the manuscript. All authors gave final approval for
publication.
• Acknowledgements
Please acknowledge anyone who contributed to the study but did not meet the authorship
criteria.
• Funding statement
Please list the source of funding for each author.
Please ensure you have prepared your revision in accordance with the guidance at
https://royalsociety.org/journals/authors/author-guidelines/ -- please note that we cannot
publish your manuscript without the end statements. We have included a screenshot example of
the end statements for reference. If you feel that a given heading is not relevant to your paper,
please nevertheless include the heading and explicitly state that it is not relevant to your work.
Because the schedule for publication is very tight, it is a condition of publication that you submit
the revised version of your manuscript before 24-Jan-2019. Please note that the revision deadline
6
will expire at 00.00am on this date. If you do not think you will be able to meet this date please let
me know immediately.
To revise your manuscript, log into https://mc.manuscriptcentral.com/rsos and enter your
Author Centre, where you will find your manuscript title listed under "Manuscripts with
Decisions". Under "Actions," click on "Create a Revision." You will be unable to make your
revisions on the originally submitted version of the manuscript. Instead, revise your manuscript
and upload a new version through your Author Centre.
When submitting your revised manuscript, you will be able to respond to the comments made by
the referees and upload a file "Response to Referees" in "Section 6 - File Upload". You can use this
to document any changes you make to the original manuscript. In order to expedite the
processing of the revised manuscript, please be as specific as possible in your response to the
referees. We strongly recommend uploading two versions of your revised manuscript:
1) Identifying all the changes that have been made (for instance, in coloured highlight, in bold
text, or tracked changes);
2) A 'clean' version of the new manuscript that incorporates the changes made, but does not
highlight them.
When uploading your revised files please make sure that you have:
1) A text file of the manuscript (tex, txt, rtf, docx or doc), references, tables (including captions)
and figure captions. Do not upload a PDF as your "Main Document";
2) A separate electronic file of each figure (EPS or print-quality PDF preferred (either format
should be produced directly from original creation package), or original software format);
3) Included a 100 word media summary of your paper when requested at submission. Please
ensure you have entered correct contact details (email, institution and telephone) in your user
account;
4) Included the raw data to support the claims made in your paper. You can either include your
data as electronic supplementary material or upload to a repository and include the relevant doi
within your manuscript. Make sure it is clear in your data accessibility statement how the data
can be accessed;
5) All supplementary materials accompanying an accepted article will be treated as in their final
form. Note that the Royal Society will neither edit nor typeset supplementary material and it will
be hosted as provided. Please ensure that the supplementary material includes the paper details
where possible (authors, article title, journal name).
Supplementary files will be published alongside the paper on the journal website and posted on
the online figshare repository (https://rs.figshare.com/). The heading and legend provided for
each supplementary file during the submission process will be used to create the figshare page,
so please ensure these are accurate and informative so that your files can be found in searches.
Files on figshare will be made available approximately one week before the accompanying article
so that the supplementary material can be attributed a unique DOI.
Please note that Royal Society Open Science charge article processing charges for all new
submissions that are accepted for publication. Charges will also apply to papers transferred to
Royal Society Open Science from other Royal Society Publishing journals, as well as papers
submitted as part of our collaboration with the Royal Society of Chemistry
(http://rsos.royalsocietypublishing.org/chemistry).
If your manuscript is newly submitted and subsequently accepted for publication, you will be
asked to pay the article processing charge, unless you request a waiver and this is approved by
7
Royal Society Publishing. You can find out more about the charges at
http://rsos.royalsocietypublishing.org/page/charges. Should you have any queries, please
contact openscience@royalsociety.org.
Once again, thank you for submitting your manuscript to Royal Society Open Science and I look
forward to receiving your revision. If you have any questions at all, please do not hesitate to get
in touch.
Kind regards,
Royal Society Open Science Editorial Office
Royal Society Open Science
openscience@royalsociety.org
on behalf of Professor Antonia Hamilton (Subject Editor)
openscience@royalsociety.org
Reviewer comments to Author:
Reviewer: 1
Comments to the Author(s)
I rarely review papers this well-written, compelling, and consequential for improving
psychological research. This is truly outstanding work, and will make a major contribution to
increasing the state of the art in cross-cultural research.
I have a few minor comments and speculations.
1) I was intrigued by the authors' speculation that there is cultural variation in norms
surrounding generosity (that giving to those without need is seen as a sign of superiority for
example). I think this is worth elaborating on further, for each of the populations. One thing that
occurred to me re the results for the U.S. sample, for example, is that it is common for middle
class U.S. adults (including college students) to live away from those closest to them. And thus
gift giving/generosity may serve a different function (to solidify social bonds in the absence of
physical contact).
I think it is also worth commenting (in the final reflections) that research of this kind is
tremendously time and labor intensive, and takes longer to publish (utterly worth doing, but this
is one of the primary reasons it isn't getting done). The expectations for high volume publication
in psychology is actively undermining doing the kind of work this research requires, and the
result is that the field is producing a large volume of low quality papers. Why has replication
gotten more traction that the "diversity problem"? The field is using the same methods with the
same convenience samples to tackle replication.
Lack of training in conducting research outside of WEIRD samples within psychology is a major
obstacle as well. Note that the authors are unusual in that they are trained in multiple disciplines
(including psychology).
The need for convergent methods is worth mentioning I think. Asking participants to explain is a
good example of the benefits of multiple measures, but I think developing multiple methods to
study a construct/process is the gold standard (and rarely done in psychological research).
8
Reviewer: 2
Comments to the Author(s)
My overall assessment is that this is a strong article worthy of publication. It articulates
generalizability concerns clearly and provides a compelling specific example of the non-
universality of a well-established finding. I include here a set of questions, critiques, and
suggestions for the authors to consider, should a revision of this manuscript be requested.
The authors seem to suggest generalizability should always be a primary concern on par with
replicability. I enjoy the “not optional” assertion, but wonder if there is a reasonable ordering of
these two research goals, even if just a temporal order. Isn’t it appropriate that replicability be
established prior to generalizability? Why attempt to generalize a finding that very well may be
spurious even in the original samples tested? Some discussion of this issue may strengthen the
paper.
On page 4, starting on line 45. The authors ask why generalizability hasn’t captured the same
attention as replicability concerns in the field. I think it’s quite obvious. BEM (discovery of the
metaphysically impossible) and Stapel (blatant and widespread fraud) are such glaring and
powerful evidence of a sick system. I would argue that generalizability concerns likely can’t have
similarly stark demonstrations of fundamental problems (much to the detriment of attention
focused on them). This doesn’t make generalizability any less important in my mind, but I think
it’s perfectly understandable that replicability has captured more attention given the direct and
mind-blowing demonstrations of problems in that sphere. I suggest removing this section.
The authors seem to occasionally conflate (or at least write with a lack of precision about)
convenience samples vs. deeper community samples and WEIRD/nonWEIRD cultures. As you
argue in the discussion, simply moving into nonWEIRD countries and then collecting
convenience samples of college students is only one step in the right direction. I suggest making
the distinction more clearly in the introduction and giving each concern to get a bit of
independent coverage (highlight problems with convenience samples and WEIRD samples as
distinct issues).
I think the authors should discuss further the possible sources of inconsistency in Bangladesh and
Indonesia responding? This seems the most problematic element of the empirical portions of the
paper, and I was a bit dissatisfied with the lack of attempted explanation. I want to be clear,
though: I’m not asking you to HARK! If it’s a real head-scratcher, fine!
The sample size planning is inconsistent the rationale is weak (mirroring earlier studies was
given as one justification). This is simply a criticism. Not much can be done now and this isn’t a
“fatal” flaw in my mind.
Consider toning down the terminology such as “inconceivable” sample sizes with the PSA.
Others have run massive studies. These sample sizes will just become more common now with
the PSA.
As I mentioned above, I really love to point about PSA not being a panacea. We need “deep”
samples moving away from college students to complement our “broad” samples collected in
many countries. No suggestion, just a comment :)
I’m glad you included coverage of COGs. The discussion could highlight how consistent use of
COGs could be an excellent mechanism for promoting tests of generalizability. By explicitly
9
specifying COGs, researchers essentially welcome others to follow-up on their work with tests of
phenomena in other samples and settings.
Given the focus on making tests of generalizability with non-convenience samples in non-WEIRD
countries, I have a nagging concern and general complaint. As the authors themselves point out,
collecting these data is very time and resource intensive. One of the best ways to incentivize more
of this work is to provide academic incentives for the researchers who help conduct such studies
in the local communities. The authors acknowledge the support they received collecting data in
Indonesia and mention that the Bangladeshi team provided “valuable contributions at all phases
of the Bangladesh study.” I find it odd that none of these individuals appear as authors on this
submission. It sounds like some of these induvial contributed across multiple CRediT taxonomy
categories, and could be considered for authorship. If we hope to increase “deep” work on
generalizability and move beyond convenience samples in nonWEIRD countries, incentivizing
the work of our local collaborators should be prioritized.
Summary statement: this article is worth publishing. It is a renewed call for more work on
generalizability of psychological science accompanied by an open empirical puzzle, and could
serve to spur on further cross-cultural work on this and other phenomena. I suggest acceptance
following minor revisions.
Christopher R. Chartier
Author's Response to Decision Letter for (RSOS-181386.R0)
See Appendix A.
label_end_comment
Decision letter (RSOS-181386.R1)
05-Feb-2019
Dear Dr Tiokhin,
I am pleased to inform you that your manuscript entitled "Generalizability is not optional:
Insights from a cross-cultural study of social discounting" is now accepted for publication in
Royal Society Open Science.
You can expect to receive a proof of your article in the near future. Please contact the editorial
office (openscience_proofs@royalsociety.org and openscience@royalsociety.org) to let us know if
you are likely to be away from e-mail contact. Due to rapid publication and an extremely tight
schedule, if comments are not received, your paper may experience a delay in publication.
Royal Society Open Science operates under a continuous publication model
(http://bit.ly/cpFAQ). Your article will be published straight into the next open issue and this
will be the final version of the paper. As such, it can be cited immediately by other researchers.
As the issue version of your paper will be the only version to be published I would advise you to
check your proofs thoroughly as changes cannot be made once the paper is published.
10
On behalf of the Editors of Royal Society Open Science, we look forward to your continued
contributions to the Journal.
Kind regards,
Royal Society Open Science Editorial Office
Royal Society Open Science
openscience@royalsociety.org
on behalf of Professor Antonia Hamilton (Subject Editor)
openscience@royalsociety.org
Follow Royal Society Publishing on Twitter: @RSocPublishing
Follow Royal Society Publishing on Facebook:
https://www.facebook.com/RoyalSocietyPublishing.FanPage/
Read Royal Society Publishing's blog: https://blogs.royalsociety.org/publishing/
Appendix A
Dear Dr Tiokhin,
On behalf of the Editors, I am pleased to inform you that your Manuscript
label_version_2
RSOS-181386 entitled "Generalizability is not optional: Insights from a cross-
cultural study of social discounting" has been accepted for publication
in Royal Society Open Science subject to minor revision in accordance with
the referee suggestions. Please find the referees' comments at the end of this
email.
The reviewers and handling editors have recommended publication, but also
suggest some minor revisions to your manuscript. Therefore, I invite you to
respond to the comments and revise your manuscript.
Thank you. We have modified out manuscript in accordance with the
suggestions of the Reviewers.
Reviewer 1
Comments to the Author(s)
I rarely review papers this well-written, compelling, and consequential for
improving psychological research. This is truly outstanding work, and will
make a major contribution to increasing the state of the art in cross-cultural
research. I have a few minor comments and speculations.
We thank Reviewer 1 for their positive assessment and comments.
1) I was intrigued by the authors' speculation that there is cultural variation in
norms surrounding generosity (that giving to those without need is seen as a
sign of superiority for example). I think this is worth elaborating on further,
for each of the populations. One thing that occurred to me re the results for the
U.S. sample, for example, is that it is common for middle class U.S. adults
(including college students) to live away from those closest to them. And thus
gift giving/generosity may serve a different function (to solidify social bonds
in the absence of physical contact).
We have elaborated further on the generosity-norms that may affect social
discounting (lines 507-514).
I think it is also worth commenting (in the final reflections) that research of
this kind is tremendously time and labor intensive, and takes longer to publish
(utterly worth doing, but this is one of the primary reasons it isn't getting
done). The expectations for high volume publication in psychology is actively
undermining doing the kind of work this research requires, and the result is
that the field is producing a large volume of low quality papers. Why has
replication gotten more traction that the "diversity problem"? The field is
using the same methods with the same convenience samples to tackle
replication.
We agree that this is a problem, and we briefly mention the difficulty of cross-
cultural research (550-556). We considered discussing the extent to which
changing criteria for judging scholars would incentivize cross-cultural
research. However, we feel that this would require substantial additional
discussion and is beyond the scope of our paper.
Lack of training in conducting research outside of WEIRD samples within
psychology is a major obstacle as well. Note that the authors are unusual in
that they are trained in multiple disciplines (including psychology).
We agree completely.
The need for convergent methods is worth mentioning I think. Asking
participants to explain is a good example of the benefits of multiple measures,
but I think developing multiple methods to study a construct/process is the
gold standard (and rarely done in psychological research).
We agree completely and we mention the importance of convergent methods
at several points in the paper (lines 68-77, 396-398, 478-480). We have added
an additional reference to other experimental economic games conducted in
rural Fiji that found a similar finding to our study: sharing is primarily
determined by relative need (lines 481-483).
Reviewer: 2
Comments to the Author(s)
My overall assessment is that this is a strong article worthy of publication. It
articulates generalizability concerns clearly and provides a compelling
specific example of the non-universality of a well-established finding. I
include here a set of questions, critiques, and suggestions for the authors to
consider, should a revision of this manuscript be requested.
We thank Reviewer 2 for this positive assessment and their comments.
The authors seem to suggest generalizability should always be a primary
concern on par with replicability. I enjoy the “not optional” assertion, but
wonder if there is a reasonable ordering of these two research goals, even if
just a temporal order. Isn’t it appropriate that replicability be established prior
to generalizability? Why attempt to generalize a finding that very well may be
spurious even in the original samples tested? Some discussion of this issue
may strengthen the paper.
This is an important point. We discuss cases in which generalizability is not
worth pursuing and cases where it is more important on lines 529-537. We do
not think that there is a general rule for a temporal order generalizability vs
establishing reliability, because establishing reliability also depends on the
population in which one tests an effect (e.g. an effect may not be reliable in
U.S. undergrads but be an important aspect of human nature more broadly).
On page 4, starting on line 45. The authors ask why generalizability hasn’t
captured the same attention as replicability concerns in the field. I think it’s
quite obvious. BEM (discovery of the metaphysically impossible) and Stapel
(blatant and widespread fraud) are such glaring and powerful evidence of a
sick system. I would argue that generalizability concerns likely can’t have
similarly stark demonstrations of fundamental problems (much to the
detriment of attention focused on them). This doesn’t make generalizability
any less important in my mind, but I think it’s perfectly understandable that
replicability has captured more attention given the direct and mind-blowing
demonstrations of problems in that sphere. I suggest removing this section.
Thanks for this suggestion. We in-part share this intuition. However, by one
metric (citations) this doesn’t appear to be the case. The Henrich et al.
WEIRD paper has been cited over 5000+ times, compared to the 3000+
citations of Simmons et al. False-Positive Psychology. So it seems like the
WEIRD paper has caught people’s attention plenty, but that nonetheless little
change has happened.
Additionally, Reviewer 1 thought that it was obvious that the reason
generalizability hasn’t captured the same attention was not due to a difference
in the glaring/powerful evidence, but rather that current incentives for high-
volume output explain the difference:
“The expectations for high volume publication in psychology is actively
undermining doing the kind of work this research requires, and the result is
that the field is producing a large volume of low-quality papers. Why has
replication gotten more traction that the "diversity problem"? The field is
using the same methods with the same convenience samples to tackle
replication.”
Instead of speculating about the causes of the lack of attention to
generalizability, we hope that this paragraph encourages productive discussion
among psychologists on this topic.
The authors seem to occasionally conflate (or at least write with a lack of
precision about) convenience samples vs. deeper community samples and
WEIRD/nonWEIRD cultures. As you argue in the discussion, simply moving
into nonWEIRD countries and then collecting convenience samples of college
students is only one step in the right direction. I suggest making the distinction
more clearly in the introduction and giving each concern to get a bit of
independent coverage (highlight problems with convenience samples and
WEIRD samples as distinct issues).
We have clarified in the introduction that convenience samples are not
synonymous with WEIRD samples. As the reviewer alludes to, there is also
an important problem of deeper engagement with communities and situating
findings in their context. This is an important point, but is not the main point
of this paper, and we worry that it will distract from the discussion of
generalizability.
I think the authors should discuss further the possible sources of inconsistency
in Bangladesh and Indonesia responding? This seems the most problematic
element of the empirical portions of the paper, and I was a bit dissatisfied with
the lack of attempted explanation. I want to be clear, though: I’m not asking
you to HARK! If it’s a real head-scratcher, fine!
We discussed this substantially in the SI (see “Inconsistent Responding
Across Sites) and have clarified the discussion of this in the main text (lines
437 – 449).
The sample size planning is inconsistent the rationale is weak (mirroring
earlier studies was given as one justification). This is simply a criticism. Not
much can be done now and this isn’t a “fatal” flaw in my mind.
We agree, but we chose to be transparent about the actual reasons for
choosing samples of varying sizes instead of generating a post-hoc rationale.
Consider toning down the terminology such as “inconceivable” sample sizes
with the PSA. Others have run massive studies. These sample sizes will just
become more common now with the PSA.
We have changed “previously inconceivable” to “large”.
As I mentioned above, I really love to point about PSA not being a panacea.
We need “deep” samples moving away from college students to complement
our “broad” samples collected in many countries. No suggestion, just a
comment :)
Thank you for this comment.
I’m glad you included coverage of COGs. The discussion could highlight how
consistent use of COGs could be an excellent mechanism for promoting tests
of generalizability. By explicitly specifying COGs, researchers essentially
welcome others to follow-up on their work with tests of phenomena in other
samples and settings.
We have now explained what COGs are and why they are useful in the main
text. We have also moved our COG statement into the main text (lines 589 –
616).
Given the focus on making tests of generalizability with non-convenience
samples in non-WEIRD countries, I have a nagging concern and general
complaint. As the authors themselves point out, collecting these data is very
time and resource intensive. One of the best ways to incentivize more of this
work is to provide academic incentives for the researchers who help conduct
such studies in the local communities. The authors acknowledge the support
they received collecting data in Indonesia and mention that the Bangladeshi
team provided “valuable contributions at all phases of the Bangladesh study.”
I find it odd that none of these individuals appear as authors on this
submission. It sounds like some of these induvial contributed across multiple
CRediT taxonomy categories, and could be considered for authorship. If we
hope to increase “deep” work on generalizability and move beyond
convenience samples in nonWEIRD countries, incentivizing the work of our
local collaborators should be prioritized.
We agree that adding local contributors as authors is one way to incentivize
their engagement in such research. Our research group has included the local
contributors from Bangladesh on papers that have focused on research at that
site:
Hruschka, D. J., Munira, S., Jesmin, K., Hackman, J., & Tiokhin, L. (2018).
Learning from failures of protocol in cross-cultural research. Proceedings of
the National Academy of Sciences, 115(45), 11428-11434.
Hackman, J., Munira, S., Jasmin, K., & Hruschka, D. (2017). Revisiting
psychological mechanisms in the anthropology of altruism. Human
Nature, 28(1), 76-91.
In retrospect, it was a mistake not to include the local contributors in
Bangladesh as co-authors on this paper. We thank Reviewer 2 for alerting us
to this. The local contributors in Bangladesh have now reviewed the
manuscript and given their approval to be added as co-authors. We have also
amended the contributions section to reflect each author’s contribution.
Summary statement: this article is worth publishing. It is a renewed call for
more work on generalizability of psychological science accompanied by an
open empirical puzzle, and could serve to spur on further cross-cultural work
on this and other phenomena. I suggest acceptance following minor revisions.
Society Open
