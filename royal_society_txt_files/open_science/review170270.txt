How effective is incidental learning of the shape of
probability distributions?
Randy Tran, Edward Vul and Harold Pashler
Article citation details
R. Soc. open sci. 4: 170270.
http://dx.doi.org/10.1098/rsos.170270
Review timeline
Original submission: 29 March 2017 Note: Reports are unedited and appear as
1st revised submission: 16 May 2017 submitted by the referee. The review history
2nd revised submission: 30 June 2017 appears in chronological order.
Final acceptance: 3 July 2017
Review History
label_version_1
RSOS-170270.R0 (Original submission)
label_author_1
Review form: Reviewer 1 (Bernhard Spitzer)
Is the manuscript scientifically sound in its present form?
Yes
Are the interpretations and conclusions justified by the results?
Yes
Is the language acceptable?
Yes
Is it clear how to access all supporting data?
Yes
Do you have any ethical concerns with this paper?
No
Have you any concerns about statistical analyses in this paper?
No
© 2017 The Authors. Published by the Royal Society under the terms of the Creative Commons
Attribution License http://creativecommons.org/licenses/by/4.0/, which permits unrestricted use,
provided the original author and source are credited
2
Recommendation?
label_recommendation_1
Major revision is needed (please make suggestions in comments)
Comments to the Author(s)
label_comment_1
This is an inventive study that sets out to address a very interesting question, how effectively
participants learn bimodal stimulus distributions, without (Exp 1 & 3) or with (Exp 2) explicit
instructions to do so. Participants’ reproduction of bimodal distributions of spatial locations was
remarkably poor, suggesting that learning was less effective than suggested in previous work.
The manuscript is relatively short, for the most part well-written, and has been an enjoyable read.
However, I do have several concerns about the experimental approach and its adequateness in
addressing the research question under study.
(1) One concern is about the primary learning task (the adaptation of ‘whack-a-mole’ game,
Phase 1) in the present context. The task is motivated as requiring “close attention to its location
as the object’s sole action-relevant property” (p 7). However, relevant to spatial distribution
learning, does the task indeed require attention to (or even knowledge of) the object’s location
within the full screen area/width? (which is the reference frame of the experimenter-manipulated
distribution, if I understand correctly).
The concern arises in combination with the (I presume) mouse-based implementation of the
game, for which several methodological details are missing: Did the mouse-pointer return to a
fixed location after each click (e.g. the screen center? and was it warranted that participants
looked at/focused on a fixed reference point before every new trial?). The concern is that the
task-relevant location per trial, from the perspective of a subject performing the task, was relative
to the last/current mouse position (which also defined the required action), which would vary
trial-by-trial. In other words, is it warranted that the experimenter-manipulated distribution and
the participant-processed locations resided in the same coordinate system? (similar to distinctions
between ego- vs. allocentric coordinates in spatial processing)?
A similar concern arises for the testing/reproduction phase (Phase 2) – might participants have
tried to reproduce clicks (locations) relative to the previous click location, rather than relative to
the full screen area ? Perhaps, this could even be tested; i.e. whether the distribution of
reproduced location distances (from the previous location) might have mimicked in some way
that of the learning phase? (even if this distribution may not necessarily be strongly bimodal?)
(2) Another point concerns the testing procedure itself (producing 180 clicks). The authors
consider it unlikely that this procedure was a significant contributor to the observed outcome,
given clear positive findings in Exp 3 (p 16). However, considering the extreme/discrete stimulus
distribution in Experiment 3 (Figure 3), the reproduction is rather poor (Figure 3). Unless it is
shown directly that participants can (could) indeed reproduce more detailed distributional
knowledge with this particular method (as would presumably be required to reproduce the
distribution in Exp 1&2), I tend to think that the testing procedure remains a major limiting factor
to the present results, and should be discussed as such.
Also here, several relevant details are missing: Did participants know how many clicks could be
made in total? Was there a counter/countdown so that participants knew how many clicks were
left? Were they allowed to count? How fast could they click, was there a forced wait time
between clicks?
(3) Instructions (as a disclaimer, English is not my first language), however, I have difficulties
seeing how the verbatim quotes on p 9 communicate the key aspect of the to-be-reproduced
distribution (the relative frequencies of disks per location, if I understand correctly). The
phrasings emphasize concepts like ‘sequence’ or ‘pattern’, which in my reading may lead
subjects to ideas of serial order, relative positions (disk-by-disk, see above) and other things.
Again, I am not a native English speaker, but can we be confident that participants understood
3
the relevant concept of ‘distribution’ (and again, in the reference frame of the full screen area, see
above) when being tested?
Related to this point, I got confused about the levels of ‘unintentional’ (title) and ‘implicit’
(running title). In my reading of the introduction, the study sets out to test if distributional
knowledge can be unintentionally acquired, and explicitly reproduced (when prompted to do so).
The testing instructions and -task got me confused whether the reproduction was intended to be
implicit as well, i.e. whether reproduced locations would reflect distributional knowledge
without subjects being aware of the relevant dimension of the distribution? The authors may
wish to clarify these points in writing and interpretation.
(4) Performance measures for Phase 1 seem to be missing and should be reported: What were the
hit rates, the average hit streak time, and did these depend on location (in absolute and/or
relative terms, see above)? How did the distribution of clicks (not disks), including hits and
misses in Phase 1 look like? Finally, did Phase 1 performance predict performance in Phase 2
(across subjects)?
Further/minor comments:
-Was there anything displayed on screen besides the disks (and the streak counters?). I.e. any
fixation cross, orientation aid, grid, etc?
-Which colour did the disks have?
-What were the dimensions (and resolution) of the screen?
-Since almost the full width of the screen was used (p 8), were participants free to move their
head (or only their eyes)?
-Just to be sure (this is not explicitly stated in methods), is it correct that all stimuli were
presented at the same vertical position, i.e. all along an imaginary horizontal ‘line’? (a
figure/schematic illustration of the task/display might be helpful for readers)
-The advantage of a single fixed sequence (p 8 l 1 and Supplement) is unclear to me – wouldn’t it
be more desirable to generalize results across different sequences/serial orders? Did this one
sequence meet any special criteria? Were characteristics of the two sequences (used in
Experiments 1&2 vs. 3) matched in some way? This seems relevant as the results differed just
between those Experiments (1&2 vs. 3) between which the sequences differed, too. In this
context, the authors might wish to consider potentially distinct serial order effects especially in
integrating spatial locations (Hubert-Wallander and Boynton, 2015, J Vis).
-p9 “..do NOT click in the same spot over and over.” Was the learning sequence design (Phase 1)
subjected to similar (or any) restrictions?
-There was no break between Phase 1 and Phase 2 (p8 bottom). Is there a possibility that in
remembering/keeping track of already-produced locations, participants might easily have
confused Phase 1 and (late) Phase 2 clicks in memory? Relatedly, did Phase 2 responses perhaps
mirror only late (and or early) portions of Phase 1?
-The results report is very descriptive and should be substantiated by statistical analysis and/or
formal model comparison. Are the differences in learning (Figure 5) between experiments
significant, after accounting for the differences in Phase 1 distribution?
Another intuitive (though somewhat less elegant) approach to statistical testing might be to
calculate simple correlations between the Phase 1 and Phase 2 distributions
4
-Given potential issues with the Phase 2 task, were participants informally debriefed after the
experiment? One would be curious to know what participants had responded when asked
directly, e.g. about regions on the screen where they thought disks occurred more frequently than
in others.
I sign my reviews, faithfully submitted
Bernhard Spitzer
label_author_2
Review form: Reviewer 2 (Ulrik Beierholm)
Is the manuscript scientifically sound in its present form?
Yes
Are the interpretations and conclusions justified by the results?
Yes
Is the language acceptable?
Yes
Is it clear how to access all supporting data?
Yes
Do you have any ethical concerns with this paper?
No
Have you any concerns about statistical analyses in this paper?
No
Recommendation?
label_recommendation_2
Accept with minor revision (please list in comments)
Comments to the Author(s)
label_comment_2
Reviewer: Ulrik Beierholm, Durham University, UK
Tran et al. examine experimentally how human subjects learn a distributions of locations and
provide a simple descriptive model of this process.
The paper can be seen as a response to previous work on learning of probabilistic priors
(including work by my collaborators and myself).
The paper is very well written and provide interesting results.
I like the devil and angel on each shoulder approach (the authors are welcome to decide who
plays which role) , debating the merits of the data.
The modeling is nice and I believe essential for the claims of the paper. However I would like to
see some more details provided, e.g. how the fitting of k was done.
It should also be explained what x_i refers to (if obvious).
One major issue here is why subjects are able to learn some distributions (simple unimodal,
clearly bimodal), but not others (more ambiguously bimodal). As suggested by the authors this
may just be because subjects have a bias towards learning unimodal distributions.
It should be possible to test for strong unimodal bias through modeling of subject learning. One
possibility would be that subjects do explicit hypotheses testing of the number of modes of the
5
distribution, with a subject specific prior preference for a single mode (as suggested by the
authors). This might provide support for a single mode in experiments 1 and 2, dual mode in
experiment 3.
This might help to disambiguate the issue of whether subject are merely better at learning the
further the distributions can be separated, or is it truly the discreteness (no values that span the
gap)
The authors highlight similarities (that I agree with) with our recent work (Sanborn & Beierholm
2016, note spelling), but there is a difference in that although the bimodal distribution was similar
across the studies (exp 1 and 2 in this study), the previous work did find evidence for bimodal
learning. This could be due to the use of explicit discrete numerical feedback, or indeed more
clearly distinguishable dual modes. I would be curious if the authors had any thoughts on this.
A side note, figure 5 (exp 1 and 2) shows that the likelihood of the kernel precision is essentially
flat over a very wide range (-10 to -1) implying that any of those values could provide equally
good fit. Using a prior (assigned or found through expectation maximisation) would avoid such
an issue. In practice the authors are not interested in exact parameter fits (just whether they are
below positive or negative), but it may be relevant for future work.
label_end_comment
Decision letter (RSOS-170270)
25th April 2017
Dear Mr Tran,
The editors assigned to your paper ("How Effective is Incidental Learning of the Shape of
Probability Distributions?") have now received comments from reviewers. We would like you to
revise your paper in accordance with the referee and Associate Editor suggestions which can be
found below (not including confidential reports to the Editor). Please note this decision does not
guarantee eventual acceptance.
Please submit a copy of your revised paper within three weeks (i.e. by the 17th May 2017). If we
do not hear from you within this time then it will be assumed that the paper has been withdrawn.
In exceptional circumstances, extensions may be possible if agreed with the Editorial Office in
advance.We do not allow multiple rounds of revision so we urge you to make every effort to
fully address all of the comments at this stage. If deemed necessary by the Editors, your
manuscript will be sent back to one or more of the original reviewers for assessment. If the
original reviewers are not available we may invite new reviewers.
To revise your manuscript, log into http://mc.manuscriptcentral.com/rsos and enter your
Author Centre, where you will find your manuscript title listed under "Manuscripts with
Decisions." Under "Actions," click on "Create a Revision." Your manuscript number has been
appended to denote a revision. Revise your manuscript and upload a new version through your
Author Centre.
When submitting your revised manuscript, you must respond to the comments made by the
referees and upload a file "Response to Referees" in "Section 6 - File Upload". Please use this to
document how you have responded to the comments, and the adjustments you have made. In
order to expedite the processing of the revised manuscript, please be as specific as possible in
your response.
6
In addition to addressing all of the reviewers' and editor's comments please also ensure that your
revised manuscript contains the following sections as appropriate before the reference list:
• Ethics statement (if applicable)
If your study uses humans or animals please include details of the ethical approval received,
including the name of the committee that granted approval. For human studies please also detail
whether informed consent was obtained. For field studies on animals please include details of all
permissions, licences and/or approvals granted to carry out the fieldwork.
• Data accessibility
It is a condition of publication that all supporting data are made available either as
supplementary information or preferably in a suitable permanent repository. The data
accessibility section should state where the article's supporting data can be accessed. This section
should also include details, where possible of where to access other relevant research materials
such as statistical tools, protocols, software etc can be accessed. If the data have been deposited in
an external repository this section should list the database, accession number and link to the DOI
for all data from the article that have been made publicly available. Data sets that have been
deposited in an external repository and have a DOI should also be appropriately cited in the
manuscript and included in the reference list.
If you wish to submit your supporting data or code to Dryad (http://datadryad.org/), or modify
your current submission to dryad, please use the following link:
http://datadryad.org/submit?journalID=RSOS&manu=RSOS-170270
• Competing interests
Please declare any financial or non-financial competing interests, or state that you have no
competing interests.
• Authors’ contributions
All submissions, other than those with a single author, must include an Authors’ Contributions
section which individually lists the specific contribution of each author. The list of Authors
should meet all of the following criteria; 1) substantial contributions to conception and design, or
acquisition of data, or analysis and interpretation of data; 2) drafting the article or revising it
critically for important intellectual content; and 3) final approval of the version to be published.
All contributors who do not meet all of these criteria should be included in the
acknowledgements.
We suggest the following format:
AB carried out the molecular lab work, participated in data analysis, carried out sequence
alignments, participated in the design of the study and drafted the manuscript; CD carried out
the statistical analyses; EF collected field data; GH conceived of the study, designed the study,
coordinated the study and helped draft the manuscript. All authors gave final approval for
publication.
• Acknowledgements
Please acknowledge anyone who contributed to the study but did not meet the authorship
criteria.
• Funding statement
Please list the source of funding for each author.
7
Once again, thank you for submitting your manuscript to Royal Society Open Science and I look
forward to receiving your revision. If you have any questions at all, please do not hesitate to get
in touch.
Yours sincerely,
Alice Power
Editorial Coordinator
Royal Society Open Science
on behalf of Essi Viding
Subject Editor, Royal Society Open Science
openscience@royalsociety.org
Comments to Author:
Reviewers' Comments to Author:
Reviewer: 1
Comments to the Author(s)
This is an inventive study that sets out to address a very interesting question, how effectively
participants learn bimodal stimulus distributions, without (Exp 1 & 3) or with (Exp 2) explicit
instructions to do so. Participants’ reproduction of bimodal distributions of spatial locations was
remarkably poor, suggesting that learning was less effective than suggested in previous work.
The manuscript is relatively short, for the most part well-written, and has been an enjoyable read.
However, I do have several concerns about the experimental approach and its adequateness in
addressing the research question under study.
(1) One concern is about the primary learning task (the adaptation of ‘whack-a-mole’ game,
Phase 1) in the present context. The task is motivated as requiring “close attention to its location
as the object’s sole action-relevant property” (p 7). However, relevant to spatial distribution
learning, does the task indeed require attention to (or even knowledge of) the object’s location
within the full screen area/width? (which is the reference frame of the experimenter-manipulated
distribution, if I understand correctly).
The concern arises in combination with the (I presume) mouse-based implementation of the
game, for which several methodological details are missing: Did the mouse-pointer return to a
fixed location after each click (e.g. the screen center? and was it warranted that participants
looked at/focused on a fixed reference point before every new trial?). The concern is that the
task-relevant location per trial, from the perspective of a subject performing the task, was relative
to the last/current mouse position (which also defined the required action), which would vary
trial-by-trial. In other words, is it warranted that the experimenter-manipulated distribution and
the participant-processed locations resided in the same coordinate system? (similar to distinctions
between ego- vs. allocentric coordinates in spatial processing)?
A similar concern arises for the testing/reproduction phase (Phase 2) – might participants have
tried to reproduce clicks (locations) relative to the previous click location, rather than relative to
the full screen area ? Perhaps, this could even be tested; i.e. whether the distribution of
reproduced location distances (from the previous location) might have mimicked in some way
that of the learning phase? (even if this distribution may not necessarily be strongly bimodal?)
(2) Another point concerns the testing procedure itself (producing 180 clicks). The authors
consider it unlikely that this procedure was a significant contributor to the observed outcome,
given clear positive findings in Exp 3 (p 16). However, considering the extreme/discrete stimulus
distribution in Experiment 3 (Figure 3), the reproduction is rather poor (Figure 3). Unless it is
shown directly that participants can (could) indeed reproduce more detailed distributional
8
knowledge with this particular method (as would presumably be required to reproduce the
distribution in Exp 1&2), I tend to think that the testing procedure remains a major limiting factor
to the present results, and should be discussed as such.
Also here, several relevant details are missing: Did participants know how many clicks could be
made in total? Was there a counter/countdown so that participants knew how many clicks were
left? Were they allowed to count? How fast could they click, was there a forced wait time
between clicks?
(3) Instructions (as a disclaimer, English is not my first language), however, I have difficulties
seeing how the verbatim quotes on p 9 communicate the key aspect of the to-be-reproduced
distribution (the relative frequencies of disks per location, if I understand correctly). The
phrasings emphasize concepts like ‘sequence’ or ‘pattern’, which in my reading may lead
subjects to ideas of serial order, relative positions (disk-by-disk, see above) and other things.
Again, I am not a native English speaker, but can we be confident that participants understood
the relevant concept of ‘distribution’ (and again, in the reference frame of the full screen area, see
above) when being tested?
Related to this point, I got confused about the levels of ‘unintentional’ (title) and ‘implicit’
(running title). In my reading of the introduction, the study sets out to test if distributional
knowledge can be unintentionally acquired, and explicitly reproduced (when prompted to do so).
The testing instructions and -task got me confused whether the reproduction was intended to be
implicit as well, i.e. whether reproduced locations would reflect distributional knowledge
without subjects being aware of the relevant dimension of the distribution? The authors may
wish to clarify these points in writing and interpretation.
(4) Performance measures for Phase 1 seem to be missing and should be reported: What were the
hit rates, the average hit streak time, and did these depend on location (in absolute and/or
relative terms, see above)? How did the distribution of clicks (not disks), including hits and
misses in Phase 1 look like? Finally, did Phase 1 performance predict performance in Phase 2
(across subjects)?
Further/minor comments:
-Was there anything displayed on screen besides the disks (and the streak counters?). I.e. any
fixation cross, orientation aid, grid, etc?
-Which colour did the disks have?
-What were the dimensions (and resolution) of the screen?
-Since almost the full width of the screen was used (p 8), were participants free to move their
head (or only their eyes)?
-Just to be sure (this is not explicitly stated in methods), is it correct that all stimuli were
presented at the same vertical position, i.e. all along an imaginary horizontal ‘line’? (a
figure/schematic illustration of the task/display might be helpful for readers)
-The advantage of a single fixed sequence (p 8 l 1 and Supplement) is unclear to me – wouldn’t it
be more desirable to generalize results across different sequences/serial orders? Did this one
sequence meet any special criteria? Were characteristics of the two sequences (used in
Experiments 1&2 vs. 3) matched in some way? This seems relevant as the results differed just
between those Experiments (1&2 vs. 3) between which the sequences differed, too. In this
context, the authors might wish to consider potentially distinct serial order effects especially in
integrating spatial locations (Hubert-Wallander and Boynton, 2015, J Vis).
9
-p9 “..do NOT click in the same spot over and over.” Was the learning sequence design (Phase 1)
subjected to similar (or any) restrictions?
-There was no break between Phase 1 and Phase 2 (p8 bottom). Is there a possibility that in
remembering/keeping track of already-produced locations, participants might easily have
confused Phase 1 and (late) Phase 2 clicks in memory? Relatedly, did Phase 2 responses perhaps
mirror only late (and or early) portions of Phase 1?
-The results report is very descriptive and should be substantiated by statistical analysis and/or
formal model comparison. Are the differences in learning (Figure 5) between experiments
significant, after accounting for the differences in Phase 1 distribution?
Another intuitive (though somewhat less elegant) approach to statistical testing might be to
calculate simple correlations between the Phase 1 and Phase 2 distributions
-Given potential issues with the Phase 2 task, were participants informally debriefed after the
experiment? One would be curious to know what participants had responded when asked
directly, e.g. about regions on the screen where they thought disks occurred more frequently than
in others.
I sign my reviews, faithfully submitted
Bernhard Spitzer
Reviewer: 2
Comments to the Author(s)
Reviewer: Ulrik Beierholm, Durham University, UK
Tran et al. examine experimentally how human subjects learn a distributions of locations and
provide a simple descriptive model of this process.
The paper can be seen as a response to previous work on learning of probabilistic priors
(including work by my collaborators and myself).
The paper is very well written and provide interesting results.
I like the devil and angel on each shoulder approach (the authors are welcome to decide who
plays which role) , debating the merits of the data.
The modeling is nice and I believe essential for the claims of the paper. However I would like to
see some more details provided, e.g. how the fitting of k was done.
It should also be explained what x_i refers to (if obvious).
One major issue here is why subjects are able to learn some distributions (simple unimodal,
clearly bimodal), but not others (more ambiguously bimodal). As suggested by the authors this
may just be because subjects have a bias towards learning unimodal distributions.
It should be possible to test for strong unimodal bias through modeling of subject learning. One
possibility would be that subjects do explicit hypotheses testing of the number of modes of the
distribution, with a subject specific prior preference for a single mode (as suggested by the
authors). This might provide support for a single mode in experiments 1 and 2, dual mode in
experiment 3.
This might help to disambiguate the issue of whether subject are merely better at learning the
further the distributions can be separated, or is it truly the discreteness (no values that span the
gap)
The authors highlight similarities (that I agree with) with our recent work (Sanborn & Beierholm
2016, note spelling), but there is a difference in that although the bimodal distribution was similar
10
across the studies (exp 1 and 2 in this study), the previous work did find evidence for bimodal
learning. This could be due to the use of explicit discrete numerical feedback, or indeed more
clearly distinguishable dual modes. I would be curious if the authors had any thoughts on this.
A side note, figure 5 (exp 1 and 2) shows that the likelihood of the kernel precision is essentially
flat over a very wide range (-10 to -1) implying that any of those values could provide equally
good fit. Using a prior (assigned or found through expectation maximisation) would avoid such
an issue. In practice the authors are not interested in exact parameter fits (just whether they are
below positive or negative), but it may be relevant for future work.
Author's Response to Decision Letter for (RSOS-170270)
See Appendix A.
label_version_2
RSOS-170270.R1 (Revision)
label_author_3
Review form: Reviewer 1 (Bernhard Spitzer)
Is the manuscript scientifically sound in its present form?
Yes
Are the interpretations and conclusions justified by the results?
Yes
Is the language acceptable?
Yes
Is it clear how to access all supporting data?
Yes
Do you have any ethical concerns with this paper?
No
Have you any concerns about statistical analyses in this paper?
No
Recommendation?
label_recommendation_3
Accept with minor revision (please list in comments)
Comments to the Author(s)
label_comment_3
The authors were generally responsive in the revision and addressed several of my previously
raised concerns.
I wish to clarify a potential misunderstanding of my previous suggestion to optionally inspect the
distribution of relative distances (in terms of the relative distance from the previous
click/stimulus), in addition to the distribution of absolute screen locations. Indeed, while I
understand that the time-series of relative distances (just like the time-series of absolute locations)
cannot be reasonably cross-correlated between training and test, my suggestion was rather to
11
inspect the distribution of these distances regardless of their time of occurrence within the
sequence. A priori, with uniform random locations, the frequency of these distances should
decrease linearly with increasing (unsigned) distance. However, if the locations are bimodally
distributed (as in the present training phase), the distribution of distances should be bimodal as
well (with a second mode/peak near the distance between the two modes of the absolute location
distribution). My concern/question was that at test, participants might have produced the
distance between the two modes more often than would be expected by chance/uniform
reproduction, which (i) would reflect a form of distributional learning and (ii) could go
undetected in the presented analyses, given some error in (and/or partial neglect of) reproducing
the absolute screen locations (see my previous comments). I do understand that the present
experiments were not designed directly for this question and I leave it to the authors whether to
pursue it further. I do appreciate that the authors included the possibility of allocentric
processing as a potential limitation of their findings in the General Discussion.
I had difficulties following the authors’ response to my suggestion to report Phase 1 performance
data, specifically “We have found previously in unrelated studies that subjects may become off
task when stimuli are presented passively on the screen. Therefore, we did not analyze these
data.” I understand that the authors had previously used passive tasks where Phase 1
performance data was not available (and I agree that an active task, like in the present
experiments, is better), however I would view this as a reason to report these data, here where
they are available, rather than a reason to not report them? The author did include means and
SDs in their response letter, but it was not specified what the numbers mean (I presume response
times in seconds, or proportions of hits?). I think readers would indeed benefit from the Phase 1
performance data being included (with a brief explanation) in the paper.
My remaining concerns have been convincingly addressed. I think that despite its potential
limitations, the paper can make a nice contribution to the existing literature and stimulate further
research into this very interesting question.
Sincerely
Bernhard Spitzer
(Dept of Experimental Psychology, University of Oxford)
label_end_comment
Decision letter (RSOS-170270.R1)
26-Jun-2017
Dear Mr Tran:
On behalf of the Editors, I am pleased to inform you that your Manuscript RSOS-170270.R1
entitled "How Effective is Incidental Learning of the Shape of Probability Distributions?" has been
accepted for publication in Royal Society Open Science subject to minor revision in accordance
with the referee suggestions. Please find the referees' comments at the end of this email.
The reviewers and Subject Editor have recommended publication, but also suggest some minor
revisions to your manuscript. Therefore, I invite you to respond to the comments and revise your
manuscript.
• Ethics statement
If your study uses humans or animals please include details of the ethical approval received,
including the name of the committee that granted approval. For human studies please also detail
whether informed consent was obtained. For field studies on animals please include details of all
permissions, licences and/or approvals granted to carry out the fieldwork.
12
• Data accessibility
It is a condition of publication that all supporting data are made available either as
supplementary information or preferably in a suitable permanent repository. The data
accessibility section should state where the article's supporting data can be accessed. This section
should also include details, where possible of where to access other relevant research materials
such as statistical tools, protocols, software etc can be accessed. If the data has been deposited in
an external repository this section should list the database, accession number and link to the DOI
for all data from the article that has been made publicly available. Data sets that have been
deposited in an external repository and have a DOI should also be appropriately cited in the
manuscript and included in the reference list.
If you wish to submit your supporting data or code to Dryad (http://datadryad.org/), or modify
your current submission to dryad, please use the following link:
http://datadryad.org/submit?journalID=RSOS&manu=RSOS-170270.R1
• Competing interests
Please declare any financial or non-financial competing interests, or state that you have no
competing interests.
• Authors’ contributions
All submissions, other than those with a single author, must include an Authors’ Contributions
section which individually lists the specific contribution of each author. The list of Authors
should meet all of the following criteria; 1) substantial contributions to conception and design, or
acquisition of data, or analysis and interpretation of data; 2) drafting the article or revising it
critically for important intellectual content; and 3) final approval of the version to be published.
All contributors who do not meet all of these criteria should be included in the
acknowledgements.
We suggest the following format:
AB carried out the molecular lab work, participated in data analysis, carried out sequence
alignments, participated in the design of the study and drafted the manuscript; CD carried out
the statistical analyses; EF collected field data; GH conceived of the study, designed the study,
coordinated the study and helped draft the manuscript. All authors gave final approval for
publication.
• Acknowledgements
Please acknowledge anyone who contributed to the study but did not meet the authorship
criteria.
• Funding statement
Please list the source of funding for each author.
Please note that we cannot publish your manuscript without these end statements included. We
have included a screenshot example of the end statements for reference. If you feel that a given
heading is not relevant to your paper, please nevertheless include the heading and explicitly state
that it is not relevant to your work.
Because the schedule for publication is very tight, it is a condition of publication that you submit
the revised version of your manuscript within 7 days (i.e. by the 05-Jul-2017). If you do not think
you will be able to meet this date please let me know immediately.
To revise your manuscript, log into https://mc.manuscriptcentral.com/rsos and enter your
Author Centre, where you will find your manuscript title listed under "Manuscripts with
Decisions". Under "Actions," click on "Create a Revision." You will be unable to make your
13
revisions on the originally submitted version of the manuscript. Instead, revise your manuscript
and upload a new version through your Author Centre.
When submitting your revised manuscript, you will be able to respond to the comments made by
the referees and upload a file "Response to Referees" in "Section 6 - File Upload". You can use this
to document any changes you make to the original manuscript. In order to expedite the
processing of the revised manuscript, please be as specific as possible in your response to the
referees.
When uploading your revised files please make sure that you have:
1) A text file of the manuscript (tex, txt, rtf, docx or doc), references, tables (including captions)
and figure captions. Do not upload a PDF as your "Main Document".
2) A separate electronic file of each figure (EPS or print-quality PDF preferred (either format
should be produced directly from original creation package), or original software format)
3) Included a 100 word media summary of your paper when requested at submission. Please
ensure you have entered correct contact details (email, institution and telephone) in your user
account
4) Included the raw data to support the claims made in your paper. You can either include your
data as electronic supplementary material or upload to a repository and include the relevant doi
within your manuscript
5) All supplementary materials accompanying an accepted article will be treated as in their final
form. Note that the Royal Society will neither edit nor typeset supplementary material and it will
be hosted as provided. Please ensure that the supplementary material includes the paper details
where possible (authors, article title, journal name).
Supplementary files will be published alongside the paper on the journal website and posted on
the online figshare repository (https://figshare.com). The heading and legend provided for each
supplementary file during the submission process will be used to create the figshare page, so
please ensure these are accurate and informative so that your files can be found in searches. Files
on figshare will be made available approximately one week before the accompanying article so
that the supplementary material can be attributed a unique DOI.
Once again, thank you for submitting your manuscript to Royal Society Open Science and I look
forward to receiving your revision. If you have any questions at all, please do not hesitate to get
in touch.
Best wishes
Alice Power
Editorial Coordinator
Royal Society Open Science
openscience@royalsociety.org
on behalf of Essi Viding
Subject Editor, Royal Society Open Science
openscience@royalsociety.org
Comments to Author:
Reviewer: 1
Comments to the Author(s)
The authors were generally responsive in the revision and addressed several of my previously
raised concerns.
14
I wish to clarify a potential misunderstanding of my previous suggestion to optionally inspect the
distribution of relative distances (in terms of the relative distance from the previous
click/stimulus), in addition to the distribution of absolute screen locations. Indeed, while I
understand that the time-series of relative distances (just like the time-series of absolute locations)
cannot be reasonably cross-correlated between training and test, my suggestion was rather to
inspect the distribution of these distances regardless of their time of occurrence within the
sequence. A priori, with uniform random locations, the frequency of these distances should
decrease linearly with increasing (unsigned) distance. However, if the locations are bimodally
distributed (as in the present training phase), the distribution of distances should be bimodal as
well (with a second mode/peak near the distance between the two modes of the absolute location
distribution). My concern/question was that at test, participants might have produced the
distance between the two modes more often than would be expected by chance/uniform
reproduction, which (i) would reflect a form of distributional learning and (ii) could go
undetected in the presented analyses, given some error in (and/or partial neglect of) reproducing
the absolute screen locations (see my previous comments). I do understand that the present
experiments were not designed directly for this question and I leave it to the authors whether to
pursue it further. I do appreciate that the authors included the possibility of allocentric
processing as a potential limitation of their findings in the General Discussion.
I had difficulties following the authors’ response to my suggestion to report Phase 1 performance
data, specifically “We have found previously in unrelated studies that subjects may become off
task when stimuli are presented passively on the screen. Therefore, we did not analyze these
data.” I understand that the authors had previously used passive tasks where Phase 1
performance data was not available (and I agree that an active task, like in the present
experiments, is better), however I would view this as a reason to report these data, here where
they are available, rather than a reason to not report them? The author did include means and
SDs in their response letter, but it was not specified what the numbers mean (I presume response
times in seconds, or proportions of hits?). I think readers would indeed benefit from the Phase 1
performance data being included (with a brief explanation) in the paper.
My remaining concerns have been convincingly addressed. I think that despite its potential
limitations, the paper can make a nice contribution to the existing literature and stimulate further
research into this very interesting question.
Sincerely
Bernhard Spitzer
(Dept of Experimental Psychology, University of Oxford)
Author's Response to Decision Letter for (RSOS-170270.R1)
Department of Psychology
University of California, San Diego
La Jolla, CA 92093
June 29, 2017
Dr. Essi Viding, Subject Editor
Psychology and Cognitive Neuroscience
Royal Society Open Science
Dear Dr. Viding,
We are happy to hear that our article entitled “How Effective is Incidental Learning of the Shape
of Probability Distributions?” has been accepted for publication. We have updated the
15
manuscript to address the minor revision by including subjects’ performance during training for
each experiment in the appropriate sections. Please find the final version of our manuscript now
submitted into Manuscript Central.
We thank you for your time and acceptance of our article. As Dr. Spitzer stated, we hope this
study will stimulate further discussions on incidental learning.
Sincerely,
Randy Tran, Ed Vul, and Hal Pashler
label_end_comment
Decision letter (RSOS-170270.R2)
03-Jul-2017
Dear Mr Tran,
I am pleased to inform you that your manuscript entitled "How Effective is Incidental Learning of
the Shape of Probability Distributions?" is now accepted for publication in Royal Society Open
Science.
You can expect to receive a proof of your article in the near future. Please contact the editorial
office (openscience_proofs@royalsociety.org and openscience@royalsociety.org) to let us know if
you are likely to be away from e-mail contact. Due to rapid publication and an extremely tight
schedule, if comments are not received, your paper may experience a delay in publication.
Royal Society Open Science operates under a continuous publication model
(http://bit.ly/cpFAQ). Your article will be published straight into the next open issue and this
will be the final version of the paper. As such, it can be cited immediately by other researchers.
As the issue version of your paper will be the only version to be published I would advise you to
check your proofs thoroughly as changes cannot be made once the paper is published.
In order to raise the profile of your paper once it is published, we can send through a PDF of your
paper to selected colleagues. If you wish to take advantage of this, please reply to this email with
the name and email addresses of up to 10 people who you feel would wish to read your article.
On behalf of the Editors of Royal Society Open Science, we look forward to your continued
contributions to the Journal.
Best wishes,
Andrew Dunn
Senior Publishing Editor
Royal Society Open Science
openscience@royalsociety.org
Appendix A
UNIVERSITY OF CALIFORNIA, SAN DIEGO UCSD
_________________________________________________________________________________________________________________________________________________________________________ _________________________________________________________
BERKELEY <U+F06C> DAVIS <U+F06C> IRVINE <U+F06C> LOS ANGELES MERCED RIVERSIDE
<U+F06C> <U+F06C> <U+F06C> SAN DIEGO <U+F06C><U+F020> SAN FRANCISCO S SANTA BARBARA <U+F06C> SANTA CRUZ
_________________________________________________________________________________________________________________________________________________________________________ _________________________________________________________
Department of Psychology
University of California, San Diego
La Jolla, CA 92093
May 15, 2017
Dr. Essi Viding, Subject Editor
Psychology and Cognitive Neuroscience
Royal Society Open Science
Dear Dr. Viding,
We appreciate the opportunity to resubmit our article entitled “How Effective is Incidental
Learning of the Shape of Probability Distributions?” We have addressed both reviewers’ thoughtful
questions and suggestions. Please find a revised version of our manuscript now submitted into
Manuscript Central. Detailed responses to the reviewers’ concerns are included below.
The first reviewer, Dr. Spitzer, raised four concerns in his review. The first is an interesting point
that learning could have occurred allocentrically from trial-tos-trial given that the mouse pointer did not
return to a fixed location at the beginning of each trial (nor was there a fixed reference point). To test
this idea, a cross-correlation analysis on the training/testing time-series of the distances could be done.
However, this would raise questions that were not considered in the current experimental design (i.e.,
the distances between disks from trial-to-trial were random). For instance, should the full-time series or
subset be used? Moreover, if subsets are used, how large and which point in the time-series should be
used? Considering these questions, we would not have an a priori reason in selecting a region for
analysis in the current design. While we agree (and have now noted this in our General Discussion) that
it is possible allocentric learning of the blue disks could have occurred, our experimental stimuli were
not designed to examine trial-by-trial learning (i.e., all subjects saw the same random order of trials).
Therefore, we do not believe we can do justice to the question given these considerations. A future
experiment would be needed to directly test this hypothesis.
Second, Dr. Spitzer considers the distribution for click production in Experiment 3 to be poor.
We believe that while the subject responses were not perfect, the larger distribution tails can be
attributed to variability from subject-to-subject and trial-to-trial given the continuous dependent measure
(i.e., subjects can click anywhere the line).
Third, while we do not have any explicit data (e.g., exit survey) to determine whether the
subjects fully understood the instructions, we did not have any reports of subjects requesting for
instruction clarification or explanation. In addition, recall that the instructions for Experiment 3, where
subjects did produce bimodal clicks, were identical to Experiment 1.
Fourth, Dr. Spitzer inquired about Phase 1 performance for the three experiments. The fast-paced
whack-a-mole game feature in Phase 1 was designed to keep subjects engaged in paying attention to
where the blue disks appeared. We have found previously in unrelated studies that subjects may become
off task when stimuli are presented passively on the screen. Therefore, we did not analyze these data.
However, for transparency, the resulting means and standard errors are as follow: Exp. 1 M = 0.51, SE =
0.02; Exp. 2 M = 0.57, SE = 0.03; Exp. 3 M = 0.58, SE = 0.03. We also followed up on the request for a
statistical analysis across the three experiments. We fitted MAP kernel densities for each subject with a
soft prior (to avoid large flat regions) in all three experiments. An ANOVA on subject minima across the
experiments was highly significant, F(2, 89) = 8.54, p < 0.001. These results are consistent with our
initial interpretation and are now included in our revised manuscript.
Finally, we have also addressed Dr. Spitzer’s methodological questions and minor comments
directly in the manuscript, where appropriate. The participants were not told how many clicks they could
make in Phase 2, but there was a counter on the top left of the screen that incremented with each click.
Subjects were not forced to wait between clicks (i.e., self-paced). The fixed sequence of blue dots
(specifically used to reduce noise in our analyses) appeared on a visible horizontal line that was static
throughout the game that was 822 pixels long on a 1024x768-resolution screen. Unfortunately, we did
not explicitly question subjects at the end of this study about the distribution of disks, though we do
investigate subject’s verbal responses and incidental learning in a different series of experiments.
Our second reviewer, Dr. Beierholm (apologies for the typographical citation error in the initial
manuscript), requested more details on the modeling approach. We addressed this in our revised
manuscript under the “Quantifying Learning” section. Specifically, in the model, x_i refers to the
position seen on a given training trial. And fitting of the kernel densities were done by obtaining the
distribution over positions as the kernel density estimate (sum over all Kernels for all training data,
normalized), for a given k. Then the likelihood of a subject’s responses under that distribution was
calculated for each k. Finally, the maximum likelihood k was taken as the estimate.
Dr. Beierholm offered a possible alternative explanation to his study that we discussed in our
General Discussion: learning may have occurred from using discrete numerical feedback. It is possible
that the discrete numbers played a role in learning the bimodal and quadrimodal distributions. This
interpretation, though, would still be inline with the general hypothesis that discreteness makes learning
easier. A future study would have to examine more specifically, whether discreteness in the response or
underlying distribution is the driving force.
We hope you agree that we have addressed both Dr. Spitzer and Dr. Beierholm’s questions and
suggestions both in this cover letter and revised manuscript, and we hope you will agree that it is much
improved and suitable for publication.
Sincerely,
Randy Tran, Ed Vul, and Hal Pashler
Society Open
