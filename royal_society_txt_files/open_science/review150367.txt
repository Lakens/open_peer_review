Estimating uncertainty and reliability of social network data
using Bayesian inference
Damien R. Farine and Ariana Strandburg-Peshkin
Article citation details
R. Soc. open sci. 2: 150367.
http://dx.doi.org/10.1098/rsos.150367
Review timeline
Original submission: 19 December 2014 Note: Reports are unedited and appear as
1st revised submission: 24 July 2015 submitted by the referee. The review history
2nd revised submission: 19 August 2015 appears in chronological order.
Final acceptance: 20 August 2015
Review History
label_version_1
RSOS-140534.R0 (Original submission)
label_author_1
Review form: Reviewer 1
Is the manuscript scientifically sound in its present form?
Yes
Are the interpretations and conclusions justified by the results?
Yes
Is the language acceptable?
Yes
Is it clear how to access all supporting data?
Yes
Do you have any ethical concerns with this paper?
No
© 2015 The Authors. Published by the Royal Society under the terms of the Creative Commons
Attribution License http://creativecommons.org/licenses/by/4.0/, which permits unrestricted use,
provided the original author and source are credited
2
Have you any concerns about statistical analyses in this paper?
No
Recommendation?
label_recommendation_1
Accept with minor revision (please list in comments)
Comments to the Author(s)
label_comment_1
Dear Authors,
I felt astonished while reading this paper. The way it was produced and the tremendous authors'
attitude for scientific works!
The authors have proposed a new method to estimate the accuracy of social network data. They
have presented the comparison, needed for such a work, with the bootstrap method showing the
degree of improvement when working with small dataset. Moreover they have estimated a
possible minimum sample size threshold when studying animal association.
I really have no critiques on this manuscript.
Three advices:
1) Bayesian approach is well introduced but I have the feeling it lacks of some clarification and
possible useful directions for naïve readers. I would suggest to develop a more didactically useful
reasoning on prior choice. Which other prior distributions authors could have used here and,
more in general, which other prior distributions can be used when working on association data!
This point is not necessary but it will probably increase the MS visibility and use in future works.
2)Explanation of the Bayesian uncertainty (i.e. credibility intervals around the estimation) seems
to occur too late in the MS.
3) I would suggest authors to carefully read the MS again in order to fix few orthographic errors
label_author_2
Review form: Reviewer 2 (Matthew Spencer)
Is the manuscript scientifically sound in its present form?
No
Are the interpretations and conclusions justified by the results?
No
Is the language acceptable?
Yes
Is it clear how to access all supporting data?
Yes
Do you have any ethical concerns with this paper?
No
Have you any concerns about statistical analyses in this paper?
Yes
Recommendation?
label_recommendation_2
Major revision is needed (please make suggestions in comments)
3
Comments to the Author(s)
label_comment_2
Attached (See Appendix A.)
label_end_comment
Decision letter (RSOS-140534)
03-Jun-2015
Dear Dr Farine:
Manuscript ID RSOS-140534 entitled "Estimating uncertainty and reliability of network data
using Bayesian inference" which you submitted to Royal Society Open Science, has been
reviewed. The comments from reviewers are included at the bottom of this letter.
In view of the criticisms of the reviewers, the manuscript has been rejected in its current form.
However, a new manuscript may be submitted which takes into consideration these comments.
Please note that resubmitting your manuscript does not guarantee eventual acceptance, and that
your resubmission will be subject to peer review before a decision is made.
You will be unable to make your revisions on the originally submitted version of your
manuscript. Instead, revise your manuscript and upload the files via your author centre.
Once you have revised your manuscript, go to https://mc.manuscriptcentral.com/rsos and login
to your Author Center. Click on "Manuscripts with Decisions," and then click on "Create a
Resubmission" located next to the manuscript number. Then, follow the steps for resubmitting
your manuscript.
Your resubmitted manuscript should be submitted by 01-Dec-2015. If you are unable to submit
by this date please contact the Editorial Office.
I look forward to a resubmission.
Sincerely,
Emilie Aime
Senior Publishing Editor, Royal Society Open Science
openscience@royalsociety.org
Author's Response to Decision Letter for (RSOS-150367)
See Appendix B.
label_version_2
RSOS-150367.R1 (Revision)
label_author_3
Review form: Reviewer 2 (Matthew Spencer)
Is the manuscript scientifically sound in its present form?
Yes
Are the interpretations and conclusions justified by the results?
Yes
4
Is the language acceptable?
Yes
Is it clear how to access all supporting data?
Some changes to code may be needed, as described in my review.
Do you have any ethical concerns with this paper?
No
Have you any concerns about statistical analyses in this paper?
No
Recommendation?
label_recommendation_3
Accept with minor revision (please list in comments)
Comments to the Author(s)
label_comment_3
Attached (See Appendix C.)
label_end_comment
Decision letter (RSOS-150367)
14-Aug-2015
Dear Dr Farine,
The Subject Editor assigned to your paper ("Estimating uncertainty and reliability of social
network data using Bayesian inference") has now received comments from reviewers. We would
like you to revise your paper in accordance with the referee and Subject Editor suggestions which
can be found below (not including confidential reports to the Editor). Please note this decision
does not guarantee eventual acceptance.
Please submit a copy of your revised paper within three weeks (i.e. by the 06-Sep-2015). If we do
not hear from you within this time then it will be assumed that the paper has been withdrawn. In
exceptional circumstances, extensions may be possible if agreed with the Editorial Office in
advance. We do not allow multiple rounds of revision so we urge you to make every effort to
fully address all of the comments at this stage. If deemed necessary by the Editors, your
manuscript will be sent back to one or more of the original reviewers for assessment. If the
original reviewers are not available we may invite new reviewers.
To revise your manuscript, log into http://mc.manuscriptcentral.com/rsos and enter your
Author Centre, where you will find your manuscript title listed under "Manuscripts with
Decisions." Under "Actions," click on "Create a Revision." Your manuscript number has been
appended to denote a revision. Revise your manuscript and upload a new version through your
Author Centre.
When submitting your revised manuscript, you must respond to the comments made by the
referees and upload a file "Response to Referees" in "Section 6 - File Upload". Please use this to
document how you have responded to each of the comments, and the adjustments you have
made. In order to expedite the processing of the revised manuscript, please be as specific as
possible in your response.
In addition to addressing all of the reviewers' and editor's comments please also ensure that your
revised manuscript contains the following sections before the reference list:
5
• Ethics statement
If your study uses humans or animals please include details of the ethical approval received,
including the name of the committee that granted approval. For human studies please also detail
whether informed consent was obtained. For field studies on animals please include details of all
permissions, licences and/or approvals granted to carry out the fieldwork.
• Data accessibility
It is a condition of publication that all supporting data are made available either as
supplementary information or preferably in a suitable permanent repository. The data
accessibility section should state where the article's supporting data can be accessed. This section
should also include details, where possible of where to access other relevant research materials
such as statistical tools, protocols, software etc can be accessed. If the data has been deposited in
an external repository this section should list the database, accession number and link to the DOI
for all data from the article that has been made publicly available. Data sets that have been
deposited in an external repository and have a DOI should also be appropriately cited in the
manuscript and included in the reference list.
• Competing interests
Please declare any financial or non-financial competing interests, or state that you have no
competing interests.
• Authors’ contributions
All submissions, other than those with a single author, must include an Authors’ Contributions
section which individually lists the specific contribution of each author. The list of Authors
should meet all of the following criteria; 1) substantial contributions to conception and design, or
acquisition of data, or analysis and interpretation of data; 2) drafting the article or revising it
critically for important intellectual content; and 3) final approval of the version to be published.
All contributors who do not meet all of these criteria should be included in the
acknowledgements.
We suggest the following format:
AB carried out the molecular lab work, participated in data analysis, carried out sequence
alignments, participated in the design of the study and drafted the manuscript; CD carried out
the statistical analyses; EF collected field data; GH conceived of the study, designed the study,
coordinated the study and helped draft the manuscript. All authors gave final approval for
publication.
• Acknowledgements
Please acknowledge anyone who contributed to the study but did not meet the authorship
criteria.
• Funding statement
Please list the source of funding for each author.
Once again, thank you for submitting your manuscript to Royal Society Open Science and I look
forward to receiving your revision. If you have any questions at all, please do not hesitate to get
in touch.
Yours sincerely,
Emilie Aime
Senior Publishing Editor, Royal Society Open Science
openscience@royalsociety.org
6
Author's Response to Decision Letter for (RSOS-150367)
See Appendix D.
ppendix A
Review of Farine and Strandburg-Peshkin, Estimating uncertainty
and reliability of network data using Bayesian inference
Matthew Spencer
May 28, 2015
mmary
s manuscript compares two simple approaches to estimating the uncertainty in edge weights of a social
work (bootstrap confidence intervals, and more or less Bayesian estimation of marginal credible intervals).
approach taken is very simple, which is not necessarily a bad thing, but I think that more could be done to
the manuscript in the context of more sophisticated methods for related problems. If I’ve understood what
done correctly, then I think more computational work is needed to establish differences in performance
ween the methods. Throughout, the manuscript is written in a rather informal way. Personally, I’d prefer to
much more precise explanation, and more use of mathematical notation. I’ve indicated below some places
re I think things could be tightened up.
Overall, I felt the manuscript could be scientifically sound with further work, and that further work is
needed before the interpretations and conclusions can be justified by the results. The language is mostly
eptable, and the supporting information is adequate and clear. I have some concerns about statistical
lyses, outlined below. I did not have any ethical concerns.
ain points
. This manuscript uses a very simple approach to the potentially very difficult problem of estimating uncer-
tainty in edge weights on incompletely-observed social networks. This isn’t my field, but perhaps it would
be worth reflecting on the much greater sophistication of existing methods for estimating social networks
(e.g. Everitt, 2012; Hunter et al., 2012). It might be productive to try and build on those methods, rather
than starting from scratch.
In particular, the authors use a beta-binomial model for the marginal distribution of each edge weight
(page 6). I wonder how much dependencies among edge weights affect the efficiency of estimation. For
example, for data simulated under the cliques model (page 5, lines 150-156), edge weights depend on both
clique membership and gregariousness, and so we would not expect weights for edges involving the same
individual to be unconditionally independent. Thus, a simple starting point for an improved model would
be to include latent variables that represent clique membership and gregariousness. This might be more
than you want to attempt now, but is worth bearing in mind.
. Page 3, lines 73-76: “Bootstrapping methods may underestimate uncertainty and lead to biased estimates
when sample sizes are very small.” This is true, but perhaps it would be worth comparing with a
non-bootstrap method. If I’ve understood the Bayesian approach correctly, it’s estimating independent
marginal distributions for each edge weight (as discussed above). If you’re going to do that, then an
equivalent non-Bayesian procedure might be one of the exact small-sample confidence intervals on a
binomial proportion described by Agresti (2002, section 1.4.4). I’d expect these to perform better than a
bootstrap for small sample sizes. This is important because the discussion (page 10, lines 313-315) makes
a point about small-sample performance.
. Page 3, line 99: “Observations are largely binary.” I think this is one place where an attempt to be more
formal would be helpful. I’m assuming that “largely binary” means the random variables observed take
non-negative integer values, with most of the probability on the values 0 and 1. But at this point, we
have not defined what exactly what is being observed. Perhaps it would help to explain early on what a
social network is, and what an observation consists of, in a reasonably formal way. I think Hunter et al.
(2012, section 2) does this quite well in general, although the case discussed here is slightly different and
more specific.
. I’m not quite clear about how much computation has been done here, but I think it’s not enough to
establish general patterns. My attempt at summarizing the procedure described in the manuscript is as
follows:
(a) Generate a true network with either 15 or 50 individuals (page 5, line 145). In the code, lines 12-31
generate this network.
(b) For a range of sampling efforts from 2 to 200 (line 38 in the code), draw an observed binary network
from the true network (page 5, lines 160-165).
(c) For each sampling effort, draw 100 bootstrap pseudosamples (page 6, line 178, code: lines 102-106)
and a sample of size 100 from the posterior distribution for each edge weight (page 7, line 223, code:
lines 117-122).
(d) Calculate the required statistics from the bootstrap pseudosamples and the sample from the posterior
(code: lines 125-185).
In the supplied code the network is generated only once, and in the manuscript, there is no mention
of multiple true networks for a given combination of number of individuals and presence/absence of
cliques. Also, in the legend to figure 2, “four example networks are shown.” In other words, unless I’ve
misunderstood, all the comparisons of methods are based on just four particular true networks, and for
any given true network and any given sampling effort, just one observed binary network.
I think it will be important to establish that the apparent differences between methods are not dependent
on the particular networks that were sampled. To do this, generate a large number (say 100 or 1000) of
true networks, and repeat the procedures described above for each of them. This will have the side effect
of helping to reduce the jaggedness in the graphs. Based on the fact that it took 557 seconds to run the
supplied code (for a true network of size 50) on a single core on my desktop computer (3.2 GHz Intel Xeon
with 16 GB RAM), 100 true networks should be perfectly feasible (557 × 100/3600 = 15.5 hours).
Also, samples of size 100 (bootstrap and posterior) are not that large. It would be better to use at least
1000, if computationally feasible (but I think having multiple true networks matters more).
. Page 7, lines 205-211. “To compute the prior for each network we estimated the distribution of edge
weights across all dyads from our sampled data. We then fit a beta distribution to these data . . . We
found that using an inappropriate prior could yield surprisingly bad results, even after many sampling
periods.” This is a somewhat unconventional approach, if I’ve understood it correctly. Looking at the
code (lines 109-115), it seems that of the two parameters of the beta distribution, shape1 is fixed at 1,
and shape2 is estimated by maximum likelihood from all of the observed data. The resulting distribution
is then used as a prior for each marginal distribution.
First, it would help a lot to write down exactly what was done in mathematical notation. Second, if my
description above is accurate, it’s not a purely Bayesian procedure (I think), because a point maximum
likelihood estimate is used for one of the parameters. This may mean that the uncertainty in posterior
distributions is underestimated. If you retain this procedure, I think a clear justification is needed,
Alternatively, it sounds as though you might be able to achieve the same thing in a more conventional
way using a hierarchical Bayesian model, in which the parameters of the marginal edge weight beta
distribution for each dyad are drawn from a suitable distribution, whose parameters are estimated along
with all the others. This shouldn’t be too hard in a language such as BUGS, JAGS or Stan.
inor comments
. Page 2, lines 60-65. Four sentences are used to give a rather imprecise explanation of the nonparametric
bootstrap. I think it would be possible to do better. For example: “In the nonparametric bootstrap, the
distribution of a statistic is estimated from its distribution in pseudosamples obtained by resampling with
replacement from the original data.”
. Page 3, lines 97-98: “Traditional approaches where edge weights are calculated using indices perform
poorly . . . ” It would be good to make this statement more precise, by explaining what those indices are
and citing a reference.
. Page 3, line 101: “c.f. the prior distribution” should be “i.e. the prior distribution”?
. Page 5, line 146: “a gregariousness score that was drawn from a Poisson distribution and normalised to
values between 0 and 1.” First, the description isn’t very precise. From the code, lines 23-24:
degree_distribution <- rpois(N,3)+0.05
ids$DEG_DIST <- degree_distribution/max(degree_distribution)
The text needs to specify the mean of the Poisson distribution, and that 0.05 was added. Second, it’s not
obvious to me why a Poisson distribution, divided by the maximum sampled value, is a natural choice
here. Why not a beta distribution, for example, if you want values between 0 and 1?
. Page 5, line 150: “individuals were also randomly allocated to 5 different communities”. Equal-sized?
Give details.
. Page 7, line 217: “95% credibility interval” should be “95% credible interval.”
. Page 7, lines 234-235. “We estimated the accuracy of each network, which we define as one minus the
mean absolute difference between the edge value in the ‘real’ network and the edge value in the b-SRI
or Bayesian network.” The use of an absolute accuracy measure might have some consequences for your
evaluation of performance, compared to (for example) a quadratic measure (e.g. Garthwaite et al., 2002,
pp. 119-121). Could you briefly justify this choice?
. Page 8, lines 241-249. It’s a bit misleading to refer to the intervals produced by the Bayesian method as
“confidence intervals.” Since you need to refer to both the bootstrap and the Bayesian intervals, perhaps
just “95% interval” would do here? Also applies to lines 308 and 363.
. Page 8, lines 253-263. It would be helpful to include references for each of the node-level metrics.
. Page 10, lines 306-306. “One potential issue with animal social networks that is rarely explored is the error
rate when estimating the presence or absence of an edge. The level of false positives or false negatives (real
association rates that fell outside the 95% confidence interval generated from the observed data) differed
strongly between the two methods.” It’s not clear to me that falling outside the 95% confidence interval
has anything to do with presence or absence of edges. I think the first sentence should read something like
“One potential issue with animal social networks that is rarely explored is the error rate when estimating
edge weights.”
. Page 10, lines 315-316: “the Bayesian networks . . . remained largely consistent”. “Consistent” isn’t a
good choice of word, because it has a well-defined statistical meaning, different from the sense in which
it’s being used here (e.g. Garthwaite et al., 2002, section 2.3).
. Page 13, lines 409-412. “if there is a non-trivial likelihood of introducing false positives in the data, then
this approach could have disproportionate impact on the resulting estimation of that edge weight. One
way of dealing with this issue is to threshold the network and remove dyads with the weakest possible
edge value.” I would have thought that a better approach would be to a state-space model, in which the
probability of false positive observations is explicitly modelled.
. The code runs. It does produce some warnings:
There were 50 or more warnings (use warnings() to see the first 50)
> warnings()
Warning messages:
1: In dbeta(observed.network[!is.na(observed.network)], shape1 = shape1, ... :
NaNs produced
.
.
.
50: In quartz(width = 6, height = 4.5) :
Quartz device is not available on this platform
The Quartz warnings can definitely be ignored (I’m using Ubuntu). The dbeta warnings are probably
also trivial.
. I don’t think Figure 1 is essential. It’s probably safe to assume that anyone reading this paper without a
working knowledge of Bayesian estimation will also look in a textbook that describes the basics.
ferences
esti, A. (2002). Categorical Data Analysis. John Wiley and Sons, Hoboken, second edition.
ritt, R. G. (2012). Bayesian parameter estimation for latent Markov random fields and social networks.
ournal of Computational and Graphical Statistics, 21(4):940–960.
thwaite, P. H., Jolliffe, I. T., and Jones, B. (2002). Statistical Inference. Oxford University Press, Oxford,
cond edition.
ter, D. R., Krivitsky, P. N., and Schweinberger, M. (2012). Computational statistical methods for social
etwork models. Journal of Computational and Graphical Statistics, 21(4):856–882.
Appendix B
OVERVIEW: We thank both reviewers for taking the time to evaluate our manuscript. The
comments were stimulating and inspired us to make some substantial improvements
throughout the manuscript. Foremost, we now replicate the evaluation of the methods using
400 simulated networks instead of 4. We also implement a more sophisticated method for
calculating the prior distribution that is more theoretically correct than our previous method.
Finally, we added a worked empirical example to help bridge the gap to help empiricists
understand the method. In addition, we have revised the text for clarity and removed errors
as highlighted by the reviewers and our own re-reading.
Reviewer 1:
I felt astonished while reading this paper. The way it was produced and the tremendous
authors' attitude for scientific works!
The authors have proposed a new method to estimate the accuracy of social network data.
They have presented the comparison, needed for such a work, with the bootstrap method
showing the degree of improvement when working with small dataset. Moreover they have
estimated a possible minimum sample size threshold when studying animal association. I
really have no critiques on this manuscript.
We thank the reviewer for their positive comments.
1) Bayesian approach is well introduced but I have the feeling it lacks of some clarification
and possible useful directions for naïve readers. I would suggest to develop a more
didactically useful reasoning on prior choice. Which other prior distributions authors could
have used here and, more in general, which other prior distributions can be used when
working on association data! This point is not necessary but it will probably increase the MS
visibility and use in future works.
We have included more discussion on possible strategies for prior choice into the methods.
For our analysis, we have chosen to employ an empirical Bayesian approach (type II
maximum likelihood).
2) Explanation of the Bayesian uncertainty (i.e. credibility intervals around the estimation)
seems to occur too late in the MS.
We now define credible intervals when we first introduce our Bayesian approach (third
paragraph of the introduction).
3) I would suggest authors to carefully read the MS again in order to fix few orthographic
errors.
Done.
Reviewer 2:
This manuscript compares two simple approaches to estimating the uncertainty in edge
weights of a social network (bootstrap confidence intervals, and more or less Bayesian
estimation of marginal credible intervals). The approach taken is very simple, which is not
necessarily a bad thing, but I think that more could be done to put the manuscript in the
context of more sophisticated methods for related problems. If I’ve understood what was
done correctly, then I think more computational work is needed to establish differences in
performance between the methods. Throughout, the manuscript is written in a rather informal
way. Personally, I’d prefer to see much more precise explanation, and more use of
mathematical notation. I’ve indicated below some places where I think things could be
tightened up. Overall, I felt the manuscript could be scientifically sound with further work, and
that further work is also needed before the interpretations and conclusions can be justified by
the results. The language is mostly acceptable, and the supporting information is adequate
and clear. I have some concerns about statistical analyses, outlined below. I did not have
any ethical concerns.
We thank the reviewer for his careful reading and detailed comments on our manuscript. In
response to his suggestions, we have made major changes to our methodology, conducted
further analyses, and updated our manuscript, as detailed below. We feel that this work has
been dramatically improved through incorporating these excellent suggestions.
1. This manuscript uses a very simple approach to the potentially very difficult problem of
estimating uncertainty in edge weights on incompletely-observed social networks. This isn’t
my field, but perhaps it would be worth reflecting on the much greater sophistication of
existing methods for estimating social networks (e.g. Everitt, 2012; Hunter et al., 2012). It
might be productive to try and build on those methods, rather than starting from scratch. In
particular, the authors use a beta-binomial model for the marginal distribution of each edge
weight (page 6). I wonder how much dependencies among edge weights affect the efficiency
of estimation. For example, for data simulated under the cliques model (page 5, lines 150-
156), edge weights depend on both clique membership and gregariousness, and so we
would not expect weights for edges involving the same individual to be unconditionally
independent. Thus, a simple starting point for an improved model would be to include latent
variables that represent clique membership and gregariousness. This might be more than
you want to attempt now, but is worth bearing in mind.
We agree with the reviewer that modeling dependencies among edge weights would be an
important next step, and have included a discussion of this idea and the suggested
references into our manuscript (Discussion section). As we note, this problem becomes
exceptionally difficult when edges are weighted rather than binary, as is often the case in
studies of animal social networks. As the reviewer notes, in this work we have employed a
very simple approach to a very complex problem. This simplicity is intentional - our intent
with this paper is mainly to illustrate a simple method for inferring uncertainty on edge
weights in social network data, as the issue of uncertainty in network data is seldom even
discussed in the field of animal social networks. By presenting this simple approach, our aim
to bring the issue of estimating uncertainty in animal social networks to the fore, and we
hope that our manuscript will stimulate further work (perhaps incorporating more
sophisticated methods) in this critical area. With this in mind, we also highlight several
possible extensions of our work in the Discussion section.
2. Page 3, lines 73-76: “Bootstrapping methods may underestimate uncertainty and lead to
biased estimates when sample sizes are very small.” This is true, but perhaps it would be
worth comparing with a non-bootstrap method. If I’ve understood the Bayesian approach
correctly, it’s estimating independent marginal distributions for each edge weight (as
discussed above). If you’re going to do that, then an equivalent non-Bayesian procedure
might be one of the exact small-sample confidence intervals on a binomial proportion
described by Agresti (2002, section 1.4.4). I’d expect these to perform better than a
bootstrap for small sample sizes. This is important because the discussion (page 10, lines
313-315) makes a point about small-sample performance.
We thank the reviewer for pointing us to these methods. We have now incorporated exact
small-sample confidence intervals, computed using the Clopper - Pearson method, into our
manuscript and figures for comparison.
3. Page 3, line 99: “Observations are largely binary.” I think this is one place where an
attempt to be more formal would be helpful. I’m assuming that “largely binary” means the
random variables observed take non-negative integer values, with most of the probability on
the values 0 and 1. But at this point, we have not defined what exactly what is being
observed. Perhaps it would help to explain early on what a social network is, and what an
observation consists of, in a reasonably formal way. I think Hunter et al. (2012, section 2)
does this quite well in general, although the case discussed here is slightly different and
more specific.
This comment highlights an unclear passage in our text. Here we were simply commenting
on the fact that in a network constructed from few observations, many edges will be binary
(only a single observation) and thus can only have a value of 0 or 1. We have followed the
reviewer’s advice here and more formally defined what is being observed and how edge
weights values arise.
4. I’m not quite clear about how much computation has been done here, but I think it’s not
enough to establish general patterns. My attempt at summarizing the procedure described in
the manuscript is as follows: (a) Generate a true network with either 15 or 50 individuals
(page 5, line 145). In the code, lines 12-31 generate this network. (b) For a range of
sampling efforts from 2 to 200 (line 38 in the code), draw an observed binary network from
the true network (page 5, lines 160-165). (c) For each sampling effort, draw 100 bootstrap
pseudosamples (page 6, line 178, code: lines 102-106) and a sample of size 100 from the
posterior distribution for each edge weight (page 7, line 223, code: lines 117-122). (d)
Calculate the required statistics from the bootstrap pseudosamples and the sample from the
posterior (code: lines 125-185). In the supplied code the network is generated only once, and
in the manuscript, there is no mention of multiple true networks for a given combination of
number of individuals and presence/absence of cliques. Also, in the legend to figure 2, “four
example networks are shown.” In other words, unless I’ve misunderstood, all the
comparisons of methods are based on just four particular true networks, and for any given
true network and any given sampling effort, just one observed binary network. I think it will
be important to establish that the apparent differences between methods are not dependent
on the particular networks that were sampled. To do this, generate a large number (say 100
or 1000) of true networks, and repeat the procedures described above for each of them. This
will have the side effect of helping to reduce the jaggedness in the graphs. Based on the fact
that it took 557 seconds to run the supplied code (for a true network of size 50) on a single
core on my desktop computer (3.2 GHz Intel Xeon with 16 GB RAM), 100 true networks
should be perfectly feasible (557 × 100/3600 = 15.5 hours). Also, samples of size 100
(bootstrap and posterior) are not that large. It would be better to use at least 1000, if
computationally feasible (but I think having multiple true networks matters more).
We have increased our sample size as suggested by the reviewer. We now create 100
instances of each of the four types of networks (N=50 nodes, with and without cliques, and
N=15 nodes, with and without cliques) as suggested, and report aggregate statistics (mean
and 95% confidence intervals) on the accuracy of edge weight estimates and on the
reliability of uncertainty estimates across all of these networks for each sampling effort. For
each type of network and each sampling effort, we compute the absolute accuracy of edge
weights (the mean absolute difference between true and estimated edge weights for each
network), the relative accuracy of edge weights (rank correlation between true edge weights
and estimated edge weights within each network, for each method), the rate of false
positives (fraction of true edge weights that fall below the estimated 95% confidence /
credible intervals), and the rate of false negatives (fraction of real edge weights that fall
above the estimated 95% confidence / credible intervals) for each method. We then report
means and 95% ranges of these values in the figures. Similarly, our evaluation of estimates
of higher-order network properties such as degree are now based on 100 replicate networks
for each network type.
5. Page 7, lines 205-211. “To compute the prior for each network we estimated the
distribution of edge weights across all dyads from our sampled data. We then fit a beta
distribution to these data . . .We found that using an inappropriate prior could yield
surprisingly bad results, even after many sampling periods.” This is a somewhat
unconventional approach, if I’ve understood it correctly. Looking at the code (lines 109-115),
it seems that of the two parameters of the beta distribution, shape1 is fixed at 1, and shape2
is estimated by maximum likelihood from all of the observed data. The resulting distribution
is then used as a prior for each marginal distribution. First, it would help a lot to write down
exactly what was done in mathematical notation. Second, if my description above is
accurate, it’s not a purely Bayesian procedure (I think), because a point maximum likelihood
estimate is used for one of the parameters. This may mean that the uncertainty in posterior
distributions is underestimated. If you retain this procedure, I think a clear justification is
needed, Alternatively, it sounds as though you might be able to achieve the same thing in a
more conventional way using a hierarchical Bayesian model, in which the parameters of the
marginal edge weight beta distribution for each dyad are drawn from a suitable distribution,
whose parameters are estimated along with all the others. This shouldn’t be too hard in a
language such as BUGS, JAGS or Stan.
We thank the reviewer for these suggestions. We have now changed our method for
constructing the prior distribution on edge weights. Rather than employing hierarchical
Bayesian approach, we chose to use an empirical Bayesian approach (type II maximum
likelihood) which is also widely used in the literature, and should approximate the results one
would obtain using a full hierarchical Bayesian approach. We have clarified this approach in
the manuscript using mathematical notation (see the SI), as suggested. We have also noted
clearly that using this method for constructing the prior brings us out of the purely-Bayesian
realm and into the empirical-Bayesian realm. We provide several suggestions for alternative
ways to construct priors that would remain purely Bayesian, but have chosen to take the
empirical Bayesian route for the sake of practicality. We also show that this approach yields
good estimates of edge weight uncertainty (the rate of false positives and false negatives fall
very close to 2.5% for both measures, as is expected if the uncertainty estimates are
correct).
Minor comments
1. Page 2, lines 60-65. Four sentences are used to give a rather imprecise explanation of the
nonparametric bootstrap. I think it would be possible to do better. For example: “In the
nonparametric bootstrap, the distribution of a statistic is estimated from its distribution in
pseudosamples obtained by resampling with replacement from the original data.”
We have added in the suggested sentence to be more precise, but have also retained a
modified version of the original description using less technical wording, which we think will
be helpful to less statistically-minded readers.
2. Page 3, lines 97-98: “Traditional approaches where edge weights are calculated using
indices perform poorly . . . ” It would be good to make this statement more precise, by
explaining what those indices are and citing a reference.
This sentence has been removed from the revised manuscript.
3. Page 3, line 101: “c.f. the prior distribution” should be “i.e. the prior distribution”? 3
Done.
4. Page 5, line 146: “a gregariousness score that was drawn from a Poisson distribution and
normalised to values between 0 and 1.” First, the description isn’t very precise. From the
code, lines 23-24: degree_distribution <- rpois(N,3)+0.05 ids$DEG_DIST <-
degree_distribution/max(degree_distribution) The text needs to specify the mean of the
Poisson distribution, and that 0.05 was added. Second, it’s not obvious to me why a Poisson
distribution, divided by the maximum sampled value, is a natural choice here. Why not a beta
distribution, for example, if you want values between 0 and 1?
We have clarified this. We acknowledge that a beta distribution would achieve the same
output, but in this case we kept the Poisson model because this is the standard distribution
for degree in most studies.
5. Page 5, line 150: “individuals were also randomly allocated to 5 different communities”.
Equal-sized? Give details.
We have clarified that individuals were allocated with equal probability (thus they are roughly
equal-sized, but this is stochastic).
6. Page 7, line 217: “95% credibility interval” should be “95% credible interval.”
Done.
7. Page 7, lines 234-235. “We estimated the accuracy of each network, which we define as
one minus the mean absolute difference between the edge value in the ‘real’ network and
the edge value in the b-SRI or Bayesian network.” The use of an absolute accuracy measure
might have some consequences for your evaluation of performance, compared to (for
example) a quadratic measure (e.g. Garthwaite et al., 2002, pp. 119-121). Could you briefly
justify this choice?
We chose to use an absolute rather than a quadratic measure because we thought this
would be more intuitively understandable to the reader (because the error can then be
simply conceptualized as absolute differences in edge weight). The main difference between
the two methods should be that a quadratic measure will disproportionately weight the
estimates that are farther off from the true value. We have no real a priori reason to prefer
one over the other mathematically, so went with intuitive appeal here.
8. Page 8, lines 241-249. It’s a bit misleading to refer to the intervals produced by the
Bayesian method as “confidence intervals.” Since you need to refer to both the bootstrap
and the Bayesian intervals, perhaps just “95% interval” would do here? Also applies to lines
308 and 363.
We thank the reviewer for this suggestion, and have referred to simply “95% intervals”
throughout the manuscript whenever both Bayesian and boostrap methods are being
compared.
9. Page 8, lines 253-263. It would be helpful to include references for each of the node-level
metrics.
We have added a reference as suggested.
10. Page 10, lines 306-306. “One potential issue with animal social networks that is rarely
explored is the error rate when estimating the presence or absence of an edge. The level of
false positives or false negatives (real association rates that fell outside the 95% confidence
interval generated from the observed data) differed strongly between the two methods.” It’s
not clear to me that falling outside the 95% confidence interval has anything to do with
presence or absence of edges. I think the first sentence should read something like “One
potential issue with animal social networks that is rarely explored is the error rate when
estimating edge weights.”
This sentence has been removed from the revised manuscript.
11. Page 10, lines 315-316: “the Bayesian networks . . . remained largely consistent”.
“Consistent” isn’t a good choice of word, because it has a well-defined statistical meaning,
different from the sense in which it’s being used here (e.g. Garthwaite et al., 2002, section
2.3).
We have changed the wording to “constant” rather than “consistent”.
12. Page 13, lines 409-412. “if there is a non-trivial likelihood of introducing false positives in
the data, then this approach could have disproportionate impact on the resulting estimation
of that edge weight. One way of dealing with this issue is to threshold the network and
remove dyads with the weakest possible edge value.” I would have thought that a better
approach would be to a state-space model, in which the probability of false positive
observations is explicitly modelled.
We have removed the second sentence (thresholding) from the manuscript as we are
generally against thresholding anyway. The reviewer is correct that the probability of false
positives could be modelled, and this could be incorporated into the Bayesian estimation as
well. Thus, have added this suggestion.
13. The code runs. It does produce some warnings: There were 50 or more warnings (use
warnings() to see the first 50) > warnings() Warning messages: 1: In
dbeta(observed.network[!is.na(observed.network)], shape1 = shape1, ... : NaNs produced . .
. 50: In quartz(width = 6, height = 4.5) : Quartz device is not available on this platform The
Quartz warnings can definitely be ignored (I’m using Ubuntu). The dbeta warnings are
probably also trivial.
We greatly appreciate that the reviewer took the time to look through and run our code. The
dbeta warning should no longer be applicable, since we have now changed our method for
constructing prior distributions, and thus removed this portion of the code. The code does
produce warnings from the mle routine, but this is unavoidable (it is caused by mle
evaluating parameter values that are outside the bounds of 0 and 1).
14. I don’t think Figure 1 is essential. It’s probably safe to assume that anyone reading this
paper without a working knowledge of Bayesian estimation will also look in a textbook that
describes the basics.
While we agree that Figure 1 is quite basic, we have left it in the manuscript because we are
attempting to make our work as accessible as possible to a wide range of practitioners in the
field of animal social networks. As it stands, it really helps the reader to understand the basic
process of generating posterior distributions.
ppendix C
Review of Farine and Strandburg-Peshkin, Estimating uncertainty
and reliability of network data using Bayesian inference
Matthew Spencer
August 7, 2015
mmary
authors have addressed all my comments (and, I think, those of the other reviewer) on the first version.
rall, while this work may not be the definitive solution to the problem, it’s certainly a useful contribution.
er than some very small details (listed below under Minor comments), I think that the manuscript would
benefit from a short paragraph in the introduction summarizing the data and terminology for a typical
lication.
ain points
. In the introduction, terms relating to networks are used before they have been defined. For example,
lines 70-72 refer to “observations”, “nodes” and “edge weights.” “Association rates” appear on line 82,
“network ties” on line 92, and “dyad” on line 96, all without definition. A short paragraph early on
defining all of these things in the context of this particular problem would be a good idea. It may also
be the case that some of these things are effectively synonyms, in which case standardizing on one term
throughout would be good.
inor comments
. Abstract, line 31, still contains the term “confidence interval” (should be “credible interval”).
. Page 3, line 95, “posterior probability of edge weights” should be “posterior probability density of edge
weights”?
. Page 4, line 107, “edges where a pair of nodes have only been observed once can only have an edge weight
of 0 or 1.” That is true for the particular estimator of edge weight considered here, but isn’t true in general.
And because an estimator with these properties is often undesirable, there are many other estimators that
give different results. See, for example, chapter 6 in Manning and Schu¨tze (1999), which discusses simple
ways of estimating the probability of occurrence of unobserved words or phrases in text. Some of those
methods might be relevant to network data. The argument that bootstrapping can’t tell you anything
about uncertainty in such cases is still valid, but it would be worth rephrasing this paragraph either to
specify the estimator, or to avoid mentioning particular estimated values.
. Page 4, line 117, “This approach commences with a large credible interval when few observations are
made (generally starting with a range from 0 to 1).” Is that true? For example, a common vague prior
for a parameter <U+03B8> <U+2208> [0, 1] would be Uniform(0, 1), for which I think most sensible methods for generating
a 95% credible interval would give [0.025, 0.975]. And if the 95% credible interval was [0, 1], where is the
remaining 5% of the probability?
. Page 5, lines 141-143. Are the terms “sets of observations”, “observation samples” and “sampling periods”
synonyms? If so, stick to one of them.
. Page 5, lines 165-166. “Thus, two individuals that have a high gregariousness score . . . , whereas two
non-social individuals . . . ” It would be clearer to use “two individuals that have a low gregariousness
score” in the second clause, to match the first clause.
. Page 6, line 183: “library” should be “package”.
. Page 6, line 184: “Edges in these networks represent the probability . . . ”. Should be “The weights of
edges in these networks . . . ”?
. Page 6, lines 186-187. “This is a widespread and common way to define either interaction or association
data . . . ” Should this say something like “This is a common way to estimate interaction or association
rates.”?
. Page 10, line 332. “For an incremental level of sampling effort S . . . ”. I’m not quite clear exactly what
this means. Is it just “For increasing levels of sampling effort . . . ”?
. Figures 2 and 3. The panel labels are (a)!, (b)! etc, rather than just (a), (b), etc. I also found that the
polygons in most of the figures didn’t render properly either in Adobe Reader or Evince (for example, in
figure 5, the blue polygon appeared as two isolated regions, one in the lower left and one on the upper
right.
. Figure 4 legend, line 729. “ration” should be “ratio”?
. Figure 5 legend should specify the colours used.
. Supplemental information, page 1. Is there a missing s in the last factor? Perhaps it should look like:
Z 1
P (d|s, a, b) = P (d|s, x, a, b)P (x|s, a, b) dx.
0
In the line below the first equation, “For binary observations” should read “For iid binary observations.
Similarly, “Across all n edge weights, x1 , x2 , . . . , xn ” should read “Across all n edge weights, x1 , x2 , . . . , xn
(assumed iid)”.
. Source file 1_Bayesian_network_beta_conjugate_evaluation_final_ml2.R did not execute on my com-
puter. It gave the following error message:
Error in degree(observed.network, ignore.eval = FALSE) :
unused argument (ignore.eval = FALSE)
Source file 2_Bayesian_network_beta_conjugate_test_final_ml2.R also failed to execute, with the
following error message:
Error in degree(observed.network.s[, , i], gmode = "graph") :
unused argument (gmode = "graph")
Perhaps this is due to differences in package versions? The result of sessionInfo() was
R version 3.2.1 (2015-06-18)
Platform: x86_64-pc-linux-gnu (64-bit)
Running under: Ubuntu precise (12.04.5 LTS)
locale:
[1] LC_CTYPE=en_GB.UTF-8 LC_NUMERIC=C
[3] LC_TIME=en_GB.UTF-8 LC_COLLATE=en_GB.UTF-8
[5] LC_MONETARY=en_GB.UTF-8 LC_MESSAGES=en_GB.UTF-8
[7] LC_PAPER=en_GB.UTF-8 LC_NAME=C
[9] LC_ADDRESS=C LC_TELEPHONE=C
[11] LC_MEASUREMENT=en_GB.UTF-8 LC_IDENTIFICATION=C
attached base packages:
[1] stats4 stats graphics grDevices utils datasets methods
[8] base
other attached packages:
[1] igraph_0.7.1 abind_1.4-3 binom_1.1-1 bbmle_1.0.17
[5] MASS_7.3-26 vegan_2.2-1 lattice_0.20-29 permute_0.8-3
[9] asnipe_0.83 sna_2.3-2
loaded via a namespace (and not attached):
[1] grid_3.2.1 nlme_3.1-109 Matrix_1.1-4 tools_3.2.1
[5] numDeriv_2012.9-1 parallel_3.2.1 cluster_2.0.3 mgcv_1.7-24
ferences
nning, C. D. and Schu¨tze, H. (1999). Foundations of statistical natural language processing. The MIT Press,
ambridge, Massachusetts.
Appendix D
The authors have addressed all my comments (and, I think, those of the other reviewer) on
the first version. Overall, while this work may not be the definitive solution to the problem,
it’s certainly a useful contribution. Other than some very small details (listed below under
Minor comments), I think that the manuscript would still benefit from a short paragraph in
the introduction summarizing the data and terminology for a typical application.
We thank the reviewer from taking the time to review our manuscript again. We have
added the paragraph as suggested, fixed up the terminology issues, and fixed up a few
typographical mistakes in addition to the points made below.
Main points
1. In the introduction, terms relating to networks are used before they have been
defined. For example, lines 70-72 refer to “observations”, “nodes” and “edge
weights.” “Association rates” appear on line 82, “network ties” on line 92, and
“dyad” on line 96, all without definition. A short paragraph early on defining all of
these things in the context of this particular problem would be a good idea. It may
also be the case that some of these things are effectively synonyms, in which case
standardizing on one term throughout would be good.
We have added this as the second paragraph in the introduction (lines 61 – 71) and
have tried to clarify the use of terminology throughout the paper. In particular, we
replaced many instances of the phrase “association rate” with “edge weight” when
discussing the networks themselves, but have retained some uses of the phrase
“association rate” and “interaction rate’ when talking about the biological reality
behind the network data.
Minor comments
1. Abstract, line 31, still contains the term “confidence interval” (should be “credible
interval”).
Changed this to “uncertainty estimates” to be more general (and to avoid using
unnecessary jargon early on).
2. Page 3, line 95, “posterior probability of edge weights” should be “posterior probability
density of edge weights”?
Done
3. Page 4, line 107, “edges where a pair of nodes have only been observed once can only
have an edge weight of 0 or 1.” That is true for the particular estimator of edge
weight considered here, but isn’t true in general. And because an estimator with
these properties is often undesirable, there are many other estimators that give
different results. See, for example, chapter 6 in Manning and Schu ¨tze (1999), which
discusses simple ways of estimating the probability of occurrence of unobserved
words or phrases in text. Some of those methods might be relevant to network
data. The argument that bootstrapping can’t tell you anything about uncertainty in
such cases is still valid, but it would be worth rephrasing this paragraph either to
specify the estimator, or to avoid mentioning particular estimated values.
We have updated this paragraph to make it clear that we are talking about using
indices (e.g. the simple ratio index) to calculate edge weights (with appropriate
citation to Cairns & Schwager 1987 who developed these). We agree that there are
many alternative estimators that could theoretically be useful for network analysis,
but their usefulness has never been explored.
4. Page 4, line 117, “This approach commences with a large credible interval when few
observations are made (generally starting with a range from 0 to 1).” Is that true?
For example, a common vague prior for a parameter <U+03B8> <U+2208> [0, 1] would be Uniform(0,
1), for which I think most sensible methods for generating a 95% credible interval
would give [0.025, 0.975]. And if the 95% credible interval was [0, 1], where is the
remaining 5% of the probability?
We have removed the phrase in parentheses.
5. Page 5, lines 141-143. Are the terms “sets of observations”, “observation samples” and
“sampling periods” synonyms? If so, stick to one of them.
Done.
6. Page 5, lines 165-166. “Thus, two individuals that have a high gregariousness score ...,
whereas two non-social individuals . . . ” It would be clearer to use “two individuals
that have a low gregariousness score” in the second clause, to match the first
clause.
Done.
7. Page 6, line 183: “library” should be “package”.
Done.
8. Page 6, line 184: “Edges in these networks represent the probability ...”. Should be “The
weights of edges in these networks . . . ”?
Done.
9. Page 6, lines 186-187. “This is a widespread and common way to define either interaction
or association data ...” Should this say something like “This is a common way to
estimate interaction or association rates.”?
Done.
10. Page 10, line 332. “For an incremental level of sampling effort S . . . ”. I’m not quite
clear exactly what this means. Is it just “For increasing levels of sampling effort . . .
”?
Done.
11. Figures 2 and 3. The panel labels are (a)!, (b)! etc, rather than just (a), (b), etc. I also
found that the polygons in most of the figures didn’t render properly either in
Adobe Reader or Evince (for example, in figure 5, the blue polygon appeared as two
isolated regions, one in the lower left and one on the upper right.
We noted that there were issues with polygon rendering by the online conversion
of the figures embedded in the text in the submission software. However, we saw
no such issues with the full-sized figures uploaded separately. We can easily re-
generate figures if required.
12. Figure 4 legend, line 729. “ration” should be “ratio”?
Done.
13. Figure 5 legend should specify the colours used.
Done.
14. Supplemental information, page 1. Is there a missing s in the last factor?
Done.
15. In the line below the first equation, “For binary observations” should read “For iid
binary observations. Similarly, “Across all n edge weights, x1, x2, . . . , xn” should
read “Across all n edge weights, x1, x2, . . . , xn (assumed iid)”.
Done.
15.
Sourcefile1_Bayesian_network_beta_conjugate_evaluation_final_ml2.Rdidnotexecuteonm
ycom- puter. It gave the following error message:
Error in degree(observed.network, ignore.eval = FALSE) :
unused argument (ignore.eval = FALSE)
Source file 2_Bayesian_network_beta_conjugate_test_final_ml2.R also failed to execute,
with the following error message:
Error in degree(observed.network.s[, , i], gmode = "graph") :
unused argument (gmode = "graph")
This is an issue of incompatibility between igraph and sna packages. This probably occurred
because reviewer ran the empirical version first (loading the sna package) and then the
evaluation file (which loaded igraph second, thus over-riding the degree function from sna)
without re-starting R, whereas each file run from scratch would not have caused an error.
We have removed the dependency on the igraph package in these files which completely
resolves this problem.
Society Open
