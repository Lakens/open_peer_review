A Bayesian bird's eye view of `Replications of important
results in social psychology'
Maarten Marsman, Felix D. Schönbrodt, Richard D. Morey, Yuling Yao, Andrew Gelman
and Eric-Jan Wagenmakers
Article citation details
R. Soc. open sci. 4: 160426.
http://dx.doi.org/10.1098/rsos.160426
Review timeline
Original submission: 20 June 2016 Note: Reports are unedited and appear as
1st revised submission: 30 November 2016 submitted by the referee. The review history
2nd revised submission: 11 December 2016 appears in chronological order.
Final acceptance: 12 December 2016
Review History
label_version_1
RSOS-160426.R0 (Original submission)
label_author_1
Review form: Reviewer 1 (Daniël Lakens)
Is the manuscript scientifically sound in its present form?
No
Are the interpretations and conclusions justified by the results?
No
Is the language acceptable?
Yes
Is it clear how to access all supporting data?
Yes
Do you have any ethical concerns with this paper?
No
Have you any concerns about statistical analyses in this paper?
No
© 2017 The Authors. Published by the Royal Society under the terms of the Creative Commons
Attribution License http://creativecommons.org/licenses/by/4.0/, which permits unrestricted use,
provided the original author and source are credited
2
Recommendation?
label_recommendation_1
Major revision is needed (please make suggestions in comments)
Comments to the Author(s)
label_comment_1
The authors have re-analyzed a special issue of social Psychology I co-edited, in which registered
replication studies were reported. I offer several suggestions for improvements below.
Since the paper mainly just presents the re-analyses, I only have minor comments on the
introduction and discussion. But I want to start by one important comment about the conclusions
the authors draw.
The current data can not be used, in any way, to draw conclusions about the state of social
psychology, or the extent to which it is in a crisis of confidence. The only population that
inferences can be generalized to (and this should be made clear by the authors) is the first ever
special issue consisting of exclusively pre-registered replication studies in the scientific literature.
Since the sample consists of the entire population, generalizing is not needed. Let me explain why
these data can not generalize to the field of social psychology. There was never a way to be sure
you could publish replication studies in 2011 when the call for papers was sent around. We can
expect many authors (but not all) to have been working on a topic, failed to replicate a finding,
not being able to publish it, and use this registered report format to share their knowledge that
the effect did not replicate. Suggesting this sample is representative of social psychology is
completely unwarranted, and making such claims unethical, because it hurts the scientific
reputation of an entire field. Currently, the best (but limited) estimate we have about the state of
the field comes from the Reproducibility Project. The current re-analysis does not contribute
anything to our understanding of the extent to which science is in a ‘crisis’. It just shows that non-
replicable results exist in the literature, and that with a guaranteed publication, researchers are
willing to empirically show this. This was not a random sample of studies, so there is no way to
conclude the findings in this special issue are a reason for ‘concern’. A more appropriate
conclusion would be that there seems to be the need for ways to publish studies that indicate
Type 1 errors in the literature.
The introduction needs some work to make main goal of the paper clearer. The abstract states
what was done, but not why this was done. The introduction states the goal is to provide a
Bayesian bird’s eye view – but the question is why we would want to do this. What question does
this answer? Yes, a non-significant p-value can be inconclusive, but papers in this special issue
with non-significant results did not argue the result was conclusive evidence of a lack of an effect
(as far as I know). I’m not questioning that Bayesian analyses on this special issue can be done –
the authors clearly show that they can be done (except when they can’t be done – see below). But
I think the authors need to show why they should be done. Alternatively, this article might have,
as a main goal, to compare three different Bayesian approaches. The special issue data might be
used, since it was open. In that case, the current article might be meant as an educational
overview about the relative strengths and weaknesses of different approaches. I think the main
goal was to show the feasibility of the reported analyses (goal 2 in the introduction). That could
be made clearer, and made the central goal, given that I found the Bayesian bird’s eye view goal
somewhat vague. Regardless of the choice of the authors, we want to prevent the impression that
we are calculating things, just because they can be calculated. If the goal is education, I don’t see a
strong contribution of the hierarchical analysis (see below).
The authors want to show the feasibility of these analyses, but then skip over analyses that are
not feasible. For example, the goal is to “highlight the feasibility of analyzing data from standard
experiments in social psychology using Bayesian tools”. The authors than relegate the fact that
there is no Bayesian equivalent of the analysis performed by Nauts to a footnote. In a more
balanced discussion, this would be mentioned as a major limitation of Bayesian analyses, but the
authors do not seem to find this a limitation with mentioning in the general discussion. They
probably should – not being able to perform the analysis one needs to perform is a rather large
obstacle. I’m sure a Bayesian method will be created in the future of this test, but for now, it
seems worth mentioning.
3
A second limitation in the current re-analysis is that “our Bayesian random-effects meta-analysis
[…] requires the assumption that the 38 effect sizes are independent realizations from a single
overarching distribution”. I’m not an expert on Bayesian meta-analyses, but I would think that
approaches exist that can deal dependent tests. Why is such a test not performed if it is superior?
Are the authors recommending that researchers perform tests for which the assumptions are
violated, justifying this as the authors do by the statement ‘this is probably the best that can be
done here’? What do the authors mean by ‘the best’? If the best is not good enough, why do this
test? I should add that the authors provide no question that is answered by the hierarchical
analysis. In grouping completely unrelated studies together (with which I mean that appearing in
the same journal is not an indication of any reasonable practical or theoretical similarity)
uncertain individual studies get an estimate that is influenced by the mean of completely
unrelated other studies. To extend an analogy the authors seem fond of: It’s like using the weight
of a feather, in the pouch of a jumping kangaroo, as measured by a bathroom scale, to estimate
the distance between the earth and the moon. I don’t see why the mean difference due to a
disgust manipulation should be used to weigh the estimate for the effect of a social exclusion
manipulation (I can come up with vagaries, but nothing that will withstand scrutiny). The
conclusions from this analysis are either trivial (there was a lot of uncertainty in individual
studies – you can just report the mean sample size for that conclusion) or don’t answer a question
anyone cares about (the group level mean for articles in a single special issue is 0, answers no
theoretically or practically interesting question). I think this analysis can be completely removed
from the manuscript – the authors violate the assumptions of the test, and the results are not
interesting, which makes it not very suited for a paper with mainly an educational focus.
The authors use the default r-scale in their analyses. The increasingly widespread criticism on this
default for social psychological research, and the empirical evidence against the plausibility of r =
0.707 as a realistic prior (e.g., see the Reproducibility project), make this a weakness in the current
manuscript that will surely be criticized, and, I fear, keep people from using Bayes factors in a
truly informative way. A better way is to report the results of a sensitivity analysis. Presenting
these would show how JASP will effortlessly provide such an analysis, which would make the
paper more informative to researchers who might want to, but don’t yet, use Bayes factors. A
more reasonable r-scale would be in 0.3 to 0.5 range, in my subjective opinion.
It is unclear why the Many Labs results are left out. The fact that it included benchmark findings
outside of social psychology implies the authors are only interested in ‘pure’ social psychology
effects. But then they could have included some of the studies that are social psychological. If the
results of the many labs project are evident from the plot of effect sizes, the authors imply that
Bayesian re-analyses are not needed if you have a Frequentist forest plot. Finally, other projects
(e.g., the Reproducibility Project) did receive Bayesian re-analyses (e.g., Etz & Vandekerckhove),
even though Frequentist overviews were available.
As a sidenote: The authors don’t seem to point out a single situation where Bayesian statistics
would lead to a different interpretation to using Frequentist statistics, nor a situation where a
Bayesian estimation approach leads to very different conclusions than a Bayesian hypothesis
testing approach. It might be worth mentioning that for all practical purposes, the approach to
statistical inferences you choose matters almost nothing, and that other aspects of doing research
(e.g., asking a valid question, collecting informative data) are much more important. This might
be a better conclusion than the current conclusion about how the field should try to make sure
findings replicate at a higher rate than they do now. I agree the field should do this, but this
conclusion does not follow from this re-analysis, nor is it related to the way we analyze our data
(and if anything, the correct use of a Neyman-Pearson approach, without publication bias, is a
better way to control the probability that studies will ‘replicate’ if we define replication as a test
statistic, p-value or Bayes factor of a certain threshold).
Signed,
Daniel Lakens
4
label_author_2
Review form: Reviewer 2 (Patrizio Tressoldi)
Is the manuscript scientifically sound in its present form?
Yes
Are the interpretations and conclusions justified by the results?
Yes
Is the language acceptable?
Yes
Is it clear how to access all supporting data?
Yes. All raw data and software syntax are available open access.
Do you have any ethical concerns with this paper?
No
Have you any concerns about statistical analyses in this paper?
No
Recommendation?
label_recommendation_2
Accept with minor revision (please list in comments)
Comments to the Author(s)
label_comment_2
This is an important contribution to the problem on how to estimate the replicability of the results
obtained by published papers.
The authors apply three different Bayesian methods to estimate the strenght of evidence of a
series of replication studies published in a special issue of Social Psychology.
My comments aim at clarifying some of their statistical choices and the methodology of
replication estimation.
- Space allowed, I suggest to add a first paragraph about "How to estimate replication results" .
I'm aware that there is not an agreed consensus, but it is important to know the authors opinion
and how it differs from other ones, in particular from those relying on the frequentist approach.
For example Verhagen and Wagenmakers (2014) suggest that a replication study should answer
the following questions: a) Is the effect present or absent in the replication attempt?; b) "“Does the
Effect Size in the replication attempt equal the Effect Size in the original study?”; c) In case a
meta-analysis is possible, "“When pooling all data, is the effect present or absent and of similar
effect size?”.
- Clarify how the 44 directed and the 59 indirected effects sizes were identified in the 38 studies
and the precise number of the original papers (14 or 15?);
- Clarify how the 60 BFs analyses were identified in the 38 studies;
- Clarify how the different effect sizes were pooled to obtain the 38 effect sizes used in the overall
Bayesian parameter estimation.
-Give a comparison about the original authors statements about the successful/failure replication
and the Bayesian results. For example Gibson et al. (2014) who replicated Shih et al. (1999)
stereotype susceptibility effect declared to "have found a significant effect with the same pattern
of means after removing participants that did not know the race or gender stereotypes, but not
when those participants were retained". Does their results interpretation fit with the Bayesian
analyses?
5
- on pag. 7, lines 34 and 56-57, when discussing the low variance explained by some studies, it is
important to point out that there is no a linear relationship between the measure of effect sizes
and their practical or theoretical importance (e.g. see Cortina, J. M., & Landis, R. S. (2009). When
small effect sizes tell a big story, and when large effect sizes don’t. Statistical and methodological
myths and urban legends: Doctrine, verity and fable in the organizational and social sciences,
287-308.)
Minor suggestions:
- on note 2, I will change Gelman's description of his emotion about the BF (I've appreciated
anyway) with a brief description of his reasons;
Patrizio Tressoldi PhD
label_end_comment
Decision letter (RSOS-160426)
4th October 2016
Dear Dr Marsman,
The editors assigned to your paper ("A Bayesian Bird's Eye View of Important Results in Social
Psychology") have now received comments from reviewers. We would like you to revise your
paper in accordance with the referee suggestions which can be found below (not including
confidential reports to the Editor). Please note this decision does not guarantee eventual
acceptance. The Subject Editor is particularly keen that the thoughtful points raised by Dr Lakens
are addressed in full.
Please submit a copy of your revised paper within three weeks (i.e. by the 25th October). If we do
not hear from you within this time then it will be assumed that the paper has been withdrawn. In
exceptional circumstances, extensions may be possible if agreed with the Editorial Office in
advance.We do not allow multiple rounds of revision so we urge you to make every effort to
fully address all of the comments at this stage. If deemed necessary by the Editors, your
manuscript will be sent back to one or more of the original reviewers for assessment. If the
original reviewers are not available we may invite new reviewers.
To revise your manuscript, log into http://mc.manuscriptcentral.com/rsos and enter your
Author Centre, where you will find your manuscript title listed under "Manuscripts with
Decisions." Under "Actions," click on "Create a Revision." Your manuscript number has been
appended to denote a revision. Revise your manuscript and upload a new version through your
Author Centre.
When submitting your revised manuscript, you must respond to the comments made by the
referees and upload a file "Response to Referees" in "Section 6 - File Upload". Please use this to
document how you have responded to the comments, and the adjustments you have made. In
order to expedite the processing of the revised manuscript, please be as specific as possible in
your response.
In addition to addressing all of the reviewers' and editor's comments please also ensure that your
revised manuscript contains the following sections as appropriate before the reference list:
• Ethics statement (if applicable)
If your study uses humans or animals please include details of the ethical approval received,
including the name of the committee that granted approval. For human studies please also detail
6
whether informed consent was obtained. For field studies on animals please include details of all
permissions, licences and/or approvals granted to carry out the fieldwork.
• Data accessibility
It is a condition of publication that all supporting data are made available either as
supplementary information or preferably in a suitable permanent repository. The data
accessibility section should state where the article's supporting data can be accessed. This section
should also include details, where possible of where to access other relevant research materials
such as statistical tools, protocols, software etc can be accessed. If the data have been deposited in
an external repository this section should list the database, accession number and link to the DOI
for all data from the article that have been made publicly available. Data sets that have been
deposited in an external repository and have a DOI should also be appropriately cited in the
manuscript and included in the reference list.
If you wish to submit your supporting data or code to Dryad (http://datadryad.org/), or modify
your current submission to dryad, please use the following link:
http://datadryad.org/submit?journalID=RSOS&manu=RSOS-160426
• Competing interests
Please declare any financial or non-financial competing interests, or state that you have no
competing interests.
• Authors’ contributions
All submissions, other than those with a single author, must include an Authors’ Contributions
section which individually lists the specific contribution of each author. The list of Authors
should meet all of the following criteria; 1) substantial contributions to conception and design, or
acquisition of data, or analysis and interpretation of data; 2) drafting the article or revising it
critically for important intellectual content; and 3) final approval of the version to be published.
All contributors who do not meet all of these criteria should be included in the
acknowledgements.
We suggest the following format:
AB carried out the molecular lab work, participated in data analysis, carried out sequence
alignments, participated in the design of the study and drafted the manuscript; CD carried out
the statistical analyses; EF collected field data; GH conceived of the study, designed the study,
coordinated the study and helped draft the manuscript. All authors gave final approval for
publication.
• Acknowledgements
Please acknowledge anyone who contributed to the study but did not meet the authorship
criteria.
• Funding statement
Please list the source of funding for each author.
Once again, thank you for submitting your manuscript to Royal Society Open Science and I look
forward to receiving your revision. If you have any questions at all, please do not hesitate to get
in touch.
Yours sincerely,
Alice Power
Editorial Coordinator, Royal Society Open Science
on behalf of Essi Viding
Subject Editor, Royal Society Open Science
openscience@royalsociety.org
7
Comments to Author:
Reviewers' Comments to Author:
Reviewer: 1
Comments to the Author(s)
label_comment_3
The authors have re-analyzed a special issue of social Psychology I co-edited, in which registered
replication studies were reported. I offer several suggestions for improvements below.
Since the paper mainly just presents the re-analyses, I only have minor comments on the
introduction and discussion. But I want to start by one important comment about the conclusions
the authors draw.
The current data can not be used, in any way, to draw conclusions about the state of social
psychology, or the extent to which it is in a crisis of confidence. The only population that
inferences can be generalized to (and this should be made clear by the authors) is the first ever
special issue consisting of exclusively pre-registered replication studies in the scientific literature.
Since the sample consists of the entire population, generalizing is not needed. Let me explain why
these data can not generalize to the field of social psychology. There was never a way to be sure
you could publish replication studies in 2011 when the call for papers was sent around. We can
expect many authors (but not all) to have been working on a topic, failed to replicate a finding,
not being able to publish it, and use this registered report format to share their knowledge that
the effect did not replicate. Suggesting this sample is representative of social psychology is
completely unwarranted, and making such claims unethical, because it hurts the scientific
reputation of an entire field. Currently, the best (but limited) estimate we have about the state of
the field comes from the Reproducibility Project. The current re-analysis does not contribute
anything to our understanding of the extent to which science is in a ‘crisis’. It just shows that non-
replicable results exist in the literature, and that with a guaranteed publication, researchers are
willing to empirically show this. This was not a random sample of studies, so there is no way to
conclude the findings in this special issue are a reason for ‘concern’. A more appropriate
conclusion would be that there seems to be the need for ways to publish studies that indicate
Type 1 errors in the literature.
The introduction needs some work to make main goal of the paper clearer. The abstract states
what was done, but not why this was done. The introduction states the goal is to provide a
Bayesian bird’s eye view – but the question is why we would want to do this. What question does
this answer? Yes, a non-significant p-value can be inconclusive, but papers in this special issue
with non-significant results did not argue the result was conclusive evidence of a lack of an effect
(as far as I know). I’m not questioning that Bayesian analyses on this special issue can be done –
the authors clearly show that they can be done (except when they can’t be done – see below). But
I think the authors need to show why they should be done. Alternatively, this article might have,
as a main goal, to compare three different Bayesian approaches. The special issue data might be
used, since it was open. In that case, the current article might be meant as an educational
overview about the relative strengths and weaknesses of different approaches. I think the main
goal was to show the feasibility of the reported analyses (goal 2 in the introduction). That could
be made clearer, and made the central goal, given that I found the Bayesian bird’s eye view goal
somewhat vague. Regardless of the choice of the authors, we want to prevent the impression that
we are calculating things, just because they can be calculated. If the goal is education, I don’t see a
strong contribution of the hierarchical analysis (see below).
The authors want to show the feasibility of these analyses, but then skip over analyses that are
not feasible. For example, the goal is to “highlight the feasibility of analyzing data from standard
experiments in social psychology using Bayesian tools”. The authors than relegate the fact that
there is no Bayesian equivalent of the analysis performed by Nauts to a footnote. In a more
balanced discussion, this would be mentioned as a major limitation of Bayesian analyses, but the
authors do not seem to find this a limitation with mentioning in the general discussion. They
probably should – not being able to perform the analysis one needs to perform is a rather large
obstacle. I’m sure a Bayesian method will be created in the future of this test, but for now, it
seems worth mentioning.
8
A second limitation in the current re-analysis is that “our Bayesian random-effects meta-analysis
[…] requires the assumption that the 38 effect sizes are independent realizations from a single
overarching distribution”. I’m not an expert on Bayesian meta-analyses, but I would think that
approaches exist that can deal dependent tests. Why is such a test not performed if it is superior?
Are the authors recommending that researchers perform tests for which the assumptions are
violated, justifying this as the authors do by the statement ‘this is probably the best that can be
done here’? What do the authors mean by ‘the best’? If the best is not good enough, why do this
test? I should add that the authors provide no question that is answered by the hierarchical
analysis. In grouping completely unrelated studies together (with which I mean that appearing in
the same journal is not an indication of any reasonable practical or theoretical similarity)
uncertain individual studies get an estimate that is influenced by the mean of completely
unrelated other studies. To extend an analogy the authors seem fond of: It’s like using the weight
of a feather, in the pouch of a jumping kangaroo, as measured by a bathroom scale, to estimate
the distance between the earth and the moon. I don’t see why the mean difference due to a
disgust manipulation should be used to weigh the estimate for the effect of a social exclusion
manipulation (I can come up with vagaries, but nothing that will withstand scrutiny). The
conclusions from this analysis are either trivial (there was a lot of uncertainty in individual
studies – you can just report the mean sample size for that conclusion) or don’t answer a question
anyone cares about (the group level mean for articles in a single special issue is 0, answers no
theoretically or practically interesting question). I think this analysis can be completely removed
from the manuscript – the authors violate the assumptions of the test, and the results are not
interesting, which makes it not very suited for a paper with mainly an educational focus.
The authors use the default r-scale in their analyses. The increasingly widespread criticism on this
default for social psychological research, and the empirical evidence against the plausibility of r =
0.707 as a realistic prior (e.g., see the Reproducibility project), make this a weakness in the current
manuscript that will surely be criticized, and, I fear, keep people from using Bayes factors in a
truly informative way. A better way is to report the results of a sensitivity analysis. Presenting
these would show how JASP will effortlessly provide such an analysis, which would make the
paper more informative to researchers who might want to, but don’t yet, use Bayes factors. A
more reasonable r-scale would be in 0.3 to 0.5 range, in my subjective opinion.
It is unclear why the Many Labs results are left out. The fact that it included benchmark findings
outside of social psychology implies the authors are only interested in ‘pure’ social psychology
effects. But then they could have included some of the studies that are social psychological. If the
results of the many labs project are evident from the plot of effect sizes, the authors imply that
Bayesian re-analyses are not needed if you have a Frequentist forest plot. Finally, other projects
(e.g., the Reproducibility Project) did receive Bayesian re-analyses (e.g., Etz & Vandekerckhove),
even though Frequentist overviews were available.
As a sidenote: The authors don’t seem to point out a single situation where Bayesian statistics
would lead to a different interpretation to using Frequentist statistics, nor a situation where a
Bayesian estimation approach leads to very different conclusions than a Bayesian hypothesis
testing approach. It might be worth mentioning that for all practical purposes, the approach to
statistical inferences you choose matters almost nothing, and that other aspects of doing research
(e.g., asking a valid question, collecting informative data) are much more important. This might
be a better conclusion than the current conclusion about how the field should try to make sure
findings replicate at a higher rate than they do now. I agree the field should do this, but this
conclusion does not follow from this re-analysis, nor is it related to the way we analyze our data
(and if anything, the correct use of a Neyman-Pearson approach, without publication bias, is a
better way to control the probability that studies will ‘replicate’ if we define replication as a test
statistic, p-value or Bayes factor of a certain threshold).
Signed,
Daniel Lakens
9
Reviewer: 2
Comments to the Author(s)
label_comment_4
This is an important contribution to the problem on how to estimate the replicability of the results
obtained by published papers.
The authors apply three different Bayesian methods to estimate the strenght of evidence of a
series of replication studies published in a special issue of Social Psychology.
My comments aim at clarifying some of their statistical choices and the methodology of
replication estimation.
- Space allowed, I suggest to add a first paragraph about "How to estimate replication results" .
I'm aware that there is not an agreed consensus, but it is important to know the authors opinion
and how it differs from other ones, in particular from those relying on the frequentist approach.
For example Verhagen and Wagenmakers (2014) suggest that a replication study should answer
the following questions: a) Is the effect present or absent in the replication attempt?; b) "“Does the
Effect Size in the replication attempt equal the Effect Size in the original study?”; c) In case a
meta-analysis is possible, "“When pooling all data, is the effect present or absent and of similar
effect size?”.
- Clarify how the 44 directed and the 59 indirected effects sizes were identified in the 38 studies
and the precise number of the original papers (14 or 15?);
- Clarify how the 60 BFs analyses were identified in the 38 studies;
- Clarify how the different effect sizes were pooled to obtain the 38 effect sizes used in the overall
Bayesian parameter estimation.
-Give a comparison about the original authors statements about the successful/failure replication
and the Bayesian results. For example Gibson et al. (2014) who replicated Shih et al. (1999)
stereotype susceptibility effect declared to "have found a significant effect with the same pattern
of means after removing participants that did not know the race or gender stereotypes, but not
when those participants were retained". Does their results interpretation fit with the Bayesian
analyses?
- on pag. 7, lines 34 and 56-57, when discussing the low variance explained by some studies, it is
important to point out that there is no a linear relationship between the measure of effect sizes
and their practical or theoretical importance (e.g. see Cortina, J. M., & Landis, R. S. (2009). When
small effect sizes tell a big story, and when large effect sizes don’t. Statistical and methodological
myths and urban legends: Doctrine, verity and fable in the organizational and social sciences,
287-308.)
Minor suggestions:
- on note 2, I will change Gelman's description of his emotion about the BF (I've appreciated
anyway) with a brief description of his reasons;
Patrizio Tressoldi PhD
Author's Response to Decision Letter for (RSOS-160426)
See Appendix A.
10
label_version_2
RSOS-160426.R1 (Revision)
label_author_3
Review form: Reviewer 1 (Daniël Lakens)
Is the manuscript scientifically sound in its present form?
Yes
Are the interpretations and conclusions justified by the results?
Yes
Is the language acceptable?
Yes
Is it clear how to access all supporting data?
Yes
Do you have any ethical concerns with this paper?
No
Have you any concerns about statistical analyses in this paper?
No
Recommendation?
label_recommendation_3
Accept with minor revision (please list in comments)
Comments to the Author(s)
label_comment_5
The authors have incorporated all my important suggestions – most importantly the
generalizability of the current results to psychology. But also discussions of alternative statistical
analyses, sensitivity analysis (although a more illustrative example could be provided here,
where the BF provides less strong evidence when the prior changes), and mentioning that not all
analyses could be performed.
I have some minor final comments.
The authors write “Classical methods, however, are unable to quantify evidence; in particular,
classical methods cannot distinguish between the absence of evidence (i.e., the data are
uninformative) or the evidence of absence (i.e., the data support a null hypothesis H0)”
These are two different issues. The first is correct. The second is ambiguous. Equivalence testing
allows researchers to do exactly this – distinguish uninformative (i.e., undetermined, neither
equivalent nor significant) findings from practically equivalent findings. Is this not ‘classical’ –
even though it is based on p-values and confidence intervals? Furthermore, the authors seem to
imply that a Bayesian analysis can ‘support a null hypothesis’. But it can not. The authors
perhaps mean ‘support a null hypothesis *more* than a specific alternative hypothesis’? The
relative nature is specified very clearly later on in the article – I think it deserves to be mentioned
here as well.
I’m pretty sure Nosek and Lakens (2014) talk about evidence, not in the narrow sense that some
Bayesians use the word (relative evidence based on the likelihood) but in a much broader way
(i.e., like ‘evidence’ is used in court – observations, data). I did not find the quote convincing
reason for a Bayesian analysis of this special issue.
A recent criticism on the use of Bayesian meta-analyses was just accepted for publication (it was a
comment on earlier Bayesian re-analysis of some other studies by some of the same authors):
11
“Bayesian Evidence Synthesis is No Substitute for Meta-analysis – A Re-analysis of Scheibehenne,
Jamil and Wagenmakers (2016)”. I’m sure it deserves to be mentioned in this article, as it points
out some limitations of one type of Bayesian meta-analysis, and highlights benefits of an
alternative approach. If the authors disagree with those criticisms, this article would be a good
place to provide some counter-arguments.
Is Figure 7 not available under CC license? Maybe this availability can be specified once in the
author note?
I still think the use of a default prior for all tests indicates a weakness. Which models are being
tested, and what is the justification for this model? The authors seem more concerned about
changes to the prior that make the results non-diagnostic – but earlier Bayesian re-analyses of
other datasets (e.g., Etz & VanderKerckhove, 2016) had uninformativeness of data as one of the
main conclusions. Why would this be different here? Or do the authors think the prior should be
chosen so that the results are informative, even when the models tested are not very interesting?
label_end_comment
Decision letter (RSOS-160426.R1)
5th December 2016
Dear Dr marsman:
On behalf of the Editors, I am pleased to inform you that your Manuscript RSOS-160426.R1
entitled "A Bayesian Bird's Eye View of ``Replications of Important Results in Social Psychology''"
has been accepted for publication in Royal Society Open Science subject to minor revision in
accordance with the referee suggestions. Please find the referees' comments at the end of this
email.
The reviewers and Subject Editor have recommended publication, but also suggest some minor
revisions to your manuscript. Therefore, I invite you to respond to the comments and revise your
manuscript. I am hopeful that the final changes will not require much further work and the
reviewer has provided helpful suggestions regarding how the points could be addressed.
• Ethics statement
If your study uses humans or animals please include details of the ethical approval received,
including the name of the committee that granted approval. For human studies please also detail
whether informed consent was obtained. For field studies on animals please include details of all
permissions, licences and/or approvals granted to carry out the fieldwork.
• Data accessibility
It is a condition of publication that all supporting data are made available either as
supplementary information or preferably in a suitable permanent repository. The data
accessibility section should state where the article's supporting data can be accessed. This section
should also include details, where possible of where to access other relevant research materials
such as statistical tools, protocols, software etc can be accessed. If the data has been deposited in
an external repository this section should list the database, accession number and link to the DOI
for all data from the article that has been made publicly available. Data sets that have been
deposited in an external repository and have a DOI should also be appropriately cited in the
manuscript and included in the reference list.
If you wish to submit your supporting data or code to Dryad (http://datadryad.org/), or modify
your current submission to dryad, please use the following link:
12
http://datadryad.org/submit?journalID=RSOS&manu=RSOS-160426.R1
• Competing interests
Please declare any financial or non-financial competing interests, or state that you have no
competing interests.
• Authors’ contributions
All submissions, other than those with a single author, must include an Authors’ Contributions
section which individually lists the specific contribution of each author. The list of Authors
should meet all of the following criteria; 1) substantial contributions to conception and design, or
acquisition of data, or analysis and interpretation of data; 2) drafting the article or revising it
critically for important intellectual content; and 3) final approval of the version to be published.
All contributors who do not meet all of these criteria should be included in the
acknowledgements.
We suggest the following format:
AB carried out the molecular lab work, participated in data analysis, carried out sequence
alignments, participated in the design of the study and drafted the manuscript; CD carried out
the statistical analyses; EF collected field data; GH conceived of the study, designed the study,
coordinated the study and helped draft the manuscript. All authors gave final approval for
publication.
• Acknowledgements
Please acknowledge anyone who contributed to the study but did not meet the authorship
criteria.
• Funding statement
Please list the source of funding for each author.
Please note that we cannot publish your manuscript without these end statements included. We
have included a screenshot example of the end statements for reference. If you feel that a given
heading is not relevant to your paper, please nevertheless include the heading and explicitly state
that it is not relevant to your work.
Because the schedule for publication is very tight, it is a condition of publication that you submit
the revised version of your manuscript within 7 days (i.e. by the 13th December 2016). If you do
not think you will be able to meet this date please let me know immediately.
To revise your manuscript, log into https://mc.manuscriptcentral.com/rsos and enter your
Author Centre, where you will find your manuscript title listed under "Manuscripts with
Decisions". Under "Actions," click on "Create a Revision." You will be unable to make your
revisions on the originally submitted version of the manuscript. Instead, revise your manuscript
and upload a new version through your Author Centre.
When submitting your revised manuscript, you will be able to respond to the comments made by
the referees and upload a file "Response to Referees" in "Section 6 - File Upload". You can use this
to document any changes you make to the original manuscript. In order to expedite the
processing of the revised manuscript, please be as specific as possible in your response to the
referees.
When uploading your revised files please make sure that you have:
1) A text file of the manuscript (tex, txt, rtf, docx or doc), references, tables (including captions)
and figure captions. Do not upload a PDF as your "Main Document".
2) A separate electronic file of each figure (EPS or print-quality PDF preferred (either format
should be produced directly from original creation package), or original software format)
13
3) Included a 100 word media summary of your paper when requested at submission. Please
ensure you have entered correct contact details (email, institution and telephone) in your user
account
4) Included the raw data to support the claims made in your paper. You can either include your
data as electronic supplementary material or upload to a repository and include the relevant doi
within your manuscript
5) All supplementary materials accompanying an accepted article will be treated as in their final
form. Note that the Royal Society will neither edit nor typeset supplementary material and it will
be hosted as provided. Please ensure that the supplementary material includes the paper details
where possible (authors, article title, journal name).
Supplementary files will be published alongside the paper on the journal website and posted on
the online figshare repository (https://figshare.com). The heading and legend provided for each
supplementary file during the submission process will be used to create the figshare page, so
please ensure these are accurate and informative so that your files can be found in searches. Files
on figshare will be made available approximately one week before the accompanying article so
that the supplementary material can be attributed a unique DOI.
Once again, thank you for submitting your manuscript to Royal Society Open Science and I look
forward to receiving your revision. If you have any questions at all, please do not hesitate to get
in touch.
Best wishes
Andrew Dunn
Senior Publishing Editor
Royal Society Open Science
openscience@royalsociety.org
on behalf of Essi Viding
Subject Editor, Royal Society Open Science
openscience@royalsociety.org
Comments to Author:
Reviewer: 1
Comments to the Author(s)
label_comment_6
The authors have incorporated all my important suggestions – most importantly the
generalizability of the current results to psychology. But also discussions of alternative statistical
analyses, sensitivity analysis (although a more illustrative example could be provided here,
where the BF provides less strong evidence when the prior changes), and mentioning that not all
analyses could be performed.
I have some minor final comments.
The authors write “Classical methods, however, are unable to quantify evidence; in particular,
classical methods cannot distinguish between the absence of evidence (i.e., the data are
uninformative) or the evidence of absence (i.e., the data support a null hypothesis H0)”
These are two different issues. The first is correct. The second is ambiguous. Equivalence testing
allows researchers to do exactly this – distinguish uninformative (i.e., undetermined, neither
equivalent nor significant) findings from practically equivalent findings. Is this not ‘classical’ –
even though it is based on p-values and confidence intervals? Furthermore, the authors seem to
imply that a Bayesian analysis can ‘support a null hypothesis’. But it can not. The authors
perhaps mean ‘support a null hypothesis *more* than a specific alternative hypothesis’? The
14
relative nature is specified very clearly later on in the article – I think it deserves to be mentioned
here as well.
I’m pretty sure Nosek and Lakens (2014) talk about evidence, not in the narrow sense that some
Bayesians use the word (relative evidence based on the likelihood) but in a much broader way
(i.e., like ‘evidence’ is used in court – observations, data). I did not find the quote convincing
reason for a Bayesian analysis of this special issue.
A recent criticism on the use of Bayesian meta-analyses was just accepted for publication (it was a
comment on earlier Bayesian re-analysis of some other studies by some of the same authors):
“Bayesian Evidence Synthesis is No Substitute for Meta-analysis – A Re-analysis of Scheibehenne,
Jamil and Wagenmakers (2016)”. I’m sure it deserves to be mentioned in this article, as it points
out some limitations of one type of Bayesian meta-analysis, and highlights benefits of an
alternative approach. If the authors disagree with those criticisms, this article would be a good
place to provide some counter-arguments.
Is Figure 7 not available under CC license? Maybe this availability can be specified once in the
author note?
I still think the use of a default prior for all tests indicates a weakness. Which models are being
tested, and what is the justification for this model? The authors seem more concerned about
changes to the prior that make the results non-diagnostic – but earlier Bayesian re-analyses of
other datasets (e.g., Etz & VanderKerckhove, 2016) had uninformativeness of data as one of the
main conclusions. Why would this be different here? Or do the authors think the prior should be
chosen so that the results are informative, even when the models tested are not very interesting?
Author's Response to Decision Letter for (RSOS-160426.R1)
See Appendix B.
label_end_comment
Decision letter (RSOS-160426.R2)
12-Dec-2016
Dear Dr marsman,
I am pleased to inform you that your manuscript entitled "A Bayesian Bird's Eye View of
``Replications of Important Results in Social Psychology''" is now accepted for publication in
Royal Society Open Science.
You can expect to receive a proof of your article in the near future. Please contact the editorial
office (openscience_proofs@royalsociety.org and openscience@royalsociety.org) to let us know if
you are likely to be away from e-mail contact. Due to rapid publication and an extremely tight
schedule, if comments are not received, your paper may experience a delay in publication.
Royal Society Open Science operates under a continuous publication model
(http://bit.ly/cpFAQ). Your article will be published straight into the next open issue and this
will be the final version of the paper. As such, it can be cited immediately by other researchers.
As the issue version of your paper will be the only version to be published I would advise you to
check your proofs thoroughly as changes cannot be made once the paper is published.
15
In order to raise the profile of your paper once it is published, we can send through a PDF of your
paper to selected colleagues. If you wish to take advantage of this, please reply to this email with
the name and email addresses of up to 10 people who you feel would wish to read your article.
On behalf of the Editors of Royal Society Open Science, we look forward to your continued
contributions to the Journal.
Best wishes,
Alice Power
Editorial Coordinator
Royal Society Open Science
openscience@royalsociety.org
Below is a point-by-point response to the reviewers’ suggestions for improvement.
Comments by Reviewer 1
The current data can not be used, in any way, to draw conclusions about the state of
social psychology, or the extent to which it is in a crisis of confidence. The only
population that inferences can be generalized to (and this should be made clear by
the authors) is the first ever special issue consisting of exclusively pre-registered
replication studies in the scientific literature. Since the sample consists of the entire
population, generalizing is not needed.
We take the reviewer’s point and we are now careful in our conclusions. We
have changed the title, abstract, and several segments in the main text.
Moreover, in a separate section in the discussion section we now mention that
the extent to which these results could generalize depends on the
representativeness of the studies in the special issue for the field as a whole.
We have repeatedly gone over the new text to ensure that we do not make
claims about generalizability that are not warranted.
Let me explain why these data can not generalize to the field of social psychology.
There was never a way to be sure you could publish replication studies in 2011 when
the call for papers was sent around. We can expect many authors (but not all) to
have been working on a topic, failed to replicate a finding, not being able to publish it,
and use this registered report format to share their knowledge that the effect did not
replicate. Suggesting this sample is representative of social psychology is completely
unwarranted, and making such claims unethical, because it hurts the scientific
reputation of an entire field. Currently, the best (but limited) estimate we have about
the state of the field comes from the Reproducibility Project.
We concede this point. It was insufficiently clear to us that the sample might
not be representative. Thank you for pointing this out so forcefully. To be
absolutely clear that the reader grasps this important point, the Discussion
section now includes a separate section titled “Dangers of Generalizing the
Results Beyond the Special Issue”.
Of course our results remain informative for the specific studies included in
the special issue. But the main point, as we now emphasize in our conclusion,
is that blind generalization to the entire field is unwarranted.
The current re-analysis does not contribute anything to our understanding of the
extent to which science is in a ‘crisis’. It just shows that non-replicable results exist in
the literature, and that with a guaranteed publication, researchers are willing to
empirically show this. This was not a random sample of studies, so there is no way to
conclude the findings in this special issue are a reason for ‘concern’. A more
appropriate conclusion would be that there seems to be the need for ways to publish
studies that indicate Type 1 errors in the literature.
As mentioned above, we accept the point and are now careful in our wording.
As the reviewer knows, we are not a fan of classical methods, and any
interpretation in terms of Type 1 errors are the responsibility of those who
enjoy their p-value null-hypothesis statistical hypothesis testing.
The introduction needs some work to make main goal of the paper clearer. The
abstract states what was done, but not why this was done. The introduction states
the goal is to provide a Bayesian bird’s eye view – but the question is why we would
want to do this. What question does this answer?
As a point of departure consider the second paragraph of the introduction:
“Although the special issue has attracted widespread attention, the data have
not yet been analyzed as a whole, across all replication attempts. In addition,
the individual replication attempts were analyzed solely with classical
statistics (i.e., p-values and confidence intervals). Classical methods, however,
are unable to quantify evidence; in particular, classical methods cannot
distinguish between the absence of evidence (i.e., the data are uninformative)
or the evidence of absence (i.e., the data support a null hypothesis H0). Hence
it is possible that even a high-powered replication with a nonsignificant p-
value can be evidentially uninformative for the question of interest (refs)”.
As we mention in the manuscript, we believe that *two* aspects are important:
the overview aspect and the Bayesian aspect. We now briefly mention the
purpose of each of the three Bayesian analyses. We have also added
references for additional reasons to conduct a Bayesian analysis.
Yes, a non-significant p-value can be inconclusive, but papers in this special issue
with non-significant results did not argue the result was conclusive evidence of a lack
of an effect (as far as I know).
We are not concerned with whether the authors interpreted their p-values
correctly. We are interested in evaluating the evidence. Indeed, the editors of
the special issue state, in their closing sentence: “This differentiates science
from other ways of knowing – confidence in claims is not based on trusting the
source, but in *evaluating the evidence* itself.” (Nosek & Lakens, 2014).
In addition, the results from the replication studies have not been evaluated as
a whole. The results were surprising to us, and we believe they will be
surprising to others as well.
I’m not questioning that Bayesian analyses on this special issue can be done – the
authors clearly show that they can be done (except when they can’t be done – see
below). But I think the authors need to show why they should be done. Alternatively,
this article might have, as a main goal, to compare three different Bayesian
approaches. The special issue data might be used, since it was open. In that case,
the current article might be meant as an educational overview about the relative
strengths and weaknesses of different approaches. I think the main goal was to show
the feasibility of the reported analyses (goal 2 in the introduction).
The educational aspect remains our secondary goal. We have added some
more background to the three Bayesian analyses.
That could be made clearer, and made the central goal, given that I found the
Bayesian bird’s eye view goal somewhat vague. Regardless of the choice of the
authors, we want to prevent the impression that we are calculating things, just
because they can be calculated. If the goal is education, I don’t see a strong
contribution of the hierarchical analysis (see below).
few remarks about what our analysis aims to accomplish.
The authors want to show the feasibility of these analyses, but then skip over
analyses that are not feasible. For example, the goal is to “highlight the feasibility of
analyzing data from standard experiments in social psychology using Bayesian
tools”. The authors than relegate the fact that there is no Bayesian equivalent of the
analysis performed by Nauts to a footnote. In a more balanced discussion, this would
be mentioned as a major limitation of Bayesian analyses, but the authors do not
seem to find this a limitation with mentioning in the general discussion. They
probably should – not being able to perform the analysis one needs to perform is a
rather large obstacle. I’m sure a Bayesian method will be created in the future of this
test, but for now, it seems worth mentioning.
Good point. We have added an explanation and a citation to the footnote. In
addition, we now mention this obstacle and several others in a new subsection
(“Alternative Statistical Analyses”).
A second limitation in the current re-analysis is that “our Bayesian random-effects
meta-analysis […] requires the assumption that the 38 effect sizes are independent
realizations from a single overarching distribution”. I’m not an expert on Bayesian
meta-analyses, but I would think that approaches exist that can deal dependent
tests. Why is such a test not performed if it is superior?
Tests whose assumptions are valid are not necessarily superior to tests whose
assumptions are invalid. In fact, our models are by definitions only
abstractions of reality, and any assumption at all is highly likely to be
incorrect. In practice, statistical modeling requires a consideration of the bias-
variance tradeoff. Complex models may correspond more closely to reality, but
their complexity comes at a price, and that price is “more data”. The
assessment of whether it is worth it to switch to a more complicated model
requires the modeler to judge how well the simple model is doing, and whether
there are sufficient data to warrant the complexity of the more complicated
model. In this case, with only a few replicates in the set, and with low
heterogeneity among the studies, it is clear that a more complex model might
either fail or else provide results that are highly similar to those we already
have. We now explain our reasoning more carefully in the revision.
Are the authors recommending that researchers perform tests for which the
assumptions are violated, justifying this as the authors do by the statement ‘this is
probably the best that can be done here’? What do the authors mean by ‘the best’? If
the best is not good enough, why do this test?
Our wording was imprecise. We now mention the bias-variance tradeoff and
explicitly indicate why we believe a more complex model will not provide
deeper insight.
I should add that the authors provide no question that is answered by the hierarchical
analysis. In grouping completely unrelated studies together (with which I mean that
appearing in the same journal is not an indication of any reasonable practical or
theoretical similarity) uncertain individual studies get an estimate that is influenced by
the mean of completely unrelated other studies. To extend an analogy the authors
seem fond of: It’s like using the weight of a feather, in the pouch of a jumping
kangaroo, as measured by a bathroom scale, to estimate the distance between the
earth and the moon. I don’t see why the mean difference due to a disgust
manipulation should be used to weigh the estimate for the effect of a social exclusion
manipulation (I can come up with vagaries, but nothing that will withstand scrutiny).
individual studies – you can just report the mean sample size for that conclusion) or
don’t answer a question anyone cares about (the group level mean for articles in a
single special issue is 0, answers no theoretically or practically interesting question).
I think this analysis can be completely removed from the manuscript – the authors
violate the assumptions of the test, and the results are not interesting, which makes it
not very suited for a paper with mainly an educational focus.
As stated above, our primary goal is the Bayesian reanalysis -- the systematic
summary of the series of studies published in the special issue. More
importantly, the reviewer raises an interesting philosophical issue about
hierarchical modeling. As we mentioned in the manuscript,
“In the hierarchical model it is assumed that the effect sizes come from a
single overarching distribution, that is, the group-level model. Note that many
experiments from the special issue concern phenomena that are conceptually
unrelated; consequently, the group-level distribution describes the location
and heterogeneity of ‘important results in social psychology’ that were deemed
suitable for preregistered replication. As will be evident below, the
heterogeneity in effect sizes is estimated to be relatively small, and this results
in a substantial shrinkage effect.”
We now expand on this issue. First, note that the hierarchical model does not
require that the studies are conceptually related (see also Efron & Morris, 1977,
for an extended discussion). All that the model assumes is that the effects are
governed by a normal distribution. Second, the hierarchical model contains a
parameter that measures the heterogeneity across studies. In our application,
the studies turn out to be highly homogeneous. This does not show that the
studies are conceptually related, but it does show that their effect sizes are
similar, and this is all that is required for an application of the model. Third, the
extent to which effects are conceptually related is not an all-or-none matter.
One may always argue that the individual case is unique and, at some level,
conceptually unrelated to the other cases. To illustrate, suppose we weight
different animals with different, noisy measurement scales. One object is a
kangaroo, the other is a zebra, the other is crocodile. These are completely
different animals. Why would their weights be related? Nevertheless, it is
appropriate here to use a hierarchical model, and this model will provide better
estimates than a non-hierarchical model (also classically, see Efron & Morris,
1977). In addition, a skeptic may argue that all of the studies in the special
issue deal with effects that are small and fragile. This commonality alone
warrants the application of the hierarchical model.
The authors use the default r-scale in their analyses. The increasingly widespread
criticism on this default for social psychological research, and the empirical evidence
against the plausibility of r = 0.707 as a realistic prior (e.g., see the Reproducibility
project), make this a weakness in the current manuscript that will surely be criticized,
and, I fear, keep people from using Bayes factors in a truly informative way. A better
way is to report the results of a sensitivity analysis. Presenting these would show
how JASP will effortlessly provide such an analysis, which would make the paper
more informative to researchers who might want to, but don’t yet, use Bayes factors.
A more reasonable r-scale would be in 0.3 to 0.5 range, in my subjective opinion.
Firstly, we are quite happy with the current default value. In our opinion,
smaller values of r can be considered but usually need to be accompanied by
shifts in the location of the prior distribution. Specifically, consider a Cauchy
indistinguishable from H0, and the data are nondiagnostic. We believe that if
there is knowledge to suggest that r should be low, there usually also is
knowledge to suggest that the mean should be non-zero. Extending the priors
to allow for this would be an interesting and worthwhile development (in fact
we are currently pursuing this in our lab), but right now JASP (and all JZS
tests, as well as 99% of Bayes factor tests in general) implements only the
“centered” or “local” priors with a median of zero. So on the spectrum of
“objective Bayes” to “subjective Bayes”, our work leans more to “objective
Bayes” (note that these are established labels in the Bayesian field, we do not
intend to suggest that one is superior to the other). We now discuss this issue
in the section that discusses the IJzerman example.
Secondly, we agree that a sensitivity analysis is a good idea. Again, it is
important not to overinterpret results with low values of prior width r, because
without concomitant changes in the mean of the prior distributions such a
setting makes the data nondiagnostic. For this reason, and because our focus
is on the default analysis, we refrain from presenting the robustness tests in
the manuscript itself. However, we have carried out the standard sensitivity
analyses and report them in the JASP files that are available online. Also, we
give an example of a sensitivity analysis in the section on the IJzerman
example.
It is unclear why the Many Labs results are left out. The fact that it included
benchmark findings outside of social psychology implies the authors are only
interested in ‘pure’ social psychology effects. But then they could have included
some of the studies that are social psychological. If the results of the many labs
project are evident from the plot of effect sizes, the authors imply that Bayesian re-
analyses are not needed if you have a Frequentist forest plot. Finally, other projects
(e.g., the Reproducibility Project) did receive Bayesian re-analyses (e.g., Etz &
Vandekerckhove), even though Frequentist overviews were available.
In sharp contrast to the RP:Psychology and the other contributions in the
Social Psychology special issue, the ManyLabs project presents multiple
replication studies. This demands a fundamentally different statistical
treatment. Moreover, with many replication studies the data are bound to pass
the interocular traumatic test. We now explicitly mention this in the revision.
As a sidenote: The authors don’t seem to point out a single situation where Bayesian
statistics would lead to a different interpretation to using Frequentist statistics,
Although this is not immediately relevant for the current paper, we do
disagree. None of our Bayesian conclusions result in the same interpretation
as the ones reached using Frequentist statistics. This is because the Bayesian
conclusions attach probabilities to models and parameters. The Bayesian
conclusions that we reach are by definition unattainable by any Frequentist
method (for better or for worse). For instance, the Frequentist conclusion “we
cannot reject the null hypothesis” is certainly not on par with the Bayesian
conclusion “the data provide evidence for the null hypothesis”. However, the
purpose of this paper is *not* to argue that Bayesian methods are superior to
Frequentist methods. I understand that the reviewer has used his prior
knowledge to reach this conclusion, but it is not reflected in the likelihood. In
this paper we merely showcase our Bayesian reanalysis and let the reader
decide whether this is useful, or whether this provides an informative new
perspective.
nor a situation where a Bayesian estimation approach leads to very different
conclusions than a Bayesian hypothesis testing approach.
The testing approach addresses a fundamentally different question than the
estimation approach. On a higher level there is a happy consistency in this
particular case, but in general this is not a certainty.
It might be worth mentioning that for all practical purposes, the approach to statistical
inferences you choose matters almost nothing, and that other aspects of doing
research (e.g., asking a valid question, collecting informative data) are much more
important.
This remark is universally true and can be attached to all papers that present
an alternative statistical perspective. Indeed, we already emphasize the value
of good research practices throughout the manuscript. To further emphasize
the point feels somewhat out of place here, and may come across as
undercutting our own contribution. With all due respect, it is also true that
curing cancer is more important than research in psychology, but that does
not mean that every paper in psychology needs to state this fact (“We found X,
Y, and Z, but of course curing cancer is more important and our time would
have been better spent working on that instead”).
This might be a better conclusion than the current conclusion about how the field
should try to make sure findings replicate at a higher rate than they do now. I agree
the field should do this, but this conclusion does not follow from this re-analysis, nor
is it related to the way we analyze our data (and if anything, the correct use of a
Neyman-Pearson approach, without publication bias, is a better way to control the
probability that studies will ‘replicate’ if we define replication as a test statistic, p-
value or Bayes factor of a certain threshold).
We have now stated our conclusion less strongly. Note that we never claimed
that the Bayesian framework is better than the Frequentist framework in
assessing reproducibility. Yes, privately and on Twitter we may espouse the
idea that Frequentism is the handiwork of the devil, but that is not the topic of
this paper. We felt we were relatively modest with respect to our claims. For
instance, note the conclusion: “We also hope that future analyses of
replication studies will be more inclusive by employing a range of different,
complementary techniques. When different statistical procedures support the
same inference this can only serve to reinforce one's confidence in the
robustness and validity of the results.”
Comments by Reviewer 2
Space allowed, I suggest to add a first paragraph about "How to estimate replication
results". I'm aware that there is not an agreed consensus, but it is important to know
the authors opinion and how it differs from other ones, in particular from those relying
on the frequentist approach. For example Verhagen and Wagenmakers (2014)
suggest that a replication study should answer the following questions: a) Is the
effect present or absent in the replication attempt?; b) "“Does the Effect Size in the
replication attempt equal the Effect Size in the original study?”; c) In case a meta-
analysis is possible, "“When pooling all data, is the effect present or absent and of
similar effect size?”.
The revision now contains a separate section “Alternative Statistical Analyses”
in which we mention that questions (a) and (b) can also be sensible. Question
best place to discuss the alternative questions. We tried different options and
felt that introducing this earlier runs the risk of confusing the reader.
Clarify how the 44 directed and the 59 indirected effects sizes were identified in the
38 studies and the precise number of the original papers (14 or 15?);
The effect sizes follow from the tests that we have performed, with the directed
effect size measures originating from the t-test and correlation test, and
undirected effect sizes from the ANOVA, t-test, correlation test, and
contingency tables.
The special issue contained 15 studies. We incorporated in our re-analysis all
but one paper, which was the Many Labs paper. We now explain why the Many
Labs project was not included at the beginning of Section 2. Note that the
number 38 refers to the number of t-tests in the 14 studies.
Clarify how the 60 BFs analyses were identified in the 38 studies;
The original studies did not include Bayes factors. The .jasp files that we have
shared online at https://osf.io/bqwzd/ makes clear what classical analysis were
considered and contains a replication of this classical result using the
available data (some issues can be found, which are described in the
corresponding .jasp file). Hereafter, a Bayesian reanalysis is provided in the
.jasp file.
Clarify how the different effect sizes were pooled to obtain the 38 effect sizes used in
the overall Bayesian parameter estimation.
In the hierarchical analyses we used a hierarchical Bayesian model which
assumes that each of the effect sizes comes from a single overarching group-
level model. The group-level model is a normal distribution with an unknown
mean and variance, where the mean and variance are determined from the
data. This is described in Section 1.(b) ``Bayesian Parameter Estimation:
Hierarchical Models’’. Note that the complete code that we have used for
estimating this model is online at https://osf.io/bqwzd/.
Give a comparison about the original authors statements about the successful/failure
replication and the Bayesian results. For example Gibson et al. (2014) who replicated
Shih et al. (1999) stereotype susceptibility effect declared to "have found a significant
effect with the same pattern of means after removing participants that did not know
the race or gender stereotypes, but not when those participants were retained". Does
their results interpretation fit with the Bayesian analyses?
In this paper we prefer to showcase our Bayesian reanalysis and let the reader
decide whether this is useful, or whether this provides an informative new
perspective. In abstract and general terms, our conclusion match with those of
the individual researchers. The conflict with Gibson arises because they
discuss a post-hoc test. In more specific terms, it is challenging to map
frequentist conclusions onto Bayesian concepts (this would be an interesting
project in itself, which would require a larger data set and the development of
some sort of rating system).
On pag. 7, lines 34 and 56-57, when discussing the low variance explained by some
studies, it is important to point out that there is no a linear relationship between the
J. M., & Landis, R. S. (2009). When small effect sizes tell a big story, and when large
effect sizes don’t. Statistical and methodological myths and urban legends: Doctrine,
verity and fable in the organizational and social sciences, 287-308.)
We agree that on an individual basis a small effect size may imply an important
theoretical or practical result. Here, however, we do not consider individual
results in the face of a theoretical contribution, but use effect sizes to describe
specific aspects of the across-study results. In this case, that the experimental
manipulations explain little of the observed variability, and for the directed
effect sizes that most of the effects are small and estimated with high
uncertainty.
On note 2, I will change Gelman's description of his emotion about the BF (I've
appreciated anyway) with a brief description of his reasons.
Done.
Amsterdam, December 6, 2016
Dear Dr. Viding, Dr. Dunn,
We are happy to submit a revision of our manuscript “A Bayesian Bird's Eye View of ‘Replications of
Important Results in Social Psychology’ ”. We greatly appreciate the swift review process and the
positive evaluation of Reviewer 1. Below we outline, in bold, how we have incorporated the remaining
suggestions for improvement.
We hope this revision is satisfactory. We look forward to your comments.
Kind regards,
Maarten Marsman (also on behalf of the co-authors)
Comments by Reviewer 1
Reviewer response: The authors have incorporated all my important suggestions – most importantly the
generalizability of the current results to psychology. But also discussions of alternative statistical
analyses, sensitivity analysis (although a more illustrative example could be provided here, where the BF
provides less strong evidence when the prior changes), and mentioning that not all analyses could be
performed.
Our response: We appreciate the positive evaluation.
Reviewer response: The authors write “Classical methods, however, are unable to quantify evidence; in
particular, classical methods cannot distinguish between the absence of evidence (i.e., the data are
uninformative) or the evidence of absence (i.e., the data support a null hypothesis H0)” These are two
different issues. The first is correct. The second is ambiguous. Equivalence testing allows researchers to
do exactly this – distinguish uninformative (i.e., undetermined, neither equivalent nor significant)
findings from practically equivalent findings. Is this not ‘classical’ – even though it is based on p-values
and confidence intervals?
Our response: It is true that equivalence testing (a decidedly non-standard classical procedure in
mainstream empirical disciplines) has been proposed as a solution. This does require that the hypothesis
at hand is no longer a point, but an interval whose endpoints need to be specified. In addition, the
problem remains that a nonsignificant result is inherently ambiguous. In order to avoid a lengthy
discussion on a topic that most readers will be unfamiliar with, we have now clarified that the null
hypothesis under consideration is a point.
Reviewer response: Furthermore, the authors seem to imply that a Bayesian analysis can ‘support a null
hypothesis’. But it can not. The authors perhaps mean ‘support a null hypothesis *more* than a specific
alternative hypothesis’? The relative nature is specified very clearly later on in the article – I think it
deserves to be mentioned here as well.
Our response: In our opinion, the concepts “support” and “evidence” are inherently relative. We have
carefully considered the suggestion to foreshadow the relative nature of Bayesian hypothesis testing, but
we believe that in the suggested location (which is about classical testing) such foreshadowing is out of
place and potentially confusing. We are pleased that the reviewer believes this issue is explained clearly
in the paper.
Reviewer response: I’m pretty sure Nosek and Lakens (2014) talk about evidence, not in the narrow
sense that some Bayesians use the word (relative evidence based on the likelihood) but in a much
broader way (i.e., like ‘evidence’ is used in court – observations, data). I did not find the quote
convincing reason for a Bayesian analysis of this special issue.
Our response: :-) As used inside and outside of a court, evidence is generally considered something that
changes one’s opinion. This interpretation is consistent with the Bayesian philosophy; as an aside, in
forensics one generally uses Bayes factors and relative likelihoods to quantify evidence. We have added
a clarifying footnote.
Reviewer response: A recent criticism on the use of Bayesian meta-analyses was just accepted for
publication (it was a comment on earlier Bayesian re-analysis of some other studies by some of the
same authors): “Bayesian Evidence Synthesis is No Substitute for Meta-analysis – A Re-analysis of
Scheibehenne, Jamil and Wagenmakers (2016)”. I’m sure it deserves to be mentioned in this article, as
it points out some limitations of one type of Bayesian meta-analysis, and highlights benefits of an
alternative approach. If the authors disagree with those criticisms, this article would be a good place to
provide some counter-arguments.
Our response: This criticism is not relevant for the current work as it relates to a specific method of
pooling data that transcends the Bayesian-frequentist divide. In addition, we will submit a rebuttal to the
journal in which the original paper and reply were published.
Reviewer response:Is Figure 7 not available under CC license? Maybe this availability can be specified
once in the author note?
Our response: We only applied CC licenses for figures of special interest. Figures from JASP, for
instance, also do not have the CC license.
Reviewer response: I still think the use of a default prior for all tests indicates a weakness. Which
models are being tested, and what is the justification for this model? The authors seem more concerned
about changes to the prior that make the results non-diagnostic – but earlier Bayesian re-analyses of
other datasets (e.g., Etz & VanderKerckhove, 2016) had uninformativeness of data as one of the main
conclusions. Why would this be different here? Or do the authors think the prior should be chosen so
that the results are informative, even when the models tested are not very interesting?
Our response: The problem is not that data are sometimes uninformative. This is a fact of life that needs
to be acknowledged and embraced. The problem is that reducing the width of the prior distribution is
certain to produce an uninformative result, irrespective of the actual data. As explained in the
manuscript, we believe that severe reductions of the default width should be accompanied by a
subjective assessment of prior location. This is an exciting statistical program that we are looking
forward to work on in the future.
Society Open
