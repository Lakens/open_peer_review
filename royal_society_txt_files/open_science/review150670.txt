Assessing recognition memory using confidence ratings
and response times
Christoph T. Weidemann and Michael J. Kahana
Article citation details
R. Soc. open sci. 3: 150670.
http://dx.doi.org/10.1098/rsos.150670
Review timeline
Original submission: 5 December 2015 Note: Reports are unedited and appear as
Revised submission: 10 March 2016 submitted by the referee. The review history
Final acceptance: 11 March 2016 appears in chronological order.
Review History
label_version_1
RSOS-150670.R0 (Original submission)
label_author_1
Review form: Reviewer 1
Is the manuscript scientifically sound in its present form?
Yes
Are the interpretations and conclusions justified by the results?
Yes
Is the language acceptable?
Yes
Is it clear how to access all supporting data?
I could download the file with the data, but could not open it.
Do you have any ethical concerns with this paper?
No
© 2016 The Authors. Published by the Royal Society under the terms of the Creative Commons
Attribution License http://creativecommons.org/licenses/by/4.0/, which permits unrestricted use,
provided the original author and source are credited
2
Have you any concerns about statistical analyses in this paper?
No
Recommendation?
label_recommendation_1
Accept with minor revision (please list in comments)
Comments to the Author(s)
label_comment_1
Review
Royal Society Open Science
Assessing recognition memory using confidence ratings and response times
Christoph Weidemann & Michael Kahana
This article investigates the extent to which response times (RTs) to an old-new recognition
memory decision can be used to quantify recognition discriminability. The authors use RTs from
old-new judgments to derive measures of sensitivity (area under the ROC curve, AUC). The main
finding is that, like rating data, RTs provide diagnostic information over and above the old-new
judgment, but that the AUC derived from RTs is lower than the AUC derived from ratings. The
authors go on to show that this pattern holds when the data for “old” and “new” responses is
analysed separately, being more pronounced within the subset of “old” responses.
Overall, I thought that this was an interesting, useful, and straightforward paper, and could be
accepted with only minor revisions. Although many researchers collect reaction time data in
recognition memory experiments, this data is often not reported, and the focus tends to be on the
rating data. It is known that RTs tend to be inversely related to confidence / distance from the
response criterion, but RTs have tended not to feature in mainstream theories of recognition. This
is changing, and any complete theory of recognition memory will also arguably need to explain
RTs. Efforts to bring the analysis of RTs from recognition memory tasks more into the
mainstream therefore seem useful, and by using RTs to construct ROCs, the article contributes to
this aim. This technique has been used in many other research areas, but is not typical in the
recognition memory literature.
Specific points
1. It would be useful to have some basic descriptive statistics about the RTs (e.g., means, variance,
nature of their distribution).
2. At first I did not understand the analysis in Section 3.3. Then I realised that participants had
completed a free recall task prior to the recognition task and the data were being analysed
according to whether items had been recalled in this task or not. Nevertheless, I couldn’t find any
real rationale for this analysis and I generally found this section difficult to follow. It wasn't clear
to me what this added.
3. The data have been averaged across multiple sessions, and we learn in the Discussion that
some of these sessions have different proportions of old and new items. This would reasonably
be expected to have an effect on a participant’s response bias, as the authors mention. It would be
useful to know a bit more about the design of the sessions that were included in the analysis. I
appreciate that the main difference of interest is between RT and rating based sensitivity
measures, but can the authors say more to allay the concern that RT and rating measures may be
differentially sensitive to particular manipulations used across sessions, such as the proportion of
old/new items?
4. In the conclusions, the authors suggest that RTs could act as a substitute for ratings in ROC
construction. In the paradigm used, however, a rating response always followed the old-new
judgment. The authors show that RT data and rating data converge to some extent with this
paradigm, but wouldn’t it be stronger to demonstrate the same L-ROC finding in a paradigm in
which no ratings are collected? It could be that the RT to the old-new response is affected by the
participant knowing they are about to make a rating.
3
5. Could the finding that the area under the C-ROC is greater than the L-ROC arise because the
mapping of the strength signal to the RT is noisier than the mapping of the signal to the
confidence rating?
6. I could not find the formula used for calculating AUC.
7. I could download a data file from the supplementary files (in .hdf5 format), but couldn’t access
it with common software packages. I gather this format is for use in Python.
8. The y-axis labels in Figures 5 and 7 were cut-off.
label_author_2
Review form: Reviewer 2 (John T. Wixted)
Is the manuscript scientifically sound in its present form?
Yes
Are the interpretations and conclusions justified by the results?
Yes
Is the language acceptable?
Yes
Is it clear how to access all supporting data?
I don't see anything about making the data available.
Do you have any ethical concerns with this paper?
No
Have you any concerns about statistical analyses in this paper?
No
Recommendation?
label_recommendation_2
Accept with minor revision (please list in comments)
Comments to the Author(s)
label_comment_2
In this paper, Weidemann and Kahana investigate the utility of RT measures in constructing
ROCs from recognition memory experiments (comparing them to traditional confidence-based
ROCs).
It took me a long time to appreciate the value of this work. I recommend that this paper be
accepted for publication because the data are important, but I also think the authors could do a
better job of explaining their importance. For example, at the end of section 3.1, they say "This
correlation, however, is inflated by the fact that both ROC functions are constrained to pass
through the classification point." A reader who does not immediately grasp what is being said
here is going to have a hard time appreciating the value of what is presented next in section 3.2
(which is the main news). Once you understand why the r = .93 statistic is artificially inflated, it
seems so simple. But I had to think about it for a long time before I finally understood, and the
authors should not require readers to do that (because most won't). The problem is that RT is
non-monotonic, so a fast RT does not in itself tell you if the decision is "old" or "new." Thus, you
need the classification response. If only a small percentage of lures is mistakenly classified as old
and a large percentage of targets is correctly classified as old, then the RT-ROC will necessarily
show a low overall false alarm rate and a high overall hit rate. That is the constraint. All of this
4
should be reiterated at the end of section 3.1. Unless this point is fully understood by the reader,
what comes next in section 3.2 just seems like a bizarre approach to take.
But it is not bizarre at all. In fact, the analyses in section 3.2 are more interesting than the authors
seem to appreciate. I have always thought the biggest hurdle faced by a threshold model is that
there is information available for items judged to be new. In a threshold model, items judged as
new are below threshold (i.e., there is no information associated with those items). Yet the "new"
ROC in Figure 3 and the "new vs. old" ROCs in Figure 4 show that the threshold view is not
viable. Yet people still rely on multinomial models of recognition, which have been shown to be
able to handle confidence-based ROCs. They can, but it is definitely newsworthy that an RT-ROC
shows (proves maybe?) that there is continuous information associated with items that have been
judged to be new. Perhaps even the confidence-based "new" ROC in Figure 3 makes that point,
but I am not entirely sure about that. The "new" RT-ROC certainly does. I think it would be useful
to show what a threshold model predicts about a "new" ROC. The theoretical importance of these
new results would be clearer. Also, recent work by Kellen, Klauer and colleagues has been
interpreted to mean that confidence ratings are not continuous readouts of a continuous memory
signal. I don't think the same argument can be applied to RT data, so that would be another
interesting theoretical angle to pursue.
Minor points:
1. On page 2, it says "The area under an ROC function (AUC), however, provides an index of
discriminability which does not depend on strong (and typically untestable) assumptions about
the distribution of internal states…" The word "untestable" is too strong. Mickes et al. (2007,
PB&R) tested the Gaussian assumption and found evidence consistent with it. I think
"unprovable" might be a fair word to use here, but not "untestable."
2. I did not see the value of the recalled-vs.-unrecalled analyses, but maybe I am missing
something (certainly possible given that I almost missed why section 3.2 was needed and why it
is so interesting).
3. With regard to the figures, different letters should be added to each graph and then used when
referring to a particular graph in the figure. I sometimes was not sure which graph the authors
wanted me to look at when they sent me to "Figure 3."
John Wixted
UC San Diego
label_end_comment
Decision letter (RSOS-150670)
2nd February 2016
Dear Dr Weidemann
On behalf of the Editors, I am pleased to inform you that your Manuscript RSOS-150670 entitled
"Assessing recognition memory using confidence ratings and response times" has been accepted
for publication in Royal Society Open Science subject to minor revision in accordance with the
referee suggestions. Please find the referees' comments at the end of this email.
5
The reviewers and handling editors have recommended publication, but also suggest some minor
revisions to your manuscript. Therefore, I invite you to respond to the comments and revise your
manuscript.
• Ethics statement
If your study uses humans or animals please include details of the ethical approval received,
including the name of the committee that granted approval. For human studies please also detail
whether informed consent was obtained. For field studies on animals please include details of all
permissions, licences and/or approvals granted to carry out the fieldwork.
• Data accessibility
It is a condition of publication that all supporting data are made available either as
supplementary information or preferably in a suitable permanent repository. The data
accessibility section should state where the article's supporting data can be accessed. This section
should also include details, where possible of where to access other relevant research materials
such as statistical tools, protocols, software etc can be accessed. If the data has been deposited in
an external repository this section should list the database, accession number and link to the DOI
for all data from the article that has been made publicly available. Data sets that have been
deposited in an external repository and have a DOI should also be appropriately cited in the
manuscript and included in the reference list.
If you wish to submit your supporting data or code to Dryad (http://datadryad.org/), or modify
your current submission to dryad, please use the following link:
http://datadryad.org/submit?journalID=RSOS&manu=RSOS-150670
• Competing interests
Please declare any financial or non-financial competing interests, or state that you have no
competing interests.
• Authors’ contributions
All submissions, other than those with a single author, must include an Authors’ Contributions
section which individually lists the specific contribution of each author. The list of Authors
should meet all of the following criteria; 1) substantial contributions to conception and design, or
acquisition of data, or analysis and interpretation of data; 2) drafting the article or revising it
critically for important intellectual content; and 3) final approval of the version to be published.
All contributors who do not meet all of these criteria should be included in the
acknowledgements.
We suggest the following format:
AB carried out the molecular lab work, participated in data analysis, carried out sequence
alignments, participated in the design of the study and drafted the manuscript; CD carried out
the statistical analyses; EF collected field data; GH conceived of the study, designed the study,
coordinated the study and helped draft the manuscript. All authors gave final approval for
publication.
• Acknowledgements
Please acknowledge anyone who contributed to the study but did not meet the authorship
criteria.
• Funding statement
Please list the source of funding for each author.
6
Because the schedule for publication is very tight, it is a condition of publication that you submit
the revised version of your manuscript within 7 days (i.e. by the 9th February). If you do not
think you will be able to meet this date please let me know immediately.
To revise your manuscript, log into https://mc.manuscriptcentral.com/rsos and enter your
Author Centre, where you will find your manuscript title listed under "Manuscripts with
Decisions". Under "Actions," click on "Create a Revision." You will be unable to make your
revisions on the originally submitted version of the manuscript. Instead, revise your manuscript
and upload a new version through your Author Centre.
When submitting your revised manuscript, you will be able to respond to the comments made by
the referees and upload a file "Response to Referees" in "Section 6 - File Upload". You can use this
to document any changes you make to the original manuscript. In order to expedite the
processing of the revised manuscript, please be as specific as possible in your response to the
referees.
When uploading your revised files please make sure that you have:
1) A text file of the manuscript (tex, txt, rtf, docx or doc), references, tables (including captions)
and figure captions. Do not upload a PDF as your "Main Document".
2) A separate electronic file of each figure (EPS or print-quality PDF preferred (either format
should be produced directly from original creation package), or original software format)
3) Included a 100 word media summary of your paper when requested at submission. Please
ensure you have entered correct contact details (email, institution and telephone) in your user
account
4) Included the raw data to support the claims made in your paper. You can either include your
data as electronic supplementary material or upload to a repository and include the relevant doi
within your manuscript
5) Included your supplementary files in a format you are happy with (no line numbers,
vancouver referencing, track changes removed etc) as these files will NOT be edited in
production
Once again, thank you for submitting your manuscript to Royal Society Open Science and I look
forward to receiving your revision. If you have any questions at all, please do not hesitate to get
in touch.
Best wishes
Matthew Allinson
Editorial Coordinator, Royal Society Open Science
on behalf of Essi Viding
Subject Editor, Royal Society Open Science
openscience@royalsociety.org
Reviewer comments to Author:
Reviewer: 1
Comments to the Author(s)
In this paper, Weidemann and Kahana investigate the utility of RT measures in constructing
ROCs from recognition memory experiments (comparing them to traditional confidence-based
ROCs).
7
It took me a long time to appreciate the value of this work. I recommend that this paper be
accepted for publication because the data are important, but I also think the authors could do a
better job of explaining their importance. For example, at the end of section 3.1, they say "This
correlation, however, is inflated by the fact that both ROC functions are constrained to pass
through the classification point." A reader who does not immediately grasp what is being said
here is going to have a hard time appreciating the value of what is presented next in section 3.2
(which is the main news). Once you understand why the r = .93 statistic is artificially inflated, it
seems so simple. But I had to think about it for a long time before I finally understood, and the
authors should not require readers to do that (because most won't). The problem is that RT is
non-monotonic, so a fast RT does not in itself tell you if the decision is "old" or "new." Thus, you
need the classification response. If only a small percentage of lures is mistakenly classified as old
and a large percentage of targets is correctly classified as old, then the RT-ROC will necessarily
show a low overall false alarm rate and a high overall hit rate. That is the constraint. All of this
should be reiterated at the end of section 3.1. Unless this point is fully understood by the reader,
what comes next in section 3.2 just seems like a bizarre approach to take.
But it is not bizarre at all. In fact, the analyses in section 3.2 are more interesting than the authors
seem to appreciate. I have always thought the biggest hurdle faced by a threshold model is that
there is information available for items judged to be new. In a threshold model, items judged as
new are below threshold (i.e., there is no information associated with those items). Yet the "new"
ROC in Figure 3 and the "new vs. old" ROCs in Figure 4 show that the threshold view is not
viable. Yet people still rely on multinomial models of recognition, which have been shown to be
able to handle confidence-based ROCs. They can, but it is definitely newsworthy that an RT-ROC
shows (proves maybe?) that there is continuous information associated with items that have been
judged to be new. Perhaps even the confidence-based "new" ROC in Figure 3 makes that point,
but I am not entirely sure about that. The "new" RT-ROC certainly does. I think it would be useful
to show what a threshold model predicts about a "new" ROC. The theoretical importance of these
new results would be clearer. Also, recent work by Kellen, Klauer and colleagues has been
interpreted to mean that confidence ratings are not continuous readouts of a continuous memory
signal. I don't think the same argument can be applied to RT data, so that would be another
interesting theoretical angle to pursue.
Minor points:
1. On page 2, it says "The area under an ROC function (AUC), however, provides an index of
discriminability which does not depend on strong (and typically untestable) assumptions about
the distribution of internal states…" The word "untestable" is too strong. Mickes et al. (2007,
PB&R) tested the Gaussian assumption and found evidence consistent with it. I think
"unprovable" might be a fair word to use here, but not "untestable."
2. I did not see the value of the recalled-vs.-unrecalled analyses, but maybe I am missing
something (certainly possible given that I almost missed why section 3.2 was needed and why it
is so interesting).
3. With regard to the figures, different letters should be added to each graph and then used when
referring to a particular graph in the figure. I sometimes was not sure which graph the authors
wanted me to look at when they sent me to "Figure 3."
John Wixted
UC San Diego
8
Reviewer: 2
Comments to the Author(s)
Review
Royal Society Open Science
Assessing recognition memory using confidence ratings and response times
Christoph Weidemann & Michael Kahana
This article investigates the extent to which response times (RTs) to an old-new recognition
memory decision can be used to quantify recognition discriminability. The authors use RTs from
old-new judgments to derive measures of sensitivity (area under the ROC curve, AUC). The main
finding is that, like rating data, RTs provide diagnostic information over and above the old-new
judgment, but that the AUC derived from RTs is lower than the AUC derived from ratings. The
authors go on to show that this pattern holds when the data for “old” and “new” responses is
analysed separately, being more pronounced within the subset of “old” responses.
Overall, I thought that this was an interesting, useful, and straightforward paper, and could be
accepted with only minor revisions. Although many researchers collect reaction time data in
recognition memory experiments, this data is often not reported, and the focus tends to be on the
rating data. It is known that RTs tend to be inversely related to confidence / distance from the
response criterion, but RTs have tended not to feature in mainstream theories of recognition. This
is changing, and any complete theory of recognition memory will also arguably need to explain
RTs. Efforts to bring the analysis of RTs from recognition memory tasks more into the
mainstream therefore seem useful, and by using RTs to construct ROCs, the article contributes to
this aim. This technique has been used in many other research areas, but is not typical in the
recognition memory literature.
Specific points
1. It would be useful to have some basic descriptive statistics about the RTs (e.g., means, variance,
nature of their distribution).
2. At first I did not understand the analysis in Section 3.3. Then I realised that participants had
completed a free recall task prior to the recognition task and the data were being analysed
according to whether items had been recalled in this task or not. Nevertheless, I couldn’t find any
real rationale for this analysis and I generally found this section difficult to follow. It wasn't clear
to me what this added.
3. The data have been averaged across multiple sessions, and we learn in the Discussion that
some of these sessions have different proportions of old and new items. This would reasonably
be expected to have an effect on a participant’s response bias, as the authors mention. It would be
useful to know a bit more about the design of the sessions that were included in the analysis. I
appreciate that the main difference of interest is between RT and rating based sensitivity
measures, but can the authors say more to allay the concern that RT and rating measures may be
differentially sensitive to particular manipulations used across sessions, such as the proportion of
old/new items?
4. In the conclusions, the authors suggest that RTs could act as a substitute for ratings in ROC
construction. In the paradigm used, however, a rating response always followed the old-new
judgment. The authors show that RT data and rating data converge to some extent with this
paradigm, but wouldn’t it be stronger to demonstrate the same L-ROC finding in a paradigm in
which no ratings are collected? It could be that the RT to the old-new response is affected by the
participant knowing they are about to make a rating.
5. Could the finding that the area under the C-ROC is greater than the L-ROC arise because the
mapping of the strength signal to the RT is noisier than the mapping of the signal to the
confidence rating?
6. I could not find the formula used for calculating AUC.
9
7. I could download a data file from the supplementary files (in .hdf5 format), but couldn’t access
it with common software packages. I gather this format is for use in Python.
8. The y-axis labels in Figures 5 and 7 were cut-off.
Author's Response to Decision Letter for (RSOS-150670)
See Appendix A.
Christoph T. Weidemann Michael J. Kahana
Swansea University University of Pennsylvania
Department of Psychology Department of Psychology
Singleton Park Suite 302c, 3401 Walnut St.
Swansea, SA2 8PP, Wales, UK Philadelphia, PA 19104, USA
phone: +1-917-512-5338 phone: +1-215-746-3501
e-mail: ctw@cogsci.info e-mail: kahana@sas.upenn.edu
March 10, 2016
. Viding:
reciate the thoughtful comments on our manuscript (RSOS-150670) from the expert reviewers. Attached,
nd a revised version of our manuscript. Below we highlight excerpts from the attached reviews that require
es along with our replies.
Comments from Reviewer 1 (John Wixted)
. ] Once you understand why the r = .93 statistic is artificially inflated, it seems so simple. But I had
think about it for a long time before I finally understood, and the authors should not require readers
do that (because most won’t). The problem is that RT is non-monotonic, so a fast RT does not in
elf tell you if the decision is “old” or “new.” Thus, you need the classification response. If only a small
rcentage of lures is mistakenly classified as old and a large percentage of targets is correctly classified
old, then the RT-ROC will necessarily show a low overall false alarm rate and a high overall hit rate.
at is the constraint. [. . . ] All of this should be reiterated at the end of section 3.1. Unless this point
fully understood by the reader, what comes next in section 3.2 just seems like a bizarre approach to
ke.
e added a new paragraph at the end of section 3.1 which explains this issue in more detail and thus better
es the following section.
ave always thought the biggest hurdle faced by a threshold model is that there is information available
r items judged to be new. In a threshold model, items judged as new are below threshold (i.e., there
no information associated with those items). Yet the “new” ROC in Figure 3 and the “new vs. old”
OCs in Figure 4 show that the threshold view is not viable. Yet people still rely on multinomial models
recognition, which have been shown to be able to handle confidence-based ROCs. They can, but it
definitely newsworthy that an RT-ROC shows (proves maybe?) that there is continuous information
sociated with items that have been judged to be new. Perhaps even the confidence-based “new” ROC
Figure 3 makes that point, but I am not entirely sure about that. The “new” RT-ROC certainly does. I
nk it would be useful to show what a threshold model predicts about a “new” ROC. The theoretical im-
rtance of these new results would be clearer. Also, recent work by Kellen, Klauer and colleagues has
en interpreted to mean that confidence ratings are not continuous readouts of a continuous memory
gnal. I don’t think the same argument can be applied to RT data, so that would be another interesting
eoretical angle to pursue.
refer to Kellen & Klauer’s recent Psych. Review paper in the motivation to our recall based analysis. The
possible threshold and signal detection models is too large for us to be comfortable to interpret our results
ding strong support for either family of models. The similarities between ROCs derived from confidence
and RTs appear to suggest that conclusions from confidence ratings would generalize to response times.
n page 2, it says “The area under an ROC function (AUC), however, provides an index of discriminabil-
which does not depend on strong (and typically untestable) assumptions about the distribution of
ernal states. . . ” The word “untestable” is too strong. Mickes et al. (2007, PB&R) tested the Gaussian
sumption and found evidence consistent with it. I think “unprovable” might be a fair word to use here,
t not “untestable.”
e changed the wording to “typically untested”.
id not see the value of the recalled-vs.-unrecalled analyses, but maybe I am missing something (cer-
nly possible given that I almost missed why section 3.2 was needed and why it is so interesting).
better motivate this section.
ith regard to the figures, different letters should be added to each graph and then used when referring
a particular graph in the figure. I sometimes was not sure which graph the authors wanted me to look
when they sent me to “Figure 3.”
e now added letters to more easily identify the different sections of Figures 1 and 3 and have taken greater
dentify which part of each figure we are referring to.
Comments from Reviewer 2
would be useful to have some basic descriptive statistics about the RTs (e.g., means, variance, nature
their distribution).
e now added some basic descriptive statistics at the beginning of the results section.
first I did not understand the analysis in Section 3.3. Then I realised that participants had completed
free recall task prior to the recognition task and the data were being analysed according to whether
ms had been recalled in this task or not. Nevertheless, I couldnt find any real rationale for this analysis
d I generally found this section difficult to follow. It wasn’t clear to me what this added.
better motivate this analysis.
e data have been averaged across multiple sessions, and we learn in the Discussion that some of
ese sessions have different proportions of old and new items. This would reasonably be expected
have an effect on a participants response bias, as the authors mention. It would be useful to know
bit more about the design of the sessions that were included in the analysis. I appreciate that the
ain difference of interest is between RT and rating based sensitivity measures, but can the authors
y more to allay the concern that RT and rating measures may be differentially sensitive to particular
anipulations used across sessions, such as the proportion of old/new items?
ants were not made aware of the varying frequency of new items and this manipulation had miniscule
n the resulting ROC functions. Furthermore this manipulation was counterbalanced across sessions such
cts of this manipulation should average out.
the conclusions, the authors suggest that RTs could act as a substitute for ratings in ROC construction.
the paradigm used, however, a rating response always followed the old-new judgment. The authors
ow that RT data and rating data converge to some extent with this paradigm, but wouldnt it be stronger
demonstrate the same L-ROC finding in a paradigm in which no ratings are collected? It could be that
e RT to the old-new response is affected by the participant knowing they are about to make a rating.
e that the RT to the old-new response could be affected by the anticipation of a confidence rating and future
should investigate L-ROC functions based on classification responses that are not followed by ratings. It
rtant to note, however, that absolute levels of RTs do not affect the corresponding L-ROCs and thus any
t increase in classification RT due to the anticipation of the rating response would not affect the results.
lyses depended on us being able to directly compare performance for these two measures and additional
e due to interindividual differences or other variability in classification performance would have complicated
mparisons.
ould the finding that the area under the C-ROC is greater than the L-ROC arise because the mapping
the strength signal to the RT is noisier than the mapping of the signal to the confidence rating?
he discussion section we explore different candidates for such noise (e.g., fast guesses).
ould not find the formula used for calculating AUC.
specify (in the appendix) that “[t]he area under the ROC functions can be calculated by adding the areas
by each pair of points and their projections onto the ordinate.” We have also supplied the analysis scripts
erated our Figures (both for the data and the tutorial in the appendix).
ould download a data file from the supplementary files (in .hdf5 format), but couldnt access it with
mmon software packages. I gather this format is for use in Python.
an open data format not specific to Python. We have added relevant information in the data accessibility
e y-axis labels in Figures 5 and 7 were cut-off.
gures have been corrected.
Sincerely,
Christoph T. Weidemann
Society Open
