Cue predictability does not modulate bottom-up attentional
capture
Erik L. Meijs, Felix H. Klaassen, Levan Bokeria, Simon van Gaal and Floris P. de Lange
Article citation details
R. Soc. open sci. 5: 180524.
http://dx.doi.org/10.1098/rsos.180524
Review timeline
Original submission: 30 March 2018 Note: Reports are unedited and appear as
1st revised submission: 26 July 2018 submitted by the referee. The review history
2nd revised submission: 14 September 2018 appears in chronological order.
Final acceptance: 27 September 2018
Review History
label_version_1
RSOS-180524.R0 (Original submission)
label_author_1
Review form: Reviewer 1
Is the manuscript scientifically sound in its present form?
Yes
Are the interpretations and conclusions justified by the results?
Yes
Is the language acceptable?
Yes
Is it clear how to access all supporting data?
Yes
Do you have any ethical concerns with this paper?
No
Reports © 2018 The Reviewers; Decision Letters © 2018 The Reviewers and Editors;
Responses © 2018 The Reviewers, Editors and Authors. Published by the Royal Society under the
terms of the Creative Commons Attribution License http://creativecommons.org/licenses/by/4.0/,
which permits unrestricted use, provided the original author and source are credited
2
Have you any concerns about statistical analyses in this paper?
Yes
Recommendation?
label_recommendation_1
Major revision is needed (please make suggestions in comments)
Comments to the Author(s)
label_comment_1
In three experiments, the authors investigated whether bottom-up attentional capture by abrupt
onset cues depended on the predictability of the cues in the spatial and in the temporal domain.
In Experiment 1, spatial predictability of the cues was manipulated by presenting the cues more
likely at one out of two possible display locations. In Experiment 2, the temporal predictability of
the cues was manipulated by presenting cues and targets in continuous blocks in which the
temporal regularity of cue presentation was varied. In Experiment 3, the temporal predictability
of the cues was varied. In addition, cues were presented in a spatially predictable way only in one
display location. As none of the manipulations of cue predictability had a significant impact on
the cueing effects, the authors concluded that bottom-up attention capture by the cues was
independent of prior expectations about cue onset.
I think the authors are addressing an interesting research question. The study has been carried
out well and the statistical data analysis is overall reasonable and sound. I have some remarks
and concerns which are mostly related to the interpretation and the statistical analyses:
1) In each experiment, the searched-for target was an abrupt onset singleton and the cue was an
abrupt onset singleton too. This allows for an explanation of the cueing effects in terms of top-
down contingent capture (Folk et al., 1992). I think this complicates the interpretation of the
present results because the cues could have captured attention due to the similarity between cue
and target features. This might explain why cues captured attention when they were predictable,
and it might also explain why no difference was found between conditions with predictable and
less predictable cues.
2) What was the result for the neutral control group in Experiment 1? Figure 1 shows the cueing
effect for this group, but was it statistically compared with the cueing effects of the test group in
which cue expectation was manipulated? It would be interesting to see this analysis.
3) It was not clear enough how the time windows of the SOAs for which the cueing effects were
analyzed in Experiment 2 and 3 were identified. Please explain the procedure in more detail.
4) The fact that the cueing effects with the 117 ms SOA were larger in Experiment 1 compared to
Experiment 2 is a bit puzzling. In fact, it casts some doubt on the notion that the attentional
capture effect by the cues is fully automatic (bottom-up).
5) I did not quite understand the motivation for spatial manipulation employed in Experiment 3.
The cues were presented only in one display location to make them predictable in the spatial
domain. Apparently, this was also the case in the blocks with low temporal predictability of the
cues? The spatial predictability of the cues in both blocks (the temporally predictable block and
the temporally unpredictable block) could have been the reason why no difference was found
between the blocks in this experiment. Wouldn’t it be more appropriate to have one block in
which the cue is fully predictable (spatially and temporally) and one block in which the cue
remained unpredictable?
6) The authors concede that “… it is possible that the task set did not optimally support
suppression of uninformative cues …” (p. 21). Please explain this in more detail. I think especially
the fact that the cues shared with the targets some of the critical searched-for features is
important in this context (see 1)).
3
7) The authors should check the analyses in Experiment 2 and 3 and the respective degrees of
freedom. The degrees of freedom do not appear to match the number of participants (n=63 and
n=58?). In addition, why are there different degrees of freedom for the late and for the early time
window (Exp 2 and 3)?
label_author_2
Review form: Reviewer 2
Is the manuscript scientifically sound in its present form?
Yes
Are the interpretations and conclusions justified by the results?
No
Is the language acceptable?
Yes
Is it clear how to access all supporting data?
Yes
Do you have any ethical concerns with this paper?
No
Have you any concerns about statistical analyses in this paper?
No
Recommendation?
label_recommendation_2
Major revision is needed (please make suggestions in comments)
Comments to the Author(s)
label_comment_2
This article reports a study of the effect of predictability on attentional capture by physically
salient stimuli. Each experiment used a cueing task in which participants were required to detect
a target that could occur in the same location as a preceding cue (valid trials) or a different
location (invalid trials). The cue was non-informative, so valid trials and invalid trials were
equally common. In Experiment 1 the cue occurred in one location more often than the other. In
Experiment 2, the cue occurred more regularly in one condition than another, but location was
unpredictable. In Experiment 3, the cue occurred regularly in a predictable location. The size of
the validity effect (difference in RT on valid vs invalid trials) was not significantly influenced by
any of these manipulations of cue predictability.
1. The manuscript is well written and the results are analyzed appropriately – what is here seems
solid. However, my concern is that the experiments may not be well set up to test what they are
designed to test. This issue is raised in the Discussion (p21). Specifically, the use of a non-
informative cue means that there is little incentive to apply top-down suppression to the cue,
since the target occurs equally often in the cue location (in which case attending to the cue will
speed responses) as in the uncued location (in which case attending to the cue will slow
responses). So overall there may be little or no net negative consequence to attending to the cue,
and hence little drive to suppress. As noted on p20 of the manuscript, this is in contrast to other
studies showing an effect of predictability in which the distractor and target are always at
4
different locations and hence there is a clear disadvantage in attending to the distractor (and
consequently a greater drive to suppress).
To reiterate, these issues are raised in the manuscript (“in all experiments the tasks we used may
have had factors that made participants attend cues (instead of ignore them). It is possible that
the fact that task set did not optimally support suppression of uninformative cues obscured any
potential effects in our tasks”). But this significant caveat is ignored in the abstract (“This
underscores the automatic and reflexive nature of bottom-up attention”) and title (“Stimulus
predictability does not modulate bottom-up attentional capture”), which make very general-
sounding claims that cannot be substantiated by these data. I think the authors need to be clearer
throughout in noting that their findings are qualified by these issues.
2. Experiment 3 found no significant attentional capture effect; this is not mentioned in the
Discussion, but is at least consistent with the idea that capture can be modulated by top-down
processes (when the cue is completely predictable). Likewise, in discussing the difference in
capture between Expts 1 and 2 the authors suggest that “participants deploy a different strategy
in Experiment 2 compared to Experiment 1, in which they focus more on the targets and less on
the cues” (p22). This again would suggest that capture is not totally inevitable/reflexive, but is
glossed over in the final conclusions paragraph which states that “in the exogenous cueing tasks
we used, bottom-up attentional capture was not altered by prior knowledge about the location or
time point of the distracting input”.
[On a related issue: The lack of a significant difference in the size of the validity effect in
Experiments 2 and 3 (p19) does not show that “the validity effect was roughly the same in both
experiments”. It shows that there was not a statistically significant difference in the size of the
effect in the two experiments – this is a somewhat different claim, e.g., the null result could be
due to a lack of power.]
3. Experiment 1 had a much larger sample size than Experiments 2 and 3 (120 vs 67/61) – was
there are reason for this? More generally, how were sample sizes determined?
4. The manuscript provides a single Methods section that deals with all three experiments, and
then a single Results section dealing with each study in turn. But the start of each section of the
Results ends up providing a reminder of the rationale and procedure of each experiment. I
suspect the manuscript would be easier to follow and avoid redundancy if the three experiments
were separated – i.e., a separate Method and Results section for each (with Method for Expts 2
and 3 being very short, since it’s largely the same as for earlier experiments).
5. Accuracy data are not analyzed (or even presented) because accuracy was “close to ceiling” in
each experiment. Regardless of its overall level, at the very least accuracy data should be
presented (and ideally analyzed). If the authors are concerned about interrupting the flow of their
manuscript, perhaps they could put these additional analyses in supplementary materials.
6. Were participants explicitly informed prior to the task that the cue location and target location
were unrelated? Also, were they informed about the predictability manipulation, e.g., that the cue
would appear most often in (say) the top location? Or did they learn this via experience?
7. P4-5, “Based on the evidence listed earlier, we anticipated that unexpected cue stimuli attract
more attention and therefore in larger cue-target validity effects”. Should be something like
“…stimuli WILL attract more attention and therefore PRODUCE larger cue-target validity
effects”.
8. P9, “This analysis only considered participants that had expectations about the most likely cue
5
location”. As far as I can tell, whether participants actually had expectations about the cue
location was not assessed, so this might be more accurately phrased as something like “…
participants for whom the cue location was predictable”. This issue is implicitly acknowledged
on p20, “Assuming that our manipulations of expectations actually instantiated priors in our
subjects…”.
9. P20: “Other studies have used similar paradigms with streams of stimulation to investigate
effects of temporal expectations and did find effects of temporal regularity”. Should this be “did
not find”?
label_end_comment
Decision letter (RSOS-180524.R0)
13-Jun-2018
Dear Mr Meijs,
The editors assigned to your paper ("Stimulus predictability does not modulate bottom-up
attentional capture") have now received comments from reviewers. We would like you to revise
your paper in accordance with the referee and Associate Editor suggestions which can be found
below (not including confidential reports to the Editor). Please note this decision does not
guarantee eventual acceptance.
Please submit a copy of your revised paper within three weeks (i.e. by the 06-Jul-2018). If we do
not hear from you within this time then it will be assumed that the paper has been withdrawn. In
exceptional circumstances, extensions may be possible if agreed with the Editorial Office in
advance.We do not allow multiple rounds of revision so we urge you to make every effort to
fully address all of the comments at this stage. If deemed necessary by the Editors, your
manuscript will be sent back to one or more of the original reviewers for assessment. If the
original reviewers are not available, we may invite new reviewers.
To revise your manuscript, log into http://mc.manuscriptcentral.com/rsos and enter your
Author Centre, where you will find your manuscript title listed under "Manuscripts with
Decisions." Under "Actions," click on "Create a Revision." Your manuscript number has been
appended to denote a revision. Revise your manuscript and upload a new version through your
Author Centre.
When submitting your revised manuscript, you must respond to the comments made by the
referees and upload a file "Response to Referees" in "Section 6 - File Upload". Please use this to
document how you have responded to the comments, and the adjustments you have made. In
order to expedite the processing of the revised manuscript, please be as specific as possible in
your response.
In addition to addressing all of the reviewers' and editor's comments please also ensure that your
revised manuscript contains the following sections as appropriate before the reference list:
• Ethics statement (if applicable)
If your study uses humans or animals please include details of the ethical approval received,
including the name of the committee that granted approval. For human studies please also detail
whether informed consent was obtained. For field studies on animals please include details of all
permissions, licences and/or approvals granted to carry out the fieldwork.
6
• Data accessibility
It is a condition of publication that all supporting data are made available either as
supplementary information or preferably in a suitable permanent repository. The data
accessibility section should state where the article's supporting data can be accessed. This section
should also include details, where possible of where to access other relevant research materials
such as statistical tools, protocols, software etc can be accessed. If the data have been deposited in
an external repository this section should list the database, accession number and link to the DOI
for all data from the article that have been made publicly available. Data sets that have been
deposited in an external repository and have a DOI should also be appropriately cited in the
manuscript and included in the reference list.
If you wish to submit your supporting data or code to Dryad (http://datadryad.org/), or modify
your current submission to dryad, please use the following link:
http://datadryad.org/submit?journalID=RSOS&manu=RSOS-180524
• Competing interests
Please declare any financial or non-financial competing interests, or state that you have no
competing interests.
• Authors’ contributions
All submissions, other than those with a single author, must include an Authors’ Contributions
section which individually lists the specific contribution of each author. The list of Authors
should meet all of the following criteria; 1) substantial contributions to conception and design, or
acquisition of data, or analysis and interpretation of data; 2) drafting the article or revising it
critically for important intellectual content; and 3) final approval of the version to be published.
All contributors who do not meet all of these criteria should be included in the
acknowledgements.
We suggest the following format:
AB carried out the molecular lab work, participated in data analysis, carried out sequence
alignments, participated in the design of the study and drafted the manuscript; CD carried out
the statistical analyses; EF collected field data; GH conceived of the study, designed the study,
coordinated the study and helped draft the manuscript. All authors gave final approval for
publication.
• Acknowledgements
Please acknowledge anyone who contributed to the study but did not meet the authorship
criteria.
• Funding statement
Please list the source of funding for each author.
Please note that Royal Society Open Science will introduce article processing charges for all new
submissions received from 1 January 2018. Charges will also apply to papers transferred to Royal
Society Open Science from other Royal Society Publishing journals, as well as papers submitted
as part of our collaboration with the Royal Society of Chemistry
(http://rsos.royalsocietypublishing.org/chemistry). If your manuscript is submitted and
accepted for publication after 1 Jan 2018, you will be asked to pay the article processing charge,
unless you request a waiver and this is approved by Royal Society Publishing. You can find out
more about the charges at http://rsos.royalsocietypublishing.org/page/charges. Should you
have any queries, please contact openscience@royalsociety.org.
7
Once again, thank you for submitting your manuscript to Royal Society Open Science and I look
forward to receiving your revision. If you have any questions at all, please do not hesitate to get
in touch.
Kind regards,
Alice Power
Royal Society Open Science
openscience@royalsociety.org
on behalf of Dr Narayanan Srinivasan (Associate Editor) and Antonia Hamilton (Subject Editor)
openscience@royalsociety.org
Associate Editor's comments (Dr Narayanan Srinivasan):
Two reviewers have now commented on the paper. While they find the paper interesting, they
also point out to major issues that need to be addressed.
Comments to Author:
Reviewers' Comments to Author:
Reviewer: 1
Comments to the Author(s)
In three experiments, the authors investigated whether bottom-up attentional capture by abrupt
onset cues depended on the predictability of the cues in the spatial and in the temporal domain.
In Experiment 1, spatial predictability of the cues was manipulated by presenting the cues more
likely at one out of two possible display locations. In Experiment 2, the temporal predictability of
the cues was manipulated by presenting cues and targets in continuous blocks in which the
temporal regularity of cue presentation was varied. In Experiment 3, the temporal predictability
of the cues was varied. In addition, cues were presented in a spatially predictable way only in one
display location. As none of the manipulations of cue predictability had a significant impact on
the cueing effects, the authors concluded that bottom-up attention capture by the cues was
independent of prior expectations about cue onset.
I think the authors are addressing an interesting research question. The study has been carried
out well and the statistical data analysis is overall reasonable and sound. I have some remarks
and concerns which are mostly related to the interpretation and the statistical analyses:
1) In each experiment, the searched-for target was an abrupt onset singleton and the cue was an
abrupt onset singleton too. This allows for an explanation of the cueing effects in terms of top-
down contingent capture (Folk et al., 1992). I think this complicates the interpretation of the
present results because the cues could have captured attention due to the similarity between cue
and target features. This might explain why cues captured attention when they were predictable,
and it might also explain why no difference was found between conditions with predictable and
less predictable cues.
2) What was the result for the neutral control group in Experiment 1? Figure 1 shows the cueing
effect for this group, but was it statistically compared with the cueing effects of the test group in
which cue expectation was manipulated? It would be interesting to see this analysis.
3) It was not clear enough how the time windows of the SOAs for which the cueing effects were
analyzed in Experiment 2 and 3 were identified. Please explain the procedure in more detail.
8
4) The fact that the cueing effects with the 117 ms SOA were larger in Experiment 1 compared to
Experiment 2 is a bit puzzling. In fact, it casts some doubt on the notion that the attentional
capture effect by the cues is fully automatic (bottom-up).
5) I did not quite understand the motivation for spatial manipulation employed in Experiment 3.
The cues were presented only in one display location to make them predictable in the spatial
domain. Apparently, this was also the case in the blocks with low temporal predictability of the
cues? The spatial predictability of the cues in both blocks (the temporally predictable block and
the temporally unpredictable block) could have been the reason why no difference was found
between the blocks in this experiment. Wouldn’t it be more appropriate to have one block in
which the cue is fully predictable (spatially and temporally) and one block in which the cue
remained unpredictable?
6) The authors concede that “… it is possible that the task set did not optimally support
suppression of uninformative cues …” (p. 21). Please explain this in more detail. I think especially
the fact that the cues shared with the targets some of the critical searched-for features is
important in this context (see 1)).
7) The authors should check the analyses in Experiment 2 and 3 and the respective degrees of
freedom. The degrees of freedom do not appear to match the number of participants (n=63 and
n=58?). In addition, why are there different degrees of freedom for the late and for the early time
window (Exp 2 and 3)?
Reviewer: 2
Comments to the Author(s)
This article reports a study of the effect of predictability on attentional capture by physically
salient stimuli. Each experiment used a cueing task in which participants were required to detect
a target that could occur in the same location as a preceding cue (valid trials) or a different
location (invalid trials). The cue was non-informative, so valid trials and invalid trials were
equally common. In Experiment 1 the cue occurred in one location more often than the other. In
Experiment 2, the cue occurred more regularly in one condition than another, but location was
unpredictable. In Experiment 3, the cue occurred regularly in a predictable location. The size of
the validity effect (difference in RT on valid vs invalid trials) was not significantly influenced by
any of these manipulations of cue predictability.
1. The manuscript is well written and the results are analyzed appropriately – what is here seems
solid. However, my concern is that the experiments may not be well set up to test what they are
designed to test. This issue is raised in the Discussion (p21). Specifically, the use of a non-
informative cue means that there is little incentive to apply top-down suppression to the cue,
since the target occurs equally often in the cue location (in which case attending to the cue will
speed responses) as in the uncued location (in which case attending to the cue will slow
responses). So overall there may be little or no net negative consequence to attending to the cue,
and hence little drive to suppress. As noted on p20 of the manuscript, this is in contrast to other
studies showing an effect of predictability in which the distractor and target are always at
different locations and hence there is a clear disadvantage in attending to the distractor (and
consequently a greater drive to suppress).
To reiterate, these issues are raised in the manuscript (“in all experiments the tasks we used may
have had factors that made participants attend cues (instead of ignore them). It is possible that
the fact that task set did not optimally support suppression of uninformative cues obscured any
9
potential effects in our tasks”). But this significant caveat is ignored in the abstract (“This
underscores the automatic and reflexive nature of bottom-up attention”) and title (“Stimulus
predictability does not modulate bottom-up attentional capture”), which make very general-
sounding claims that cannot be substantiated by these data. I think the authors need to be clearer
throughout in noting that their findings are qualified by these issues.
2. Experiment 3 found no significant attentional capture effect; this is not mentioned in the
Discussion, but is at least consistent with the idea that capture can be modulated by top-down
processes (when the cue is completely predictable). Likewise, in discussing the difference in
capture between Expts 1 and 2 the authors suggest that “participants deploy a different strategy
in Experiment 2 compared to Experiment 1, in which they focus more on the targets and less on
the cues” (p22). This again would suggest that capture is not totally inevitable/reflexive, but is
glossed over in the final conclusions paragraph which states that “in the exogenous cueing tasks
we used, bottom-up attentional capture was not altered by prior knowledge about the location or
time point of the distracting input”.
[On a related issue: The lack of a significant difference in the size of the validity effect in
Experiments 2 and 3 (p19) does not show that “the validity effect was roughly the same in both
experiments”. It shows that there was not a statistically significant difference in the size of the
effect in the two experiments – this is a somewhat different claim, e.g., the null result could be
due to a lack of power.]
3. Experiment 1 had a much larger sample size than Experiments 2 and 3 (120 vs 67/61) – was
there are reason for this? More generally, how were sample sizes determined?
4. The manuscript provides a single Methods section that deals with all three experiments, and
then a single Results section dealing with each study in turn. But the start of each section of the
Results ends up providing a reminder of the rationale and procedure of each experiment. I
suspect the manuscript would be easier to follow and avoid redundancy if the three experiments
were separated – i.e., a separate Method and Results section for each (with Method for Expts 2
and 3 being very short, since it’s largely the same as for earlier experiments).
5. Accuracy data are not analyzed (or even presented) because accuracy was “close to ceiling” in
each experiment. Regardless of its overall level, at the very least accuracy data should be
presented (and ideally analyzed). If the authors are concerned about interrupting the flow of their
manuscript, perhaps they could put these additional analyses in supplementary materials.
6. Were participants explicitly informed prior to the task that the cue location and target location
were unrelated? Also, were they informed about the predictability manipulation, e.g., that the cue
would appear most often in (say) the top location? Or did they learn this via experience?
7. P4-5, “Based on the evidence listed earlier, we anticipated that unexpected cue stimuli attract
more attention and therefore in larger cue-target validity effects”. Should be something like
“…stimuli WILL attract more attention and therefore PRODUCE larger cue-target validity
effects”.
8. P9, “This analysis only considered participants that had expectations about the most likely cue
location”. As far as I can tell, whether participants actually had expectations about the cue
location was not assessed, so this might be more accurately phrased as something like “…
participants for whom the cue location was predictable”. This issue is implicitly acknowledged
on p20, “Assuming that our manipulations of expectations actually instantiated priors in our
subjects…”.
10
9. P20: “Other studies have used similar paradigms with streams of stimulation to investigate
effects of temporal expectations and did find effects of temporal regularity”. Should this be “did
not find”?
Author's Response to Decision Letter for (RSOS-180524.R0)
See Appendix A.
label_version_2
RSOS-180524.R1 (Revision)
label_author_3
Review form: Reviewer 1
Is the manuscript scientifically sound in its present form?
Yes
Are the interpretations and conclusions justified by the results?
Yes
Is the language acceptable?
Yes
Is it clear how to access all supporting data?
Yes
Do you have any ethical concerns with this paper?
No
Have you any concerns about statistical analyses in this paper?
No
Recommendation?
label_recommendation_3
Accept with minor revision (please list in comments)
Comments to the Author(s)
label_comment_3
The authors have addressed my concerns and I think the manuscript has clearly improved. Two
minor issues are remaining:
1) In the abstract, the authors write: “…However, while we observed robust attentional capture,
…”. I think this statement is not accurate because in Experiment 3 no evidence of capture was
found in the early time window.
2) It seems that some important information is still missing in the methods section of Experiment
1 and 2: How many trials where there in total in Experiment 2 and 3? How often was the target
presented on average?
11
label_end_comment
Decision letter (RSOS-180524.R1)
13-Sep-2018
Dear Mr Meijs:
Manuscript ID RSOS-180524.R1 entitled "Cue predictability does not modulate bottom-up
attentional capture" which you submitted to Royal Society Open Science, has been reviewed. The
comments of the reviewer(s) are included at the bottom of this letter.
Please submit a copy of your revised paper before 06-Oct-2018. Please note that the revision
deadline will expire at 00.00am on this date. If we do not hear from you within this time then it
will be assumed that the paper has been withdrawn. In exceptional circumstances, extensions
may be possible if agreed with the Editorial Office in advance. We do not allow multiple rounds
of revision so we urge you to make every effort to fully address all of the comments at this stage.
We hope to be able to make an editorial decision (without further review) if your revisions
appropriately address all the comments.
To revise your manuscript, log into http://mc.manuscriptcentral.com/rsos and enter your
Author Centre, where you will find your manuscript title listed under "Manuscripts with
Decisions." Under "Actions," click on "Create a Revision." Your manuscript number has been
appended to denote a revision. Revise your manuscript and upload a new version through your
Author Centre.
When submitting your revised manuscript, you must respond to the comments made by the
referees and upload a file "Response to Referees" in "Section 6 - File Upload". Please use this to
document how you have responded to the comments, and the adjustments you have made. In
order to expedite the processing of the revised manuscript, please be as specific as possible in
your response.
In addition to addressing all of the reviewers' and editor's comments please also ensure that your
revised manuscript contains the following sections before the reference list:
• Ethics statement
If your study uses humans or animals please include details of the ethical approval received,
including the name of the committee that granted approval. For human studies please also detail
whether informed consent was obtained. For field studies on animals please include details of all
permissions, licences and/or approvals granted to carry out the fieldwork.
• Data accessibility
It is a condition of publication that all supporting data are made available either as
supplementary information or preferably in a suitable permanent repository. The data
accessibility section should state where the article's supporting data can be accessed. This section
should also include details, where possible of where to access other relevant research materials
such as statistical tools, protocols, software etc can be accessed. If the data have been deposited in
an external repository this section should list the database, accession number and link to the DOI
for all data from the article that have been made publicly available. Data sets that have been
deposited in an external repository and have a DOI should also be appropriately cited in the
manuscript and included in the reference list.
• Competing interests
Please declare any financial or non-financial competing interests, or state that you have no
competing interests.
12
• Authors’ contributions
All submissions, other than those with a single author, must include an Authors’ Contributions
section which individually lists the specific contribution of each author. The list of Authors
should meet all of the following criteria; 1) substantial contributions to conception and design, or
acquisition of data, or analysis and interpretation of data; 2) drafting the article or revising it
critically for important intellectual content; and 3) final approval of the version to be published.
All contributors who do not meet all of these criteria should be included in the
acknowledgements.
We suggest the following format:
AB carried out the molecular lab work, participated in data analysis, carried out sequence
alignments, participated in the design of the study and drafted the manuscript; CD carried out
the statistical analyses; EF collected field data; GH conceived of the study, designed the study,
coordinated the study and helped draft the manuscript. All authors gave final approval for
publication.
• Acknowledgements
Please acknowledge anyone who contributed to the study but did not meet the authorship
criteria.
• Funding statement
Please list the source of funding for each author.
Please note that Royal Society Open Science charge article processing charges for all new
submissions that are accepted for publication. Charges will also apply to papers transferred to
Royal Society Open Science from other Royal Society Publishing journals, as well as papers
submitted as part of our collaboration with the Royal Society of Chemistry
(http://rsos.royalsocietypublishing.org/chemistry). If your manuscript is newly submitted and
subsequently accepted for publication, you will be asked to pay the article processing charge,
unless you request a waiver and this is approved by Royal Society Publishing. You can find out
more about the charges at http://rsos.royalsocietypublishing.org/page/charges. Should you
have any queries, please contact openscience@royalsociety.org.
Once again, thank you for submitting your manuscript to Royal Society Open Science and I look
forward to receiving your revision. If you have any questions at all, please do not hesitate to get
in touch.
Kind regards,
Royal Society Open Science Editorial Office
Royal Society Open Science
openscience@royalsociety.org
on behalf of Dr Narayanan Srinivasan (Associate Editor) and Prof. Antonia Hamilton (Subject
Editor)
openscience@royalsociety.org
Associate Editor Comments to Author (Dr Narayanan Srinivasan):
Associate Editor: 1
Comments to the Author:
The manuscript is much improved but there are still some concerns. In addition to the minor
concerns of one of the reviewers, i have the following comments.
13
(1)
Reviewer 2 had pointed out that the lack of validity effect in Exp 3 and presence of validity effect
in Exp 2. You responded by saying that “However, the validity effect was not significantly
different from that in Experiment 2 (where it was significant) “. I am not sure how you concluded
this. A look at figures 2 and 3 (early window) does indicate that the validity effects may be
different. Please clarify.
(2)
In experiment 2 (perhaps 3 as well), there were trials in which only cue appeared. Perhaps i
missed it, it was not clear what was the percentage of such cue only trials. If this is correct, there
would have been target trials preceded by cue-only trials and target trials preceded by target
trials. I was wondering whether it makes sense to analyse at least the target trials preceded by
target trials (assuming percentage of cue-only trials are small). The rationale for this was not fully
clear. If sometimes there were no targets, cue would have predicted appearance or lack of target
(depending again on the percentage of cue-only trials). This expectation was not part of
manipulation and i am bothered by this.
(3) In page 12, late window F(1,59)=2.85, p = 0.097 is presumably the interaction effect between
cue validity and regularity. Am i correct?
Reviewer comments to Author:
Reviewer: 1
Comments to the Author(s)
The authors have addressed my concerns and I think the manuscript has clearly improved. Two
minor issues are remaining:
1) In the abstract, the authors write: “…However, while we observed robust attentional capture,
…”. I think this statement is not accurate because in Experiment 3 no evidence of capture was
found in the early time window.
2) It seems that some important information is still missing in the methods section of Experiment
1 and 2: How many trials where there in total in Experiment 2 and 3? How often was the target
presented on average?
Author's Response to Decision Letter for (RSOS-180524.R1)
See Appendix B.
label_end_comment
Decision letter (RSOS-180524.R2)
27-Sep-2018
Dear Mr Meijs,
I am pleased to inform you that your manuscript entitled "Cue predictability does not modulate
bottom-up attentional capture" is now accepted for publication in Royal Society Open Science.
14
You can expect to receive a proof of your article in the near future. Please contact the editorial
office (openscience_proofs@royalsociety.org and openscience@royalsociety.org) to let us know if
you are likely to be away from e-mail contact. Due to rapid publication and an extremely tight
schedule, if comments are not received, your paper may experience a delay in publication.
Royal Society Open Science operates under a continuous publication model
(http://bit.ly/cpFAQ). Your article will be published straight into the next open issue and this
will be the final version of the paper. As such, it can be cited immediately by other researchers.
As the issue version of your paper will be the only version to be published I would advise you to
check your proofs thoroughly as changes cannot be made once the paper is published.
On behalf of the Editors of Royal Society Open Science, we look forward to your continued
contributions to the Journal.
Kind regards,
Royal Society Open Science Editorial Office
Royal Society Open Science
openscience@royalsociety.org
on behalf of Dr Narayanan Srinivasan (Associate Editor) and Prof. Antonia Hamilton (Subject
Editor)
openscience@royalsociety.org
Associate Editor Comments to Author (Dr Narayanan Srinivasan):
Associate Editor
Comments to the Author:
We thank the author for responding to the comments. The paper is accepted for publication in
label_version_3
RSOS. Congratulations to the authors.
Follow Royal Society Publishing on Twitter: @RSocPublishing
Follow Royal Society Publishing on Facebook:
https://www.facebook.com/RoyalSocietyPublishing.FanPage/
Read Royal Society Publishing's blog: https://blogs.royalsociety.org/publishing/
Appendix A
Reply to reviewers
We thank both reviewers for their helpful comments, which we believe helped us to
significantly improve the paper. In the following, we respond to each of the
comments in detail, referring to the specific changes in the manuscript when
applicable.
Reviewer 1:
1) In each experiment, the searched-for target was an abrupt onset singleton
and the cue was an abrupt onset singleton too. This allows for an explanation of
the cueing effects in terms of top-down contingent capture (Folk et al., 1992). I
think this complicates the interpretation of the present results because the cues
could have captured attention due to the similarity between cue and target
features. This might explain why cues captured attention when they were
predictable, and it might also explain why no difference was found between
conditions with predictable and less predictable cues.
We agree with the reviewer that cues may have captured attention partially because
of feature overlap between the cue stimuli and target stimuli (i.e. both were
“defined” as abrupt onsets). Still, the similarity between cues and targets was equal
in all experimental conditions. Therefore, we believe this did not necessarily prevent
us from observing modulations of attentional capture by top-down expectations
about the cue stimulus. In particular, it does not seem unlikely that temporal
expectations (Experiment 2 and 3) would affect the processing of sudden-onset
distractor stimuli.
Nevertheless, we cannot rule out that in a different task attentional capture would
have been modulated by prior expectations about the cue. This is something that we
already referred to in the discussion section (page 18/19), but now we also explicitly
link this to contingent capture (see also response to comment 6).
2) What was the result for the neutral control group in Experiment 1? Figure 1
shows the cueing effect for this group, but was it statistically compared with the
cueing effects of the test group in which cue expectation was manipulated? It
would be interesting to see this analysis.
In the original manuscript we did not statistically compare the neutral condition with
the expected or unexpected condition. The neutral control group was a separate
group of participant who only received neutral trials. Therefore, we could not easily
incorporate them in the repeated-measures ANOVA.
Nevertheless, the reviewer’s question shows that it may be interesting for readers to
have this information. Therefore, we have now added results from two independent
2
samples t-tests to the results section (page 8) of Experiment 1: “Further post-hoc
analyses showed that the validity effect in the neutral control group was not
significantly different from that on either expected (t116=-0.96, p=0.338) or
unexpected (t116=-1.15, p=0.252) trials for participants who had expectations about
cues.”.
3) It was not clear enough how the time windows of the SOAs for which the
cueing effects were analyzed in Experiment 2 and 3 were identified. Please
explain the procedure in more detail.
We have now extended and rewritten the “Behavioural analysis” paragraph that
pertains to Experiment 2 and 3 (page 11) with the aim of increasing the readability
and clarity. The relevant paragraph now reads: “Based on the smoothed data, we
computed the overall validity effect for each SOA. We then identified two SOAs of
interest based on these data: (1) an early maximally facilitatory validity effect (SOA
with a maximally positive overall validity effect); and (2) a late maximally inhibitory
validity effect (SOA with a maximally negative overall validity effect). We then
averaged the (non-smoothed) data around these SOAs of interest, using the selected
SOA and the two SOA’s preceding or following it, to create an early and a late window
of interest.”
3
4) The fact that the cueing effects with the 117 ms SOA were larger in
Experiment 1 compared to Experiment 2 is a bit puzzling. In fact, it casts some
doubt on the notion that the attentional capture effect by the cues is fully
automatic (bottom-up).
Indeed, this is an interesting but somewhat puzzling effect. It is hard to
unambiguously interpret the effect because there are multiple differences between
Experiment 1 and Experiment 2. As outlined in the manuscript, we think the most
likely explanation is the fact Experiment 1 had a fixed SOA between cue and target,
whereas the SOA between cue and target was variable in Experiment 2. Therefore,
the cue was a good temporal predictor of target onset in Experiment 1, and hence
subjects may have processed it more attentively, leading to stronger general bottom-
up capture effects.
We tentatively interpret this as evidence that participants’ strategy may have been
affected by the likelihood and predictability of target onsets. Based on the current
data set, it is difficult to determine how exactly this should be interpreted. It is
possible that participants’ focus on targets reduced attentional capture. Alternatively,
it is possible that in Experiment 1 participants explicitly tried to use the temporal
information the cue provided to try and predict when the target would be presented.
4
Importantly, we do not believe it is likely that the difference in validity effects
between the two experiments can be explained by differences in expectations about
the cue, meaning our main results and conclusions are still valid. Nevertheless, we
agree with the reviewer that some of the interpretations of the validity effect
difference between the experiments argue against an interpretation of the bottom-
up attention effects as being fully automatic and stimulus-driven. We have therefore
deleted these claims from our abstract (page 2) and discussion (page 18/19).
Moreover, we have changed the manuscript title (now: results “Cue predictability
does not modulate bottom-up attentional capture”) to more precisely reflect our
results.
5) I did not quite understand the motivation for spatial manipulation employed
in Experiment 3. The cues were presented only in one display location to make
them predictable in the spatial domain. Apparently, this was also the case in the
blocks with low temporal predictability of the cues? The spatial predictability of
the cues in both blocks (the temporally predictable block and the temporally
unpredictable block) could have been the reason why no difference was found
between the blocks in this experiment. Wouldn’t it be more appropriate to have
one block in which the cue is fully predictable (spatially and temporally) and one
block in which the cue remained unpredictable?
5
Experiment 3 was set up to investigate attentional capture in a situation where the
cue was completely spatially predictable but where temporal predictability was
manipulated. We agree with the reviewer that the difference between the
predictable and unpredictable conditions, and thus possibly the chance of observing
an effect, would have been larger when using a condition with both low spatial and
temporal predictability.
We chose to use the current design for the following reasons: (1) We wanted
Experiment 2 and 3 to be as comparable as possible, such that we could directly
compare them to test for effects of spatial expectation and (2) early pilot studies for
this series of experiments suggested large carry-over effects when manipulating
spatial expectations within one participant (which is also why we used a between-
subjects design for Experiment 1).
In response to a comment by reviewer 2, we have reordered the methods and results
sections of the manuscript. At the end of the Results section for Experiment 2 (page
12), we have now added a section explicitly describing the rationale of how
Experiment 3 builds on Experiment 2.
6
6) The authors concede that “… it is possible that the task set did not optimally
support suppression of uninformative cues …” (p. 21). Please explain this in more
detail. I think especially the fact that the cues shared with the targets some of the
critical searched-for features is important in this context (see 1)).
We have now added a section to the discussion with the aim of making a clearer link
between this statement and the examples offered in the paragraph. Additionally, we
now also list the feature overlap between cues and targets as one of the reasons we
may not have observed the effects we anticipated.
The paragraph in the discussion now ends with: “Consequently, in all experiments the
tasks we used may have had factors that made participants attend cues (instead of
ignore them). In addition, cues and targets were both defined as abrupt onset stimuli,
meaning cue features to some extent overlapped with the searched-for target feature
[3]. It is conceivable that this overlap caused attention to be captured regardless of
experimental condition. As a result, potential effects may have been obscured
because task set did not optimally support suppression of uninformative cues [9].”
(page 18).
7
7) The authors should check the analyses in Experiment 2 and 3 and the
respective degrees of freedom. The degrees of freedom do not appear to match
the number of participants (n=63 and n=58?). In addition, why are there different
degrees of freedom for the late and for the early time window (Exp 2 and 3)?
We thank the reviewer for noticing this potential mistake. We have carefully checked
all the degree of freedom values for Experiment 2 and 3 and concluded that we
reported the correct values. The varying df values are due to the fact that the onsets
of cues and targets were defined randomly (within certain ranges) and independently
of each other. As a result, not every participant has data in every condition for every
SOA. Participants with missing values in one of the conditions of a certain analysis are
not included for that specific analysis, but may be included in other analyses.
To make sure this is clear to future readers, we have added a few sentences to the
behavioural analysis part of the method section for Experiment 2 (page 11): “Please
note that since cue onsets and target onsets were determined independently, not
every participant had observations for every condition at every SOA. In case data was
missing in one of the conditions of an analysis, we excluded the respective participant
from that analysis.
8
Reviewer 2:
1. The manuscript is well written and the results are analyzed appropriately –
what is here seems solid. However, my concern is that the experiments may not
be well set up to test what they are designed to test. This issue is raised in the
Discussion (p21). Specifically, the use of a non-informative cue means that there is
little incentive to apply top-down suppression to the cue, since the target occurs
equally often in the cue location (in which case attending to the cue will speed
responses) as in the uncued location (in which case attending to the cue will slow
responses). So overall there may be little or no net negative consequence to
attending to the cue, and hence little drive to suppress. As noted on p20 of the
manuscript, this is in contrast to other studies showing an effect of predictability
in which the distractor and target are always at different locations and hence
there is a clear disadvantage in attending to the distractor (and consequently a
greater drive to suppress).
9
To reiterate, these issues are raised in the manuscript (“in all experiments the
tasks we used may have had factors that made participants attend cues (instead
of ignore them). It is possible that the fact that task set did not optimally support
suppression of uninformative cues obscured any potential effects in our tasks”).
But this significant caveat is ignored in the abstract (“This underscores the
automatic and reflexive nature of bottom-up attention”) and title (“Stimulus
predictability does not modulate bottom-up attentional capture”), which make
very general-sounding claims that cannot be substantiated by these data. I think
the authors need to be clearer throughout in noting that their findings are
qualified by these issues.
We agree with the reviewer that we may have overstated our conclusion at a few
points in the manuscript. We have therefore revised the manuscript (especially the
discussion) and deleted any claims that are not warranted based on the current data.
Furthermore, we have made sure the conclusions are now focused on the effects of
the predictability of the cue (instead of more general claims) on attentional capture.
In addition, we deleted the general-sounding claim from the abstract and rephrased
our title to “Cue predictability does not modulate bottom-up attentional capture” to
make it fit our results better.
10
2. Experiment 3 found no significant attentional capture effect; this is not
mentioned in the Discussion, but is at least consistent with the idea that capture
can be modulated by top-down processes (when the cue is completely
predictable). Likewise, in discussing the difference in capture between Expts 1 and
2 the authors suggest that “participants deploy a different strategy in Experiment
2 compared to Experiment 1, in which they focus more on the targets and less on
the cues” (p22). This again would suggest that capture is not totally
inevitable/reflexive, but is glossed over in the final conclusions paragraph which
states that “in the exogenous cueing tasks we used, bottom-up attentional
capture was not altered by prior knowledge about the location or time point of
the distracting input”.
[On a related issue: The lack of a significant difference in the size of the validity
effect in Experiments 2 and 3 (p19) does not show that “the validity effect was
roughly the same in both experiments”. It shows that there was not a statistically
significant difference in the size of the effect in the two experiments – this is a
somewhat different claim, e.g., the null result could be due to a lack of power.]
It is true that the absence of a validity effect in Experiment 3 somewhat suggests that
there may be less capture for completely predictable cues. However, the validity
effect was not significantly different from that in Experiment 2 (where it was
significant) and within Experiment 3 there was no modulation by cue predictability.
11
Therefore, we did not consider this to be convincing evidence that cue predictability
affected the validity effect in this experiment. We acknowledge that our statement
“the validity effect was roughly the same in both experiments” was imprecise and
have therefore deleted this part of the sentence from the respective paragraph.
As also stated in our response to comment 1, we agree with the reviewer that our
initial conclusions were a bit too strong and general. We have revised our conclusion
paragraph (page 19) to more specifically focus on the predictability of the cue stimuli.
This paragraph now reads: “In conclusion, we did not find evidence for modulations
of bottom-up capture by spatial or temporal expectations about the cue. We
therefore conclude that, at least in the exogenous cueing tasks we used, bottom-up
attentional capture does not appear to be altered by prior knowledge about the
location or time point of the distracting input.”
In addition, we have made sure that the manuscript title and abstract do not make
any claims that are not supported by the data presented in the manuscript.
3. Experiment 1 had a much larger sample size than Experiments 2 and 3 (120 vs
67/61) – was there are reason for this? More generally, how were sample sizes
determined?
12
We included 120 participants in Experiment 1 because this was a between-subjects
design in which we wanted to compare the three different groups. With 120
participants (N=40 per group) our design had a power of approximately 80% to detect
between-subject effects with an effect size of d=0.6. Of course higher sensitivity
would have been better, but to us this was the optimal compromise between
statistical power and practical considerations. Since Experiment 2 and Experiment 3
were within-subjects designs, we needed less participants and aimed to test roughly
60-70 participants for those experiments. We have now added these considerations
to the Participants sections of the manuscript (page 5 and page 10).
4. The manuscript provides a single Methods section that deals with all three
experiments, and then a single Results section dealing with each study in turn. But
the start of each section of the Results ends up providing a reminder of the
rationale and procedure of each experiment. I suspect the manuscript would be
easier to follow and avoid redundancy if the three experiments were separated –
i.e., a separate Method and Results section for each (with Method for Expts 2 and
3 being very short, since it’s largely the same as for earlier experiments).
In line with the reviewers’ suggestion, we have now separated the methods and
results sections to improve readability and reduce redundancy. For this reason, large
parts of the methods and results were reordered and rewritten to prevent
unnecessary repetition.
13
5. Accuracy data are not analyzed (or even presented) because accuracy was
“close to ceiling” in each experiment. Regardless of its overall level, at the very
least accuracy data should be presented (and ideally analyzed). If the authors are
concerned about interrupting the flow of their manuscript, perhaps they could put
these additional analyses in supplementary materials.
In response to the reviewer’s comment, we have now also run the main analysis for
each of the Experiments on percentage correct. As anticipated by the reviewer, we
were concerned that adding the percentage correct results would negatively
influence the flow of our manuscript. Therefore, we present the new figure and
analysis results in a supplementary file.
6. Were participants explicitly informed prior to the task that the cue location and
target location were unrelated? Also, were they informed about the predictability
manipulation, e.g., that the cue would appear most often in (say) the top
location? Or did they learn this via experience?
We instructed participants that the cues were irrelevant to their task and hence could
be ignored. We agree with the reviewer that this is important and relevant
information and have therefore added it to the manuscript in the “procedure”
paragraphs on page 6 and page 10.
14
7. P4-5, “Based on the evidence listed earlier, we anticipated that unexpected cue
stimuli attract more attention and therefore in larger cue-target validity effects”.
Should be something like “…stimuli WILL attract more attention and therefore
PRODUCE larger cue-target validity effects”.
We thank the reviewer for this suggestion and have adjusted the sentence to “…we
anticipated that unexpected cue stimuli will attract more attention and therefore
result in larger cue-target validity effects” (page 4/5).
8. P9, “This analysis only considered participants that had expectations about the
most likely cue location”. As far as I can tell, whether participants actually had
expectations about the cue location was not assessed, so this might be more
accurately phrased as something like “… participants for whom the cue location
was predictable”. This issue is implicitly acknowledged on p20, “Assuming that our
manipulations of expectations actually instantiated priors in our subjects…”.
We thank the reviewer for this suggestion and have adjusted the phrasing of the
sentence in the methods to “This analysis only considered participants for whom the
cue was more likely to appear in one of the locations (N=78)” (page 7).
15
9. P20: “Other studies have used similar paradigms with streams of stimulation to
investigate effects of temporal expectations and did find effects of temporal
regularity”. Should this be “did not find”?
We meant to compare our results with studies in which temporal regularities were
successfully manipulated and affected behaviour. Thus, while the original sentence
was correct, it was poorly phrased. Therefore, we have adjusted the phrasing of this
part of the discussion to “Other studies have used similar paradigms with streams of
stimulation to investigate effects of temporal expectations and did find effects of
temporal regularity on subsequent behavioural performance [20,37]. Nevertheless,
temporal expectations about cues do not appear to influence the processing of
subsequently presented target stimuli in situations where targets are salient and
uncoupled from the cues.” (page 16).
16
Appendix B
Reply to reviewers
We thank the reviewer and the editor for their comments. In the following, we
respond to each of the comments in detail, referring to the specific changes in the
manuscript when applicable.
Reviewer 1:
1) In the abstract, the authors write: “…However, while we observed robust
attentional capture, …”. I think this statement is not accurate because in
Experiment 3 no evidence of capture was found in the early time window.
We agree with the reviewer that we may have overstated our conclusions in the
abstract and have removed the word “robust” from the Abstract.
2) It seems that some important information is still missing in the methods section
of Experiment 1 and 2: How many trials where there in total in Experiment 2 and
3? How often was the target presented on average?
The amount of trials (800) for Experiment 1 was reported on page 7 in the Materials
paragraphs. Indeed, the amount of trials for Experiments 2 and 3 were not yet
reported in the paper. In those experiments, we post-hoc used every target
presentation as a trial. This means the number of trials is equal to the number of
targets that was presented. There were 1371 trials for Experiment 2 and 1365 trials
for Experiment 3. We do now report these numbers in the Behavioural analysis
sections on page 10 and page 14.
Editor:
1) Reviewer 2 had pointed out that the lack of validity effect in Exp 3 and presence
of validity effect in Exp 2. You responded by saying that “However, the validity
effect was not significantly different from that in Experiment 2 (where it was
significant) “. I am not sure how you concluded this. A look at figures 2 and 3
(early window) does indicate that the validity effects may be different. Please
clarify.
While there may appear to be a difference in the sizes of the validity effects of
Experiment 2 and 3, this difference is not statistically significant. The statistics for
the t-test in which this was tested are reported in the Comparison between
experiments paragraph on page 15/16: “… to Experiment 3 cue location was
unpredictable in Experiment 2, the comparison between those experiments can be
used to test this hypothesis. An independent samples t-test showed that there was
no significant difference between the validity effects (t119=1.182, p=0.240) of both
experiments.”
2) In experiment 2 (perhaps 3 as well), there were trials in which only cue
appeared. Perhaps i missed it, it was not clear what was the percentage of such
cue only trials. If this is correct, there would have been target trials preceded by
2
cue-only trials and target trials preceded by target trials. I was wondering whether
it makes sense to analyse at least the target trials preceded by target trials
(assuming percentage of cue-only trials are small). The rationale for this was not
fully clear. If sometimes there were no targets, cue would have predicted
appearance or lack of target (depending again on the percentage of cue-only
trials). This expectation was not part of manipulation and i am bothered by this.
We apologize that it was not clear how trials were defined for Experiments 2 and 3.
We have added a few sentences to the Behavioural analyses paragraph for
Experiment 2 to explain this in more detail: “During the experiment, the task was
presented to participants in continuous streams, without a clearly discernible trial
structure. For analyses purposes, trials were defined post-hoc by isolating at all
target presentations (on average 1371 trials per participant).”
Because trials are defined by a target being present (otherwise there is nothing to
respond to), there cannot be any “cue-only” trials. Further, while sometimes a cue
was not followed by a target (because targets were presented less often than cues),
this cannot have biased participants. Any individual cue was uninformative about
the presence, onset or location of upcoming targets, because the onsets and
locations for cues and targets were determined completely independently.
Therefore, while participants may have had an overall low expectation that a target
would be presented, this expectation would be constant throughout the experiment
3
and not affected by individual cues. When being presented with a cue, a participant
could not know whether or not a target would follow.
3) In page 12, late window F(1,59)=2.85, p = 0.097 is presumably the interaction
effect between cue validity and regularity. Am I correct?
No, as mentioned in the text of the paragraph this effect concerns a main effect of
expectations. That is, the reaction time was not significantly different between
regular and irregular blocks.
The interaction between cue validity and expectations/regularity is reported earlier
in the same paragraph: “… temporal expectations did not modulate bottom-up
attentional capture, as shown by the absence of an influence of cue onset regularity
on the validity effect (early window: F1,60=0.02, p=0.883; late window: F1,59=2.85,
p=0.097).”
4
Society Open
