Emo, love and god: making sense of Urban Dictionary, a
crowd-sourced online dictionary
Dong Nguyen, Barbara McGillivray and Taha Yasseri
Article citation details
R. Soc. open sci. 5: 172320.
http://dx.doi.org/10.1098/rsos.172320
Review timeline
Original submission: 21 December 2017 Note: Reports are unedited and appear as
Revised submission: 22 March 2018 submitted by the referee. The review history
Final acceptance: 27 March 2018 appears in chronological order.
Review History
label_version_1
RSOS-172320.R0 (Original submission)
label_author_1
Review form: Reviewer 1
Is the manuscript scientifically sound in its present form?
Yes
Are the interpretations and conclusions justified by the results?
Yes
Is the language acceptable?
Yes
Is it clear how to access all supporting data?
Yes
Do you have any ethical concerns with this paper?
No
© 2018 The Authors. Published by the Royal Society under the terms of the Creative Commons
Attribution License http://creativecommons.org/licenses/by/4.0/, which permits unrestricted use,
provided the original author and source are credited
2
Have you any concerns about statistical analyses in this paper?
No
Recommendation?
label_recommendation_1
Accept with minor revision (please list in comments)
Comments to the Author(s)
label_comment_1
The paper reports an analysis (the first one to date, as far as I can assess) of a full snapshot of the
content of the urbandictionary website. The motivation of the work, as it is stated in the
introduction, is quite clear and relatable: to understand how language evolves in a fast-paced and
globally connected society it is key to look at how self-organized online communities gather and
process the knowledge about new words and expressions. Several questions can be explored
under the lightbeam of this general research question, such as for example "whether the content
[of these communities] reflects real language innovation, as opposed to the concerns of a specific
community of users". Although the paper falls short on delivering compelling results along these
lines (as I will detail), I still believe it brings valuable contribution because: i) it is the first one
studying this particular dataset; ii) it freely provides the data gathered to the research
community; iii) the analysis is in general sound and well-conducted (except for a few things
listed below) and iv) the results reported give some insights that, even if not especially surprising
or groundbreaking, is functional to start a line of research on the broader topic of language
evolution in self-organized systems.
Some detailed comments.
- This paper is basically a dataset paper. A very well-done one, but it mostly focuses on
describing a new dataset from different angles and making it available to the community. In my
book, this is a perfectly acceptable and useful contribution. However, the actual scope of the
contribution must emerge clearly from the introduction and abstract. At the moment, in my
opinion, it doesn't. The introduction should be reframed and toned down a bit to clearly reflect
that the contribution is to collect, publish and conduct a preliminary analysis on a dataset that
could be used in the future to do a bunch of cool stuff (which is well summarized by the last
paragraph in the conclusions).
- The technical part that is most unclear is the word matching with the Wiki terms. I did not
understand why, after filtering out all the UD words with only one definition, the number of
Wiki words increases. My understanding is that the set of Wiki words was fixed (modulo the
upper-lower case variations) and then the UD words are matched against that fixed set of terms,
with a varying overlap %. This is also why I found inaccurate all the discussion around the Wiki
words havign higher relative volume in Table 3 than in Table 2: that should follow directly by the
fact that the authors filtered out most of the UD words and kept more or less the same Wiki
words. Either I am missing something (in which case, the writing should be revised) or that part
is not 100% correct.
- I did not find the "emo love god" part in the title is not particularly appropriate just because
there is no compelling running example about those words in the paper. They are just the most
popular terms in this specific platform. I find it a bit misleading, so I would remove that part
altogether from the title. But I do realize this is mostly an editorial choice.
- I understand that the RSOS format led the authors to split the paper in main sections +
appendix, but the most of the details on how the crowdsourcing was conducted are functional to
understand properly the results. So I would suggest to move as much as possible from the
crowdsourcing section in the main text (it not all of it). At the very least the questions that the
crowdworkers have been asked should be in the main body.
3
- The related work seems quite relevant but, along the lines of "Over time, new words enter the
lexicon, others become obsolete, and existing words acquire new meanings.", I think that the list
of related papers could be expanded a bit, for example including other papers that studied
language variations in other contexts. This is one: https://arxiv.org/pdf/1707.00781.pdf but I
believe there are others. I don't require the authors to add new references but I leave this here as a
suggestion to them in case they would like to improve the related work coverage in their
manuscript).
- Figure 6 (middle) could perhaps be more informative if the values were averaged for each x
value and error bars were reported. To make it extra-nice, individual average points could be
colored with different intensity/alpha to encode how many datapoints are in each bin. As it is
now, it is hard to see a trend because we don't know how many point are actually in the black
cloud in the middle.
- The methodology that the authors adopted for data analysis is a bit mixed and not always in the
way one would expect. In the study quantitative statistics are reported, a crowdsourcing study
has been conducted and a bit of manual qualitative assessment has been performed too.
Sometimes, to answer some specific questions, I would have used different tools that the ones the
authors used. For example, these two points : "However, there seems to be a systematic deviation
from a perfect correlation in which the number of up votes generally outperforms the number of
down votes" and "Based on a qualitative analysis of a small sample of entries we can say that
headwords with only one definition in UD often relate to words concerning the specific
community of UD users" are probably good questions for the crowdworkers. Another example is
the crowd assessment of a word bein a noun or not. I would probably have gone with a
quantitative approach there, matching the words with an external knowledge base or dictionary.
- The last part on voting dynamics felt a little bit out of place and probably not as insightful as the
rest of the results. Voting dynamics are influenced by a myriad of factors (starting from the
interface, as the authors correctly point out) that are often not easy to control. Some proxies might
have a decent predictive power (which is fine if your goal is to predict popularity) but not much
explanation power. Also, I feel that the voting dynamics are a bit different, in terms of scope,
from the main focus of the paper which is supposed to be the language part. My gut feeling
would be to remove that subsection altogether but it's ok if the authors want to keep it.
label_author_2
Review form: Reviewer 2
Is the manuscript scientifically sound in its present form?
No
Are the interpretations and conclusions justified by the results?
No
Is the language acceptable?
Yes
Is it clear how to access all supporting data?
Yes
4
Do you have any ethical concerns with this paper?
No
Have you any concerns about statistical analyses in this paper?
No
Recommendation?
label_recommendation_2
Major revision is needed (please make suggestions in comments)
Comments to the Author(s)
label_comment_2
In this manuscript, the authors measure and report properties of "Urban Dictionary" regarding its
size, content, and other characteristics such as familiarity and offensiveness. The work is novel in
its subject of study and aligns with recent approaches from computational social science to gain a
better understanding of systems emerging from the interaction of thousands or even millions of
individuals.
However, I find that the manuscript does not meet RSOS' criterion of "high-quality research"
which "sufficiently advance[s] scientific knowledge" due to i) a lack of context of the presented
results; ii) questions regarding the choices made in the analysis; and iii) the many claims that are
not supported by the empirical analysis (see below for details).
Therefore, I cannot recommend publication of this manuscript in RSOS in its current form and
would ask the authors to address the issues raised above.
1) Motivation & Lack of context
The authors make a convincing argument why the data from "Urban Dictionary" provides new
possibilities for, e.g., i) studying language change; or ii) developing/refining natural language
processing systems.
However, I fail to see a strong connection between the presented analysis&results and the stated
motivation. While the authors show many correlations between pairs of variables (e.g. Fig. 6,7 or
Table 4,5), the implications (or even reasons) of these correlations remain almost completely
unclear.
Furthermore, what is the importance of the variables chosen by the authors? While it is clear that
one cannot study all aspects, it seems that some of the characteristics/variables are chosen
arbitrarily or simply because they are easily accessible, e.g. why focus on proper nouns, and not
on adjectives; why formality and not the length of the word? This relates to the point above --
how are these variables helping to approach the points raised in the motivation? Additional
information here would considerably improve the applicability of the results.
The properties that are being measured are not compared with measurements in other and/or
'traditional' dictionaries (besides the 'coverage' in Table 2,3). A comparison is not only necessary
to put these numbers into context, but also would possibly make a much stronger case why
Urban Dictionary is important as a reasearch object.
2) Analysis
Some of the choices in the analysis were unclear from the description of the Main text/Methods.
Additional information would clarify how to interpret the results.
Section 2b): In the results of Table 3, the authors apply a threshold to the Urban Dictionary
showing that "it appears that the coverage of Wiktionary is higher than that of UD". Do the
authors also apply the threshold to Wiktionary? If not, it seems to me to be an unfair comparison.
Section 2b): I find the term "lexical coverage" highly misleading. What is being covered here? The
space of all possible sequences of letters? The measurement only concerns the number of unique
strings, thus it simply reflects the size of the database (in terms of entries) that is being measured.
5
Section 2c,ii) The measurement of 'offensiveness' is unclear to me due to 2 points. First, it is not
clear whether an objective notion of offensiveness exists -- the measured value is intrinsically tied
to who is asked in the survey. However, it seems the analysis here assumes (without proof or
reference to prior work) that, indeed, the reported values reflect an objective measurable quantity
that is related to the word (independent of the person who rated). Second, the process of ranking
3 options seems arbitrary with respect to their offensiveness seems arbitrary (why not 5 or 7 or
10?). Furthermore, the relative measurement (ranking) makes the offensiveness of a word
dependent upon the offensiveness of other words. It is not clear if and how these choices
introduce potential bias and artefacts, especially taking into account the rather small effect sizes
(e.g. Table 4,5). In my opinion, it is thus necessary to convince the readers that the proposed
measure of offensiveness is indeed meaningful. Beyond the 2 examples in Table 12, the authors
do not provide any evidence for that.
Reliability of overall results: "The majority of the entries in UD were not familiar to the
crowdworkers." (p.7,l.39).
From what I understood, all answers of the crowdworkers are part of the analysis independent of
the familiarity. However, if workers are not familiar with an entry, how reliable is the judgement
with respect to proper noun, formal/informal, etc.?
3) Main claims are not supported by results
Throughout the manuscript the authors compare Urban Dictionary to 'traditional' dictionaries --
some of these are main claims contained in the abstract/discussion and this aspect serves as a
main motivation for the current study. However, these comparisons are not supported by any
empirical results as the 'traditional' dictionaries are not analyzed in this manuscript. Additional
data/analysis or more precise statements would clarify the importance of the results.
Examples are:
- Abstract, l.52: "Our study highlights that Urban Dictionary has a higher content heterogeneity
than found in traditional dictionaries"
- p.7, l.25: "manual inspection suggested that UD has a higher coverage of informal and
infrequent words and of proper nouns"
- p.7, l.35: "While most entries are marked as describing a meaning, the considerable presence of
opinions shows that the type of content in UD is different than in traditional dictionaries."
- p.10,l.21: "UD contains more offensive content than traditional dictionaries"
- p.10, l.49: "Differently from the more traditional dictionaries, we found that most contributors of
UD only added one entry and very few added a high number of entries"; in fact, the authors
speculate without reference or data that (p.4,l.45) "However, this feature is fundamentally
different to what one would expect from a more traditional dictionary, in which we expect a
more even distribution of information per headword."
- Discussion, p.10, l.49: "Differently from the more traditional dictionaries, we found that most
contributors of UD only added one entry and very few added a high number of entries"
- Discussion, p.11, l.17: "Our study highlights that UD has a higher content heterogeneity than
traditional dictionaries" (same as abstract)
- Discussion, p.11, l.20: "However, UD is unique in capturing many infrequent, informal words
and it could therefore complement the traditional dictionaries." In which sense is it unique?
Without any comparison based on data, this is pure speculation.
- Discussion, p.11, l.21: " Furthermore, while there is more offensive content in UD, ..." more
offensive than where?
6
Other examples of unsupported claims are:
- p.8, l.20: "In contrast, [...] in UD and one of its distinctive characteristics is its often offensive
content." - How do we know that offensive content is one of its distinctive features?
- Figure 11 & p.7,l.48: "We also observe a slight trend regarding formality, with entries in higher
frequency bins more often marked to be appropriate for use in formal conversations."
In view of the absence of any (estimated) errorbars, I am not convinced of this claim as the blue
curve is clearly non-monotonic.
label_end_comment
Decision letter (RSOS-172320)
12-Feb-2018
Dear Ms Nguyen,
The editors assigned to your paper ("Emo, Love, and God: Making Sense of Urban Dictionary, a
Crowd-Sourced Online Dictionary") have now received comments from reviewers. We would
like you to revise your paper in accordance with the referee and Associate Editor suggestions
which can be found below (not including confidential reports to the Editor). Please note this
decision does not guarantee eventual acceptance.
Please submit a copy of your revised paper within three weeks (i.e. by the 07-Mar-2018). If we do
not hear from you within this time then it will be assumed that the paper has been withdrawn. In
exceptional circumstances, extensions may be possible if agreed with the Editorial Office in
advance.We do not allow multiple rounds of revision so we urge you to make every effort to
fully address all of the comments at this stage. If deemed necessary by the Editors, your
manuscript will be sent back to one or more of the original reviewers for assessment. If the
original reviewers are not available, we may invite new reviewers.
To revise your manuscript, log into http://mc.manuscriptcentral.com/rsos and enter your
Author Centre, where you will find your manuscript title listed under "Manuscripts with
Decisions." Under "Actions," click on "Create a Revision." Your manuscript number has been
appended to denote a revision. Revise your manuscript and upload a new version through your
Author Centre.
When submitting your revised manuscript, you must respond to the comments made by the
referees and upload a file "Response to Referees" in "Section 6 - File Upload". Please use this to
document how you have responded to the comments, and the adjustments you have made. In
order to expedite the processing of the revised manuscript, please be as specific as possible in
your response.
In addition to addressing all of the reviewers' and editor's comments please also ensure that your
revised manuscript contains the following sections as appropriate before the reference list:
• Ethics statement (if applicable)
If your study uses humans or animals please include details of the ethical approval received,
including the name of the committee that granted approval. For human studies please also detail
whether informed consent was obtained. For field studies on animals please include details of all
permissions, licences and/or approvals granted to carry out the fieldwork.
7
• Data accessibility
It is a condition of publication that all supporting data are made available either as
supplementary information or preferably in a suitable permanent repository. The data
accessibility section should state where the article's supporting data can be accessed. This section
should also include details, where possible of where to access other relevant research materials
such as statistical tools, protocols, software etc can be accessed. If the data have been deposited in
an external repository this section should list the database, accession number and link to the DOI
for all data from the article that have been made publicly available. Data sets that have been
deposited in an external repository and have a DOI should also be appropriately cited in the
manuscript and included in the reference list.
If you wish to submit your supporting data or code to Dryad (http://datadryad.org/), or modify
your current submission to dryad, please use the following link:
http://datadryad.org/submit?journalID=RSOS&manu=RSOS-172320
• Competing interests
Please declare any financial or non-financial competing interests, or state that you have no
competing interests.
• Authors’ contributions
All submissions, other than those with a single author, must include an Authors’ Contributions
section which individually lists the specific contribution of each author. The list of Authors
should meet all of the following criteria; 1) substantial contributions to conception and design, or
acquisition of data, or analysis and interpretation of data; 2) drafting the article or revising it
critically for important intellectual content; and 3) final approval of the version to be published.
All contributors who do not meet all of these criteria should be included in the
acknowledgements.
We suggest the following format:
AB carried out the molecular lab work, participated in data analysis, carried out sequence
alignments, participated in the design of the study and drafted the manuscript; CD carried out
the statistical analyses; EF collected field data; GH conceived of the study, designed the study,
coordinated the study and helped draft the manuscript. All authors gave final approval for
publication.
• Acknowledgements
Please acknowledge anyone who contributed to the study but did not meet the authorship
criteria.
• Funding statement
Please list the source of funding for each author.
Please note that Royal Society Open Science will introduce article processing charges for all new
submissions received from 1 January 2018. Charges will also apply to papers transferred to Royal
Society Open Science from other Royal Society Publishing journals, as well as papers submitted
as part of our collaboration with the Royal Society of Chemistry
(http://rsos.royalsocietypublishing.org/chemistry). If your manuscript is submitted and
accepted for publication after 1 Jan 2018, you will be asked to pay the article processing charge,
unless you request a waiver and this is approved by Royal Society Publishing. You can find out
more about the charges at http://rsos.royalsocietypublishing.org/page/charges. Should you
have any queries, please contact openscience@royalsociety.org.
8
Once again, thank you for submitting your manuscript to Royal Society Open Science and I look
forward to receiving your revision. If you have any questions at all, please do not hesitate to get
in touch.
Kind regards,
Alice Power
Editorial Coordinator
Royal Society Open Science
openscience@royalsociety.org
on behalf of Dr Hamed Haddadi (Associate Editor) and Marta Kwiatkowska (Subject Editor)
openscience@royalsociety.org
Associate Editor's comments (Dr Hamed Haddadi):
Dear authors,
As you see attached, the reviewers found the dataset interesting, but the methodology needs
rethinking and work. We look forward to receiving your submission after having addressed the
reviewer comments.
Comments to Author:
Reviewers' Comments to Author:
Reviewer: 1
Comments to the Author(s)
label_comment_3
The paper reports an analysis (the first one to date, as far as I can assess) of a full snapshot of the
content of the urbandictionary website. The motivation of the work, as it is stated in the
introduction, is quite clear and relatable: to understand how language evolves in a fast-paced and
globally connected society it is key to look at how self-organized online communities gather and
process the knowledge about new words and expressions. Several questions can be explored
under the lightbeam of this general research question, such as for example "whether the content
[of these communities] reflects real language innovation, as opposed to the concerns of a specific
community of users". Although the paper falls short on delivering compelling results along these
lines (as I will detail), I still believe it brings valuable contribution because: i) it is the first one
studying this particular dataset; ii) it freely provides the data gathered to the research
community; iii) the analysis is in general sound and well-conducted (except for a few things
listed below) and iv) the results reported give some insights that, even if not especially surprising
or groundbreaking, is functional to start a line of research on the broader topic of language
evolution in self-organized systems.
Some detailed comments.
- This paper is basically a dataset paper. A very well-done one, but it mostly focuses on
describing a new dataset from different angles and making it available to the community. In my
book, this is a perfectly acceptable and useful contribution. However, the actual scope of the
contribution must emerge clearly from the introduction and abstract. At the moment, in my
opinion, it doesn't. The introduction should be reframed and toned down a bit to clearly reflect
that the contribution is to collect, publish and conduct a preliminary analysis on a dataset that
9
could be used in the future to do a bunch of cool stuff (which is well summarized by the last
paragraph in the conclusions).
- The technical part that is most unclear is the word matching with the Wiki terms. I did not
understand why, after filtering out all the UD words with only one definition, the number of
Wiki words increases. My understanding is that the set of Wiki words was fixed (modulo the
upper-lower case variations) and then the UD words are matched against that fixed set of terms,
with a varying overlap %. This is also why I found inaccurate all the discussion around the Wiki
words havign higher relative volume in Table 3 than in Table 2: that should follow directly by the
fact that the authors filtered out most of the UD words and kept more or less the same Wiki
words. Either I am missing something (in which case, the writing should be revised) or that part
is not 100% correct.
- I did not find the "emo love god" part in the title is not particularly appropriate just because
there is no compelling running example about those words in the paper. They are just the most
popular terms in this specific platform. I find it a bit misleading, so I would remove that part
altogether from the title. But I do realize this is mostly an editorial choice.
- I understand that the RSOS format led the authors to split the paper in main sections +
appendix, but the most of the details on how the crowdsourcing was conducted are functional to
understand properly the results. So I would suggest to move as much as possible from the
crowdsourcing section in the main text (it not all of it). At the very least the questions that the
crowdworkers have been asked should be in the main body.
- The related work seems quite relevant but, along the lines of "Over time, new words enter the
lexicon, others become obsolete, and existing words acquire new meanings.", I think that the list
of related papers could be expanded a bit, for example including other papers that studied
language variations in other contexts. This is one: https://arxiv.org/pdf/1707.00781.pdf but I
believe there are others. I don't require the authors to add new references but I leave this here as a
suggestion to them in case they would like to improve the related work coverage in their
manuscript).
- Figure 6 (middle) could perhaps be more informative if the values were averaged for each x
value and error bars were reported. To make it extra-nice, individual average points could be
colored with different intensity/alpha to encode how many datapoints are in each bin. As it is
now, it is hard to see a trend because we don't know how many point are actually in the black
cloud in the middle.
- The methodology that the authors adopted for data analysis is a bit mixed and not always in the
way one would expect. In the study quantitative statistics are reported, a crowdsourcing study
has been conducted and a bit of manual qualitative assessment has been performed too.
Sometimes, to answer some specific questions, I would have used different tools that the ones the
authors used. For example, these two points : "However, there seems to be a systematic deviation
from a perfect correlation in which the number of up votes generally outperforms the number of
down votes" and "Based on a qualitative analysis of a small sample of entries we can say that
headwords with only one definition in UD often relate to words concerning the specific
community of UD users" are probably good questions for the crowdworkers. Another example is
the crowd assessment of a word bein a noun or not. I would probably have gone with a
quantitative approach there, matching the words with an external knowledge base or dictionary.
- The last part on voting dynamics felt a little bit out of place and probably not as insightful as the
rest of the results. Voting dynamics are influenced by a myriad of factors (starting from the
interface, as the authors correctly point out) that are often not easy to control. Some proxies might
10
have a decent predictive power (which is fine if your goal is to predict popularity) but not much
explanation power. Also, I feel that the voting dynamics are a bit different, in terms of scope,
from the main focus of the paper which is supposed to be the language part. My gut feeling
would be to remove that subsection altogether but it's ok if the authors want to keep it.
Reviewer: 2
Comments to the Author(s)
label_comment_4
In this manuscript, the authors measure and report properties of "Urban Dictionary" regarding its
size, content, and other characteristics such as familiarity and offensiveness. The work is novel in
its subject of study and aligns with recent approaches from computational social science to gain a
better understanding of systems emerging from the interaction of thousands or even millions of
individuals.
However, I find that the manuscript does not meet RSOS' criterion of "high-quality research"
which "sufficiently advance[s] scientific knowledge" due to i) a lack of context of the presented
results; ii) questions regarding the choices made in the analysis; and iii) the many claims that are
not supported by the empirical analysis (see below for details).
Therefore, I cannot recommend publication of this manuscript in RSOS in its current form and
would ask the authors to address the issues raised above.
1) Motivation & Lack of context
The authors make a convincing argument why the data from "Urban Dictionary" provides new
possibilities for, e.g., i) studying language change; or ii) developing/refining natural language
processing systems.
However, I fail to see a strong connection between the presented analysis&results and the stated
motivation. While the authors show many correlations between pairs of variables (e.g. Fig. 6,7 or
Table 4,5), the implications (or even reasons) of these correlations remain almost completely
unclear.
Furthermore, what is the importance of the variables chosen by the authors? While it is clear that
one cannot study all aspects, it seems that some of the characteristics/variables are chosen
arbitrarily or simply because they are easily accessible, e.g. why focus on proper nouns, and not
on adjectives; why formality and not the length of the word? This relates to the point above --
how are these variables helping to approach the points raised in the motivation? Additional
information here would considerably improve the applicability of the results.
The properties that are being measured are not compared with measurements in other and/or
'traditional' dictionaries (besides the 'coverage' in Table 2,3). A comparison is not only necessary
to put these numbers into context, but also would possibly make a much stronger case why
Urban Dictionary is important as a reasearch object.
2) Analysis
Some of the choices in the analysis were unclear from the description of the Main text/Methods.
Additional information would clarify how to interpret the results.
Section 2b): In the results of Table 3, the authors apply a threshold to the Urban Dictionary
showing that "it appears that the coverage of Wiktionary is higher than that of UD". Do the
authors also apply the threshold to Wiktionary? If not, it seems to me to be an unfair comparison.
Section 2b): I find the term "lexical coverage" highly misleading. What is being covered here? The
11
space of all possible sequences of letters? The measurement only concerns the number of unique
strings, thus it simply reflects the size of the database (in terms of entries) that is being measured.
Section 2c,ii) The measurement of 'offensiveness' is unclear to me due to 2 points. First, it is not
clear whether an objective notion of offensiveness exists -- the measured value is intrinsically tied
to who is asked in the survey. However, it seems the analysis here assumes (without proof or
reference to prior work) that, indeed, the reported values reflect an objective measurable quantity
that is related to the word (independent of the person who rated). Second, the process of ranking
3 options seems arbitrary with respect to their offensiveness seems arbitrary (why not 5 or 7 or
10?). Furthermore, the relative measurement (ranking) makes the offensiveness of a word
dependent upon the offensiveness of other words. It is not clear if and how these choices
introduce potential bias and artefacts, especially taking into account the rather small effect sizes
(e.g. Table 4,5). In my opinion, it is thus necessary to convince the readers that the proposed
measure of offensiveness is indeed meaningful. Beyond the 2 examples in Table 12, the authors
do not provide any evidence for that.
Reliability of overall results: "The majority of the entries in UD were not familiar to the
crowdworkers." (p.7,l.39).
From what I understood, all answers of the crowdworkers are part of the analysis independent of
the familiarity. However, if workers are not familiar with an entry, how reliable is the judgement
with respect to proper noun, formal/informal, etc.?
3) Main claims are not supported by results
Throughout the manuscript the authors compare Urban Dictionary to 'traditional' dictionaries --
some of these are main claims contained in the abstract/discussion and this aspect serves as a
main motivation for the current study. However, these comparisons are not supported by any
empirical results as the 'traditional' dictionaries are not analyzed in this manuscript. Additional
data/analysis or more precise statements would clarify the importance of the results.
Examples are:
- Abstract, l.52: "Our study highlights that Urban Dictionary has a higher content heterogeneity
than found in traditional dictionaries"
- p.7, l.25: "manual inspection suggested that UD has a higher coverage of informal and
infrequent words and of proper nouns"
- p.7, l.35: "While most entries are marked as describing a meaning, the considerable presence of
opinions shows that the type of content in UD is different than in traditional dictionaries."
- p.10,l.21: "UD contains more offensive content than traditional dictionaries"
- p.10, l.49: "Differently from the more traditional dictionaries, we found that most contributors of
UD only added one entry and very few added a high number of entries"; in fact, the authors
speculate without reference or data that (p.4,l.45) "However, this feature is fundamentally
different to what one would expect from a more traditional dictionary, in which we expect a
more even distribution of information per headword."
- Discussion, p.10, l.49: "Differently from the more traditional dictionaries, we found that most
contributors of UD only added one entry and very few added a high number of entries"
- Discussion, p.11, l.17: "Our study highlights that UD has a higher content heterogeneity than
traditional dictionaries" (same as abstract)
- Discussion, p.11, l.20: "However, UD is unique in capturing many infrequent, informal words
and it could therefore complement the traditional dictionaries." In which sense is it unique?
Without any comparison based on data, this is pure speculation.
- Discussion, p.11, l.21: " Furthermore, while there is more offensive content in UD, ..." more
offensive than where?
12
Other examples of unsupported claims are:
- p.8, l.20: "In contrast, [...] in UD and one of its distinctive characteristics is its often offensive
content." - How do we know that offensive content is one of its distinctive features?
- Figure 11 & p.7,l.48: "We also observe a slight trend regarding formality, with entries in higher
frequency bins more often marked to be appropriate for use in formal conversations."
In view of the absence of any (estimated) errorbars, I am not convinced of this claim as the blue
curve is clearly non-monotonic.
Author's Response to Decision Letter for (RSOS-172320)
See Appendix A.
label_end_comment
Decision letter (RSOS-172320.R1)
27-Mar-2018
Dear Ms Nguyen,
I am pleased to inform you that your manuscript entitled "Emo, Love, and God: Making Sense of
Urban Dictionary, a Crowd-Sourced Online Dictionary" is now accepted for publication in Royal
Society Open Science.
You can expect to receive a proof of your article in the near future. Please contact the editorial
office (openscience_proofs@royalsociety.org and openscience@royalsociety.org) to let us know if
you are likely to be away from e-mail contact. Due to rapid publication and an extremely tight
schedule, if comments are not received, your paper may experience a delay in publication.
Royal Society Open Science operates under a continuous publication model
(http://bit.ly/cpFAQ). Your article will be published straight into the next open issue and this
will be the final version of the paper. As such, it can be cited immediately by other researchers.
As the issue version of your paper will be the only version to be published I would advise you to
check your proofs thoroughly as changes cannot be made once the paper is published.
In order to raise the profile of your paper once it is published, we can send through a PDF of your
paper to selected colleagues. If you wish to take advantage of this, please reply to this email with
the name and email addresses of up to 10 people who you feel would wish to read your article.
Please note that Royal Society Open Science will introduce article processing charges for all new
submissions received from 1 January 2018. Charges will also apply to papers transferred to Royal
Society Open Science from other Royal Society Publishing journals, as well as papers submitted
as part of our collaboration with the Royal Society of Chemistry
(http://rsos.royalsocietypublishing.org/chemistry). If your manuscript is submitted and
accepted for publication after 1 Jan 2018, you will be asked to pay the article processing charge,
unless you request a waiver and this is approved by Royal Society Publishing. You can find out
13
more about the charges at http://rsos.royalsocietypublishing.org/page/charges. Should you
have any queries, please contact openscience@royalsociety.org.
On behalf of the Editors of Royal Society Open Science, we look forward to your continued
contributions to the Journal.
Kind regards,
Andrew Dunn
Royal Society Open Science
openscience@royalsociety.org
on behalf of Dr Hamed Haddadi (Associate Editor) and Marta Kwiatkowska (Subject Editor)
openscience@royalsociety.org
Associate Editor Comments to Author (Dr Hamed Haddadi):
Thanks for your effort, I believe the paper is now in a better shape, but do feel free to improve it
before publication.
pendix A
r editors,
ny thanks to you and the reviewers for your careful evaluation of our submission 'Emo, Love, and God:
ing Sense of Urban Dictionary, a Crowd-Sourced Online Dictionary'. Based on the suggestions, we have
hrased statements in the original manuscript and expanded the motivation of our methodology. Changes
shown in red in the “annotated” document that we include with this letter. Our response to the reviewers'
ments are shown below.
d regards,
g Nguyen, Barbara McGillivray, Taha Yasseri
iewer 1
s paper is basically a dataset paper. A very well-done one, but it mostly focuses on describing a new
aset from different angles and making it available to the community. In my book, this is a perfectly
eptable and useful contribution. However, the actual scope of the contribution must emerge clearly from the
oduction and abstract. At the moment, in my opinion, it doesn't. The introduction should be reframed and
ed down a bit to clearly reflect that the contribution is to collect, publish and conduct a preliminary analysis
a dataset that could be used in the future to do a bunch of cool stuff (which is well summarized by the last
agraph in the conclusions)."
have rephrased parts of the introduction and abstract to make our contribution clearer.
e technical part that is most unclear is the word matching with the Wiki terms. I did not understand why,
r filtering out all the UD words with only one definition, the number of Wiki words increases. My
erstanding is that the set of Wiki words was fixed (modulo the upper-lower case variations) and then the UD
ds are matched against that fixed set of terms, with a varying overlap %. This is also why I found inaccurate
he discussion around the Wiki words havign higher relative volume in Table 3 than in Table 2: that should
w directly by the fact that the authors filtered out most of the UD words and kept more or less the same
i words. Either I am missing something (in which case, the writing should be revised) or that part is not
% correct."
was also raised by reviewer 2.
have expanded our explanation of our approach in the paper (section 2b). We indeed do not apply any
ring on the Wiki terms, because every headword in Wikipedia has one single entry (i.e. page). In
parison, in Urban Dictionary, headwords can have multiple entries. In Wiktionary, there is a deeper curation
headwords that do not meet Wiktionary guidelines are removed. In Urban Dictionary, there are fewer
elines, and we therefore use the number of entries for a headword as a proxy to whether the word has a
e spread usage. If the word has only one entry, it is likely to be of little interest to a wider audience.
thus indeed keep the set of Wiki words fixed and it is therefore expected that the percentage of unique UD
dwords decreases. However, the magnitude of the change (e.g. from 72% to 25%) is, in our opinion,
ing.
d not find the "emo love god" part in the title is not particularly appropriate just because there is no
pelling running example about those words in the paper. They are just the most popular terms in this
cific platform. I find it a bit misleading, so I would remove that part altogether from the title. But I do realize
is mostly an editorial choice."
would prefer keeping the current title. Although we do not focus on these terms specifically, they indeed
ear as the top words in Table 1 and they signal the informal character of Urban Dictionary and the type of
tent it contains.
nderstand that the RSOS format led the authors to split the paper in main sections + appendix, but the most
he details on how the crowdsourcing was conducted are functional to understand properly the results. So I
ld suggest to move as much as possible from the crowdsourcing section in the main text (it not all of it). At
very least the questions that the crowdworkers have been asked should be in the main body."
have expanded the main text (2c and 2d) by adding more details about the crowdsourcing, such as the
ivation of using a crowdsourcing approach and the questions asked.
he related work seems quite relevant but, along the lines of "Over time, new words enter the lexicon, others
ome obsolete, and existing words acquire new meanings.", I think that the list of related papers could be
anded a bit, for example including other papers that studied language variations in other contexts. This is
: https://arxiv.org/pdf/1707.00781.pdf but I believe there are others. I don't require the authors to add new
rences but I leave this here as a suggestion to them in case they would like to improve the related work
erage in their manuscript)."
have added a few references related to language change on social media, with a focus on the emergence
ew words and how this has been studied using online sources (we found this a bit more relevant to this
icular paper than the study on the americanization of English).
ure 6 (middle) could perhaps be more informative if the values were averaged for each x value and error
s were reported. To make it extra-nice, individual average points could be colored with different
nsity/alpha to encode how many datapoints are in each bin. As it is now, it is hard to see a trend because
don't know how many point are actually in the black cloud in the middle.
have modified the plot by adding points and confidence intervals based on bins.
he methodology that the authors adopted for data analysis is a bit mixed and not always in the way one
ld expect. In the study quantitative statistics are reported, a crowdsourcing study has been conducted and
t of manual qualitative assessment has been performed too. Sometimes, to answer some specific
stions, I would have used different tools that the ones the authors used. For example, these two points :
wever, there seems to be a systematic deviation from a perfect correlation in which the number of up votes
erally outperforms the number of down votes" and "Based on a qualitative analysis of a small sample of
ies we can say that headwords with only one definition in UD often relate to words concerning the specific
munity of UD users" are probably good questions for the crowdworkers. Another example is the crowd
essment of a word bein a noun or not. I would probably have gone with a quantitative approach there,
ching the words with an external knowledge base or dictionary."
have updated the text to motivate our choices better. We have chosen a crowdsourcing approach because
ny words are not included in external knowledge bases or dictionaries. Furthermore, there are many
biguities (e.g. apple vs Apple) while capitalization is not consistent in Urban Dictionary, making it challenging
xtract such info automatically.
e last part on voting dynamics felt a little bit out of place and probably not as insightful as the rest of the
ults. Voting dynamics are influenced by a myriad of factors (starting from the interface, as the authors
ectly point out) that are often not easy to control. Some proxies might have a decent predictive power
ich is fine if your goal is to predict popularity) but not much explanation power. Also, I feel that the voting
amics are a bit different, in terms of scope, from the main focus of the paper which is supposed to be the
uage part. My gut feeling would be to remove that subsection altogether but it's ok if the authors want to
p it."
admit that this is a preliminary analysis (and we are planning follow-up work), but it does provide a more
plete view of Urban Dictionary and builds on the other analyses presented in the paper (i.e. it uses the
otations we collected and analyzed).
iewer 2
Motivation & Lack of context
authors make a convincing argument why the data from "Urban Dictionary" provides new possibilities for,
, i) studying language change; or ii) developing/refining natural language processing systems.
ever, I fail to see a strong connection between the presented analysis&results and the stated motivation.
le the authors show many correlations between pairs of variables (e.g. Fig. 6,7 or Table 4,5), the
lications (or even reasons) of these correlations remain almost completely unclear.
hermore, what is the importance of the variables chosen by the authors? While it is clear that one cannot
y all aspects, it seems that some of the characteristics/variables are chosen arbitrarily or simply because
are easily accessible, e.g. why focus on proper nouns, and not on adjectives; why formality and not the
th of the word? This relates to the point above -- how are these variables helping to approach the points
ed in the motivation? Additional information here would considerably improve the applicability of the
ults."
have expanded our motivation in the content analysis section. The aspects that we focused on were the
s that we expected would be different from content typically found in dictionaries. In particular, the ones that
focus on are not easily accessible (in contrast to e.g., the length of the word, for which no additional
otations would be needed). For example, we focus on formality because Urban Dictionary is known for its
s on slang and non-standard language, and thus we therefore expect it to describe more words that are not
ropriate in formal settings.
e properties that are being measured are not compared with measurements in other and/or 'traditional'
ionaries (besides the 'coverage' in Table 2,3). A comparison is not only necessary to put these numbers into
text, but also would possibly make a much stronger case why Urban Dictionary is important as a research
ct.”
le we do not provide an empirical comparison with traditional dictionaries, we rely on previous lexicographic
ies and dictionary guidelines which describe the type of content that is typically found in traditional
ionaries.
example:
• Traditional dictionaries’ coverage of proper nouns (Marconi, 1990: 77)
• Traditional dictionaries’ coverage of opinions (Rundell 2016:3-4)
• The presence of offensive language in dictionaries (Rundell 2016:7 for Macmillan English Dictionary)
• Number of editors involved in creating dictionaries (Rundell 2016:11)
• Policy for defining senses in traditional dictionaries (Rundell 2016:2)
• Typology of crowdsourcing in lexicography (Rundell 2016:3-6)
Analysis
e of the choices in the analysis were unclear from the description of the Main text/Methods. Additional
rmation would clarify how to interpret the results.
tion 2b): In the results of Table 3, the authors apply a threshold to the Urban Dictionary showing that "it
ears that the coverage of Wiktionary is higher than that of UD". Do the authors also apply the threshold to
tionary? If not, it seems to me to be an unfair comparison."
our response to the second comment of reviewer 1.
ction 2b): I find the term "lexical coverage" highly misleading. What is being covered here? The space of all
sible sequences of letters? The measurement only concerns the number of unique strings, thus it simply
ects the size of the database (in terms of entries) that is being measured."
have rephrased it to 'the number of headwords'
ction 2c,ii) The measurement of 'offensiveness' is unclear to me due to 2 points. First, it is not clear whether
objective notion of offensiveness exists -- the measured value is intrinsically tied to who is asked in the
ey. However, it seems the analysis here assumes (without proof or reference to prior work) that, indeed,
reported values reflect an objective measurable quantity that is related to the word (independent of the
son who rated). Second, the process of ranking 3 options seems arbitrary with respect to their offensiveness
ms arbitrary (why not 5 or 7 or 10?). Furthermore, the relative measurement (ranking) makes the
nsiveness of a word dependent upon the offensiveness of other words. It is not clear if and how these
ices introduce potential bias and artefacts, especially taking into account the rather small effect sizes (e.g.
le 4,5). In my opinion, it is thus necessary to convince the readers that the proposed measure of
nsiveness is indeed meaningful. Beyond the 2 examples in Table 12, the authors do not provide any
ence for that."
agree that someone's judgements regarding offensiveness is highly contextual and depends on many
ors, including the background of the reader. Furthermore, people might have different thresholds what they
sider to be offensive. This complicates the task of asking for a binary judgement. However, even if people
e different thresholds they could still agree when asked to rank the definitions for the same word. In our
lyses, we focus indeed on a relative measurement of offensiveness, but we believe that this makes our
roach actually more robust, since many different factors would come into play when we would want to
pare between words (e.g., words vary in their popularity, etc. etc. ). As reported in the paper, we find a high
eement between annotators in this setting. Furthermore, we average across five annotators for each data
t. We note that statistical tests are reported throughout the paper, including in our analyses on
nsiveness.
choice of presenting 3 items is motivated by wanting to include the top ranked, second ranked, and a
dom definition for each word. Having more than 3 items would complicate the task for annotators.
liability of overall results: "The majority of the entries in UD were not familiar to the crowdworkers."
,l.39).
m what I understood, all answers of the crowdworkers are part of the analysis independent of the familiarity.
ever, if workers are not familiar with an entry, how reliable is the judgement with respect to proper noun,
al/informal, etc.?"
question about familiarity is about prior familiarity. However, even if the crowdworkers were not familiar
the described meaning beforehand, they could still judge aspects such as proper noun, formality, etc. as
are presented with the word and definition when asked these questions.
roughout the manuscript the authors compare Urban Dictionary to 'traditional' dictionaries -- some of these
main claims contained in the abstract/discussion and this aspect serves as a main motivation for the
ent study. However, these comparisons are not supported by any empirical results as the 'traditional'
ionaries are not analyzed in this manuscript. Additional data/analysis or more precise statements would
ify the importance of the results."
have rephrased statements involving comparisons with traditional dictionaries. While we do not provide an
pirical comparison with traditional dictionaries, we rely on previous lexicographic studies and dictionary
elines which describe the type of content that is typically found in traditional dictionaries. For example,
t dictionaries aim for objective content and have strong guidelines to enforce that.
amples are:
stract, l.52: "Our study highlights that Urban Dictionary has a higher content heterogeneity than found in
itional dictionaries"
nged.
7, l.25: "manual inspection suggested that UD has a higher coverage of informal and infrequent words and
roper nouns"
his sentence we already acknowledge that this is based on manual inspection. In fact, we use this sentence
otivate our quantitative analysis of informality and proper nouns in UD.
7, l.35: "While most entries are marked as describing a meaning, the considerable presence of opinions
ws that the type of content in UD is different than in traditional dictionaries."
have rephrased this sentence slightly but also back up our statement by that fact that most dictionary aim
objective content and have strong guidelines to enforce this. We have added references to scholarly
lications such as Rundell (2016) and Marconi (1990) and links to guidelines (e.g. from Wiktionary) to
trate this.
10,l.21: "UD contains more offensive content than traditional dictionaries""
hrased.
10, l.49: "Differently from the more traditional dictionaries, we found that most contributors of UD only
ed one entry and very few added a high number of entries"; in fact, the authors speculate without reference
ata that (p.4,l.45) "However, this feature is fundamentally different to what one would expect from a more
itional dictionary, in which we expect a more even distribution of information per headword."
hrased.
iscussion, p.10, l.49: "Differently from the more traditional dictionaries, we found that most contributors of
only added one entry and very few added a high number of entries""
hrased.
iscussion, p.11, l.17: "Our study highlights that UD has a higher content heterogeneity than traditional
ionaries" (same as abstract)"
hrased.
iscussion, p.11, l.20: "However, UD is unique in capturing many infrequent, informal words and it could
efore complement the traditional dictionaries." In which sense is it unique? Without any comparison based
data, this is pure speculation."
hrased.
iscussion, p.11, l.21: " Furthermore, while there is more offensive content in UD, ..." more offensive than
re?"
hrased.
8, l.20: "In contrast, [...] in UD and one of its distinctive characteristics is its often offensive content." - How
we know that offensive content is one of its distinctive features?"
hrased.
igure 11 & p.7,l.48: "We also observe a slight trend regarding formality, with entries in higher frequency bins
e often marked to be appropriate for use in formal conversations."
iew of the absence of any (estimated) errorbars, I am not convinced of this claim as the blue curve is clearly
-monotonic."
added 95% confidence intervals.
Society Open
