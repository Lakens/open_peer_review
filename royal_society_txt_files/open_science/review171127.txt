Does it actually feel right? A replication attempt of the
rounded price effect
Christopher Harms, Hanna A. Genau, Carolin Meschede and André Beauducel
Article citation details
R. Soc. open sci. 5: 171127.
http://dx.doi.org/10.1098/rsos.171127
Review timeline
Original submission: 11 August 2017 Note: Reports are unedited and appear as
1st revised submission: 6 March 2018 submitted by the referee. The review history
2nd revised submission: 20 March 2018 appears in chronological order.
Final acceptance: 21 March 2018
Review History
label_version_1
RSOS-171127.R0 (Original submission)
label_author_1
Review form: Reviewer 1
Is the manuscript scientifically sound in its present form?
No
Are the interpretations and conclusions justified by the results?
No
Is the language acceptable?
No
Is it clear how to access all supporting data?
Yes
Do you have any ethical concerns with this paper?
No
© 2018 The Authors. Published by the Royal Society under the terms of the Creative Commons
Attribution License http://creativecommons.org/licenses/by/4.0/, which permits unrestricted use,
provided the original author and source are credited
2
Have you any concerns about statistical analyses in this paper?
Yes
Recommendation?
label_recommendation_1
Reject
Comments to the Author(s)
label_comment_1
Please find my comments to the author(s) in the attached file. (Appendix A)
label_author_2
Review form: Reviewer 2 (Casper Albers)
Is the manuscript scientifically sound in its present form?
Yes
Are the interpretations and conclusions justified by the results?
Yes
Is the language acceptable?
Yes
Is it clear how to access all supporting data?
Yes
Do you have any ethical concerns with this paper?
No
Have you any concerns about statistical analyses in this paper?
No
Recommendation?
label_recommendation_2
Accept with minor revision (please list in comments)
Comments to the Author(s)
label_comment_2
The replication crisis has taught us that we shouldn't take results from a single paper at face
value: there are too many reasons why the result could be incorrect or over-estimated. Repeating
the experiments behind important scientific claims is an important part of science. The benefit
(for the reviewer) of a paper describing such a replication, is that I don't have to think about
whether the results are interesting/important enough to publish. If the theory of the original
study is deemed publishable, then so should all (large scale) replications: they share exactly the
same theory.
It was a true pleasure for me to read this paper. In my experience, this paper is a textbook
example of how a paper on a replication study should be: i) look at an important (recent) claim in
theory; ii) announce the steps you will take to validate this theory (pre-register); iii) run the study
and report results; iv) share all materials. All these steps are taken.
The authors describe what they do in a clear, transparent and thorough fashion. I had not read
the Wadhwa & Zhang (W&Z) paper before (my background is not in consumer psychology), but
the description of their studies in this paper was largely clear to me. The sample size of the
3
replication was suffiently large (much larger than the original study!). The addition of a Bayesian
analysis on top of the frequentist one is a nice bonus.
Some minor comments:
- p. 5: You describe studies 1-3 by W&Z, but it is unclear exactly what the difference between a
rounded and unrounded number is. I assume 9.99 and 10.00 would be examples, but would e.g.
4.49 and 4.50 also qualify? Some more explicit information would be useful.
- Footnote 1 is 'used' in p.5, line 14, but the footnote itself is placed on p.6
- p. 5/6: It's interesting to p-curve the original results, but it might also be relevant to note that the
technique of p-curving itself is not without theoretical criticism (cf.
http://richarddmorey.org/content/Psynom17/pcurve/#/ , I believe dr. Morey also has a pre-
print on this topic, but couldn't find it).
- p.5, l.52: no need to mention first names (unless another Schimmack also derived some R-index)
- p.7, l. 7: APA-style is to report 3 digits, so .352 rather than .3523.
- p.7, bottom half: you (correctly) mention some favourable qualities of MTurk participants over
'regular' lab-participants, but MTurk has its drawbacks as well and for the balanced view, it
might be good to (shortly) mention this. See the recent column by Van Bavel & Rand for some
background and references (http://www.psychologicalscience.org/observer/restocking-our-
subject-pools).
- Footnote 3/8: no need to mention here that materials are available at OSF: you mention at the
end of the paper that 'everything' can be found there.
- p.10, l.52: N=588 --> N = 588 (add spaces to the equality sign)
- p.11, l.5: "Md" is not a well-known abbreviation of median; please write in full.
- p.18, l.14: please add a reference for O'Donnell & Nelson.
- References: You refer to both R-Index.org (2014) and Schimmack (2014); are these indeed two
distinct references?
- Fig 2: the two lines are not distinguishable when the pdf is printed (in black and white). Please
adjust the colours, or e.g. make one line dashed.
Signed,
Casper Albers
University of Groningen
label_author_3
Review form: Reviewer 3 (Michele Nuijten)
Is the manuscript scientifically sound in its present form?
Yes
Are the interpretations and conclusions justified by the results?
No
Is the language acceptable?
Yes
Is it clear how to access all supporting data?
Yes
Do you have any ethical concerns with this paper?
No
4
Have you any concerns about statistical analyses in this paper?
Yes
Recommendation?
label_recommendation_3
Accept with minor revision (please list in comments)
Comments to the Author(s)
label_comment_3
This study is a close replication attempt of the “rounded price effect” mediated by “feeling right”
as studied by Wadhwa & Zhang (2015). In the article, the authors both statistically analyze the
reliability of the original findings, and try to replicate it in a new sample.
Psychology can definitely use more close replications of published findings, so I applaud the
authors for performing one. In analyzing and interpreting the differences between the original
and replication findings, I think it was a good choice to give several “measures” of replicability,
rather than relying on one.
Some major strengths of this manuscript are that the authors preregistered their study, that
deviations from the preregistration are explicitly described, and that they shared all their data
and materials online. Furthermore, the authors are clear about the limitations of their study
(mainly caused by unforeseen differences between the original study and their replication). This
overall high level of transparency greatly increases the scientific quality and reliability of this
paper.
Overall, I think the manuscript was well-written, although some paragraphs need some further
clarification or need some rewriting to add more nuance to some of the conclusions.
My main point of critique concerns the section in which the authors evaluate the “evidential
value” in the original studies. In general, I’m interested in methods such as p-curve and R-Index,
and I can see how they can show interesting things about a set of results. However, I think it’s
important to take the limitations of these methods into account, and not take their results at face
value. I would therefore urge the authors to tone down their interpretation of these analyses. At
the moment the authors are strongly implying that the original authors p-hacked their results,
but there is no way of knowing that for sure (unless the original authors say they did). A p-curve
only gives you information about the likeliness of a set of significant p-values, given that there is
no effect in the population. If it turns out that a set of results is unlikely, you can’t simply equate
this with evidence for p-hacking or selective reporting. The same holds for the R-Index. I think
the manuscript would benefit from (1) an explanation of what these two methods do (especially
for the R-Index, this is not clear at all, but for the p-curve I also miss details such as: it only
focuses on significant p-values, how many p-values are included here, …), (2) what their
limitations are (there is plenty of debate in the literature about these methods, please refer to
that), and (3) a more careful and nuanced interpretation of the results. Right now, replications
have a questionable reputation, and “replicators” are frequently accused of being “shameless
little bullies”. I think this is a very sad and dangerous trend, and I would urge the authors not to
give anyone a reason to call them bullies. Adding more nuance to the interpretation of results,
and sticking to facts (rather than speculation about p-hacking) is an important part of that.
My second main point of critique is that the description of the replication study is not always
clear. Specifically, it is not entirely clear to me how the variables of interest were measured. For
instance, the dependent variable has to do with how participants in different conditions rated (?)
products, but how was this done exactly? Did they simply have to indicate if they would buy it
(yes/no), or give an evaluation of the product on a categorical/continuous scale? In Figure 2 and
Table 1, the dependent variable is expressed as the “average purchase likelihood” and
“Likelihood of Purchase”, but what does this mean exactly? Based on the stimulus materials, it
5
seems that a combination of questions is used, but it is unclear to me how this information is
summarized (or whether 1 of these questions is used as dependent variable). In short, the
variables that were used in the final, main analyses need to be explained in such a way that the
reader can understand where the numbers come from.
If these two main issues are solved, I think this paper can be a great example of “Psychology 2.0”.
I added some minor comments below.
Signed,
Michèle Nuijten
Minor remarks
• It might be an interesting additional analysis to calculate the Bayes factors for the original
study, using the reported frequentist test results. This way you can directly compare the results
with the replication study.
• In the pilot study, the authors tested the attractiveness of the binoculars among German
students, but in the actual study, the sample was much more heterogeneous.
• If it’s the policy of the Royal Society to place Figures & Tables at the end of the manuscript, I
would strongly encourage them to let go of that requirement; this makes reviewing much harder
• P. 8, line 50; maybe specify that the study was performed in Germany
• P. 11, line 21; “Baby” probably does not have to be capitalized.
• P. 12, line 22; I think adding some references here will strengthen your point. Some
suggestions: https://link.springer.com/article/10.3758/BF03194105,
https://www.ncbi.nlm.nih.gov/pubmed/21280965
• I would like to see an extra paragraph that shortly explains what a Bayes factor is and how you
should interpret one(optionally by giving the “categories” of evidential value by Jeffreys, see e.g.,
Table 1 in https://link.springer.com/article/10.3758%2Fs13428-014-0470-2)
• P. 13, line 39; “BF10 = 0.0026 ± 1.6%”, please specify what the 1.6% means. Is this a credible
interval?
• P. 13, line 48; “Unsurprisingly, …”, well, it is probably surprising for the original authors. I
would steer clear of language like this.
• Please be consistent in the number of decimals you report in statistical results (I think 3
decimals is sufficient)
• P. 15, line 15; actually, the BF indicates even more than that there is “no evidence for
mediation”, that is, it indicates that there is “evidence that there is no mediation”. This might
seem just semantics, but it is a crucial difference. In the first case, the data give no evidence for
mediation, which might mean that 1) there is no mediation, or 2) the data are ambiguous. In the
latter case, you can clearly see that the data are not ambiguous at all, and contain evidence that
there is no mediation.
• P. 16, line 26; I think you can just say ANOVA rather than Analysis of Variance
• P. 17, line 28: “except for a single contrast that was significant”, please specify which contrast
that was
• P. 19: “Researchers who would like to find further evidence in favor of the “Rounded Price
Effect” should expect an effect size considerably smaller and thus plan for a sample of at least 800
participants.” Considerably smaller than what? Also, this is quite strongly stated, especially after
listing all differences between the replication and the original study. The authors should either
explain (shortly) why others should expect a much smaller effect, or change the language of this
sentence to reflect more uncertainty about the true effect. The same holds for the sentence “Based
on our findings […], we conclude that [the effect] is – if existent at all – considerably smaller than
originally reported.” This should be toned down a bit (“we suspect that the effect is considerably
smaller”, or “the effect is likely to be smaller”)
• Figure 2;
6
o please use different line types rather than different colors. Especially the difference between red
and green is hard to see for people who are color blind.
o The figure would also improve if the y-axes are cut off at 5.
o Finally, I would consider changing the error bars from representing the SE, to representing a
95% confidence interval. This way, it is immediately visible which effects significantly different
from each other.
• Figure 3;
o Something seems to have gone wrong with labelling the paths. Path “beta” in the first panel is
labelled “alpha”.
o This figure also does not reflect that this is a moderated mediation. Where is the moderation in
this figure?
• Figure 4;
o To increase readability of the figure, I would add a vertical line with the meta-analytic effect
size. I also think it could be interesting to order the studies based on effect size (then you will see
in one glance that effect size seems to depend on the research team)
o Panel b: I don’t understand why this figure is rotated compared to the panel above. I would
keep this consistent, and have the effect size on the x-axis. As a matter of fact, I would make
forest plots of these findings. The advantage of a forest plot, is that the size of the dot reflects
sample size/precision.
label_author_4
Review form: Reviewer 4
Is the manuscript scientifically sound in its present form?
No
Are the interpretations and conclusions justified by the results?
No
Is the language acceptable?
Yes
Is it clear how to access all supporting data?
Yes
Do you have any ethical concerns with this paper?
No
Have you any concerns about statistical analyses in this paper?
Yes
Recommendation?
label_recommendation_4
Reject
Comments to the Author(s)
label_comment_4
label_version_2
RSOS-171127
Reviewer Comments
This paper attempts to replicate one Study (Study 5) from Wadhwa and Zhang (2015) paper.
Replications, especially conceptual replications, held advance science. However, I believe that
one should be careful of being overly enthusiastic of all replication efforts. The current paper tries
to draw conclusions based on a single replication effort, which is neither an exact replication nor
7
is it a conceptual replication attempt. The positive is that authors recognize this limitation.
However, as discussed below, there are many issues that stand out with this replication, which
one cannot ignore.
Differences between Studies: When a replication effort is made, it is important to understand how
similar the two studies are. Comparing the replication and the original study elucidates several
key differences between the two studies. The stimuli used, sample sources, the priming
procedure and the mediator measures employed are all different.
In terms of the stimuli, authors used two products in the replication study. The first product, as in
the Wadhwa and Zhang (2015) study, was a digital camera binoculars. However, what should be
noted is that the pretest clearly indicates that the baseline attitude toward binoculars for the
German population was very different from the American population. Specifically, the baseline
attitude for the binoculars was low for German participants. We cannot ignore this difference as
the baseline attitudes for the products can significantly impact results, especially because the
study is about product evaluations.
Moreover, authors used a second product, different from the one used in original paper, which
was rated more attractive by the German participants, compared with the binoculars. However,
authors chose to use the same price as the price they used for the digital binocular camera and
always presented the unpopular binoculars first. Doing so could have led to multiple problems 1)
presenting binoculars first would lead to a potential carry over effect to the other product (instant
camera) and thus could have contaminated the results and 2) it would also lead to a contrast
effect with people comparing the binoculars with the camera. Both of these problems can severely
impact the results of the second product.
One should also note that willingness to pay indicated for both products in the pretest was much
lower than the price of the products used in the main study. It seems like the author(s) did not
change the price to match the pretest.
Finally, in the original study prices, which is the most important variable for this study, were
presented as dollar amounts ($80.00). It seems like in the current study prices were presented as
Euros (81,43 €). Thus, the price format also is different.
Differences in Samples: Participants in the two studies are recruited from very different samples.
Participants in Wadhwa and Zhang (2015) studies were from a group of professional survey
respondents. Specifically, Wadhwa & Zhang (2015) recruited US based English speaking
participants from Amazon mechanical turk, which is a popularly used online data tool.
Participants in the current replication study were recruited from diverse samples. Moreover,
given the authors used different online channels to recruit their participants, their sample is also
likely to be significantly more heterogeneous, compared with the original Wadhwa and Zhang
study. Finally, while all the participants in the original Wadhwa and Zhang study were
compensated, participants in the current study were entered into a lottery for a gift card. As I
discuss below, these differences can significantly impact the conditions needed for the
replication.
Contingency of the Original Effect on Different Factors: The effect documented in the Wadhwa
and Zhang (2015) paper is an extension of the results of many other papers by different authors.
Specifically, different authors using different datasets, including real world datasets, have shown
that precise numbers are preferable in negotiation related situations (e.g., real estate priced at
precise numbers sells at higher prices). Wadhwa and Zhang (2015) suggested a moderator,
degree to which consumers rely on feeling, for the price effect shown in past research. The effect
8
suggests that precise numbers increase preferences when people have a cognitive mindset, while
round numbers increase preferences when people have an affective mindset.
The aforementioned discussion makes it clear that Wadhwa and Zhang’s (2015) effect seems to be
contingent on many different factors-1) participants should be in an affective mindset/cognitive
mindset, 2) prices should be perceived as rounded (round condition) or precise (precise
condition) and 3) price cues should be very salient. However, as noted above, given so many
differences between samples, stimuli and priming procedure used in the two studies, it is not
clear if these conditions, which are necessary for the effect to be demonstrated, were met.
It is likely that given the data source was online pools, such as social networks, participants were
already in an affective mindset. As indicated above, for the original results to be demonstrated, a
clean manipulation of affective-cognitive mindset seems to be important. However, for all the
reasons noted above, it is difficult to say how clean the manipulation of affective versus cognitive
mindset was in the replication study.
It is also likely that given the different price formats, prices considered precise by US population
might not be considered so by the German population. It could also be that digital instant
cameras are perceived as hedonic/affective by the German population, which would further
impact the results. In the original study presented in Wadhwa and Zhang (2015) paper, authors
pilot test shows that the product was considered neutral (that is not perceived as primarily
hedonic or cognitive). Did the authors calibrate the stimuli? Finally, it is not clear how salient the
price cues were.
Support for the original effect: Despite several differences between the two studies noted above,
authors do find some support for the original effect. In the feeling conditions, participants seem
to prefer the product with the rounded price. Simple contrast for the instant camera in the feeling
condition seems to be significant. This is consistent with the original results.
P-Curve Analysis: The authors conclude that the data reported in the original paper has a bias,
even though the test for the null hypothesis of a small effect was not rejected in the analysis. This
is a misleading conclusion.
Conclusion Based on A Single Replication Attempt for One Study: The authors are drawing
conclusions based on a single replication effect of one study from the paper. As recent reports
suggest, conclusions drawn on such kind of limited replication attempts can be misleading as
such replication effort suffer from low power, which is a really important issue (Gilbert et al.
2016). Additionally, differences in the samples could have introduced multiple random errors,
which again is a significant concern when attempting to replicate a study.
In sum, one needs to ask whether this study is indeed a faithful replication of the original study
and if any meaningful conclusions could be drawn from this study. Given, the differences, related
to stimuli, priming procedure and samples between the two studies, it seems like we cannot
drawn meaningful conclusions from the study.
9
label_end_comment
Decision letter (RSOS-171127)
07-Feb-2018
Dear Mr Harms,
The editors assigned to your paper ("Does it Actually Feel Right? A Replication Attempt of the
Rounded Price Effect") have now received comments from reviewers. We would like you to
revise your paper in accordance with the referee and Associate Editor suggestions which can be
found below (not including confidential reports to the Editor). Please note this decision does not
guarantee eventual acceptance.
In particular, we recommend that you tone down your critique of the previous paper and do not
imply that the authors p-hacked their data (see comments from Reviewer 3).
Please submit a copy of your revised paper within three weeks (i.e. by the 02-Mar-2018). If we do
not hear from you within this time then it will be assumed that the paper has been withdrawn. In
exceptional circumstances, extensions may be possible if agreed with the Editorial Office in
advance.We do not allow multiple rounds of revision so we urge you to make every effort to
fully address all of the comments at this stage. If deemed necessary by the Editors, your
manuscript will be sent back to one or more of the original reviewers for assessment. If the
original reviewers are not available, we may invite new reviewers.
To revise your manuscript, log into http://mc.manuscriptcentral.com/rsos and enter your
Author Centre, where you will find your manuscript title listed under "Manuscripts with
Decisions." Under "Actions," click on "Create a Revision." Your manuscript number has been
appended to denote a revision. Revise your manuscript and upload a new version through your
Author Centre.
When submitting your revised manuscript, you must respond to the comments made by the
referees and upload a file "Response to Referees" in "Section 6 - File Upload". Please use this to
document how you have responded to the comments, and the adjustments you have made. In
order to expedite the processing of the revised manuscript, please be as specific as possible in
your response.
In addition to addressing all of the reviewers' and editor's comments please also ensure that your
revised manuscript contains the following sections as appropriate before the reference list:
• Ethics statement (if applicable)
If your study uses humans or animals please include details of the ethical approval received,
including the name of the committee that granted approval. For human studies please also detail
whether informed consent was obtained. For field studies on animals please include details of all
permissions, licences and/or approvals granted to carry out the fieldwork.
• Data accessibility
It is a condition of publication that all supporting data are made available either as
supplementary information or preferably in a suitable permanent repository. The data
accessibility section should state where the article's supporting data can be accessed. This section
should also include details, where possible of where to access other relevant research materials
such as statistical tools, protocols, software etc can be accessed. If the data have been deposited in
an external repository this section should list the database, accession number and link to the DOI
for all data from the article that have been made publicly available. Data sets that have been
10
deposited in an external repository and have a DOI should also be appropriately cited in the
manuscript and included in the reference list.
If you wish to submit your supporting data or code to Dryad (http://datadryad.org/), or modify
your current submission to dryad, please use the following link:
http://datadryad.org/submit?journalID=RSOS&manu=RSOS-171127
• Competing interests
Please declare any financial or non-financial competing interests, or state that you have no
competing interests.
• Authors’ contributions
All submissions, other than those with a single author, must include an Authors’ Contributions
section which individually lists the specific contribution of each author. The list of Authors
should meet all of the following criteria; 1) substantial contributions to conception and design, or
acquisition of data, or analysis and interpretation of data; 2) drafting the article or revising it
critically for important intellectual content; and 3) final approval of the version to be published.
All contributors who do not meet all of these criteria should be included in the
acknowledgements.
We suggest the following format:
AB carried out the molecular lab work, participated in data analysis, carried out sequence
alignments, participated in the design of the study and drafted the manuscript; CD carried out
the statistical analyses; EF collected field data; GH conceived of the study, designed the study,
coordinated the study and helped draft the manuscript. All authors gave final approval for
publication.
• Acknowledgements
Please acknowledge anyone who contributed to the study but did not meet the authorship
criteria.
• Funding statement
Please list the source of funding for each author.
Please note that Royal Society Open Science will introduce article processing charges for all new
submissions received from 1 January 2018. Charges will also apply to papers transferred to Royal
Society Open Science from other Royal Society Publishing journals, as well as papers submitted
as part of our collaboration with the Royal Society of Chemistry
(http://rsos.royalsocietypublishing.org/chemistry). If your manuscript is submitted and
accepted for publication after 1 Jan 2018, you will be asked to pay the article processing charge,
unless you request a waiver and this is approved by Royal Society Publishing. You can find out
more about the charges at http://rsos.royalsocietypublishing.org/page/charges. Should you
have any queries, please contact openscience@royalsociety.org.
Once again, thank you for submitting your manuscript to Royal Society Open Science and I look
forward to receiving your revision. If you have any questions at all, please do not hesitate to get
in touch.
Kind regards,
Alice Power
Editorial Coordinator
Royal Society Open Science
11
openscience@royalsociety.org
on behalf of Antonia Hamilton (Subject Editor)
openscience@royalsociety.org
Comments to Author:
Reviewers' Comments to Author:
Reviewer: 1
Comments to the Author(s)
Please find my comments to the author(s) in the attached file.
Reviewer: 2
Comments to the Author(s)
The replication crisis has taught us that we shouldn't take results from a single paper at face
value: there are too many reasons why the result could be incorrect or over-estimated. Repeating
the experiments behind important scientific claims is an important part of science. The benefit
(for the reviewer) of a paper describing such a replication, is that I don't have to think about
whether the results are interesting/important enough to publish. If the theory of the original
study is deemed publishable, then so should all (large scale) replications: they share exactly the
same theory.
It was a true pleasure for me to read this paper. In my experience, this paper is a textbook
example of how a paper on a replication study should be: i) look at an important (recent) claim in
theory; ii) announce the steps you will take to validate this theory (pre-register); iii) run the study
and report results; iv) share all materials. All these steps are taken.
The authors describe what they do in a clear, transparent and thorough fashion. I had not read
the Wadhwa & Zhang (W&Z) paper before (my background is not in consumer psychology), but
the description of their studies in this paper was largely clear to me. The sample size of the
replication was suffiently large (much larger than the original study!). The addition of a Bayesian
analysis on top of the frequentist one is a nice bonus.
Some minor comments:
- p. 5: You describe studies 1-3 by W&Z, but it is unclear exactly what the difference between a
rounded and unrounded number is. I assume 9.99 and 10.00 would be examples, but would e.g.
4.49 and 4.50 also qualify? Some more explicit information would be useful.
- Footnote 1 is 'used' in p.5, line 14, but the footnote itself is placed on p.6
- p. 5/6: It's interesting to p-curve the original results, but it might also be relevant to note that the
technique of p-curving itself is not without theoretical criticism (cf.
http://richarddmorey.org/content/Psynom17/pcurve/#/ , I believe dr. Morey also has a pre-
print on this topic, but couldn't find it).
- p.5, l.52: no need to mention first names (unless another Schimmack also derived some R-index)
- p.7, l. 7: APA-style is to report 3 digits, so .352 rather than .3523.
- p.7, bottom half: you (correctly) mention some favourable qualities of MTurk participants over
'regular' lab-participants, but MTurk has its drawbacks as well and for the balanced view, it
might be good to (shortly) mention this. See the recent column by Van Bavel & Rand for some
12
background and references (http://www.psychologicalscience.org/observer/restocking-our-
subject-pools).
- Footnote 3/8: no need to mention here that materials are available at OSF: you mention at the
end of the paper that 'everything' can be found there.
- p.10, l.52: N=588 --> N = 588 (add spaces to the equality sign)
- p.11, l.5: "Md" is not a well-known abbreviation of median; please write in full.
- p.18, l.14: please add a reference for O'Donnell & Nelson.
- References: You refer to both R-Index.org (2014) and Schimmack (2014); are these indeed two
distinct references?
- Fig 2: the two lines are not distinguishable when the pdf is printed (in black and white). Please
adjust the colours, or e.g. make one line dashed.
Signed,
Casper Albers
University of Groningen
Reviewer: 3
Comments to the Author(s)
This study is a close replication attempt of the “rounded price effect” mediated by “feeling right”
as studied by Wadhwa & Zhang (2015). In the article, the authors both statistically analyze the
reliability of the original findings, and try to replicate it in a new sample.
Psychology can definitely use more close replications of published findings, so I applaud the
authors for performing one. In analyzing and interpreting the differences between the original
and replication findings, I think it was a good choice to give several “measures” of replicability,
rather than relying on one.
Some major strengths of this manuscript are that the authors preregistered their study, that
deviations from the preregistration are explicitly described, and that they shared all their data
and materials online. Furthermore, the authors are clear about the limitations of their study
(mainly caused by unforeseen differences between the original study and their replication). This
overall high level of transparency greatly increases the scientific quality and reliability of this
paper.
Overall, I think the manuscript was well-written, although some paragraphs need some further
clarification or need some rewriting to add more nuance to some of the conclusions.
My main point of critique concerns the section in which the authors evaluate the “evidential
value” in the original studies. In general, I’m interested in methods such as p-curve and R-Index,
and I can see how they can show interesting things about a set of results. However, I think it’s
important to take the limitations of these methods into account, and not take their results at face
value. I would therefore urge the authors to tone down their interpretation of these analyses. At
the moment the authors are strongly implying that the original authors p-hacked their results,
but there is no way of knowing that for sure (unless the original authors say they did). A p-curve
only gives you information about the likeliness of a set of significant p-values, given that there is
no effect in the population. If it turns out that a set of results is unlikely, you can’t simply equate
this with evidence for p-hacking or selective reporting. The same holds for the R-Index. I think
the manuscript would benefit from (1) an explanation of what these two methods do (especially
for the R-Index, this is not clear at all, but for the p-curve I also miss details such as: it only
focuses on significant p-values, how many p-values are included here, …), (2) what their
limitations are (there is plenty of debate in the literature about these methods, please refer to
that), and (3) a more careful and nuanced interpretation of the results. Right now, replications
13
have a questionable reputation, and “replicators” are frequently accused of being “shameless
little bullies”. I think this is a very sad and dangerous trend, and I would urge the authors not to
give anyone a reason to call them bullies. Adding more nuance to the interpretation of results,
and sticking to facts (rather than speculation about p-hacking) is an important part of that.
My second main point of critique is that the description of the replication study is not always
clear. Specifically, it is not entirely clear to me how the variables of interest were measured. For
instance, the dependent variable has to do with how participants in different conditions rated (?)
products, but how was this done exactly? Did they simply have to indicate if they would buy it
(yes/no), or give an evaluation of the product on a categorical/continuous scale? In Figure 2 and
Table 1, the dependent variable is expressed as the “average purchase likelihood” and
“Likelihood of Purchase”, but what does this mean exactly? Based on the stimulus materials, it
seems that a combination of questions is used, but it is unclear to me how this information is
summarized (or whether 1 of these questions is used as dependent variable). In short, the
variables that were used in the final, main analyses need to be explained in such a way that the
reader can understand where the numbers come from.
If these two main issues are solved, I think this paper can be a great example of “Psychology 2.0”.
I added some minor comments below.
Signed,
Michèle Nuijten
Minor remarks
• It might be an interesting additional analysis to calculate the Bayes factors for the original
study, using the reported frequentist test results. This way you can directly compare the results
with the replication study.
• In the pilot study, the authors tested the attractiveness of the binoculars among German
students, but in the actual study, the sample was much more heterogeneous.
• If it’s the policy of the Royal Society to place Figures & Tables at the end of the manuscript, I
would strongly encourage them to let go of that requirement; this makes reviewing much harder
• P. 8, line 50; maybe specify that the study was performed in Germany
• P. 11, line 21; “Baby” probably does not have to be capitalized.
• P. 12, line 22; I think adding some references here will strengthen your point. Some
suggestions: https://link.springer.com/article/10.3758/BF03194105,
https://www.ncbi.nlm.nih.gov/pubmed/21280965
• I would like to see an extra paragraph that shortly explains what a Bayes factor is and how you
should interpret one(optionally by giving the “categories” of evidential value by Jeffreys, see e.g.,
Table 1 in https://link.springer.com/article/10.3758%2Fs13428-014-0470-2)
• P. 13, line 39; “BF10 = 0.0026 ± 1.6%”, please specify what the 1.6% means. Is this a credible
interval?
• P. 13, line 48; “Unsurprisingly, …”, well, it is probably surprising for the original authors. I
would steer clear of language like this.
• Please be consistent in the number of decimals you report in statistical results (I think 3
decimals is sufficient)
• P. 15, line 15; actually, the BF indicates even more than that there is “no evidence for
mediation”, that is, it indicates that there is “evidence that there is no mediation”. This might
seem just semantics, but it is a crucial difference. In the first case, the data give no evidence for
mediation, which might mean that 1) there is no mediation, or 2) the data are ambiguous. In the
latter case, you can clearly see that the data are not ambiguous at all, and contain evidence that
there is no mediation.
• P. 16, line 26; I think you can just say ANOVA rather than Analysis of Variance
14
• P. 17, line 28: “except for a single contrast that was significant”, please specify which contrast
that was
• P. 19: “Researchers who would like to find further evidence in favor of the “Rounded Price
Effect” should expect an effect size considerably smaller and thus plan for a sample of at least 800
participants.” Considerably smaller than what? Also, this is quite strongly stated, especially after
listing all differences between the replication and the original study. The authors should either
explain (shortly) why others should expect a much smaller effect, or change the language of this
sentence to reflect more uncertainty about the true effect. The same holds for the sentence “Based
on our findings […], we conclude that [the effect] is – if existent at all – considerably smaller than
originally reported.” This should be toned down a bit (“we suspect that the effect is considerably
smaller”, or “the effect is likely to be smaller”)
• Figure 2;
o please use different line types rather than different colors. Especially the difference between red
and green is hard to see for people who are color blind.
o The figure would also improve if the y-axes are cut off at 5.
o Finally, I would consider changing the error bars from representing the SE, to representing a
95% confidence interval. This way, it is immediately visible which effects significantly different
from each other.
• Figure 3;
o Something seems to have gone wrong with labelling the paths. Path “beta” in the first panel is
labelled “alpha”.
o This figure also does not reflect that this is a moderated mediation. Where is the moderation in
this figure?
• Figure 4;
o To increase readability of the figure, I would add a vertical line with the meta-analytic effect
size. I also think it could be interesting to order the studies based on effect size (then you will see
in one glance that effect size seems to depend on the research team)
o Panel b: I don’t understand why this figure is rotated compared to the panel above. I would
keep this consistent, and have the effect size on the x-axis. As a matter of fact, I would make
forest plots of these findings. The advantage of a forest plot, is that the size of the dot reflects
sample size/precision.
Reviewer: 4
Comments to the Author(s)
label_version_3
RSOS-171127
Reviewer Comments
This paper attempts to replicate one Study (Study 5) from Wadhwa and Zhang (2015) paper.
Replications, especially conceptual replications, held advance science. However, I believe that
one should be careful of being overly enthusiastic of all replication efforts. The current paper tries
to draw conclusions based on a single replication effort, which is neither an exact replication nor
is it a conceptual replication attempt. The positive is that authors recognize this limitation.
However, as discussed below, there are many issues that stand out with this replication, which
one cannot ignore.
Differences between Studies: When a replication effort is made, it is important to understand how
similar the two studies are. Comparing the replication and the original study elucidates several
key differences between the two studies. The stimuli used, sample sources, the priming
procedure and the mediator measures employed are all different.
In terms of the stimuli, authors used two products in the replication study. The first product, as in
15
the Wadhwa and Zhang (2015) study, was a digital camera binoculars. However, what should be
noted is that the pretest clearly indicates that the baseline attitude toward binoculars for the
German population was very different from the American population. Specifically, the baseline
attitude for the binoculars was low for German participants. We cannot ignore this difference as
the baseline attitudes for the products can significantly impact results, especially because the
study is about product evaluations.
Moreover, authors used a second product, different from the one used in original paper, which
was rated more attractive by the German participants, compared with the binoculars. However,
authors chose to use the same price as the price they used for the digital binocular camera and
always presented the unpopular binoculars first. Doing so could have led to multiple problems 1)
presenting binoculars first would lead to a potential carry over effect to the other product (instant
camera) and thus could have contaminated the results and 2) it would also lead to a contrast
effect with people comparing the binoculars with the camera. Both of these problems can severely
impact the results of the second product.
One should also note that willingness to pay indicated for both products in the pretest was much
lower than the price of the products used in the main study. It seems like the author(s) did not
change the price to match the pretest.
Finally, in the original study prices, which is the most important variable for this study, were
presented as dollar amounts ($80.00). It seems like in the current study prices were presented as
Euros (81,43 €). Thus, the price format also is different.
Differences in Samples: Participants in the two studies are recruited from very different samples.
Participants in Wadhwa and Zhang (2015) studies were from a group of professional survey
respondents. Specifically, Wadhwa & Zhang (2015) recruited US based English speaking
participants from Amazon mechanical turk, which is a popularly used online data tool.
Participants in the current replication study were recruited from diverse samples. Moreover,
given the authors used different online channels to recruit their participants, their sample is also
likely to be significantly more heterogeneous, compared with the original Wadhwa and Zhang
study. Finally, while all the participants in the original Wadhwa and Zhang study were
compensated, participants in the current study were entered into a lottery for a gift card. As I
discuss below, these differences can significantly impact the conditions needed for the
replication.
Contingency of the Original Effect on Different Factors: The effect documented in the Wadhwa
and Zhang (2015) paper is an extension of the results of many other papers by different authors.
Specifically, different authors using different datasets, including real world datasets, have shown
that precise numbers are preferable in negotiation related situations (e.g., real estate priced at
precise numbers sells at higher prices). Wadhwa and Zhang (2015) suggested a moderator,
degree to which consumers rely on feeling, for the price effect shown in past research. The effect
suggests that precise numbers increase preferences when people have a cognitive mindset, while
round numbers increase preferences when people have an affective mindset.
The aforementioned discussion makes it clear that Wadhwa and Zhang’s (2015) effect seems to be
contingent on many different factors-1) participants should be in an affective mindset/cognitive
mindset, 2) prices should be perceived as rounded (round condition) or precise (precise
condition) and 3) price cues should be very salient. However, as noted above, given so many
differences between samples, stimuli and priming procedure used in the two studies, it is not
clear if these conditions, which are necessary for the effect to be demonstrated, were met.
It is likely that given the data source was online pools, such as social networks, participants were
16
already in an affective mindset. As indicated above, for the original results to be demonstrated, a
clean manipulation of affective-cognitive mindset seems to be important. However, for all the
reasons noted above, it is difficult to say how clean the manipulation of affective versus cognitive
mindset was in the replication study.
It is also likely that given the different price formats, prices considered precise by US population
might not be considered so by the German population. It could also be that digital instant
cameras are perceived as hedonic/affective by the German population, which would further
impact the results. In the original study presented in Wadhwa and Zhang (2015) paper, authors
pilot test shows that the product was considered neutral (that is not perceived as primarily
hedonic or cognitive). Did the authors calibrate the stimuli? Finally, it is not clear how salient the
price cues were.
Support for the original effect: Despite several differences between the two studies noted above,
authors do find some support for the original effect. In the feeling conditions, participants seem
to prefer the product with the rounded price. Simple contrast for the instant camera in the feeling
condition seems to be significant. This is consistent with the original results.
P-Curve Analysis: The authors conclude that the data reported in the original paper has a bias,
even though the test for the null hypothesis of a small effect was not rejected in the analysis. This
is a misleading conclusion.
Conclusion Based on A Single Replication Attempt for One Study: The authors are drawing
conclusions based on a single replication effect of one study from the paper. As recent reports
suggest, conclusions drawn on such kind of limited replication attempts can be misleading as
such replication effort suffer from low power, which is a really important issue (Gilbert et al.
2016). Additionally, differences in the samples could have introduced multiple random errors,
which again is a significant concern when attempting to replicate a study.
In sum, one needs to ask whether this study is indeed a faithful replication of the original study
and if any meaningful conclusions could be drawn from this study. Given, the differences, related
to stimuli, priming procedure and samples between the two studies, it seems like we cannot
drawn meaningful conclusions from the study.
Author's Response to Decision Letter for (RSOS-171127)
See Appendix B.
label_version_4
RSOS-171127.R1 (Revision)
label_author_5
Review form: Reviewer 2 (Casper Albers)
Is the manuscript scientifically sound in its present form?
Yes
17
Are the interpretations and conclusions justified by the results?
Yes
Is the language acceptable?
Yes
Is it clear how to access all supporting data?
Yes
Do you have any ethical concerns with this paper?
No
Have you any concerns about statistical analyses in this paper?
No
Recommendation?
label_recommendation_5
Accept as is
Comments to the Author(s)
label_comment_5
Dear authors,
Thank you for your detailed response to my review. In my view, you revised the paper
adequately based on my comments, and I have no further comments.
Casper Albers
label_author_6
Review form: Reviewer 3 (Michele Nuijten)
Is the manuscript scientifically sound in its present form?
Yes
Are the interpretations and conclusions justified by the results?
Yes
Is the language acceptable?
Yes
Is it clear how to access all supporting data?
Yes
Do you have any ethical concerns with this paper?
No
Have you any concerns about statistical analyses in this paper?
No
Recommendation?
label_recommendation_6
Accept with minor revision (please list in comments)
Comments to the Author(s)
label_comment_6
The authors have incorporated my comments in their updated manuscript to my satisfaction. I
think the manuscript greatly improved by the added nuance in the conclusions. I noted two
18
(very) minor things in the reporting of prices (see below), but other than that I think this is a great
manuscript, as is.
Signed,
Michèle Nuijten
• P. 3-4: I think it’s European to use a comma as a decimal point; in English I think you’d have to
write “40.00” euros rather than “40,00” euros.
• P. 14: I think it should be “€80.00” rather than “80.00 €”
label_end_comment
Decision letter (RSOS-171127.R1)
20-Mar-2018
Dear Mr Harms:
On behalf of the Editors, I am pleased to inform you that your Manuscript RSOS-171127.R1
entitled "Does it Actually Feel Right? A Replication Attempt of the Rounded Price Effect" has
been accepted for publication in Royal Society Open Science subject to minor revision in
accordance with the referee suggestions. Please find the referees' comments at the end of this
email.
The reviewers and Subject Editor have recommended publication, but also suggest some minor
revisions to your manuscript. Therefore, I invite you to respond to the comments and revise your
manuscript.
• Ethics statement
If your study uses humans or animals please include details of the ethical approval received,
including the name of the committee that granted approval. For human studies please also detail
whether informed consent was obtained. For field studies on animals please include details of all
permissions, licences and/or approvals granted to carry out the fieldwork.
• Data accessibility
It is a condition of publication that all supporting data are made available either as
supplementary information or preferably in a suitable permanent repository. The data
accessibility section should state where the article's supporting data can be accessed. This section
should also include details, where possible of where to access other relevant research materials
such as statistical tools, protocols, software etc can be accessed. If the data has been deposited in
an external repository this section should list the database, accession number and link to the DOI
for all data from the article that has been made publicly available. Data sets that have been
deposited in an external repository and have a DOI should also be appropriately cited in the
manuscript and included in the reference list.
If you wish to submit your supporting data or code to Dryad (http://datadryad.org/), or modify
your current submission to dryad, please use the following link:
http://datadryad.org/submit?journalID=RSOS&manu=RSOS-171127.R1
• Competing interests
Please declare any financial or non-financial competing interests, or state that you have no
competing interests.
19
• Authors’ contributions
All submissions, other than those with a single author, must include an Authors’ Contributions
section which individually lists the specific contribution of each author. The list of Authors
should meet all of the following criteria; 1) substantial contributions to conception and design, or
acquisition of data, or analysis and interpretation of data; 2) drafting the article or revising it
critically for important intellectual content; and 3) final approval of the version to be published.
All contributors who do not meet all of these criteria should be included in the
acknowledgements.
We suggest the following format:
AB carried out the molecular lab work, participated in data analysis, carried out sequence
alignments, participated in the design of the study and drafted the manuscript; CD carried out
the statistical analyses; EF collected field data; GH conceived of the study, designed the study,
coordinated the study and helped draft the manuscript. All authors gave final approval for
publication.
• Acknowledgements
Please acknowledge anyone who contributed to the study but did not meet the authorship
criteria.
• Funding statement
Please list the source of funding for each author.
Please note that we cannot publish your manuscript without these end statements included. We
have included a screenshot example of the end statements for reference. If you feel that a given
heading is not relevant to your paper, please nevertheless include the heading and explicitly state
that it is not relevant to your work.
Because the schedule for publication is very tight, it is a condition of publication that you submit
the revised version of your manuscript within 7 days (i.e. by the 29-Mar-2018). If you do not think
you will be able to meet this date please let me know immediately.
To revise your manuscript, log into https://mc.manuscriptcentral.com/rsos and enter your
Author Centre, where you will find your manuscript title listed under "Manuscripts with
Decisions". Under "Actions," click on "Create a Revision." You will be unable to make your
revisions on the originally submitted version of the manuscript. Instead, revise your manuscript
and upload a new version through your Author Centre.
When submitting your revised manuscript, you will be able to respond to the comments made by
the referees and upload a file "Response to Referees" in "Section 6 - File Upload". You can use this
to document any changes you make to the original manuscript. In order to expedite the
processing of the revised manuscript, please be as specific as possible in your response to the
referees.
When uploading your revised files please make sure that you have:
1) A text file of the manuscript (tex, txt, rtf, docx or doc), references, tables (including captions)
and figure captions. Do not upload a PDF as your "Main Document".
2) A separate electronic file of each figure (EPS or print-quality PDF preferred (either format
should be produced directly from original creation package), or original software format)
20
3) Included a 100 word media summary of your paper when requested at submission. Please
ensure you have entered correct contact details (email, institution and telephone) in your user
account
4) Included the raw data to support the claims made in your paper. You can either include your
data as electronic supplementary material or upload to a repository and include the relevant doi
within your manuscript
5) All supplementary materials accompanying an accepted article will be treated as in their final
form. Note that the Royal Society will neither edit nor typeset supplementary material and it will
be hosted as provided. Please ensure that the supplementary material includes the paper details
where possible (authors, article title, journal name).
Supplementary files will be published alongside the paper on the journal website and posted on
the online figshare repository (https://figshare.com). The heading and legend provided for each
supplementary file during the submission process will be used to create the figshare page, so
please ensure these are accurate and informative so that your files can be found in searches. Files
on figshare will be made available approximately one week before the accompanying article so
that the supplementary material can be attributed a unique DOI.
Please note that Royal Society Open Science will introduce article processing charges for all new
submissions received from 1 January 2018. Charges will also apply to papers transferred to Royal
Society Open Science from other Royal Society Publishing journals, as well as papers submitted
as part of our collaboration with the Royal Society of Chemistry
(http://rsos.royalsocietypublishing.org/chemistry). If your manuscript is submitted and
accepted for publication after 1 Jan 2018, you will be asked to pay the article processing charge,
unless you request a waiver and this is approved by Royal Society Publishing. You can find out
more about the charges at http://rsos.royalsocietypublishing.org/page/charges. Should you
have any queries, please contact openscience@royalsociety.org.
Once again, thank you for submitting your manuscript to Royal Society Open Science and I look
forward to receiving your revision. If you have any questions at all, please do not hesitate to get
in touch.
Kind regards,
Alice Power
Royal Society Open Science
openscience@royalsociety.org
on behalf of Dr Antonia Hamilton (Subject Editor)
openscience@royalsociety.org
Reviewer comments to Author:
Reviewer: 3
Comments to the Author(s)
The authors have incorporated my comments in their updated manuscript to my satisfaction. I
think the manuscript greatly improved by the added nuance in the conclusions. I noted two
(very) minor things in the reporting of prices (see below), but other than that I think this is a great
manuscript, as is.
Signed,
Michèle Nuijten
21
• P. 3-4: I think it’s European to use a comma as a decimal point; in English I think you’d have to
write “40.00” euros rather than “40,00” euros.
• P. 14: I think it should be “€80.00” rather than “80.00 €”
Reviewer: 2
Comments to the Author(s)
Dear authors,
Thank you for your detailed response to my review. In my view, you revised the paper
adequately based on my comments, and I have no further comments.
Casper Albers
Author's Response to Decision Letter for (RSOS-171127.R1)
Dear Editor,
thank you for the kind acceptance of our paper. We also extend our thanks to the editorial team
for the coordination and handling of the manuscript as well as to the four reviewers.
We have revised the manuscript according to the requests by reviewer #3 regarding the display
of the currency symbol and numbers.
With kind regards
on behalf of the authors
Christopher Harms
label_end_comment
Decision letter (RSOS-171127.R2)
21-Mar-2018
Dear Mr Harms,
I am pleased to inform you that your manuscript entitled "Does it Actually Feel Right? A
Replication Attempt of the Rounded Price Effect" is now accepted for publication in Royal Society
Open Science.
You can expect to receive a proof of your article in the near future. Please contact the editorial
office (openscience_proofs@royalsociety.org and openscience@royalsociety.org) to let us know if
you are likely to be away from e-mail contact. Due to rapid publication and an extremely tight
schedule, if comments are not received, your paper may experience a delay in publication.
22
Royal Society Open Science operates under a continuous publication model
(http://bit.ly/cpFAQ). Your article will be published straight into the next open issue and this
will be the final version of the paper. As such, it can be cited immediately by other researchers.
As the issue version of your paper will be the only version to be published I would advise you to
check your proofs thoroughly as changes cannot be made once the paper is published.
In order to raise the profile of your paper once it is published, we can send through a PDF of your
paper to selected colleagues. If you wish to take advantage of this, please reply to this email with
the name and email addresses of up to 10 people who you feel would wish to read your article.
Please note that Royal Society Open Science will introduce article processing charges for all new
submissions received from 1 January 2018. Charges will also apply to papers transferred to Royal
Society Open Science from other Royal Society Publishing journals, as well as papers submitted
as part of our collaboration with the Royal Society of Chemistry
(http://rsos.royalsocietypublishing.org/chemistry).
If your manuscript is newly submitted and subsequently accepted for publication after 1 Jan 2018,
you will be asked to pay the article processing charge, unless you request a waiver and this is
approved by Royal Society Publishing. Manuscripts originally submitted prior to 1 Jan 2018 will
not subject to a charge, even if they are accepted in 2018. You can find out more about the charges
at http://rsos.royalsocietypublishing.org/page/charges. Should you have any queries, please
contact openscience@royalsociety.org.
On behalf of the Editors of Royal Society Open Science, we look forward to your continued
contributions to the Journal.
Kind regards,
Alice Power
Royal Society Open Science
openscience@royalsociety.org
on behalf of Dr Antonia Hamilton (Subject Editor)
openscience@royalsociety.org
pendix A
mments to the Author(s):
s paper sought to replicate the rounded price effect documented in Wadhwa & Zhang (2015).
do so, the authors attempted to replicate Study 5 in the original paper, which tested both the
ic effects and the mediation analysis. I applaud the authors for their efforts. However, as
cussed below, there are several major concerns I have with this paper:
Stimuli and Experiment Design:
re are noticeable differences in the stimuli and experiment design between this replication
dy and the original study in Wadhwa & Zhang (2015).
t, the authors of this paper found that the digital camera binoculars product that they had used
he main study was poorly rated in a pretest. Consistent with the pretest result, in the main
dy the purchase likelihood of all the conditions was on an extremely low end, indicating that
product was not liked by the participants. This is important because the original paper shows
rounded (nonrounded) price only leads to more favorable evaluations when the product
its positive feelings (favorable cognitions).
reover, the authors used a second product (digital instant camera), which was rated more
active but priced the same as the digital camera binoculars in the study. This could have
entially led to a contrast effect and affect participants’ evaluation for the second product.
Data Source:
authors used a different sample population from a different culture to conduct the replication
dy. While it is good to replicate the research across different cultures, it raises questions on
close the replication study is to the original study. Wadhwa & Zhang (2015) recruited
lish-speaking participants in the United States via the Amazon Mechanical Turk participant
l. In contrast, the authors recruited a convenience sample of German-speaking participants
ng several different online-channels, such as social media and newsletters of online magazines
r a two-month period. Participants did not receive a participation fee but were instead entered
a lottery for an Amazon gift card worth of 20 euros. Given the cultural difference, the price
s were also different in these two studies, one in US dollars (e.g., $80.00) and the other in
os (e.g., presented as EUR 80,00 in this study).
ike Amazon Mechanical Turk that has been widely used and proven to be a credible online
a source for academic research (e,g., Buhrmester, Kwang, & Gosling, 2011; Paolacci &
andler, 2014), there is no validation for whether the participants recruited from various
erent sources (e.g., social media, newsletters of online magazines) in this study actually paid
ntion to the stimuli. Given the authors are manipulating only the price information, which
a non-prominent visual cue on the screen, it is unclear if the participants really paid attention
he manipulation (i.e., price information).
authors claim that the data quality is an issue on Amazon MTurk, but did not provide any
ification for why the data quality is better for the data source they used for this replication
dy. Data for this study was collected via different sources and over a period of two months.
ated to this issue, authors’ claims such as “our sample seems to be more homogenous than the
inal sample” is not supported by facts. The demographics of the study (e.g., age range 18-86)
s not appear more homogenous than MTurk samples.
Interpretation of the Results:
authors seem to misinterpret several results, which reflect a lack of an objective assessment
he evidence.
1) The p-curve analysis the authors did on the original paper found that the test of the right-
skewness of the p-curve was not significant. According to Simonsohn et al. (2014), a null
finding (a p-curve that is not significantly right-skewed) may indicate that the set of
studies lack evidential value or that there is not enough information to make inferences
about evidential value. However, the authors directly concluded that there is bias in the
reported data due to either selective reporting or p-hacking, even though the test for
whether the set of studies lack evidential value was also not significant (i.e., null of 33%
power was not rejected) in the analysis.
2) The authors did replicate the positive effect of rounded price versus nonrounded price in
the feeling priming condition for both products (weak support for the poorly-rated digital
camera binoculars and strong support for the digital instant camera). This is consistent
with the original results given the original results show that the rounded price only have a
positive effect on evaluations when the product elicits positive feelings (e.g., when the
product is attractive).
3) The authors used the contrast results from Wadhwa & Zhang (2015) for the p-curve
analysis, but instead used the effect size of the interaction terms for comparison analysis
with that paper (e.g., Figure 4). For consistency of the analyses, the authors should
present the effect sizes for the contrast results instead.
Conclusion Based on A Single Replication Attempt:
ed on a single replication attempt of one out of the five studies in the original paper, it is also
icult to conclude that an effect is considerably smaller or even does not exist, especially when
re are various differences in the experiment stimuli, data source, and procedure. In a recent
ort in Science, Daniel Gilbert and his colleagues (Gilbert, King, Pettigrew, & Wilson, 2016)
e shown that there is still a lack of power when replicating a single study only once and this
hod can severely underestimate the replication rate.
ummary, replications of recent research can contribute to understanding of the emerging
earch findings. However, a weak replication attempt can be problematic as it could lead to
neous, incomplete conclusions.
Appendix B
Responses to the Reviewers
We thank Casper Albers, Michéle Nuijten and the two anonymous reviewers for their
eedback and criticism of our manuscript. In the following we will address their remarks
ndividually point by point. Text in italics is taken directly from the review. Changes to the
ext have been highlighted in red in the manuscript.
We would also like to thank the editorial team for the professional handling of the
manuscript.
n general, we have toned down our critique of the original study as requested by reviewer
#3 and the editor. We further included criticisms on the methods we used to highlight that
our findings can be interpreted from different perspectives as suggested by reviewer #2.
Since we cannot know the intentions and actions of the original authors, we did not intend
o imply that p-hacking and questionable research practices are the most probable reason
or the results of the original study. Rather we believe there is reason to doubt the original
indings based on the evaluation of the original study and the performed replication. There
are different explanations for the discrepancies in results and p-hacking and questionable
esearch practices are only two among several others. Hidden moderators and differences in
stimuli and sample are other sensible reasons. We have updated the manuscript to better
eflect the different perspectives.
Reviewer #1
This paper sought to replicate the rounded price effect documented in Wadhwa & Zhang
2015). To do so, the authors attempted to replicate Study 5 in the original paper, which
ested both the basic effects and the mediation analysis. I applaud the authors for their
efforts. However, as discussed below, there are several major concerns I have with this
paper:
1. Stimuli and Experiment Design:
There are noticeable differences in the stimuli and experiment design between this
eplication study and the original study in Wadhwa & Zhang (2015).
First, the authors of this paper found that the digital camera binoculars product that they
had used in the main study was poorly rated in a pretest. Consistent with the pretest result,
n the main study the purchase likelihood of all the conditions was on an extremely low end,
ndicating that the product was not liked by the participants. This is important because the
original paper shows that rounded (nonrounded) price only leads to more favorable
evaluations when the product elicits positive feelings (favorable cognitions).
Moreover, the authors used a second product (digital instant camera), which was rated more
attractive but priced the same as the digital camera binoculars in the study. This could have
potentially led to a contrast effect and affect participants’ evaluation for the second product.
Response:
As the reviewer correctly states, the participants did not rate the product from the
original study very favorably. We therefore included a second product as an additional
stimulus to the study.
We made the decision to include it as an additional product and not replace the
original product as we tried to provide a replication as direct as possible with a
reasonable extension.
Based on our experimental design we cannot rule out that contrast effects affect
participants’ evaluation of the second product. We have made this point clearer in the
manuscript and highlighted the need for further investigation of this proposed
confounder in the discussion.
We have kept the price of the products constant to exclude possible anchoring effects
of the price (Tversky & Kahneman, 1974). The actual prices were very similar to the
prices used in the experiment.
2. Data Source:
The authors used a different sample population from a different culture to conduct the
eplication study. While it is good to replicate the research across different cultures, it raises
questions on how close the replication study is to the original study. Wadhwa & Zhang (2015)
ecruited English-speaking participants in the United States via the Amazon Mechanical Turk
participant pool. In contrast, the authors recruited a convenience sample of German-
speaking participants using several different online-channels, such as social media and
newsletters of online magazines over a two-month period. Participants did not receive a
participation fee but were instead entered into a lottery for an Amazon gift card worth of 20
euros. Given the cultural difference, the price cues were also different in these two studies,
one in US dollars (e.g., $80.00) and the other in Euros (e.g., presented as EUR 80,00 in this
study).
Unlike Amazon Mechanical Turk that has been widely used and proven to be a credible online
data source for academic research (e,g., Buhrmester, Kwang, & Gosling, 2011; Paolacci &
Chandler, 2014), there is no validation for whether the participants recruited from various
different sources (e.g., social media, newsletters of online magazines) in this study actually
paid attention to the stimuli. Given the authors are manipulating only the price information,
which was a non-prominent visual cue on the screen, it is unclear if the participants really
paid attention to the manipulation (i.e., price information).
The authors claim that the data quality is an issue on Amazon MTurk, but did not provide any
ustification for why the data quality is better for the data source they used for this
eplication study. Data for this study was collected via different sources and over a period of
wo months. Related to this issue, authors’ claims such as “our sample seems to be more
homogenous than the original sample” is not supported by facts. The demographics of the
study (e.g., age range 18-86) does not appear more homogenous than MTurk samples.
Response:
The reviewer notes important differences in the sampling method and the
characteristics of the sample. Since the original data is not openly available and only
limited information on the sample descriptives are reported in the original paper, it is
impossible for us to gauge the precise differences between our samples.
Our sample is not representative of the general population, but more heterogeneous
as a study solely conducted with undergraduate students. More advanced sampling
strategies to allow for better generalizations to the population are to be desired in
psychological research in general but are very uncommon as of today.
The research literature on how different incentive strategies affect psychological
effects is sparse. There is literature on how incentives affect data quality and drop-out
rates, but the effects on self-selection and the hypothesis under investigation have not
been investigated, so a conclusion would be speculation and requires investigation in
further research.
We disagree with the reviewer on the unambiguous credibility of MTurk samples.
While the referenced studies (which are also included in our manuscript) show that
MTurk samples are of higher representativeness than samples of psychology students,
there is evidence that MTurk participants are not working as diligently as researchers
hope. This has been shown in the referenced investigation by Rouse (2015) and was
also raised by reviewer #2 who gave further references on this issue (which have been
included in the manuscript).
Since we did not conduct the study in a controlled and observed laboratory
environment, we cannot be absolutely certain if participants have paid attention to the
stimuli. In our view, this is also case in the original study. In fact, we have included the
same checks for data quality as the original authors (e.g. excluding participants who
did not correctly answer the priming questions). We further excluded participants who
guessed the purpose of the study and based on reaction times. Both is common
practice to validate the quality of online research data.
Regarding the hetero- or homogeneity of our sample, we recognize the limitation of
our study that MTurk samples are more heterogeneous than our sample is – with the
caveat that sample descriptives and open data are lacking in the original study for
further investigation on this.
3. Interpretation of the Results:
The authors seem to misinterpret several results, which reflect a lack of an objective
assessment of the evidence.
1) The p-curve analysis the authors did on the original paper found that the test of the right-
skewness of the p-curve was not significant. According to Simonsohn et al. (2014), a null
inding (a p-curve that is not significantly right-skewed) may indicate that the set of studies
ack evidential value or that there is not enough information to make inferences about
evidential value. However, the authors directly concluded that there is bias in the reported
data due to either selective reporting or p-hacking, even though the test for whether the set
of studies lack evidential value was also not significant (i.e., null of 33% power was not
ejected) in the analysis.
Response:
Thank you for this important correction!
We have corrected the description of the p-curve results to reflect the interpretation
recommended by Simonsohn et al. (2014, 2015) and also noted by reviewer #3.
2) The authors did replicate the positive effect of rounded price versus nonrounded price in
he feeling priming condition for both products (weak support for the poorly-rated digital
camera binoculars and strong support for the digital instant camera). This is consistent with
he original results given the original results show that the rounded price only have a positive
effect on evaluations when the product elicits positive feelings (e.g., when the product is
attractive).
Response:
While the descriptives might go in the predicted direction, the statistical tests are
either inconclusive or provide evidence in favor of the null hypothesis of no effect –
with the exception for the contrast in the feeling condition for the instant camera (BF =
1.653).
We have updated the Results and Discussion section and hope to better state and
consider this finding.
3) The authors used the contrast results from Wadhwa & Zhang (2015) for the p-curve
analysis, but instead used the effect size of the interaction terms for comparison analysis
with that paper (e.g., Figure 4). For consistency of the analyses, the authors should present
he effect sizes for the contrast results instead.
Response:
The Rounded Price Effect is stated as an interaction between Context and Price
Roundedness. Thus, in our view, the interaction effect size is of interest for the
comparison of the studies.
The contrasts are relevant to understand whether the interaction is of the kind
predicted by theory, but the effect size for the interaction term is relevant for
comparison between different studies.
Selecting the t-tests for the p-curve analysis is based on the recommendations by
Simonsohn et al. (2014). The recommendation is based on the observation, that
different types of interaction will lead to different biases in the p-curve. This does not
relate to the comparison of effect sizes across studies.
4. Conclusion Based on A Single Replication Attempt:
Based on a single replication attempt of one out of the five studies in the original paper, it is
also difficult to conclude that an effect is considerably smaller or even does not exist,
especially when there are various differences in the experiment stimuli, data source, and
procedure. In a recent report in Science, Daniel Gilbert and his colleagues (Gilbert, King,
Pettigrew, & Wilson, 2016) have shown that there is still a lack of power when replicating a
single study only once and this method can severely underestimate the replication rate.
n summary, replications of recent research can contribute to understanding of the emerging
esearch findings. However, a weak replication attempt can be problematic as it could lead
o erroneous, incomplete conclusions.
Response:
The reviewer notes an important point: The question of how to interpret the outcome
of a single replication study is an important one if the field of psychology wants to
establish replication as a standard procedure to improve the reliability of research (see
Zwaan et al., 2007, for many arguments in favor of replications becoming mainstream).
We agree with the reviewer (and Gilbert et al., 2016, at least in this regard) that a
single replication study is not sufficient for a final verdict about a substantive theory as
we have also stated in the discussion section. This is why we have integrated the
available information in a comparative overview to show, that two independent
replications were not able to show an overall similar pattern of results as the original
study did. It is our point of view that further studies on this effect should be conducted
in a principled and less biased way (compared to research and publishing practices
commonly found in psychological science) by adhering to open science principles (pre-
registration of hypotheses, open sharing of data and material) to which we would like
to contribute with our study.
On theses premises we disagree with the reviewers that the present replication study
is “weak”.
Reviewer #2 (Casper Albers)
The replication crisis has taught us that we shouldn't take results from a single paper at face
value: there are too many reasons why the result could be incorrect or over-estimated.
Repeating the experiments behind important scientific claims is an important part of science.
The benefit (for the reviewer) of a paper describing such a replication, is that I don't have to
hink about whether the results are interesting/important enough to publish. If the theory of
he original study is deemed publishable, then so should all (large scale) replications: they
share exactly the same theory.
t was a true pleasure for me to read this paper. In my experience, this paper is a textbook
example of how a paper on a replication study should be: i) look at an important (recent)
claim in theory; ii) announce the steps you will take to validate this theory (pre-register); iii)
un the study and report results; iv) share all materials. All these steps are taken.
The authors describe what they do in a clear, transparent and thorough fashion. I had not
ead the Wadhwa & Zhang (W&Z) paper before (my background is not in consumer
psychology), but the description of their studies in this paper was largely clear to me. The
sample size of the replication was suffiently large (much larger than the original study!). The
addition of a Bayesian analysis on top of the frequentist one is a nice bonus.
Response:
Thank you for this positive evaluation of our work!
Some minor comments:
Response:
- p. 5: You describe studies 1-3 by W&Z, but The introduction has been made clearer to
it is unclear exactly what the difference explain the conditions of the experiment.
between a rounded and unrounded number
is. I assume 9.99 and 10.00 would be
examples, but would e.g. 4.49 and 4.50 also
qualify? Some more explicit information
would be useful.
- Footnote 1 is 'used' in p.5, line 14, but the We have corrected this in the word version
footnote itself is placed on p.6 of the manuscript.
- p. 5/6: It's interesting to p-curve the We have updated the interpretation of the
original results, but it might also be relevant p-curve results according to the suggestions
to note that the technique of p-curving itself by reviewer #1 and have further included a
is not without theoretical criticism (cf. reference to the talk by Dr. Morey.
http://richarddmorey.org/content/ Unfortunately, we could not find a pre-print
Psynom17/pcurve/#/ , I believe dr. Morey on his criticism either and he did not
also has a pre-print on this topic, but respond timely to our enquiry.
couldn't find it).
- p.5, l.52: no need to mention first names Has been fixed where references in the text
(unless another Schimmack also derived were made.
some R-index)
- p.7, l. 7: APA-style is to report 3 digits, so All numbers are now reported to the
.352 rather than .3523. precision of three decimals.
- p.7, bottom half: you (correctly) mention Thank you for this valuable suggestion! We
some favourable qualities of MTurk have included the recommended
participants over 'regular' lab-participants, references for different perspectives on this
but MTurk has its drawbacks as well and for question.
the balanced view, it might be good to
(shortly) mention this. See the recent
column by Van Bavel & Rand for some
background and references
(http://www.psychologicalscience.org/obse
rver/ restocking-our-subject-pools).
- Footnote 3/8: no need to mention here We have updated the references to the OSF
that materials are available at OSF: you repository throughout the text.
mention at the end of the paper that
'everything' can be found there.
- p.10, l.52: N=588 --> N = 588 (add spaces References to the statistical summaries
to the equality sign) have been corrected and displays of
- p.11, l.5: "Md" is not a well-known formula have been unified.
abbreviation of median; please write in full.
- p.18, l.14: please add a reference for The reference has been added.
O'Donnell & Nelson.
- References: You refer to both R-Index.org These are two different references indeed:
(2014) and Schimmack (2014); are these The R-Index.org (2014) refers to the
indeed two distinct references? website where the Excel spreadsheet is
provided (without an author clearly stated)
while Schimmack (2014) refers to the
unpublished manuscript which explains the
method.
- Fig 2: the two lines are not distinguishable All figures in the manuscript have been
when the pdf is printed (in black and white). updated to be greyscale.
Please adjust the colours, or e.g. make one
line dashed.
Signed,
Casper Albers
University of Groningen
Reviewer #3 (Michelle Nuijten)
This study is a close replication attempt of the “rounded price effect” mediated by “feeling
ight” as studied by Wadhwa & Zhang (2015). In the article, the authors both statistically
analyze the reliability of the original findings, and try to replicate it in a new sample.
Psychology can definitely use more close replications of published findings, so I applaud the
authors for performing one. In analyzing and interpreting the differences between the
original and replication findings, I think it was a good choice to give several “measures” of
eplicability, rather than relying on one.
Some major strengths of this manuscript are that the authors preregistered their study, that
deviations from the preregistration are explicitly described, and that they shared all their
data and materials online. Furthermore, the authors are clear about the limitations of their
study (mainly caused by unforeseen differences between the original study and their
eplication). This overall high level of transparency greatly increases the scientific quality and
eliability of this paper.
Overall, I think the manuscript was well-written, although some paragraphs need some
urther clarification or need some rewriting to add more nuance to some of the conclusions.
Response:
Thank you for this positive evaluation of our work!
My main point of critique concerns the section in which the authors evaluate the “evidential
value” in the original studies. In general, I’m interested in methods such as p-curve and R-
ndex, and I can see how they can show interesting things about a set of results. However, I
hink it’s important to take the limitations of these methods into account, and not take their
esults at face value. I would therefore urge the authors to tone down their interpretation of
hese analyses. At the moment the authors are strongly implying that the original authors p-
hacked their results, but there is no way of knowing that for sure (unless the original authors
say they did). A p-curve only gives you information about the likeliness of a set of significant
p-values, given that there is no effect in the population. If it turns out that a set of results is
unlikely, you can’t simply equate this with evidence for p-hacking or selective reporting. The
same holds for the R-Index. I think the manuscript would benefit from (1) an explanation of
what these two methods do (especially for the R-Index, this is not clear at all, but for the p-
curve I also miss details such as: it only focuses on significant p-values, how many p-values
are included here, …), (2) what their limitations are (there is plenty of debate in the literature
about these methods, please refer to that), and (3) a more careful and nuanced
nterpretation of the results. Right now, replications have a questionable reputation, and
“replicators” are frequently accused of being “shameless little bullies”. I think this is a very
sad and dangerous trend, and I would urge the authors not to give anyone a reason to call
hem bullies. Adding more nuance to the interpretation of results, and sticking to facts
rather than speculation about p-hacking) is an important part of that.
Response:
In line with the suggestions by reviewers #1 and #3 we have updated the section about
p-curve and R-Index to better convey the interpretation also suggested by Simonsohn
et al. (2014).
We have also included additional paragraphs underlying the rationale of the p-curve
and the R-Index, but refer to the original papers for a more technical elaboration.
We also toned down the introduction and interpretation of the p-curve and R-Index.
We explain that using these indicators could simply be regarded as a routine when
evaluating a study before conducting a replication study.
My second main point of critique is that the description of the replication study is not always
clear. Specifically, it is not entirely clear to me how the variables of interest were measured.
For instance, the dependent variable has to do with how participants in different conditions
ated (?) products, but how was this done exactly? Did they simply have to indicate if they
would buy it (yes/no), or give an evaluation of the product on a categorical/continuous
scale? In Figure 2 and Table 1, the dependent variable is expressed as the “average purchase
ikelihood” and “Likelihood of Purchase”, but what does this mean exactly? Based on the
stimulus materials, it seems that a combination of questions is used, but it is unclear to me
how this information is summarized (or whether 1 of these questions is used as dependent
variable). In short, the variables that were used in the final, main analyses need to be
explained in such a way that the reader can understand where the numbers come from.
Response:
We have made the language of the Methods section more consistent and added
paragraphs to better explain the scales and materials used.
f these two main issues are solved, I think this paper can be a great example of “Psychology
2.0”. I added some minor comments below.
Signed,
Michèle Nuijten
Minor remarks
• It might be an interesting additional analysis to calculate the Bayes factors for the
original study, using the reported frequentist test results. This way you can directly compare
he results with the replication study.
Response:
Unfortunately, it is – to our knowledge – not possible to calculate Bayes factors based
on the reported information alone, since the cell sizes for the study are not reported in
the text and cannot be easily estimated from the figures or test-statistics. The formulas
presented in Rouder, Speckman, Sun, & Morey (2009), Rouder, Morey, Speckman, &
Province (2012) and Johnson (2005) cannot be calculated without this information.
Further, we believe that it is not sensible for comparison: Since the effect sizes in the
published literature are generally inflated and one has to account for this bias when
planning a study (as we did and report in our paper), the Bayes factor based on the
reported statistic will also overstate the relative evidence in favor of the effect (when
default, noninformative priors are used which do not account for publication bias). For
the interaction term (rounded price effect) it is possible to get an approximation of a
Bayes factor based on just the test-statistic (Falkenberry, 2017) which yields a value of
35.674 in favor of the alternative (i.e. of an effect). In comparison, the JZS Bayes factor
for the same test in our replication study is about 384.615 in favor of the null. Despite
our replication yielding 10-times the evidence in the opposite direction, this
comparison is lacking a lot of relevant context information. We believe that the
Replication Bayes factor is more sensible for comparing the original with the
replication study in Bayes factor framework.
On a similar basis one could critique the use of effect sizes for comparison (those are
also allegedly inflated due to publication bias). For this reason, we have included the
forest plot (as suggested by reviewer #3) and Simonsohn’s Small Telescope approach,
which is – in our point view – a more sensible comparison.
Response:
• In the pilot study, the authors tested the The difference has been made
attractiveness of the binoculars among German clearer in the section on sample.
students, but in the actual study, the sample was
much more heterogeneous.
• If it’s the policy of the Royal Society to place
Figures & Tables at the end of the manuscript, I
would strongly encourage them to let go of that
requirement; this makes reviewing much harder
• P. 8, line 50; maybe specify that the study We have clarified that our study was
was performed in Germany to be conducted in Germany.
• P. 11, line 21; “Baby” probably does not This typo has been corrected.
have to be capitalized.
• P. 12, line 22; I think adding some Thank you for the valuable
references here will strengthen your point. Some suggestion. We have added further
suggestions: references to the recent literature
https://link.springer.com/article/10.3758/BF03194 on the use of Bayes factors in
105, addition to p-values.
https://www.ncbi.nlm.nih.gov/pubmed/21280965
• I would like to see an extra paragraph that The section on our statistical
shortly explains what a Bayes factor is and how you analyses now briefly explains Bayes
should interpret one(optionally by giving the factors and gives references to
“categories” of evidential value by Jeffreys, see accessible introductions to their use
e.g., Table 1 in https://link.springer.com/article/ and interpretation.
10.3758%2Fs13428-014-0470-2)
• P. 13, line 39; “BF10 = 0.0026 ± 1.6%”, The new subsection on Bayes factors
please specify what the 1.6% means. Is this a now includes the explanation of the
credible interval? proportional error for Bayes factors.
• P. 13, line 48; “Unsurprisingly, …”, well, it is The language of this section has
probably surprising for the original authors. I would been improved.
steer clear of language like this.
• Please be consistent in the number of This point has also been mentioned
decimals you report in statistical results (I think 3 by reviewer #2 and all numbers are
decimals is sufficient) now reported to 3 decimals.
• P. 15, line 15; actually, the BF indicates even This is indeed an important
more than that there is “no evidence for distinction and we have updated the
mediation”, that is, it indicates that there is interpretation in the results section
“evidence that there is no mediation”. This might to reflect this.
seem just semantics, but it is a crucial difference. In
the first case, the data give no evidence for
mediation, which might mean that 1) there is no
mediation, or 2) the data are ambiguous. In the
latter case, you can clearly see that the data are
not ambiguous at all, and contain evidence that
there is no mediation.
• P. 16, line 26; I think you can just say Changed “Analysis of Variance” to
ANOVA rather than Analysis of Variance ANOVA throughout the manuscript.
• P. 17, line 28: “except for a single contrast In order to also address the concern
that was significant”, please specify which contrast by reviewer #1 about this single
that was significant test result, we have
updated the paragraph.
• P. 19: “Researchers who would like to find In line with the reviewer’s
further evidence in favor of the “Rounded Price comments above we have updated
Effect” should expect an effect size considerably the interpretation of the outcome of
smaller and thus plan for a sample of at least 800 our replication. In particular, we
participants.” Considerably smaller than what? have also included an explanation
Also, this is quite strongly stated, especially after why we believe that the replication
listing all differences between the replication and studies warrant the need for larger
the original study. The authors should either samples if the study is further
explain (shortly) why others should expect a much investigated by other researchers.
smaller effect, or change the language of this Namely, the replications are not
sentence to reflect more uncertainty about the true effected by publication bias.
effect. The same holds for the sentence “Based on
our findings […], we conclude that [the effect] is – if
existent at all – considerably smaller than originally
reported.” This should be toned down a bit (“we
suspect that the effect is considerably smaller”, or
“the effect is likely to be smaller”)
• Figure 2; The figures have been changed to
o please use different line types rather than greyscale and different lines are now
different colors. Especially the difference between distinguished by their line type.
red and green is hard to see for people who are We have kept the scaling of the y-
color blind. axes to represent the full scale (1 to
o The figure would also improve if the y-axes 9), thus providing readers with a
are cut off at 5. visual indication on the tendency of
o Finally, I would consider changing the error the results along the scale.
bars from representing the SE, to representing a
95% confidence interval. This way, it is immediately Further we prefer plotting Standard
visible which effects significantly different from Errors as the confidence intervals do
each other. not represent the result of a
significance test directly (CI’s can
overlap, and the means still be
significantly different).
• Figure 3; The labelling of the figure has been
o Something seems to have gone wrong with corrected.
labelling the paths. Path “beta” in the first panel is The moderation is effectively
labelled “alpha”. represented by the variable used for
o This figure also does not reflect that this is a “Fit Rounded/Price”, which is in line
moderated mediation. Where is the moderation in with the method used by Wadhwa &
this figure? Zhang (2015). The caption of the
figure has been updated to better
convey this.
• Figure 4; We have updated Figure 4 to
o To increase readability of the figure, I would comprise both Figure 4a and 4b into
add a vertical line with the meta-analytic effect one graph. The new figure is also
size. I also think it could be interesting to order the presented as a forest plot, so points
studies based on effect size (then you will see in in the graph highlight the different
one glance that effect size seems to depend on the sample sizes.
research team)
o Panel b: I don’t understand why this figure
is rotated compared to the panel above. I would
keep this consistent, and have the effect size on the
x-axis. As a matter of fact, I would make forest
plots of these findings. The advantage of a forest
plot, is that the size of the dot reflects sample
size/precision.
Reviewer #4
This paper attempts to replicate one Study (Study 5) from Wadhwa and Zhang (2015) paper.
Replications, especially conceptual replications, held advance science. However, I believe that
one should be careful of being overly enthusiastic of all replication efforts. The current paper
ries to draw conclusions based on a single replication effort, which is neither an exact
eplication nor is it a conceptual replication attempt. The positive is that authors recognize
his limitation. However, as discussed below, there are many issues that stand out with this
eplication, which one cannot ignore.
Differences between Studies: When a replication effort is made, it is important to understand
how similar the two studies are. Comparing the replication and the original study elucidates
several key differences between the two studies. The stimuli used, sample sources, the
priming procedure and the mediator measures employed are all different.
Response:
Several of the important points raised by reviewer #4 have also been made by
reviewer #1. Where this is the case we will kindly refer to the comments already made
in response to reviewer #1.
n terms of the stimuli, authors used two products in the replication study. The first product,
as in the Wadhwa and Zhang (2015) study, was a digital camera binoculars. However, what
should be noted is that the pretest clearly indicates that the baseline attitude toward
binoculars for the German population was very different from the American population.
Specifically, the baseline attitude for the binoculars was low for German participants. We
cannot ignore this difference as the baseline attitudes for the products can significantly
mpact results, especially because the study is about product evaluations.
Moreover, authors used a second product, different from the one used in original paper,
which was rated more attractive by the German participants, compared with the binoculars.
Response:
As in response to reviewer #1 and detailed in our paper this was in order to be
consistent with the original study. The low baseline attitude toward the original
product is precisely the reason we did include a second product, which was rated more
favorably.
This indeed is in difference to the original study, but the differences are clearly stated
in the text and results are consistently across both products.
However, authors chose to use the same price as the price they used for the digital binocular
camera and always presented the unpopular binoculars first. Doing so could have led to
multiple problems 1) presenting binoculars first would lead to a potential carry over effect to
he other product (instant camera) and thus could have contaminated the results and 2) it
would also lead to a contrast effect with people comparing the binoculars with the camera.
Both of these problems can severely impact the results of the second product.
Response:
Regarding carry over and contrast effects kindly see our response to a similar
comment by reviewer #1.
One should also note that willingness to pay indicated for both products in the pretest was
much lower than the price of the products used in the main study. It seems like the author(s)
did not change the price to match the pretest.
Response:
We did not change the prices in order to have realistic prices for the products and
match the original study as closely as possible. Further, the prices used in the study are
close to the actual prices for the products.
Finally, in the original study prices, which is the most important variable for this study, were
presented as dollar amounts ($80.00). It seems like in the current study prices were
presented as Euros (81,43 €). Thus, the price format also is different.
Response:
This is indeed a difference due to an adaption to our sample. However, we believe that
this difference is minor compared to other differences between the original and the
replication study. Please note that the exchange course was less than 1.2 USD for 1
EUR during the investigation.
Differences in Samples: Participants in the two studies are recruited from very different
samples. Participants in Wadhwa and Zhang (2015) studies were from a group of
professional survey respondents. Specifically, Wadhwa & Zhang (2015) recruited US based
English speaking participants from Amazon mechanical turk, which is a popularly used online
data tool. Participants in the current replication study were recruited from diverse samples.
Moreover, given the authors used different online channels to recruit their participants, their
sample is also likely to be significantly more heterogeneous, compared with the original
Wadhwa and Zhang study. Finally, while all the participants in the original Wadhwa and
Zhang study were compensated, participants in the current study were entered into a lottery
or a gift card. As I discuss below, these differences can significantly impact the conditions
needed for the replication.
Response:
Please see our response to reviewer #1, especially regarding the discussion of MTurk
participants.
Contingency of the Original Effect on Different Factors: The effect documented in the
Wadhwa and Zhang (2015) paper is an extension of the results of many other papers by
different authors. Specifically, different authors using different datasets, including real world
datasets, have shown that precise numbers are preferable in negotiation related situations
e.g., real estate priced at precise numbers sells at higher prices). Wadhwa and Zhang (2015)
suggested a moderator, degree to which consumers rely on feeling, for the price effect shown
n past research. The effect suggests that precise numbers increase preferences when people
have a cognitive mindset, while round numbers increase preferences when people have an
affective mindset.
The aforementioned discussion makes it clear that Wadhwa and Zhang’s (2015) effect seems
o be contingent on many different factors-1) participants should be in an affective
mindset/cognitive mindset, 2) prices should be perceived as rounded (round condition) or
precise (precise condition) and 3) price cues should be very salient. However, as noted above,
given so many differences between samples, stimuli and priming procedure used in the two
studies, it is not clear if these conditions, which are necessary for the effect to be
demonstrated, were met.
t is likely that given the data source was online pools, such as social networks, participants
were already in an affective mindset. As indicated above, for the original results to be
demonstrated, a clean manipulation of affective-cognitive mindset seems to be important.
However, for all the reasons noted above, it is difficult to say how clean the manipulation of
affective versus cognitive mindset was in the replication study.
t is also likely that given the different price formats, prices considered precise by US
population might not be considered so by the German population. It could also be that digital
nstant cameras are perceived as hedonic/affective by the German population, which would
urther impact the results. In the original study presented in Wadhwa and Zhang (2015)
paper, authors pilot test shows that the product was considered neutral (that is not perceived
as primarily hedonic or cognitive). Did the authors calibrate the stimuli? Finally, it is not clear
how salient the price cues were.
Response:
The reviewer raises relevant questions about the theoretical interpretation of
replication studies. It is important for psychology and its sub-disciplines as a science to
have a network of theories and explanations for effects (a line of research or research
program in a Lakatosian sense), so we do not need to refer to ad hoc explanations of
single instances of an effect.
We agree that a single replication study cannot falsify a whole substantive theory or
research program – and we do not aim to do this with our replication study.
The original paper is one part in the research program on the link of number
perception and consumer behavior and reports an effect and proposes an explanation
for the effect. It does so in a general way, see p. 1182 of the original paper: “To
elaborate, we show that when the purchase decision is driven by feelings, rounded
prices intensify evaluative judgments related to the target product, whereas when the
purchase decision is driven by cognition, nonrounded prices intensify evaluative
judgments related to the target product. We propose that this rounded price effect is
driven by the sense of feeling right induced as a result of the fit between two factors:
roundedness of the price and nature of the decision context.”
The contingencies mentioned in the reviewer’s comment are possible explanations
why we failed to replicate the effect and the proposed mediation, and we are happy to
include them into our discussion. They have, however, to be considered as a post hoc
explanation for the results and are thus up for further investigation in (pre-registered)
follow-up studies.
Support for the original effect: Despite several differences between the two studies noted
above, authors do find some support for the original effect. In the feeling conditions,
participants seem to prefer the product with the rounded price. Simple contrast for the
nstant camera in the feeling condition seems to be significant. This is consistent with the
original results.
Response:
The significant effect and its relevance in context with the other statistical results are
considered in the discussion and our response to reviewer #1 above.
P-Curve Analysis: The authors conclude that the data reported in the original paper has a
bias, even though the test for the null hypothesis of a small effect was not rejected in the
analysis. This is a misleading conclusion.
Response:
Thank you for this important suggestion! The language of this section has been
corrected accordingly.
Conclusion Based on A Single Replication Attempt for One Study: The authors are drawing
conclusions based on a single replication effect of one study from the paper. As recent
eports suggest, conclusions drawn on such kind of limited replication attempts can be
misleading as such replication effort suffer from low power, which is a really important issue
Gilbert et al. 2016). Additionally, differences in the samples could have introduced multiple
andom errors, which again is a significant concern when attempting to replicate a study.
Response:
Our conclusions are based on our study, the evaluation of the original study and the
replication performed by O’Donnell & Nelson. As further studies are not available to
our knowledge, a meta-analytic theory appraisal is out of scope of the present article
but a fruitful perspective for the research program.
This is, of course, only a very limited basis for general claims but offers an additional
perspective on the original effect. Since our study was pre-registered and provides all
material and data openly, however, we hope to provide the research community with
a basis for further investigation.
We recommend the growing literature on the role and need of direct and conceptual
replications in empirical sciences (Zwaan et al., 2017; Thompson, 1994; Brandt et al.,
2014; Heino et al., 2017).
n sum, one needs to ask whether this study is indeed a faithful replication of the original
study and if any meaningful conclusions could be drawn from this study. Given, the
differences, related to stimuli, priming procedure and samples between the two studies, it
seems like we cannot drawn meaningful conclusions from the study.
Response:
Even when a single replication attempt does not allow to decide whether there is a
relevant effect in the population, we believe that even replication attempts in different
samples/populations and with stimulus material that is adapted to the population at
hand provide a basis for further meta-analytic investigation of the effect.
References
Faulkenberry, T. J. (2017). Approximating Bayes factors from minimal ANOVA summaries:
An extension of the BIC method, 1–6. Retrieved from http://arxiv.org/abs/1710.02351
Heino, M. T. J., Fried, E. I., & LeBel, E. P. (2017). Commentary: Reproducibility in
Psychological Science: When Do Psychological Phenomena Exist? Frontiers in
Psychology, 8, 1004. http://doi.org/10.3389/fpsyg.2017.01004
Brandt, M. J., IJzerman, H., Dijksterhuis, A., Farach, F. J., Geller, J., Giner-Sorolla, R., …
van ’t Veer, A. (2014). The Replication Recipe: What makes for a convincing
replication? Journal of Experimental Social Psychology, 50(1), 217–224.
http://doi.org/10.1016/j.jesp.2013.10.005
Zwaan, R. A., Etz, A., Lucas, R. E., & Donnellan, M. B. (2017). Making Replication
Mainstream. Behavioral and Brain Sciences, 1–50.
http://doi.org/10.1017/S0140525X17001972
Thompson, B. (1994). The Pivotal Role of Replication in Psychological Research:
Empirically Evaluating the Replicability of Sample Results. Journal of Personality,
62(2), 157–176. http://doi.org/10.1111/j.1467-6494.1994.tb00289.x
Society Open
