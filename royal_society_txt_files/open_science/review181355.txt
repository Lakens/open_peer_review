The unreliability of egocentric bias across self–other
and memory–belief distinctions in the Sandbox Task
Steven Samuel, Edward W. Legg, Robert Lurz and Nicola S. Clayton
Article citation details
R. Soc. open sci. 5: 181355.
http://dx.doi.org/10.1098/rsos.181355
Review timeline
Original submission: 22 January 2018 Note: Reports are unedited and appear as
1st revised submission: 16 August 2018 submitted by the referee. The review history
2nd revised submission: 28 September 2018 appears in chronological order.
Final acceptance: 2 October 2018
Review History
label_version_1
RSOS-180102.R0 (Original submission)
label_author_1
Review form: Reviewer 1
Is the manuscript scientifically sound in its present form?
Yes
Are the interpretations and conclusions justified by the results?
Yes
Is the language acceptable?
Yes
Is it clear how to access all supporting data?
Yes
Do you have any ethical concerns with this paper?
No
Reports © 2018 The Reviewers; Decision Letters © 2018 The Reviewers and Editors;
Responses © 2018 The Reviewers, Editors and Authors. Published by the Royal Society under the
terms of the Creative Commons Attribution License http://creativecommons.org/licenses/by/4.0/,
which permits unrestricted use, provided the original author and source are credited
2
Have you any concerns about statistical analyses in this paper?
No
Recommendation?
label_recommendation_1
Accept with minor revision (please list in comments)
Comments to the Author(s)
label_comment_1
In this manuscript, Samuel et al aimed to uncover the cognitive processes leading to egocentric
biases in a change-of-location task. A recent previous study by the same authors showed that this
task was sensitive to a belief versus memory reasoning manipulation (participants showed
egocentric bias on the belief but not the memory trials) and also, crucially, that participants’
egocentric bias resulted from domain general representational processes: reasoning from multiple
types of representations (videos as well as mental states) was affected by egocentric bias.
The current manuscript tried to unpick the precise source of the egocentric bias found in the
previous study. The authors therefore tested three hypotheses in Experiment 1:
1. The egocentric bias results from reasoning about the other versus the self, tested by comparing
self memory versus other memory within participants;
2. The egocentric bias results from reasoning about beliefs versus memories, tested by comparing
other belief versus other memory between participants;
3. The egocentric bias results from reasoning about where the other will look versus where the
other believes the object is, tested by comparing other action versus other belief between
participants.
Around 50 participants per condition were tested in an online study with 4 trials per condition
per participant, in a blocked design.
Results: Participants were biased towards the true location of object (i.e. showed egocentric bias)
in all conditions, including in the self memory trials. This latter condition is in contrast to the
effect found in the authors’ previous study.
In terms of the hypotheses above, the authors found no effect on the magnitude of the egocentric
bias of reasoning about the other versus the self; no effect of reasoning about other belief vs other
memory when controlling for self memory bias; and no effect of reasoning about other belief vs
other action when controlling for self memory bias.
A second experiment ruled out that a change in the wording in the self memory trials could
explain the emergence of an egocentric bias on these trials.
The manuscript is well written and presents an extensive discussion of the discrepancy between
these data and previous studies. I therefore think that it is a useful addition to the literature on
this task, despite the contrast between these results and previous results, if the following points
are addressed:
1. One clear difference between the present manuscript and study 1 of the previous paper is that
in the previous paper the authors presented the different conditions in a mixed trial order,
whereas in the present paper they blocked the order of conditions. This difference should be
discussed and another experiment should be run to check that blocking vs mixing trial types does
not generate the egocentric bias on self memory trials.
2. The authors could compare the magnitude of the self memory bias in this study to that in their
previous study to support the claim that there is a significant difference between their findings
across these two papers.
3
label_author_2
Review form: Reviewer 2 (Daniel Bernstein)
Is the manuscript scientifically sound in its present form?
No
Are the interpretations and conclusions justified by the results?
No
Is the language acceptable?
Yes
Is it clear how to access all supporting data?
Not Applicable
Do you have any ethical concerns with this paper?
No
Have you any concerns about statistical analyses in this paper?
Yes
Recommendation?
label_recommendation_2
Reject
Comments to the Author(s)
label_comment_2
See attached review (Appendix A).
label_author_3
Review form: Reviewer 3
Is the manuscript scientifically sound in its present form?
Yes
Are the interpretations and conclusions justified by the results?
Yes
Is the language acceptable?
Yes
Is it clear how to access all supporting data?
Yes
Do you have any ethical concerns with this paper?
No
Have you any concerns about statistical analyses in this paper?
Yes
Recommendation?
label_recommendation_3
Major revision is needed (please make suggestions in comments)
4
Comments to the Author(s)
label_comment_3
This paper reports two experiments which use the sandbox (false belief) task to investigate theory
of mind in adults. The first experiment cleverly examined whether adults’ performance on the
task differs depending on whether they are asked about their own memories or about a story
character’s memory, belief, or future action. Somewhat surprisingly, performance did not vary
across these 4 test questions! Because the null difference between the “own memory” question
and another condition did not replicate the authors’ own earlier findings, they conducted a
second experiment to test whether wording changes to this question were responsible, but found
no evidence for this.
This work is important. It reveals major concerns about use of the sandbox task, and other
researchers need to be aware of these concerns. However, I think the paper may need to do more
to highlight differences between the present version of the sandbox and previous versions. Also, I
think the paper is much longer than it needs to be, especially given the results and take-home
message. I suspect it could be reduced to “brief report” length (i.e., 4000 words). I elaborate on
these points below.
-One concern with the current computerized version of the sandbox task is that participants did
many trials of the same sort, and this could have led them to adopt the strategy of trying to
remember each objects’ original location (as this was the correct location across most trials). It is
important to note that this is somewhat different than a related concern ruled out by the authors,
namely the concern that participants blindly responded the same way regardless of which
question was asked. My concern is that because the same questions were repeated, participants
paid special attention to the initial location in each trial, because they knew they’d probably be
asked to indicate the first location. One way to see if this concern affected the results might be to
only examine the first 2 trials completed by each participant, as in the first two trials they could
not have known that they would be repeatedly asked about the original location. I think that
providing this data could strengthen the paper.
My concern would have alleviated in a version of the sandbox task in which participants
more often had to indicate the current location of the object. If the study included such trials (i.e.,
more than just once), participants could not prepare by trying to pay greater attention to one
location than the other. I believe Bernstein et al. (2011) did include more trials of this sort. They
also had fewer trials overall, perhaps reducing the likelihood of strategic responding. I think the
authors need to be explicit about these differences between their procedure and Bernstein et al.’s
(and any other studies that have used the sandbox task). To really see the value of the present
paper, we need to know whether it reveals problems with the sandbox task in general, or
problems that only pertain to the authors’ version of the task.
INTRODUCTION.
-On p. 3, please consider softening “we fail to comprehend” to something like “we have difficulty
recognizing”. As great as our difficulties may be, we often do comprehend that others don’t share
our visual perspective.
-I’m not sure the first paragraph is needed. I suspect it could be shortened to one or two sentences
and merged with the next paragraph. Ultimately, this paper is about the suitability of the
sandbox task as a measure of theory of mind in (non-senior) adults and so I think the paper
should just get to this as quickly as possible. For this reason, it might also be possible to greatly
condense (or even remove) discussion of the theoretical issues the authors originally intended to
address (e.g., whether difficulty with theory of mind tasks is specific to reasoning about mental
states) and the corresponding literature (e.g., review of the false/outdated photos task). All of
this content is interesting. But ultimately the paper wasn’t able to address the issues it set out to
5
investigate, and they aren’t revisited in the General Discussion. So why take the reader through a
long review of these issues?
METHODS SECTION
The Methods Section of Experiment 1 repeatedly mentions ways in which the methods relate to
those from a previous paper by the authors (e.g., “The procedure for all three conditions was
identical to experiment 2 in our earlier study”; “Following our previous study [7, experiment 2],
we organized…”). Many readers won’t be familiar with the authors’ previous study, or at least
not familiar enough to remember all the details. So these comments will be lost on these readers.
Moreover, I think they will distract readers from their main goal of attempting to understand the
procedure that was actually used in the present studies. As noted above, I think it’s important to
know how the current methods relate to those from previous studies. But perhaps this should be
summarized outside of the Methods. (And again, comparisons of procedures should include
other versions of the sandbox task, including those from Bernstein et al., 2011).
RESULTS
-The results section of Experiment 1 is very long. One reason is that initially the data from each
question type are analyzed separately. However, I don’t think there is a justification for
separately analyzing these results, unless it were first shown that findings actually differed by
condition (they did not). So I suggest the authors begin the results begin with the “Comparing
conditions” section, and this would eliminate the need for most of the preceding analyses.
-The Results section of Experiment 1 begins with tests for order effects and ends with some
Bayesian analyses. Unless I’m overlooking something, neither type of analysis is found in
Experiment 2. Why not?
-I understand the data are non-normal and so parametric statistics cannot be used. However, this
makes the Results quite cumbersome, and also raises questions about whether some of the
analyses are justified. For example, in Experiment 2, I’m not sure that the authors are justified in
separately analyzing the short- and long-distance trials for each question wording. If an omnibus
test were run, it could inform us of whether there was an interaction between distance and
question type, which would then justify the separate analyses for long and short-distance. Would
it be possible to conduct omnibus analyses using GEEs or GLMMs?
-In the Discussion, the authors make some speculations about data from Bernstein et al (2011).
This is fine, though the paper would be even stronger if the authors requested the data from
Bernstein (or one of his co-authors) and then tested their conjecture about the data.
label_end_comment
Decision letter (RSOS-180102.R0)
05-Jun-2018
Dear Dr Samuel:
Manuscript ID RSOS-180102 entitled "Castles made of sand? The vanishing of egocentric bias
across self-other and memory-belief distinctions in the Sandbox Task." which you submitted to
Royal Society Open Science, has been reviewed. The comments from reviewers are included at
the bottom of this letter.
6
In view of the criticisms of the reviewers, the manuscript has been rejected in its current form.
However, a new manuscript may be submitted which takes into consideration these comments.
Please note that resubmitting your manuscript does not guarantee eventual acceptance, and that
your resubmission will be subject to peer review before a decision is made.
You will be unable to make your revisions on the originally submitted version of your
manuscript. Instead, revise your manuscript and upload the files via your author centre.
Once you have revised your manuscript, go to https://mc.manuscriptcentral.com/rsos and login
to your Author Center. Click on "Manuscripts with Decisions," and then click on "Create a
Resubmission" located next to the manuscript number. Then, follow the steps for resubmitting
your manuscript.
Your resubmitted manuscript should be submitted by 03-Dec-2018. If you are unable to submit
by this date please contact the Editorial Office.
Please note that Royal Society Open Science will introduce article processing charges for all new
submissions received from 1 January 2018. Charges will also apply to papers transferred to Royal
Society Open Science from other Royal Society Publishing journals, as well as papers submitted
as part of our collaboration with the Royal Society of Chemistry
(http://rsos.royalsocietypublishing.org/chemistry). If your manuscript is submitted and
accepted for publication after 1 Jan 2018, you will be asked to pay the article processing charge,
unless you request a waiver and this is approved by Royal Society Publishing. You can find out
more about the charges at http://rsos.royalsocietypublishing.org/page/charges. Should you
have any queries, please contact openscience@royalsociety.org.
We look forward to receiving your resubmission.
Kind regards,
Thadcha Retneswaran
Royal Society Open Science
openscience@royalsociety.org
on behalf of Dr Shirley-Ann Rüschemeyer (Associate Editor) and Prof. Antonia Hamilton (Subject
Editor)
openscience@royalsociety.org
Associate Editor Comments to Author (Dr Shirley-Ann Rüschemeyer):
Associate Editor: 1
Comments to the Author:
Dear Dr. Samuel,
Thank you very much for submitting your work to the Royal Society Open Science journal for
consideration. Three reviewers and I have read your work. While we all recognize the value of
the work presented, there are a number of issues that would need to be addressed before
publication could be considered.
1. Reviewer 2 highlights an issue with the power analysis used to justify the number of
participants in the current experiment (effect size used for power analysis is perhaps larger than
should have been). This is an important point to consider, particularly given that Bayesian
7
statistics also provide only weak support for the null result in some conditions. Can you provide
convincing evidence that the lack of an effect is not simply the result of an under-powered study?
2. Reviewers 1 and 3 both note substantial differences between the implementation of the
sandbox task in the current vs. previous studies. In particular, these reviewers have noted the
blocking of trials may have had an influence on the results. Please could you comment on this?
3. Lastly, all reviewers comment on the length of the manuscript: Reviewer 1 in particular makes
some very useful suggestions about what sections could be shortened.
The reviewers have provided many useful comments and suggestions, please take these into
consideration in your revised manuscript.
Best wishes, and thank you again for considering RSOS as an outlet for your work.
Reviewers' Comments to Author:
Reviewer: 1
Comments to the Author(s)
In this manuscript, Samuel et al aimed to uncover the cognitive processes leading to egocentric
biases in a change-of-location task. A recent previous study by the same authors showed that this
task was sensitive to a belief versus memory reasoning manipulation (participants showed
egocentric bias on the belief but not the memory trials) and also, crucially, that participants’
egocentric bias resulted from domain general representational processes: reasoning from multiple
types of representations (videos as well as mental states) was affected by egocentric bias.
The current manuscript tried to unpick the precise source of the egocentric bias found in the
previous study. The authors therefore tested three hypotheses in Experiment 1:
1. The egocentric bias results from reasoning about the other versus the self, tested by comparing
self memory versus other memory within participants;
2. The egocentric bias results from reasoning about beliefs versus memories, tested by comparing
other belief versus other memory between participants;
3. The egocentric bias results from reasoning about where the other will look versus where the
other believes the object is, tested by comparing other action versus other belief between
participants.
Around 50 participants per condition were tested in an online study with 4 trials per condition
per participant, in a blocked design.
Results: Participants were biased towards the true location of object (i.e. showed egocentric bias)
in all conditions, including in the self memory trials. This latter condition is in contrast to the
effect found in the authors’ previous study.
In terms of the hypotheses above, the authors found no effect on the magnitude of the egocentric
bias of reasoning about the other versus the self; no effect of reasoning about other belief vs other
memory when controlling for self memory bias; and no effect of reasoning about other belief vs
other action when controlling for self memory bias.
A second experiment ruled out that a change in the wording in the self memory trials could
explain the emergence of an egocentric bias on these trials.
The manuscript is well written and presents an extensive discussion of the discrepancy between
these data and previous studies. I therefore think that it is a useful addition to the literature on
this task, despite the contrast between these results and previous results, if the following points
are addressed:
8
1. One clear difference between the present manuscript and study 1 of the previous paper is that
in the previous paper the authors presented the different conditions in a mixed trial order,
whereas in the present paper they blocked the order of conditions. This difference should be
discussed and another experiment should be run to check that blocking vs mixing trial types does
not generate the egocentric bias on self memory trials.
2. The authors could compare the magnitude of the self memory bias in this study to that in their
previous study to support the claim that there is a significant difference between their findings
across these two papers.
Reviewer: 2
Comments to the Author(s)
See attached review.
Reviewer: 3
Comments to the Author(s)
This paper reports two experiments which use the sandbox (false belief) task to investigate theory
of mind in adults. The first experiment cleverly examined whether adults’ performance on the
task differs depending on whether they are asked about their own memories or about a story
character’s memory, belief, or future action. Somewhat surprisingly, performance did not vary
across these 4 test questions! Because the null difference between the “own memory” question
and another condition did not replicate the authors’ own earlier findings, they conducted a
second experiment to test whether wording changes to this question were responsible, but found
no evidence for this.
This work is important. It reveals major concerns about use of the sandbox task, and other
researchers need to be aware of these concerns. However, I think the paper may need to do more
to highlight differences between the present version of the sandbox and previous versions. Also, I
think the paper is much longer than it needs to be, especially given the results and take-home
message. I suspect it could be reduced to “brief report” length (i.e., 4000 words). I elaborate on
these points below.
-One concern with the current computerized version of the sandbox task is that participants did
many trials of the same sort, and this could have led them to adopt the strategy of trying to
remember each objects’ original location (as this was the correct location across most trials). It is
important to note that this is somewhat different than a related concern ruled out by the authors,
namely the concern that participants blindly responded the same way regardless of which
question was asked. My concern is that because the same questions were repeated, participants
paid special attention to the initial location in each trial, because they knew they’d probably be
asked to indicate the first location. One way to see if this concern affected the results might be to
only examine the first 2 trials completed by each participant, as in the first two trials they could
not have known that they would be repeatedly asked about the original location. I think that
providing this data could strengthen the paper.
My concern would have alleviated in a version of the sandbox task in which participants
more often had to indicate the current location of the object. If the study included such trials (i.e.,
more than just once), participants could not prepare by trying to pay greater attention to one
location than the other. I believe Bernstein et al. (2011) did include more trials of this sort. They
also had fewer trials overall, perhaps reducing the likelihood of strategic responding. I think the
authors need to be explicit about these differences between their procedure and Bernstein et al.’s
(and any other studies that have used the sandbox task). To really see the value of the present
9
paper, we need to know whether it reveals problems with the sandbox task in general, or
problems that only pertain to the authors’ version of the task.
INTRODUCTION.
-On p. 3, please consider softening “we fail to comprehend” to something like “we have difficulty
recognizing”. As great as our difficulties may be, we often do comprehend that others don’t share
our visual perspective.
-I’m not sure the first paragraph is needed. I suspect it could be shortened to one or two sentences
and merged with the next paragraph. Ultimately, this paper is about the suitability of the
sandbox task as a measure of theory of mind in (non-senior) adults and so I think the paper
should just get to this as quickly as possible. For this reason, it might also be possible to greatly
condense (or even remove) discussion of the theoretical issues the authors originally intended to
address (e.g., whether difficulty with theory of mind tasks is specific to reasoning about mental
states) and the corresponding literature (e.g., review of the false/outdated photos task). All of
this content is interesting. But ultimately the paper wasn’t able to address the issues it set out to
investigate, and they aren’t revisited in the General Discussion. So why take the reader through a
long review of these issues?
METHODS SECTION
The Methods Section of Experiment 1 repeatedly mentions ways in which the methods relate to
those from a previous paper by the authors (e.g., “The procedure for all three conditions was
identical to experiment 2 in our earlier study”; “Following our previous study [7, experiment 2],
we organized…”). Many readers won’t be familiar with the authors’ previous study, or at least
not familiar enough to remember all the details. So these comments will be lost on these readers.
Moreover, I think they will distract readers from their main goal of attempting to understand the
procedure that was actually used in the present studies. As noted above, I think it’s important to
know how the current methods relate to those from previous studies. But perhaps this should be
summarized outside of the Methods. (And again, comparisons of procedures should include
other versions of the sandbox task, including those from Bernstein et al., 2011).
RESULTS
-The results section of Experiment 1 is very long. One reason is that initially the data from each
question type are analyzed separately. However, I don’t think there is a justification for
separately analyzing these results, unless it were first shown that findings actually differed by
condition (they did not). So I suggest the authors begin the results begin with the “Comparing
conditions” section, and this would eliminate the need for most of the preceding analyses.
-The Results section of Experiment 1 begins with tests for order effects and ends with some
Bayesian analyses. Unless I’m overlooking something, neither type of analysis is found in
Experiment 2. Why not?
-I understand the data are non-normal and so parametric statistics cannot be used. However, this
makes the Results quite cumbersome, and also raises questions about whether some of the
analyses are justified. For example, in Experiment 2, I’m not sure that the authors are justified in
separately analyzing the short- and long-distance trials for each question wording. If an omnibus
test were run, it could inform us of whether there was an interaction between distance and
question type, which would then justify the separate analyses for long and short-distance. Would
it be possible to conduct omnibus analyses using GEEs or GLMMs?
-In the Discussion, the authors make some speculations about data from Bernstein et al (2011).
This is fine, though the paper would be even stronger if the authors requested the data from
Bernstein (or one of his co-authors) and then tested their conjecture about the data.
10
Author's Response to Decision Letter for (RSOS-180102.R0)
See Appendix B.
label_version_2
RSOS-181355.R0
label_author_4
Review form: Reviewer 1
Is the manuscript scientifically sound in its present form?
Yes
Are the interpretations and conclusions justified by the results?
Yes
Is the language acceptable?
Yes
Is it clear how to access all supporting data?
Yes
Do you have any ethical concerns with this paper?
No
Have you any concerns about statistical analyses in this paper?
No
Recommendation?
label_recommendation_4
Accept as is
Comments to the Author(s)
label_comment_4
The authors have addressed my comments and I think the manuscript is much improved by their
work to address all the reviewers' points.
label_author_5
Review form: Reviewer 2 (Daniel Bernstein)
Is the manuscript scientifically sound in its present form?
Yes
Are the interpretations and conclusions justified by the results?
Yes
11
Is the language acceptable?
Yes
Is it clear how to access all supporting data?
Yes
Do you have any ethical concerns with this paper?
No
Have you any concerns about statistical analyses in this paper?
No
Recommendation?
label_recommendation_5
Accept with minor revision (please list in comments)
Comments to the Author(s)
label_comment_5
Note that I reviewed a prior version of this manuscript with a graduate student. We co-reviewed
the current manuscript too.
Summary: The manuscript shows a null effect of egocentric belief bias (compared to a memory
control) using a computerized version of the Sandbox task. The conclusions point to two
possibilities: (1) the Sandbox task produces unreliable effects; or (2) the magnitude of effect is
smaller than previous studies suggest.
Overall opinion: The authors have been very responsive to reviewer concerns, including our own.
They have done a fine job revising their manuscript, and we think that this work will make a
valuable contribution to the literature. Below, we list our remaining, minor concerns with the
current manuscript.
Minor:
1. Additional editing needed throughout, especially on newly-added text, e.g., “those
participants’ whose…”; “differences scores” ; “indicating and that the data…” ; “admit of a
difference cognitive process”
2. Response to reviewer points (#18, pp. 50-51 in the pdf): Why not move all subsequent analyses
to an Appendix or Supplementary Materials to streamline the text? The Results sections are
complete, but could be shortened, without losing any crucial data.
3. Excluding participants’ responses selectively based on whether they passed the true belief trial
is potentially problematic. If participants have noted the two locations on the screen, but are not
following the story, they will respond randomly to either Location A or Location B. If this occurs,
participants who are responding randomly, but happen to respond correctly in the True Belief
trial will have their data included, while those who are responding in this manner but happen to
respond incorrectly to the true belief trial will have their data excluded. Although this wouldn’t
create systematic issues if it occurred, it does mean that exclusion in this manner does not
guarantee that participants who were included were necessarily paying attention to the task.
Therefore, it is possible that respondents were at times, responding randomly, and that these
random responses resulted in “subdivision of space” and “subjective experience of distance”
biases discussed in the Discussion section. The authors use a computerized version of the
Sandbox task, and use online recruitment, a crucial difference to other published literature, which
may have increased random responses. That said, I do realize that samples from online
recruitment have, in the past, responded similar to lab-based participants.
4. Reviewer 3 made issue of the fact that only one True Belief trial was included. Although we are
sensitive to the fact that this issue can’t feasibly be addressed without running another
12
experiment, it remains unaddressed. The issue is that someone can adopt a response strategy
while still passing the true belief trial. Someone who is sensitive to changes in wording will notice
when a true belief trial is present without having to process the entirety of the vignette. As such,
it is better to include multiple true belief trials, so that bias in the true belief trials can be
compared directly to both false belief trials and memory control trials. This comparison allows for
a better control than blanket exclusion based on a single trial, and arguably makes it harder (but
still not impossible) for participants to adopt a response strategy based on something other than
reasoning about the character’s beliefs. That said, we realize that the authors likely chose a single
true-belief attention check because that has been the most common procedure used with the
Sandbox task to date.
Daniel M. Bernstein and Daniel G. Derksen
label_end_comment
Decision letter (RSOS-181355.R0)
27-Sep-2018
Dear Dr Samuel
On behalf of the Editor, I am pleased to inform you that your Manuscript RSOS-181355 entitled
"Castles made of sand? The vanishing of egocentric bias across self-other and memory-belief
distinctions in the Sandbox Task." has been accepted for publication in Royal Society Open
Science subject to minor revision in accordance with the referee suggestions. Please find the
referees' comments at the end of this email.
The reviewers and Subject Editor have recommended publication, but also suggest some minor
revisions to your manuscript. Therefore, I invite you to respond to the comments and revise your
manuscript.
• Ethics statement
If your study uses humans or animals please include details of the ethical approval received,
including the name of the committee that granted approval. For human studies please also detail
whether informed consent was obtained. For field studies on animals please include details of all
permissions, licences and/or approvals granted to carry out the fieldwork.
• Data accessibility
It is a condition of publication that all supporting data are made available either as
supplementary information or preferably in a suitable permanent repository. The data
accessibility section should state where the article's supporting data can be accessed. This section
should also include details, where possible of where to access other relevant research materials
such as statistical tools, protocols, software etc can be accessed. If the data has been deposited in
an external repository this section should list the database, accession number and link to the DOI
for all data from the article that has been made publicly available. Data sets that have been
deposited in an external repository and have a DOI should also be appropriately cited in the
manuscript and included in the reference list.
If you wish to submit your supporting data or code to Dryad (http://datadryad.org/), or modify
your current submission to dryad, please use the following link:
http://datadryad.org/submit?journalID=RSOS&manu=RSOS-181355
13
• Competing interests
Please declare any financial or non-financial competing interests, or state that you have no
competing interests.
• Authors’ contributions
All submissions, other than those with a single author, must include an Authors’ Contributions
section which individually lists the specific contribution of each author. The list of Authors
should meet all of the following criteria; 1) substantial contributions to conception and design, or
acquisition of data, or analysis and interpretation of data; 2) drafting the article or revising it
critically for important intellectual content; and 3) final approval of the version to be published.
All contributors who do not meet all of these criteria should be included in the
acknowledgements.
We suggest the following format:
AB carried out the molecular lab work, participated in data analysis, carried out sequence
alignments, participated in the design of the study and drafted the manuscript; CD carried out
the statistical analyses; EF collected field data; GH conceived of the study, designed the study,
coordinated the study and helped draft the manuscript. All authors gave final approval for
publication.
• Acknowledgements
Please acknowledge anyone who contributed to the study but did not meet the authorship
criteria.
• Funding statement
Please list the source of funding for each author.
Please note that we cannot publish your manuscript without these end statements included. We
have included a screenshot example of the end statements for reference. If you feel that a given
heading is not relevant to your paper, please nevertheless include the heading and explicitly state
that it is not relevant to your work.
Because the schedule for publication is very tight, it is a condition of publication that you submit
the revised version of your manuscript before 06-Oct-2018. Please note that the revision deadline
will expire at 00.00am on this date. If you do not think you will be able to meet this date please let
me know immediately.
To revise your manuscript, log into https://mc.manuscriptcentral.com/rsos and enter your
Author Centre, where you will find your manuscript title listed under "Manuscripts with
Decisions". Under "Actions," click on "Create a Revision." You will be unable to make your
revisions on the originally submitted version of the manuscript. Instead, revise your manuscript
and upload a new version through your Author Centre.
When submitting your revised manuscript, you will be able to respond to the comments made by
the referees and upload a file "Response to Referees" in "Section 6 - File Upload". You can use this
to document any changes you make to the original manuscript. In order to expedite the
processing of the revised manuscript, please be as specific as possible in your response to the
referees.
When uploading your revised files please make sure that you have:
14
1) A text file of the manuscript (tex, txt, rtf, docx or doc), references, tables (including captions)
and figure captions. Do not upload a PDF as your "Main Document".
2) A separate electronic file of each figure (EPS or print-quality PDF preferred (either format
should be produced directly from original creation package), or original software format)
3) Included a 100 word media summary of your paper when requested at submission. Please
ensure you have entered correct contact details (email, institution and telephone) in your user
account
4) Included the raw data to support the claims made in your paper. You can either include your
data as electronic supplementary material or upload to a repository and include the relevant doi
within your manuscript
5) All supplementary materials accompanying an accepted article will be treated as in their final
form. Note that the Royal Society will neither edit nor typeset supplementary material and it will
be hosted as provided. Please ensure that the supplementary material includes the paper details
where possible (authors, article title, journal name).
Supplementary files will be published alongside the paper on the journal website and posted on
the online figshare repository (https://figshare.com). The heading and legend provided for each
supplementary file during the submission process will be used to create the figshare page, so
please ensure these are accurate and informative so that your files can be found in searches. Files
on figshare will be made available approximately one week before the accompanying article so
that the supplementary material can be attributed a unique DOI.
Please note that Royal Society Open Science charge article processing charges for all new
submissions that are accepted for publication. Charges will also apply to papers transferred to
Royal Society Open Science from other Royal Society Publishing journals, as well as papers
submitted as part of our collaboration with the Royal Society of Chemistry
(http://rsos.royalsocietypublishing.org/chemistry). If your manuscript is newly submitted and
subsequently accepted for publication, you will be asked to pay the article processing charge,
unless you request a waiver and this is approved by Royal Society Publishing. You can find out
more about the charges at http://rsos.royalsocietypublishing.org/page/charges. Should you
have any queries, please contact openscience@royalsociety.org.
Once again, thank you for submitting your manuscript to Royal Society Open Science and I look
forward to receiving your revision. If you have any questions at all, please do not hesitate to get
in touch.
Kind regards,
Royal Society Open Science Editorial Office
Royal Society Open Science
openscience@royalsociety.org
on behalf of Dr Shirley-Ann Rüschemeyer (Associate Editor) and Prof. Antonia Hamilton (Subject
Editor)
openscience@royalsociety.org
Associate Editor Comments to Author (Dr Shirley-Ann Rüschemeyer):
Associate Editor
Comments to the Author:
Dear Authors,
Thank you for submitting your work to the Royal Society Open Science journal for consideration.
15
I’m pleased to tell you the two reviewers (who also reviewed an early version of this manuscript)
and I are happy to accept your manuscript for publication pending minor revisions.
Reviewer 1 highlights two points in particular which you may wish to make note of in your
discussion.
Thanks again for considering the reviewer’s points in such detail, and for considering RSOS as an
outlet for your work.
Best wishes,
Reviewer comments to Author:
Reviewer: 2
Comments to the Author(s)
Note that I reviewed a prior version of this manuscript with a graduate student. We co-reviewed
the current manuscript too.
Summary: The manuscript shows a null effect of egocentric belief bias (compared to a memory
control) using a computerized version of the Sandbox task. The conclusions point to two
possibilities: (1) the Sandbox task produces unreliable effects; or (2) the magnitude of effect is
smaller than previous studies suggest.
Overall opinion: The authors have been very responsive to reviewer concerns, including our own.
They have done a fine job revising their manuscript, and we think that this work will make a
valuable contribution to the literature. Below, we list our remaining, minor concerns with the
current manuscript.
Minor:
1. Additional editing needed throughout, especially on newly-added text, e.g., “those
participants’ whose…”; “differences scores” ; “indicating and that the data…” ; “admit of a
difference cognitive process”
2. Response to reviewer points (#18, pp. 50-51 in the pdf): Why not move all subsequent analyses
to an Appendix or Supplementary Materials to streamline the text? The Results sections are
complete, but could be shortened, without losing any crucial data.
3. Excluding participants’ responses selectively based on whether they passed the true belief trial
is potentially problematic. If participants have noted the two locations on the screen, but are not
following the story, they will respond randomly to either Location A or Location B. If this occurs,
participants who are responding randomly, but happen to respond correctly in the True Belief
trial will have their data included, while those who are responding in this manner but happen to
respond incorrectly to the true belief trial will have their data excluded. Although this wouldn’t
create systematic issues if it occurred, it does mean that exclusion in this manner does not
guarantee that participants who were included were necessarily paying attention to the task.
Therefore, it is possible that respondents were at times, responding randomly, and that these
random responses resulted in “subdivision of space” and “subjective experience of distance”
biases discussed in the Discussion section. The authors use a computerized version of the
Sandbox task, and use online recruitment, a crucial difference to other published literature, which
may have increased random responses. That said, I do realize that samples from online
recruitment have, in the past, responded similar to lab-based participants.
4. Reviewer 3 made issue of the fact that only one True Belief trial was included. Although we are
sensitive to the fact that this issue can’t feasibly be addressed without running another
experiment, it remains unaddressed. The issue is that someone can adopt a response strategy
while still passing the true belief trial. Someone who is sensitive to changes in wording will notice
16
when a true belief trial is present without having to process the entirety of the vignette. As such,
it is better to include multiple true belief trials, so that bias in the true belief trials can be
compared directly to both false belief trials and memory control trials. This comparison allows for
a better control than blanket exclusion based on a single trial, and arguably makes it harder (but
still not impossible) for participants to adopt a response strategy based on something other than
reasoning about the character’s beliefs. That said, we realize that the authors likely chose a single
true-belief attention check because that has been the most common procedure used with the
Sandbox task to date.
Daniel M. Bernstein and Daniel G. Derksen
Reviewer: 1
Comments to the Author(s)
The authors have addressed my comments and I think the manuscript is much improved by their
work to address all the reviewers' points.
Author's Response to Decision Letter for (RSOS-181355.R0)
See Appendix C.
label_end_comment
Decision letter (RSOS-181355.R1)
02-Oct-2018
Dear Dr Samuel,
I am pleased to inform you that your manuscript entitled "The unreliability of egocentric bias
across self-other and memory-belief distinctions in the Sandbox Task." is now accepted for
publication in Royal Society Open Science.
You can expect to receive a proof of your article in the near future. Please contact the editorial
office (openscience_proofs@royalsociety.org and openscience@royalsociety.org) to let us know if
you are likely to be away from e-mail contact. Due to rapid publication and an extremely tight
schedule, if comments are not received, your paper may experience a delay in publication.
Royal Society Open Science operates under a continuous publication model
(http://bit.ly/cpFAQ). Your article will be published straight into the next open issue and this
will be the final version of the paper. As such, it can be cited immediately by other researchers.
As the issue version of your paper will be the only version to be published I would advise you to
check your proofs thoroughly as changes cannot be made once the paper is published.
17
On behalf of the Editors of Royal Society Open Science, we look forward to your continued
contributions to the Journal.
Kind regards,
Andrew Dunn
Royal Society Open Science Editorial Office
Royal Society Open Science
openscience@royalsociety.org
on behalf of Dr Shirley-Ann Rüschemeyer (Associate Editor) and Antonia Hamilton (Subject
Editor)
openscience@royalsociety.org
Follow Royal Society Publishing on Twitter: @RSocPublishing
Follow Royal Society Publishing on Facebook:
https://www.facebook.com/RoyalSocietyPublishing.FanPage/
Read Royal Society Publishing's blog: https://blogs.royalsociety.org/publishing/
Appendix A
Please note that I reviewed this manuscript with two students who have
backgrounds in theory-of-mind research and the Sandbox task. We reviewed
the manuscript separately, and then combined our reviews.
Overall opinion: This is an elegant study that challenges the small extant
literature on the Sandbox task. The experiments are well done. The
manuscript is well written, and the analyses are thorough. As with many null
effects, however, the null effects reported here are hard to interpret. We have
identified a major issue with the power analysis, as well as errors in the
literature review that prevent us from recommending this work for
publication. We otherwise have a few minor suggestions for improving the
work. We present major followed by minor suggestions for improvement.
Major concerns
1. Power. The authors of the current study performed a power analysis,
using the effect size from their prior study (Samuel et al., 2017,
Experiment 1). However, the authors use an r of .39 in their current
power analysis, whereas Samuel et al., (2017) only report an effect
size of magnitude r = .39 in Experiment 2, when comparing whether
bias in false film trials was greater than 0. Using this r value makes no
theoretical sense. The relevant r values reported in Experiment 1 of
Samuel et al., 2017 are in fact, much smaller in comparison (r = .17).
Thus, the current power analysis is incorrectly reported and vastly
underpowered for the intended false-belief versus memory control
effect (N = 439, based on r = .17, alpha = .05, power = .95). Many of
the analyses also split the sample between short and long distances.
This may have resulted in further decreased power. Thus, the current
study does not have adequate power to observe many of the effects
that are tested. Note that one of my student co-reviewers caught this
error. I gave the authors the benefit of the doubt that their reported
effect size was correct. If we are correct, here, then what can we
suggest to remedy this problem? We are heistant to recommend
doubling the sample size, because of issues related to p-hacking. We
are also sympathetic to the work already put into this study. Tough
call.
2. Errors in literature review. The authors do a nice job of reviewing the
small extant literature on the Sandbox task, but incorrectly report a few
key details. First, in Footnote 5, the authors claim that Bernstein et al.
(2017) fail to provide statistical tests of the difference between false
belief and memory control trials. From Table 3 in Bernstein et al.
(2017), right-most column, all age groups except for 5-year-olds
showed significant egocentric bias: all mean differences between false
belief and memory control yielded scores above 0, with 95%
confidence intervals excluding 0. This is equivalent to a one-sample t-
test. The data posted with that article also permit researchers to test
this for themselves. Second, p. 32, lines 11-19: When discussing
Sommerville et al. (2013, Experiment 2), the authors should note that
Sommerville et al. tested children and adults and that the average
memory bias was driven almost entirely by 3- and 5-year olds. Adults
showed little to no bias on memory control trials. Regarding power
concerns (Major point 1), Bernstein has several additional unpublished
data sets on adults’ performance on the Sandbox task, including a large
lifespan replication of Bernstein et al. (2017). In nearly all of these
datasets, false belief bias is greater than memory control bias. We say
“nearly,” because there are a few datasets that do not show a
significant difference in adults. Based on our published and
unpublished work, we believe that the actual effect size for adults is
small to medium. The null effects appear to be due to lack of power.
3. Title is overly provocative. We like the title, but think that it might be
a tad too provocative and premature, especially given our two prior
points. The authors appear to have used the incorrect effect size in
their power calculation and mistakenly report a slightly more muddled
literature to date on the Sandbox task than is the case.
Minor concerns
1. If this were submitted to a print journal, we’d suggest cutting sections
of the Introduction and General Discussion to save space. The authors
are thorough, but could streamline the work. Some of the General
Discussion includes unlikely explanations for their null effects which
could be eliminated. The authors often leave the most likely counter-
explanations to last (e.g., “crucially, if egocentric bias is stronger when
reasoning from
representations, or about other people, we should not expect results to
occur only when using a specific set of stories.”).
2. Awkward wording and grammatical errors in places, e.g., “attributing
false belief require that…”; “even for groups adults”; “Despite these
changes, we can rule out is that”
3. Negative bias is unclear: “further from the true location than even the
distance to the first location.” We suggest that the authors include an
example when introducing the concept of negative bias scores in the
Sandbox task.
4. Mention preschool versions of Sally-Anne and other false belief tasks
that produce similar results regardless of changes in wording and
procedure (cite Wellman, Cross, & Watson, 2001).
5. Table 1 needs further information. What does Trial type Left, Right
signify? I presume that L2 is to the left or right of L1, but this isn’t
anywhere in the Table or caption.
6. P. 33, lines 41-44: The authors note that reaction time data may be
more sensitive than accuracy data when measuring mental state versus
control trials. If the authors have the data, why not analyze RT data in
the computerized sandbox task?
7. Some further discussion on why participants were differently biased
on short vs long trials would be appreciated.
8. The authors use “location A”, “first location”, and “original location”
to refer to where a protagonist hid an object. Consistency would
increase readability and comprehension.
9. On the title page, “Theory of Mind” is listed as a keyword twice.
10. A visual depiction of sandbox dimensions, short vs long distances, and
positive vs negative bias calculations would be helpful for readers.
Appendix B
Response to reviews.
We thank the reviewers and editor for their time and thought in considering
our manuscript. In creating our revised submission, we have taken into account all of
the points made. We detail our responses and changes below. Where reviewers have
made similar points, these have been grouped under one number for ease of reading.
The revised text is labelled according to the point number for convenient referencing,
and where possible important edits in the text have also been pasted here. We begin
by addressing an important misunderstanding concerning our power analysis.
1. Ed. 1. Reviewer 2 highlights an issue with the power analysis used to justify the number of
participants in the current experiment (effect size used for power analysis is perhaps larger
than should have been). This is an important point to consider, particularly given that
Bayesian statistics also provide only weak support for the null result in some conditions. Can
you provide convincing evidence that the lack of an effect is not simply the result of an under-
powered study?
R2. Major. Power. The authors of the current study performed a power analysis, using the
effect size from their prior study (Samuel et al., 2017, Experiment 1). However, the authors
use an r of .39 in their current power analysis, whereas Samuel et al., (2017) only report an
effect size of magnitude r = .39 in Experiment 2, when comparing whether bias in false film
trials was greater than 0. Using this r value makes no theoretical sense. The relevant r values
reported in Experiment 1 of Samuel et al., 2017 are in fact, much smaller in comparison (r =
.17). Thus, the current power analysis is incorrectly reported and vastly underpowered for the
intended false-belief versus memory control effect (N = 439, based on r = .17, alpha = .05,
power = .95). Many of the analyses also split the sample between short and long distances.
This may have resulted in further decreased power. Thus, the current study does not have
adequate power to observe many of the effects that are tested. Note that one of my student co-
reviewers caught this error. I gave the authors the benefit of the doubt that their reported
effect size was correct. If we are correct, here, then what can we suggest to remedy this
problem? We are heistant to recommend doubling the sample size, because of issues related
to p-hacking. We are also sympathetic to the work already put into this study. Tough call.
After a bit of digging, we have discovered that this point hinges on a
confusion between Pearson’s r (i.e. correlation between two variables required to
conduct a power analysis), and r as an effect size computed for a Wilcoxon’s test.
Both of course use the same notation. What seems to have happened is that the
reviewer saw the correlation r = .39 in our power analysis and guessed that we had
in fact erroneously used the effect size r = .39 for False Film trials from our 2017
paper (which we agree would make no theoretical sense). In fact, we reported the
correlation between variables r = .39 (page 8), as this forms a crucial part of a power
analysis. Specifically, the power test requires:
1. The mean of Memory trials (M = 0.01421)
2. The mean of False Belief trials (M = 0.05089)
3. The standard deviation of Memory trials (SD = 0.0546)
4. The standard deviation of False Belief trials (SD = 0.0677)
5. The correlation between groups (r = 0.39)
The outcome of the a priori power test is an N of precisely 50. I can make a
screenshot of this power test (using G*Power) available upon request.
We hope we have now cleared up this confusion and demonstrated that the computed
N of 50 is thus sound. Nevertheless, we understand how this could occur, and given
the importance of our power analysis to our paper we have edited the text to make
clear that the r reported is the correlation between variables (please see page 8):
“The analysis revealed a required sample size of 50 participants to detect medium effect
sizes (r correlation between variables = .39, alpha = .05, power = .95).” (p.11)
We have also added all the info required to conduct the power analysis in a footnote
attached to that sentence in the main text.
“For completeness, mean bias on memory trials was 1.4% SD = 5.5), on false belief
trials 5.1% (SD = 6.8).”
Finally, we understand that statistical power is a crucial issue to consider also in
our interpretation of our results. We have added some text around what might
explain the variability in findings with the Sandbox task (please see page 29).
“One possibility is that publication biases, whereby studies that report statistically
significant findings are more likely to be published than studies which do not [e.g. 27],
have unintentionally inflated the perceived the reliability of the task. Another
possibility is that people do show greater bias on false belief trials, but the true effect
size has been overestimated, perhaps in part due to the previous point.”
2. Ed. 2. Reviewers 1 and 3 both note substantial differences between the implementation of the
sandbox task in the current vs. previous studies. In particular, these reviewers have noted the
blocking of trials may have had an influence on the results. Please could you comment on
this?
R1. 1. One clear difference between the present manuscript and study 1 of the previous paper
is that in the previous paper the authors presented the different conditions in a mixed trial
order, whereas in the present paper they blocked the order of conditions. This difference
should be discussed and another experiment should be run to check that blocking vs mixing
trial types does not generate the egocentric bias on self memory trials.
R2. Minor. Mention preschool versions of Sally-Anne and other false belief tasks that produce
similar results regardless of changes in wording and procedure (cite Wellman, Cross, &
Watson, 2001).
R3. My concern would have alleviated in a version of the sandbox task in which participants
more often had to indicate the current location of the object. If the study included such trials
(i.e., more than just once), participants could not prepare by trying to pay greater attention to
one location than the other. I believe Bernstein et al. (2011) did include more trials of this
sort. They also had fewer trials overall, perhaps reducing the likelihood of strategic
responding. I think the authors need to be explicit about these differences between their
procedure and Bernstein et al.’s (and any other studies that have used the sandbox task). To
really see the value of the present paper, we need to know whether it reveals problems with
the sandbox task in general, or problems that only pertain to the authors’ version of the task.
R3. This work is important. It reveals major concerns about use of the sandbox task, and
other researchers need to be aware of these concerns. However, I think the paper may need to
do more to highlight differences between the present version of the sandbox and previous
versions.
R3. One concern with the current computerized version of the sandbox task is that
participants did many trials of the same sort, and this could have led them to adopt the
strategy of trying to remember each objects’ original location (as this was the correct
location across most trials). It is important to note that this is somewhat different than a
related concern ruled out by the authors, namely the concern that participants blindly
responded the same way regardless of which question was asked. My concern is that because
the same questions were repeated, participants paid special attention to the initial location in
each trial, because they knew they’d probably be asked to indicate the first location. One way
to see if this concern affected the results might be to only examine the first 2 trials completed
by each participant, as in the first two trials they could not have known that they would be
repeatedly asked about the original location. I think that providing this data could strengthen
the paper.
Reviewers 1 and 3 make important points about the generalisability of our findings to
other Sandbox tasks. Reviewer 2 makes the equally important point, and somewhat
opposite point, that false belief tasks should and do show a fair degree of consistency
across different procedural variations.
When it comes to comparing our version to others, we have added/edited text to
make these comparisons (whether involving similarities or differences) clearer. These
sections are labelled in the main text (please see pages 6, 7, 9, 10, 12, 13, 19, 23, 24,
25). On page 13 (footnote) we have added an analysis of first trials only, which
supports the absence of a difference in bias between experimental and control trials
before any repetition or order effect can possibly occur. On page 25 we also note
Reviewer 2’s point that false belief tasks are usually (and should be) robust across
procedural variations, and we discuss how our blocked design has indeed been used
before and found differences. We would also like to highlight already existing text
which draws important comparisons, particularly in the General Discussion (pages
23, 24). Given that our results were somewhat unexpected, we feel the GD is the
most appropriate place to spend more considerable time drawing comparisons with
previous research, be it about results or methods. Page 25 is particularly crucial, as
we highlight the details included in the current task that are based on improvements
made as the literature on the sandbox task has evolved, or improvements which are
deemed ‘good-practice’ whether they have appeared in other studies or not. These
together ensure that the task is appropriate for testing our hypothesis. These points
include:
a) Power analyses to inform our Ns.
b) Eliminating those participants who failed the attention check (studies except
our own have not – see in particular point 3 within this answer and point 5
further down)
c) Use of appropriate statistical tests for non-normally distributed data (most
other studies have not)
d) Bayesian tests to assess the strength of nulls (other studies have not done
this)
e) Reporting all statistical comparisons that have previously been reported.
Others have tended to focus on one or two comparisons being key, which
have not always been consistent across different studies (see also Point 18 for
further discussion).
f) Multiple trials per participants and (in this revision) analyses of first trials.
Our analysis of first trials corroborated our analyses of all trials.
g) Mixed order of presentation so that order effects can in any case be checked
for. We found no evidence of order effects that might explain our findings in
any case.
h) Always making data available (our 2017 data were made public the day of
manuscript acceptance).
We would also like to add some more general points that we feel are relevant in light
of these comments:
1) Reviewer 2 makes the excellent point that procedural variations should in
any case not really matter in false belief tasks. In a sense, the fact that we
found no evidence of greater bias on false belief trials could be because i)
participants are not more biased on experimental trials or ii) the task is less
reliable at teasing them out than hitherto supposed. We have opted for the
more conservative latter option rather than making strong claims about false
belief reasoning in adults more generally.
2) If the order in which participants performed different trial types did have an
effect (our evidence suggests it does not), what this would indicate is that the
Sandbox task is highly sensitive to variations in task structure, which has
important implications for the validity and power of the Sandbox Task.
3) Our analyses of the data provided by Bernstein et al. (2017) indicate the
majority of their participants would have been eliminated if our own
attention check procedures had been followed. That they were not suggests
that over half of their participants may have not been paying attention to the
wording of trials (please see footnote added to page 29, as part of our
response to point 4 below). A similar proportion would have been removed
from their young adult group.
4) Note also that given i) Sommerville and colleagues (2013) used precisely the
same trial structure as we did and did find a significant difference between
false belief trials and memory control trials; ii) we found no effect of order,
iii) we found no difference between experimental and control trials across
three conditions in one paper; and iv) our analysis of first trials corroborated
our findings, the idea of running a fourth study with a different order of trials
appears superfluous.
3. Ed. 3. Lastly, all reviewers comment on the length of the manuscript: Reviewer 1 in particular
makes some very useful suggestions about what sections could be shortened.
R2. Minor. If this were submitted to a print journal, we’d suggest cutting sections of the
Introduction and General Discussion to save space. The authors are thorough, but could
streamline the work. Some of the General Discussion includes unlikely explanations for their
null effects which could be eliminated. The authors often leave the most likely counter-
explanations to last (e.g., “crucially, if egocentric bias is stronger when reasoning from
representations, or about other people, we should not expect results to occur only when using
a specific set of stories.”).
R3. Also, I think the paper is much longer than it needs to be, especially given the results and
take-home message. I suspect it could be reduced to “brief report” length (i.e., 4000 words). I
elaborate on these points below.
R3. I’m not sure the first paragraph is needed. I suspect it could be shortened to one or two
sentences and merged with the next paragraph. Ultimately, this paper is about the suitability
of the sandbox task as a measure of theory of mind in (non-senior) adults and so I think the
paper should just get to this as quickly as possible. For this reason, it might also be possible
to greatly condense (or even remove) discussion of the theoretical issues the authors
originally intended to address (e.g., whether difficulty with theory of mind tasks is specific to
reasoning about mental states) and the corresponding literature (e.g., review of the
false/outdated photos task). All of this content is interesting. But ultimately the paper wasn’t
able to address the issues it set out to investigate, and they aren’t revisited in the General
Discussion. So why take the reader through a long review of these issues?
We thank the reviewers for their comments on this matter. We have edited sections of
the introduction (merging of first and second paragraphs as recommended; text
concerning non-social controls on theory of mind tasks removed, visual perspective-
taking section removed, early understanding of false belief task results removed), and
in the General Discussion (text on our use of more adult-appropriate stories removed;
text on how our data filtering procedure could not account for any discrepancy
between our results and others’ (except in a positive way) removed). We have
condensed the section of the General Discussion that talks about explanations we can
rule out, but have also managed to incorporate comparisons with other versions of the
Sandbox task in this section. We have shortened the Conclusion. We have also cut a
number of what in hindsight appear to be unnecessary repetitions and ‘over
clarifications’. However, we have retained our original three hypotheses (related to
the source of egocentric bias), if only for the reason that our choice of conditions
(Other Memory, Other Belief, Other Action) would not otherwise have any
motivation to them. As such, we feel reducing the manuscript to a brief report
(Reviewer 3) would be detrimental to the work. It is also exceedingly difficult to
condense four conditions over two experiments into a brief report. Nevertheless, our
changes have cut the word count by about 2,000 words, to 9,467.
4. R1. 2. The authors could compare the magnitude of the self memory bias in this study to that
in their previous study to support the claim that there is a significant difference between their
findings across these two papers.
We thank the reviewer for this point. We have added detail to the Discussion section
to Experiment 1 (please see page 19) concerning the difference in bias on Own
Memory trials across the experiments. We have restricted this discussion to a
comparison of effect sizes to avoid adding too much more in the way of results (see
other comments regarding length of the manuscript), and also because the most
important statistic is not bias on Own Memory trials but bias on experimental trials
relative to Own Memory trials.
5. R2. Major. Errors in literature review. The authors do a nice job of reviewing the small
extant literature on the Sandbox task, but incorrectly report a few key details. First, in
Footnote 5, the authors claim that Bernstein et al. (2017) fail to provide statistical tests of the
difference between false belief and memory control trials. From Table 3 in Bernstein et al.
(2017), right-most column, all age groups except for 5-year-olds showed significant
egocentric bias: all mean differences between false belief and memory control yielded scores
above 0, with 95% confidence intervals excluding 0. This is equivalent to a one- sample t-test.
The data posted with that article also permit researchers to test this for themselves.
We originally left the Bernstein et al. (2017) finding out of our count of studies
supporting greater bias on false belief because of the absence of statistical tests of the
relevant data in the paper. First, we would like to highlight that our choice of words
was accurate; there were indeed no statistical tests of the difference between memory
and FB trials reported in that paper. The confidence intervals also turn out to be
slightly unreliable measures of statistical significance. By our subsequent analyses of
the data available we found that egocentric bias in 5 year-olds was indeed statistically
significant in a one-sample t-test against zero, but paired-samples Memory vs. False
Belief was not, presumably because of the inclusion of more variance in the latter
analysis.
We looked at the dataset (Bernstein et al., 2017) to clarify whether this study did
show statistically reliable support for the hypothesis that participants (young adults in
particular) showed more bias on false belief than memory control trials. We report a
brief summary of our findings in footnote 6 (page 29):
“The descriptive results of the study by Bernstein and colleagues [15] also
suggested that bias was greater on false belief than memory trials, and in all
age groups tested, but statistical tests of the difference were not provided. We
analysed the data, which was made available with the publication. We first
found that the data were not normally distributed, so we performed non-
parametric tests. These confirmed that participants showed more bias on False
belief trials than Memory trials in the age groups 1-3 year-olds, 9-10 year-olds,
younger adults and older adults, but not in 5 year-olds and 11-12 year-olds.
However, when we exclude those participants who indicated a location closer
to the original than final location of the hidden object on true belief trials (i.e.
those who would have been excluded by our procedures), we find first that the
number of exclusions renders statistical analyses almost impossible in 9-10
year-olds (N reduced from 26 to 6) and 11-12 year-olds (N reduced from 15 to
4). In pre-schoolers, only 4 of 43 even gave a location within Location A and
Location B on the true belief trial. In younger and older adults, the N was cut
by half (from 111 to 55 and from 70 to 36 respectively). Young adults were still
more biased on false belief than memory trials (effect size .094).”
In younger adults (the age group with which we ourselves are most concerned with
here), there was still more bias in False Belief than memory trials, though the effect
size for this declines as a result from .116 to .094, in both cases lower than what we
found in 2017 (r = .170). Additionally, a test of egocentric bias scores (young adults
again) against zero revealed only a marginally significant result (p = .051). Overall,
we feel that our own analysis indicates the potential pitfalls of making inferences
from data that have not been analysed, or have not been analysed using the
appropriate tests. Nevertheless, these data support the hypothesis that there was
greater bias on False Belief than memory trials in the young adult sample, and we
have added it to the tally, making 6 studies in support (previously five) (please see
page 29).
6. R2. Major. Second, p. 32, lines 11-19: When discussing Sommerville et al. (2013, Experiment
2), the authors should note that Sommerville et al. tested children and adults and that the
average memory bias was driven almost entirely by 3- and 5-year olds. Adults showed little to
no bias on memory control trials. Regarding power concerns (Major point 1), Bernstein has
several additional unpublished data sets on adults’ performance on the Sandbox task,
including a large lifespan replication of Bernstein et al. (2017). In nearly all of these datasets,
false belief bias is greater than memory control bias. We say “nearly,” because there are a
few datasets that do not show a significant difference in adults. Based on our published and
unpublished work, we believe that the actual effect size for adults is small to medium. The null
effects appear to be due to lack of power.
We address the misunderstanding over the power analysis in point 1. Note also that
our power analysis was geared at medium effect sizes in each condition, and our
omnibus analysis (please see page 18) combining the data from all three conditions
also found support for the null. Regarding the point about Somerville et al. testing
children, we have made the relevant text clearer (pages 27-28):
“In every trial type in the present experiment, participants were more biased towards
the second ‘true’ location than the first, even on control trials, a result consistent with
findings by Sommerville et al. [13, experiment 2], who reported bias on control trials in
children that indicated responses on average half way between the first and second
locations, and Begeer et al. [42], also with children, who reported bias on memory trials
larger even than the difference between bias on false belief trials and memory trials. In
adults too there are suggestions in their raw data of bias towards the true location even
on control trials [11, 13 (experiment 1), 15–16]. This bias is not merely greater
dispersion of responses around a point; it is directional (i.e. towards the true location).”
Note that for the adults in Sommerville et al. (2013), we have to refer to our reading
of the graphs (‘there are suggestions in their raw data’) in the absence of statistical
tests of memory trial bias against zero. The graph in exp 1 suggest adults showed bias
on memory trials, though not in exp 2. This is as we originally reported it.
We are not in a position to comment on unpublished data, but we appreciate the
reviewer’s sharing of their mixed findings with adults here.
7. R2. 3. Major. Title is overly provocative. We like the title, but think that it might be a tad too
provocative and premature, especially given our two prior points. The authors appear to have
used the incorrect effect size in their power calculation and mistakenly report a slightly more
muddled literature to date on the Sandbox task than is the case.
We address the misunderstanding around the power calculation in point 1, but we do
agree nevertheless that the title could be softened. We have opted for: “The
unreliability of egocentric bias in the Sandbox task.”
8. R2. Minor. Awkward wording and grammatical errors in places, e.g., “attributing false belief
require that...”; “even for groups adults”; “Despite these changes, we can rule out is that”
We thank the reviewer for noting these errors. They have been corrected.
9. R2. Minor. Negative bias is unclear: “further from the true location than even the distance to
the first location.” We suggest that the authors include an example when introducing the
concept of negative bias scores in the Sandbox task.
We have clarified negative bias in the ‘analyses’ section, and have now provided an
example in the text of both positive and negative bias using the illustration in Figure
1 (Page 12):
“Bias scores were converted from pixels into percentages of the total length of the
trough, with positive scores indicating that the participant gave a location x% closer to
the Location B relative to Location A, and negative scores a location in the opposite
direction (i.e. further from Location B than Location A). For example, a location
between Location A and Location B in the example given in Figure 1 would indicate
positive bias. A location to the right of Location A in this example would indicate
negative bias. As in our previous study [7] the response data were not normally
distributed, so we opted for non-parametric tests. Any exceptions are clearly stated in
the text alongside the analyses in question.”
10. R2. Minor. Table 1 needs further information. What does Trial type Left, Right signify? I
presume that L2 is to the left or right of L1, but this isn’t anywhere in the Table or caption.
We thank the reviewer for spotting this omission. We have clarified the meaning of
the information in the Table.
11. R2. Minor. P. 33, lines 41-44: The authors note that reaction time data may be more sensitive
than accuracy data when measuring mental state versus control trials. If the authors have the
data, why not analyze RT data in the computerized sandbox task?
Our point about RT data was in relation to tasks where such data is more
interpretable, such as on the Director task, or timed false belief tasks using videos
(e.g. Cohen, Sasaki & German, 2015). There is no real time pressure on a Sandbox
task, and as a result an analysis of RTs is unlikely to reveal anything of importance
(indeed, they have not been measured or incorporated into any other Sandbox Task
report). In any case, given the relatively low applicability to the present research and
concerns over manuscript length, the relevant comment has been removed from the
General Discussion.
12. R2. Minor. Some further discussion on why participants were differently biased on short vs
long trials would be appreciated.
We have added some text related to this point, although we admit we can only
speculate given the design we have (and the fact that the results do not point towards
any genuinely consistent effect of distance). Please see page 27.
13. R2. Minor. The authors use “location A”, “first location”, and “original location” to refer to
where a protagonist hid an object. Consistency would increase readability and
comprehension.
We have edited the text so that Location A and Location B are the main terms, with
only occasional deviations for emphasis or to avoid repetition.
14. R2. Minor. On the title page, “Theory of Mind” is listed as a keyword twice.
We have corrected this repetition. We thank the reviewer for their attention.
15. R2. Minor. A visual depiction of sandbox dimensions, short vs long distances, and positive vs
negative bias calculations would be helpful for readers.
We have amended the ‘analyses’ section to provide examples of positive and
negative bias that relate to the illustration (Figure 1) already provided (see point 9
above). This figure also offers a visual example of the sandbox task stimuli. Figures 2
and 5 provide visual reference lines for short/long distance points in the trough. We
feel these are most useful on the histograms as they show how responses sometimes
cluster around the second location (hence the data is not normally distributed).
However, we have also added information to the caption to Figure 1 to indicate that
the example is of a long-distance trial, and that a rapid visualisation of a short-
distance trial is easily extrapolated (see page 36):
“Figure 1. An example long distance, Own Memory trial. On short-distance trials
the area between Locations A and B was always half the area shown here.”
16. R3. INTRODUCTION.
-On p. 3, please consider softening “we fail to comprehend” to something like “we have
difficulty recognizing”. As great as our difficulties may be, we often do comprehend that
others don’t share our visual perspective.
We agree that we could soften this comment. We have amended the text as advised
(see Page 3).
17. R3. METHODS SECTION
The Methods Section of Experiment 1 repeatedly mentions ways in which the methods relate
to those from a previous paper by the authors (e.g., “The procedure for all three conditions
was identical to experiment 2 in our earlier study”; “Following our previous study [7,
experiment 2], we organized…”). Many readers won’t be familiar with the authors’ previous
study, or at least not familiar enough to remember all the details. So these comments will be
lost on these readers. Moreover, I think they will distract readers from their main goal of
attempting to understand the procedure that was actually used in the present studies. As noted
above, I think it’s important to know how the current methods relate to those from previous
studies. But perhaps this should be summarized outside of the Methods. (And again,
comparisons of procedures should include other versions of the sandbox task, including those
from Bernstein et al., 2011).
We thank the reviewer for their comments here. We have removed superfluous
references to our previous paper. We have also made comparisons/differences
between our study and previous ones clearer throughout (see response to point 2).
18. R3. RESULTS
-The results section of Experiment 1 is very long. One reason is that initially the data from
each question type are analyzed separately. However, I don’t think there is a justification for
separately analyzing these results, unless it were first shown that findings actually differed by
condition (they did not). So I suggest the authors begin the results begin with the
“Comparing conditions” section, and this would eliminate the need for most of the preceding
analyses.
R3. -I understand the data are non-normal and so parametric statistics cannot be used.
However, this makes the Results quite cumbersome, and also raises questions about whether
some of the analyses are justified. For example, in Experiment 2, I’m not sure that the authors
are justified in separately analyzing the short- and long-distance trials for each question
wording. If an omnibus test were run, it could inform us of whether there was an interaction
between distance and question type, which would then justify the separate analyses for long
and short-distance. Would it be possible to conduct omnibus analyses using GEEs or
GLMMs?
We have thought carefully about these comments, as we are sympathetic to the idea
that describing finer-grained analyses is perhaps unnecessary in the absence of a
superordinate interaction mandating them. We also considered this prior to
submission. In the end we opted to retain these analyses for reasons that, it appears to
us, remain strong enough to warrant their continued inclusion in the present
manuscript. These are:
1) Regarding the separate analyses of long- and short-distance trials, we
would like to highlight that we begin each results section with the results collapsed
over distance prior to splitting by distance. Previous Sandbox task results have also
always reported results from short and long trials separately, so if we had left these
out, we may well have been requested to put them in. Indeed, Reviewer 2 has
requested discussion of those minor differences related to distance that we report. It
is always a difficult balancing act between presenting too many results and not
enough; we felt that given previous studies had reported the split analyses that we
should do so, particularly given that we are describing results which in some cases
contradict others’.
2) We applied the same logic to the reporting of results by each trial type
(other memory, other belief, other action). Regarding an omnibus analysis, we did
perform a test of all participants combined (please see page 18) in the original text,
an analysis that again corroborated the pattern of results for each study individually
(support for the null). Nevertheless, we have removed from the results section the
Kruskal-Wallis tests, as these are essentially duplicated by the one-way ANOVA that
follows. The original Figure 4 has been removed as a result.
3) Regarding the non-parametric tests, we are glad that the reviewer agrees
these are the appropriate ones to run given the data. Thus, the issue of whether they
may be cumbersome or otherwise (we think not) is thus moot. Given that we have
now added a test of first trials, and Bayesian analyses and order effects for
Experiment 2 (at Reviewer 3’s request, see point 19 below), we feel that any further
tests (e.g. mixed models) would add little, would not allow for direct comparison to
the analyses in our previous (2017) paper (or others’), and make graphical
representations of results more difficult.
4) Finally, our results are certainly complete; that is, it is highly unlikely that
readers will not find a full report of the relevant statistic they are looking for. This is
important given focus in different studies on different analyses of the data.
This said, however, if the consensus view is that we should remove some of the finer-
grained analyses then we are open to doing so. Our data will in any case be available
should anyone wish to look at the detail.
19. R3. -The Results section of Experiment 1 begins with tests for order effects and ends with
some Bayesian analyses. Unless I’m overlooking something, neither type of analysis is found
in Experiment 2. Why not?
We thank the reviewer for noticing this omission. This was an error on our part. We
have added the relevant analyses, and can confirm that they back up the
interpretations of those analyses already conducted (please see pages 21 and 22).
20/26. R3. -In the Discussion, the authors make some speculations about data from Bernstein et al
(2011). This is fine, though the paper would be even stronger if the authors requested the data
from Bernstein (or one of his co-authors) and then tested their conjecture about the data.
We have been in contact with Daniel Bernstein recently. He has been extremely
helpful, especially given that we contacted him in the summer, and has made what
data he could available to us upon request. At present this does not include the data
for the 2011 paper, though this is an issue of timing. Though we feel that these data
could be made available to us in the future, we feel that our language concerning the
potential for a null result with young adults in that paper is already sufficiently
carefully worded. We would also like to highlight that this demonstrates the
importance of reporting all relevant statistical comparisons.
FINAL POINTS:
1) We have added test statistics (Wilcoxon Ws, Mann-Whitney Us) in addition to Z
scores, for completeness.
2) New reference added (De Bruin et al., 2017) which speaks to the question of
publication biases in research.
Appendix C
Authors’ Response: We are grateful to the Reviewers and the Editors for
their useful comments throughout this process. Please see below for our
responses.
Reviewer comments on R1
Reviewer: 2
Minor:
1. Additional editing needed throughout, especially on newly-added text, e.g.,
“those participants’ whose…”; “differences scores” ; “indicating and that the
data…” ; “admit of a difference cognitive process”
Authors’ Response: We thank the Reviewers for their attention. The errors
have been rectified.
2. Response to reviewer points (#18, pp. 50-51 in the pdf): Why not
move all subsequent analyses to an Appendix or Supplementary Materials
to streamline the text? The Results sections are complete, but could be
shortened, without losing any crucial data.
Authors’ Response: We have considered this at length before, and the main
difficulty we have with moving results to an appendix or supplemental
material is that each of our analyses does appear to be crucial in one way or
another, or to one interest or another. At the least, it appears that their
inclusion in the results section is no more an issue than their movement would
be. We detail these reasons below.
For example, short- and long-distance trials have been used in
previous versions of the task (including our own), and they also form an
important part of our graphical representations of our results. One Reviewer
also asked that we expand on our findings of subtle differences between short-
and long-distance trials. None of the Reviewers appeared to suggest that we
should change our graphical figures, all of which make a distinction between
short- and long-distance trials.
If we removed the analyses by question type (other memory, other
belief, other action), we would have a methods section that motivates the need
for three conditions but makes no distinction between them.
The analyses of first trials was an addition that was included at a
Reviewer’s (very reasonable) request.
The Bayesian analyses are critical in weighing up the strength or
otherwise of null results.
We are also not certain of which analyses the Reviewer is referring to
by ‘subsequent’.
3. Excluding participants’ responses selectively based on whether they
passed the true belief trial is potentially problematic. If participants have
noted the two locations on the screen, but are not following the story,
they will respond randomly to either Location A or Location B. If this
occurs, participants who are responding randomly, but happen to respond
correctly in the True Belief trial will have their data included, while those
who are responding in this manner but happen to respond incorrectly to
the true belief trial will have their data excluded. Although this wouldn’t
create systematic issues if it occurred, it does mean that exclusion in this
manner does not guarantee that participants who were included were
necessarily paying attention to the task. Therefore, it is possible that
respondents were at times, responding randomly, and that these random
responses resulted in “subdivision of space” and “subjective experience of
distance” biases discussed in the Discussion section. The authors use a
computerized version of the Sandbox task, and use online recruitment, a
crucial difference to other published literature, which may have increased
random responses. That said, I do realize that samples from online
recruitment have, in the past, responded similar to lab-based participants.
4. Reviewer 3 made issue of the fact that only one True Belief trial was
included. Although we are sensitive to the fact that this issue can’t feasibly
be addressed without running another experiment, it remains
unaddressed. The issue is that someone can adopt a response strategy
while still passing the true belief trial. Someone who is sensitive to
changes in wording will notice when a true belief trial is present without
having to process the entirety of the vignette. As such, it is better to
include multiple true belief trials, so that bias in the true belief trials can
be compared directly to both false belief trials and memory control trials.
This comparison allows for a better control than blanket exclusion based
on a single trial, and arguably makes it harder (but still not impossible) for
participants to adopt a response strategy based on something other than
reasoning about the character’s beliefs. That said, we realize that the
authors likely chose a single true-belief attention check because that has
been the most common procedure used with the Sandbox task to date.
Authors’ Response: We have added a footnote (footnote 6, page 25) in the
General Discussion to address these two points. We repeat it here for convenience.
“One reviewer suggested that our attention check trial would not filter out
participants who were responding randomly and made a ‘false hit’, or who were
responding strategically and opted to indicate points on the trough close to Location
A except when they perceived some change in the wording of the critical question
(without necessarily comprehending the question). However, random responding
would have been picked up across performance on experimental trials, and would
indexed by an average location approximately halfway between Locations A and B,
but this was not the case. As for strategic responding, we see no reason to believe that
participants opted to base their responses on changes in the perceptual patterns of
words on the screen rather than simply reading the instructions. We would also like
to highlight that our analysis of first trials corroborate our overall pattern of results.
Ultimately, our attention check procedure means our results are less susceptible to
such criticisms than those from studies which included participants’ data regardless
of their performance on the attention check trial.”
Reviewer: 1
Comments to the Author(s)
The authors have addressed my comments and I think the manuscript is
much improved by their work to address all the reviewers' points.
Response: We are very grateful to the Reviewer for their words.
FINAL POINTS:
We have removed one line from the very first paragraph of the introduction as
the reference was missing, and it was felt that given the comments about
length that it would be better to remove it as it was not essential:
“Even when we can verify it with our own eyes, we have difficulty
recognising that others cannot see what we can see [4].”
We have also made a subtle change to the Title, changing it from
“The Unreliability of egocentric bias in the Sandbox Task”
To
“The unreliability of egocentric bias across self-other and memory-belief
distinctions in the Sandbox Task.”
We feel this is a more informative title.
Society Open
