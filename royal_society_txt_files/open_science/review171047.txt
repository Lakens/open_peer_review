Beyond ‘significance’: principles and practice of the
Analysis of Credibility
Robert A. J. Matthews
Article citation details
R. Soc. open sci. 4: 171047.
http://dx.doi.org/10.1098/rsos.171047
Review timeline
Original submission: 2 August 2017 Note: Reports are unedited and appear as
Revised submission: 15 November 2017 submitted by the referee. The review history
Final acceptance: 6 December 2017 appears in chronological order.
Review History
label_version_1
RSOS-171047.R0 (Original submission)
label_author_1
Review form: Reviewer 1 (Leonhard Held)
Is the manuscript scientifically sound in its present form?
Yes
Are the interpretations and conclusions justified by the results?
Yes
Is the language acceptable?
Yes
Is it clear how to access all supporting data?
Not Applicable
Do you have any ethical concerns with this paper?
No
Have you any concerns about statistical analyses in this paper?
No
© 2017 The Authors. Published by the Royal Society under the terms of the Creative Commons
Attribution License http://creativecommons.org/licenses/by/4.0/, which permits unrestricted use,
provided the original author and source are credited
2
Recommendation?
label_recommendation_1
Major revision is needed (please make suggestions in comments)
Comments to the Author(s)
label_comment_1
Please see the attached file for my comments. (Appendix A)
label_author_2
Review form: Reviewer 2 (David Colquhoun)
Is the manuscript scientifically sound in its present form?
Yes
Are the interpretations and conclusions justified by the results?
Yes
Is the language acceptable?
Yes
Is it clear how to access all supporting data?
No
Do you have any ethical concerns with this paper?
No
Have you any concerns about statistical analyses in this paper?
No
Recommendation?
label_recommendation_2
Accept with minor revision (please list in comments)
Comments to the Author(s)
label_comment_2
See uploaded file. (Appendix B)
label_end_comment
Decision letter (RSOS-171047)
09-Oct-2017
Dear Professor Matthews,
The editors assigned to your paper ("Beyond “significance”: principles and practice of the
Analysis of Credibility") have now received comments from reviewers. We would like you to
revise your paper in accordance with the referee and Associate Editor suggestions which can be
found below (not including confidential reports to the Editor). Please note this decision does not
guarantee eventual acceptance.
Please submit a copy of your revised paper within three weeks (i.e. by the 01-Nov-2017). If we do
not hear from you within this time then it will be assumed that the paper has been withdrawn. In
exceptional circumstances, extensions may be possible if agreed with the Editorial Office in
advance.We do not allow multiple rounds of revision so we urge you to make every effort to
fully address all of the comments at this stage. If deemed necessary by the Editors, your
3
manuscript will be sent back to one or more of the original reviewers for assessment. If the
original reviewers are not available we may invite new reviewers.
To revise your manuscript, log into http://mc.manuscriptcentral.com/rsos and enter your
Author Centre, where you will find your manuscript title listed under "Manuscripts with
Decisions." Under "Actions," click on "Create a Revision." Your manuscript number has been
appended to denote a revision. Revise your manuscript and upload a new version through your
Author Centre.
When submitting your revised manuscript, you must respond to the comments made by the
referees and upload a file "Response to Referees" in "Section 6 - File Upload". Please use this to
document how you have responded to the comments, and the adjustments you have made. In
order to expedite the processing of the revised manuscript, please be as specific as possible in
your response.
In addition to addressing all of the reviewers' and editor's comments please also ensure that your
revised manuscript contains the following sections as appropriate before the reference list:
• Ethics statement (if applicable)
If your study uses humans or animals please include details of the ethical approval received,
including the name of the committee that granted approval. For human studies please also detail
whether informed consent was obtained. For field studies on animals please include details of all
permissions, licences and/or approvals granted to carry out the fieldwork.
• Data accessibility
It is a condition of publication that all supporting data are made available either as
supplementary information or preferably in a suitable permanent repository. The data
accessibility section should state where the article's supporting data can be accessed. This section
should also include details, where possible of where to access other relevant research materials
such as statistical tools, protocols, software etc can be accessed. If the data have been deposited in
an external repository this section should list the database, accession number and link to the DOI
for all data from the article that have been made publicly available. Data sets that have been
deposited in an external repository and have a DOI should also be appropriately cited in the
manuscript and included in the reference list.
If you wish to submit your supporting data or code to Dryad (http://datadryad.org/), or modify
your current submission to dryad, please use the following link:
http://datadryad.org/submit?journalID=RSOS&manu=RSOS-171047
• Competing interests
Please declare any financial or non-financial competing interests, or state that you have no
competing interests.
• Authors’ contributions
All submissions, other than those with a single author, must include an Authors’ Contributions
section which individually lists the specific contribution of each author. The list of Authors
should meet all of the following criteria; 1) substantial contributions to conception and design, or
acquisition of data, or analysis and interpretation of data; 2) drafting the article or revising it
critically for important intellectual content; and 3) final approval of the version to be published.
All contributors who do not meet all of these criteria should be included in the
acknowledgements.
We suggest the following format:
AB carried out the molecular lab work, participated in data analysis, carried out sequence
alignments, participated in the design of the study and drafted the manuscript; CD carried out
4
the statistical analyses; EF collected field data; GH conceived of the study, designed the study,
coordinated the study and helped draft the manuscript. All authors gave final approval for
publication.
• Acknowledgements
Please acknowledge anyone who contributed to the study but did not meet the authorship
criteria.
• Funding statement
Please list the source of funding for each author.
Once again, thank you for submitting your manuscript to Royal Society Open Science and I look
forward to receiving your revision. If you have any questions at all, please do not hesitate to get
in touch.
Yours sincerely,
Alice Power
Editorial Coordinator
Royal Society Open Science
on behalf of Mark Chaplain
Subject Editor, Royal Society Open Science
openscience@royalsociety.org
Comments to Author:
Reviewers' Comments to Author:
Reviewer: 1
Comments to the Author(s)
Please see the attached file for my comments.
Reviewer: 2
Comments to the Author(s)
See uploaded file.
Author's Response to Decision Letter for (RSOS-171047)
See Appendix C.
label_version_2
RSOS-171047.R1 (Revision)
label_author_3
Review form: Reviewer 1 (Leonhard Held)
Is the manuscript scientifically sound in its present form?
Yes
5
Are the interpretations and conclusions justified by the results?
Yes
Is the language acceptable?
Yes
Is it clear how to access all supporting data?
Not Applicable
Do you have any ethical concerns with this paper?
No
Have you any concerns about statistical analyses in this paper?
No
Recommendation?
label_recommendation_3
Accept as is
Comments to the Author(s)
label_comment_3
I am happy with this revision, thanks for a much inspiring paper. Just a minor catch: In reference
30 parts of author and title are missing.
label_end_comment
Decision letter (RSOS-171047.R1)
06-Dec-2017
Dear Professor Matthews,
I am pleased to inform you that your manuscript entitled "Beyond “significance”: principles and
practice of the Analysis of Credibility" is now accepted for publication in Royal Society Open
Science.
You can expect to receive a proof of your article in the near future. Please contact the editorial
office (openscience_proofs@royalsociety.org and openscience@royalsociety.org) to let us know if
you are likely to be away from e-mail contact. Due to rapid publication and an extremely tight
schedule, if comments are not received, your paper may experience a delay in publication.
Royal Society Open Science operates under a continuous publication model
(http://bit.ly/cpFAQ). Your article will be published straight into the next open issue and this
will be the final version of the paper. As such, it can be cited immediately by other researchers.
As the issue version of your paper will be the only version to be published I would advise you to
check your proofs thoroughly as changes cannot be made once the paper is published.
In order to raise the profile of your paper once it is published, we can send through a PDF of your
paper to selected colleagues. If you wish to take advantage of this, please reply to this email with
the name and email addresses of up to 10 people who you feel would wish to read your article.
Please note that Royal Society Open Science will introduce article processing charges for all new
submissions received from 1 January 2018. Charges will also apply to papers transferred to Royal
Society Open Science from other Royal Society Publishing journals, as well as papers submitted
as part of our collaboration with the Royal Society of Chemistry
(http://rsos.royalsocietypublishing.org/chemistry). If your manuscript is submitted and
6
accepted for publication after 1 Jan 2018, you will be asked to pay the article processing charge,
unless you request a waiver and this is approved by Royal Society Publishing. You can find out
more about the charges at http://rsos.royalsocietypublishing.org/page/charges. Should you
have any queries, please contact openscience@royalsociety.org.
On behalf of the Editors of Royal Society Open Science, we look forward to your continued
contributions to the Journal.
Kind regards,
Alice Power
Editorial Coordinator
Royal Society Open Science
openscience@royalsociety.org
on behalf of Mark Chaplain (Subject Editor)
openscience@royalsociety.org
Reviewer comments to Author:
Reviewer: 1
Comments to the Author(s)
I am happy with this revision, thanks for a much inspiring paper. Just a minor catch: In reference
30 parts of author and title are missing.
ppendix A
ments on
ond “significance”: principles and practice of the Analysis of
dibility
Robert A.J. Matthews
s interesting paper makes an intriguing proposal to accompany the
ditional reporting of statistical results (in terms of effect size,
confidence interval and p-value) with an Analysis of Credibility.
paper stands out from much of the current debate on the misuse of
alues as it really proposes something new. Part of the proposal
ction 5) dates back to Matthews (2001b), whereas Section 6 and 7
ear to be new contributions.
ike the concept of intrinsic credibility (for significant findings)
think this should be published (perhaps with some additional
cussion as outlined below). However, the specific development of
concept (intrinsic) credibility for non-significant findings is
onvincing to me. I may have misunderstood things, but it seems that
proposal needs some amendments.
a side comment, I should mention that I had no time to check the
hematical derivations outlined in the Appendix.
ments:
-----
entral feature of the proposed approach is the "critical prior
erval" (CPI): "Only if existing knowledge supports values lying
side the CPI can the claim of statistical significance (or
-significance) be deemed to be credible at the 95% level." But
t is "existing knowledge"? Knowledge from previous studies?
jective assessments from clinicians? A real example with historical
dence would help to make this point clearer. If existing knowledge
es from historical studies, how is the uncertainty of the effect
e estimates incorporated?
m wondering if equations (3) and (5) are really necessary, as they
nothing more that applying (2) and (4), respectively, to ln(U) and
L) with subsequent application of the antilog.
tion 6: I found the "negative" reasoning of the "advocacy CPI" for
-significant findings more difficult to follow than of the
eptical CPI" for significant findings. Specifically, does prior
wledge on effect sizes *larger* than AL really imply that a claim
*non-significance* is credible? This seems counter-intuitive. Perhaps
"advocacy CPI" should be interpreted not "negatively" but
sitively", in the sense that existing knowledge on effect sizes
ide the CPI would make the result significant. Specifically, on
e 10 I can follow the reasoning of the sentence starting "Put
ther way, ..." in line 19-21 but the argument in the preceding
tence seems strange to me.
m a more general perspective, I am wondering if the proposed class
prior distribution for non-significant findings with 95% CPI [0,AL]
really sensible. Intuitively, I would consider a non-significant
ding from a large two-sided p-value to be credible if existing
wledge points to effect sizes in the range [-eps, eps]. Therefore,
eel that the class of prior distributions should be bimodal and
metric around zero, in the spirit of non-local priors (Johnson and
sell, 2010, JASA, 143-170). The current proposal is perhaps more
table for one-sided p-values, but all p-values shown in Table 1 are
-sided and the sceptical prior for significant findings is also
metric so designed for two-sided p-values.
discussion of small versus large trials needs perhaps some
inement. For example, on page 9 the author states
st researchers would nevertheless regard the outcome of L1 as in
e sense more convincing, on the grounds that it is based on a much
ger trial. Thus intuition cannot be justified by appeal to the
alues, as they are the same for both studies."
s is true since the CI for the smaller trial is much larger than
the larger trial. However - without knowledge of the CI - there is
ll some subtle dependence on sample size of the evidence contained
a single p-value (Royall, American Statistician, 1986, 40, 313–315,
d and Ott, American Statistician, 2016, 70, 335-341). Perhaps this
ds some additional discussion (but it is not central, since your
ument is always based on normal priors and posteriors).
e of the entries in Table 1 seem to need more digits to make the
ults reproducible, for example the p-value for study L2 seems to be
.0245 and the lower bound of the 95% CI should be reported as
447 to make the value SL=1.69 reproducible.
tion 7: The derivation of the p-value cut-off (8) is very
riguing. Perhaps a reference to a recent paper
ps://osf.io/preprints/psyarxiv/mky9j,
be published in Nature Human Behaviour, is appropriate. These
hors propose to label results with 0.05>p>0.005 as "suggestive"
her than "significant". This seems to parallel the current proposal
"significant but without intrinsic credibility" versus "intrinsic
dibility", of course with the slightly larger threshold 0.01 rather
n 0.005.
e additional discussion may be useful on the corresponding value of
Bayes factor for H0 vs H1 for a p-value of 0.0127. The lower
it on the Bayes factor under a sceptical local prior is 1/5.43
g. Berger & Sellke, 1987), so not very small. What is the value of
Bayes factor for your sceptical prior? Is it independent of the
alue (since it is always just "non-significant")?
derivation of (8) ignores the uncertainty attached to the observed
ect size M. This needs to be justified. Can you argue that the
ertainty is already incorporated in U and L and hence in the CPI,
you don't need/want to incorporate it twice?
e 8, "intrinsic credibility for non-significant findings": the
uirement M<0 seems like a self-fulfilling prophecy, since this is
case only because you assume that the advocacy limit takes the
m (0, AL).
e 9, line 33: "both S1 and L2" --> "both "S1 and L1"
tion 10: I am wondering how the analysis of credibility outlined
e is related to a meta-analysis of studies S1 and L3.
tion 11: Some discussion of the limitations of the approach might
helpful for the reader. For example, statistical procedures that
duce a p-value but not an effect size with CI (e.g. non-parametric
ts) seem not suitable for AnCred.
e 14, line 44: "is is" --> "is"
e 16, Appendix E: I am really confused here. The advocacy CPI takes
form (0, AL). The following derivation then shows that AL<0 for
. What does this mean? Is the advocacy CPI now from AL to 0 (in
negative part of the real line) or is AL not defined (N/A as in
le 1)? Also, what happens if the result is non-significant, but M >
Is this now credibly non-significant??? Or can you show
hematically that M <= AL always holds?
nhard Held
August 2017
Appendix B
Report on Matthews
This is a timely paper on a topic that is attracting enormous interest at the moment. It
should undoubtedly be published. It could be published more or less as it stands.
But before that is done, I have a few questions which, if acted on, might make the
paper more widely read and influential.
(1) To my distress, I found the paper exceptionally hard to read. I guess it is written
for professional statisticians, not fot users, and that’s fine. The price, though, is that
few users will be able to follow the paper.
(2) I’m quite happy to admit that it is may be my deficiencies that make the paper
hard for me. Although numerate, I’m not a professional statistician, and I have
limited familiarity with Bayesian analysis. Some of the crucial results, eg eq 2, are
given with references to books, which only professionals are likely to possess. It
would help a lot of the references were to papers where step by step derivations are
given, If such papers don’t exist, it might be worth deriving them in an appendix
despite the fact that this would be superfluous for professional statisticians.
(3) I like the reverse Bayesian approach very much, I have advocated a version of it
myself and that was inspired by the author of this paper, R. Matthews. My account is
simpler than that proposed here. I assume the conventional point null, and calculate
the prior that’s needed to get a specified false positive risk. It would be up to the
author to persuade the reader that this prior was reasonable. Personally I’m not
happy about proposing prior distributions (because there is hardly any evidence for
them), and I prefer replications to be independent so I distrust incorporation of prior
information in analysis of experiments. That’s why I prefer my approach, but this field
is so contentious that it doesn’t bear on the publishability of other views. We need all
the views that we can get.
(4) As a consequence of my views about use of prior information, I’m particularly
interested in the section on “Unprecedented findings and intrinsic credibility”. The
value of P = 0.013 seems surprisingly high, compared with the results of other
people. Valen Johnson (Benjamin et al) recommends P = 0.005 as a stop-gap
threshold. According to my approach, even this may not be safe.
According to my calculations, observing P = 0.013, in a well-powered experiment
would require a prior probability of 0.6 in favour of H1 in order to achieve a false
positive risk of 5%. This would be hard to justify. Alternatively, to achieve a 5%
false positive risk, you would need to observe P = 0.008 if the prior probability = 0.5
(prior odds = 1). That’s not far from your number of P = 0.01. But if the prior were
only 0.1 then you’d need to observe P = 0.00045 to achieve an FPR of 5%. These
calculations are done most easily with our web app, at present the url is http://fpr-
calculator.uksouth.cloudapp.azure.com:3838/
(5) The one thing that I’d recommend before publication is addition of a thorough
discussion of the many other approaches that have been proposed to ameliorate the
myth of P = 0.05. For example, the results of J. Berger and of V. Johnson should be
compared, and Benjamin et al has attracted a lot of attention. My own paper has
been at https://www.biorxiv.org/content/early/2017/08/07/144337 since May and has
been downloaded over 11,000 times (it was submitted to RSOS in August). (My
response to Benjamin et al. is on pp 18-19 of the pdf). Your paper describes a more
sophisticated approach, which sounds as though it has much to recommend it. But
readers will be left baffled about how it relates to other proposed approaches. The
numerical inconsistencies mentioned above will cause puzzlement to many people, I
fear. So will the lack of any mention of false positives in your paper, or the distinction
between what I call (rather clumsily) the p-equals and the p-less-than interpretations.
A good discussion would be a valuable contribution to what has become a
contentious area.
Appendix C
Response to referees for paper RSOS-171047
Beyond “significance”: principles and practice of
the Analysis of Credibility
I thank the referees for their stimulating and very helpful comments and suggestions. I am
especially grateful for their encouraging response to this proposal for advancing the long-
standing and sometimes acrimonious debate over significance testing.
Responses to Referee 1
Comment 1: A central feature of the proposed approach is the "critical prior interval" (CPI):
"Only if existing knowledge supports values lying outside the CPI can the claim of statistical
significance (or non-significance) be deemed to be credible at the 95% level." But what is
"existing knowledge"? Knowledge from previous studies? Subjective assessments from
clinicians? A real example with historical evidence would help to make this point clearer. If
existing knowledge comes from historical studies, how is the uncertainty of the effect size
estimates incorporated?
Reply: This is an important point, and I have now addressed it at the end of Section 5
Comment 2: I am wondering if equations (3) and (5) are really necessary, as they are
nothing more that applying (2) and (4), respectively, to ln(U) and ln(L) with subsequent
application of the antilog.
Reply: In view of comments from both the second referee and others concerning the
accessibility of the paper by (non-mathematical) readers, the use of (3) and (5) in derivation
of the results in Table 1, and the resulting loss of clarity in return for a relatively small saving
in space, I feel these should remain.
Comment 3: Does prior knowledge on effect sizes *larger* than AL really imply that a claim
of *non-significance* is credible? This seems counter-intuitive. Perhaps the "advocacy CPI"
should be interpreted not "negatively" but "positively", in the sense that existing knowledge
on effect sizes inside the CPI would make the result significant. Specifically, on page 10 I can
follow the reasoning of the sentence starting "Put another way, ..." in line 19-21 but the
argument in the preceding sentence seems strange to me.
Reply: The original analysis of studies S4 and L4 was confusing, and I have amended it
accordingly.
Comment 4: From a more general perspective, I am wondering if the proposed class of prior
distribution for non-significant findings with 95% CPI [0,AL] is really sensible. Intuitively, I
would consider a non-significant finding from a large two-sided p-value to be credible if
existing knowledge points to effect sizes in the range [-eps, eps]. Therefore, I feel that the
class of prior distributions should be bimodal and symmetric around zero, in the spirit of
non-local priors (Johnson and Rossell, 2010, JASA, 143-170). The current proposal is perhaps
more suitable for one-sided p-values, but all p-values shown in Table 1 are two-sided and
the sceptical prior for significant findings is also symmetric so designed for two-sided p-
values.
Reply: The form of advocacy prior distribution used in AnCred is indeed asymmetric; this
reflects the inherent asymmetry of the concept of advocacy defined in the paper.
Specifically, an advocate believes in the existence of some positive (negative) effect; this is
captured by a distribution whose peak is to the right (left) of the no-effect line, and whose
lower (upper) 95% tail touches that line, consistent with the AnCred model of fair-minded
advocacy. While this model may be more suitable for one-sided p-values, the latter play a
wholly different role in AnCred (as emphasised below). Table 1 uses (conservative) two-
sided p-values solely to illustrate their use as simple measures of the level of credibility
contained within a 95% CI.
Comment 5: The discussion of small versus large trials needs perhaps some refinement. For
example, on page 9 the author states "Most researchers would nevertheless regard the
outcome of L1 as in some sense more convincing, on the grounds that it is based on a much
larger trial. Thus intuition cannot be justified by appeal to the p-values, as they are the same
for both studies." This is true since the CI for the smaller trial is much larger than for the
larger trial. However - without knowledge of the CI - there is still some subtle dependence
on sample size of the evidence contained in a single p-value (Royall, American Statistician,
1986, 40, 313–315, Held and Ott, American Statistician, 2016, 70, 335-341). Perhaps this
needs some additional discussion (but it is not central, since your argument is always based
on normal priors and posteriors).
Reply: I agree that the evidential content of p-values is somewhat tangential to the point
made in this section; nevertheless, I have amended some of the relevant text.
Comment 6: Some of the entries in Table 1 seem to need more digits to make the results
reproducible, for example the p-value for study L2 seems to be p=0.0245 and the lower
bound of the 95% CI should be reported as 1.0447 to make the value SL=1.69 reproducible.
Reply: Agreed; the table has been amended accordingly.
Comment 7: The derivation of the p-value cut-off (8) is very intriguing. Perhaps a reference
to a recent paper https://osf.io/preprints/psyarxiv/mky9j, to be published in Nature Human
Behaviour, is appropriate. These authors propose to label results with 0.05>p>0.005 as
"suggestive" rather than "significant". This seems to parallel the current proposal of
"significant but without intrinsic credibility" versus "intrinsic credibility", of course with the
slightly larger threshold 0.01 rather than 0.005. Some additional discussion may be useful on
the corresponding value of the Bayes factor for H0 vs H1 for a p-value of 0.0127. The lower
limit on the Bayes factor under a sceptical local prior is 1/5.43 (e.g. Berger & Sellke, 1987),
so not very small. What is the value of the Bayes factor for your sceptical prior? Is it
independent of the p-value (since it is always just "non-significant")?
Reply: I have now included a reference to the Benjamin et al paper. As the referee notes,
the authors of that paper have proposed the description of results with 0.005 < p < 0.05 as
“suggestive” . However, the apparent parallel with the AnCred concept of “significant but
lacking intrinsic credibility” is misleading – but very helpful in highlighting the risk of
misinterpretation of the use of p-values in the AnCred methodology. Under AnCred, p-
values flag up studies which require external support in order for their claim of statistical
significance to achieve credibility. This is clearly of relevance for unprecedented “out of the
blue” findings from exploratory studies, where there is no evidence other than that
provided by the study. However, in light of the referees’ comments, I believe that for studies
where prior evidence does exist, the concept of intrinsic credibility is inappropriate, and its
use of p-value thresholds is likely to lead to the misinterpretations and the pass/fail
dichotomisation that AnCred is designed to obviate. I have therefore amended the text,
confining the concept of intrinsic credibility to its original role as a solution to the notorious
“Problem of Priors”, allowing the assessment of unprecedented findings lacking prior
evidence.
Comment 8: The derivation of (8) ignores the uncertainty attached to the observed effect
size M. This needs to be justified. Can you argue that the uncertainty is already incorporated
in U and L and hence in the CPI, so you don't need/want to incorporate it twice?
Reply: The derivation in Appendix D uses the relation U = kL for a CI of (L, U). As such, the
uncertainty in the CI is reflected in the value of k, which is explicitly incorporated into the
statement for M via M = (k + 1)L/2 .
Comment 9: On "intrinsic credibility for non-significant findings": the requirement M < 0
seems like a self-fulfilling prophecy, since this is the case only because you assume that the
advocacy limit takes the form (0, AL).
Reply: The form of the advocacy prior used in AnCred does indeed imply that it is not
possible for advocates of an effect to successfully challenge non-significance if the likelihood
has M < 0 (where the substantive hypothesis is that effect sizes are positive). It is also true
that such a challenge could, however, be mounted by modelling the advocacy using more
complex distributions (eg as generated through elicitation). However, one of the design
goals of AnCred is to provide working researchers with a simple but robust means of
assessing findings. Given that the consequences of M < 0 on AnCred’s assessment of
credibility do not appear unreasonable, I would argue additional sophistication would
produce diminishing returns.
Comment 10: page 9, line 33: "both S1 and L2" --> "both "S1 and L1"
Reply: Fixed.
Comment 11: Section 10: I am wondering how the analysis of credibility outlined here is
related to a meta-analysis of studies S1 and L3.
Reply: This is an excellent point with important ramifications; I have now addressed both it
and a related point in the final two paragraphs of the section.
Comment 12: Section 10: Some discussion of the limitations of the approach might be
helpful for the reader. For example, statistical procedures that produce a p-value but not an
effect size with CI (e.g. non-parametric tests) seem not suitable for AnCred.
Reply: I have amended the final section to include a discussion of limitations.
Comment 13: page 14, line 44: "is is" --> "is"
Reply: Fixed
Comment 14: I am really confused here. The advocacy CPI takes the form (0, AL). The
following derivation then shows that AL<0 for M<0. What does this mean? Is the advocacy
CPI now from AL to 0 (in the negative part of the real line) or is AL not defined (N/A as in
Table 1)? Also, what happens if the result is non-significant, but M > AL? Is this now credibly
non-significant??? Or can you show mathematically that M <= AL always holds?
Reply: As Table 1 indicates – but which should have been clearer – in such cases the AL is
defined, but lies outside the CPI modelling advocacy, and so is not available (N/A) to
advocates of effect sizes > 0. As a result – and as noted at the end of Section 8 – advocates
of an effect can still challenge the credibility of the non-significant finding, but not under the
model of fair-minded advocacy used in AnCred. Instead, they must base their challenge on
a custom-based prior which models their belief – and which, as ever, they can justify in the
light of prior evidence.
On the final point: if a result is non-significant, M cannot exceed AL:
Proof Suppose the non-significant findings are differences summarised by a mean and CI of
M (L, U) with M > 0 and where M = (U + L)/2
Write L = M – k , U = M + k (k > 0) ;
then from eqn (A13) in Appendix E:
AL = – M(U – L)2/UL = 4Mk2/(M + k)(k – M)
Thus for M < AL we require
k2 – M2 < 4k2 so that M2 > -3k2 which is true for all M.
The analogous result for ratios follows via application of the usual log transformation.
Responses to Referee 2
Comment 1: Some of the crucial results, eg eq 2, are given with references to books, which
only professionals are likely to possess. It would help a lot of the references were to papers
where step by step derivations are given, If such papers don’t exist, it might be worth
deriving them in an appendix despite the fact that this would be superfluous for
professional statisticians.
Reply: The books and papers cited in support of the derivations are not difficult to access,
and provide the necessary guidance. In giving derivations of the new relationships in the
current paper, I have necessarily had to strike a balance between step-by-step simplicity,
the readership of the journal, and the constraints of space.
Comment 2: I like the reverse Bayesian approach very much, I have advocated a version of it
myself and that was inspired by the author of this paper, R. Matthews. My account is
simpler than that proposed here. I assume the conventional point null, and calculate the
prior that’s needed to get a specified false positive risk. It would be up to the author to
persuade the reader that this prior was reasonable. Personally I’m not happy about
proposing prior distributions (because there is hardly any evidence for them), and I prefer
replications to be independent so I distrust incorporation of prior information in analysis of
experiments. That’s why I prefer my approach, but this field is so contentious that it doesn’t
bear on the publishability of other views. We need all the views that we can get.
Reply: I am delighted the referee sees merit in the reverse Bayesian approach, which
obviates the potentially contentious incorporation of quantitative prior insight at the outset.
I also wholeheartedly agree there is a need for a variety of approaches to the current crisis
in the use of inference in research.
Comment 3: As a consequence of my views about use of prior information, I’m particularly
interested in the section on “Unprecedented findings and intrinsic credibility”. The value of
P = 0.013 seems surprisingly high, compared with the results of other people. Valen Johnson
(Benjamin et al) recommends P = 0.005 as a stop-gap threshold. According to my approach,
even this may not be safe. According to my calculations, observing P = 0.013, in a well-
powered experiment would require a prior probability of 0.6 in favour of H1 in order to
achieve a false positive risk of 5%. This would be hard to justify. Alternatively, to achieve a 5%
false positive risk, you would need to observe P = 0.008 if the prior probability = 0.5 (prior
odds = 1). That’s not far from your number of P = 0.01. But if the prior were only 0.1 then
you’d need to observe P = 0.00045 to achieve an FPR of 5%. These calculations are done
most easily with our web app, at present the url is
http://fprcalculator.uksouth.cloudapp.azure.com:3838/
Reply: AnCred makes no use of p-values for a point-null hypothesis, and so comparisons
with methods that do are somewhat moot. That a comparison was made, however,
highlights an issue I had not fully appreciated: the extent to which the use of p-values in the
AnCred methodology is open to mis-interpretation. I have therefore amended the text as
described in my reply to Comment 7 of the first referee.
Comment 4: The one thing that I’d recommend before publication is addition of a thorough
discussion of the many other approaches that have been proposed to ameliorate the myth
of P = 0.05. For example, the results of J. Berger and of V. Johnson should be compared, and
Benjamin et al has attracted a lot of attention. My own paper has been at
https://www.biorxiv.org/content/early/2017/08/07/144337 since May and has been
downloaded over 11,000 times (it was submitted to RSOS in August). (My response to
Benjamin et al. is on pp 18-19 of the pdf).
Reply: As requested, I have now included appropriate references on alternatives to p-
values in the final section. However, a “thorough discussion of the many approaches” along
with comparisons and also detailed discussion of Benjamin et al. is, I would argue, both
impracticable and inappropriate in the present paper.
Comment 5: Your paper describes a more sophisticated approach, which sounds as though
it has much to recommend it. But readers will be left baffled about how it relates to other
proposed approaches. The numerical inconsistencies mentioned above will cause
puzzlement to many people, I fear. So will the lack of any mention of false positives in your
paper, or the distinction between what I call (rather clumsily) the p-equals and the p-less-
than interpretations. A good discussion would be a valuable contribution to what has
become a contentious area.
Reply: This relates to the interpretation of p-values under AnCred; I have amended the text
as per my reply to Comment 3.
Society Open
