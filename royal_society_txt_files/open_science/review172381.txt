More than a sum of parts: robust face recognition by
integrating variation
Nadia Menon, Richard I. Kemp and David White
Article citation details
R. Soc. open sci. 5: 172381.
http://dx.doi.org/10.1098/rsos.172381
Review timeline
Original submission: 27 December 2017 Note: Reports are unedited and appear as
Revised submission: 31 March 2018 submitted by the referee. The review history
Final acceptance: 24 April 2018 appears in chronological order.
Review History
label_version_1
RSOS-172381.R0 (Original submission)
label_author_1
Review form: Reviewer 1 (Peter Hancock)
Is the manuscript scientifically sound in its present form?
Yes
Are the interpretations and conclusions justified by the results?
No
Is the language acceptable?
Yes
Is it clear how to access all supporting data?
Yes
Do you have any ethical concerns with this paper?
No
© 2018 The Authors. Published by the Royal Society under the terms of the Creative Commons
Attribution License http://creativecommons.org/licenses/by/4.0/, which permits unrestricted use,
provided the original author and source are credited
2
Have you any concerns about statistical analyses in this paper?
No
Recommendation?
label_recommendation_1
Accept with minor revision (please list in comments)
Comments to the Author(s)
label_comment_1
This is a simple study comparing the effect of learning a face as one or two different people. In
this, it mirrors some work that we did several years ago and could not get coherent results.
Looking at this, I wondered why we didn't do something a simple as this and realised that the
reason is for the obvious confound that people are being asked to learn either about one person or
about two and so there is obviously a greater demand in the two ID case. Our approach was more
subtle and ... didn't work!
So, I'd like to see at least some discussion of the task demands: being presented with two clips
and two names causes the additional encoding work of trying to learn the difference. When then
asked if the probe could be either of them, the task is clearly harder than in the one ID case, as
they have two, weaker memory traces to compare to. The result found certainly isn't surprising;
I'd like to see some effect sizes in amongst the traditional stats.
I wasn't quite clear about the procedure: 'Immediately following this naming phase, the names
and clips were presented in the same sequence as before.' Is this a complete repeat of the training
phase? Was the naming phase repeated after it?
I also don't quite understand the numbers. There were 30 ids, split into three groups of 10. The
dataset shows each score as a multiple of 10%, implying 10 trials per condition. But there were TP
and TA trials, so should there not be only 5 per half condition? Or was each sequence shown
twice, once as TP and once as TA? That would seem to pose other confounds.
Peter Hancock
label_author_2
Review form: Reviewer 2 (Markus Bindemann)
Is the manuscript scientifically sound in its present form?
Yes
Are the interpretations and conclusions justified by the results?
Yes
Is the language acceptable?
Yes
Is it clear how to access all supporting data?
Yes
Do you have any ethical concerns with this paper?
No
Have you any concerns about statistical analyses in this paper?
Yes
3
Recommendation?
label_recommendation_2
Accept with minor revision (please list in comments)
Comments to the Author(s)
label_comment_2
In this study, observers are exposed to a video clip or pair of different clips of the same person,
with pairs associated either with a single identity (Betty-Sue) or dual identities (Betty and Sue).
The results show that performance was best for paired-video single identities, providing evidence
for integration of identify information for face recognition.
This is a short but interesting study, with a neat design, that will make a nice addition to the
literature. I would like to see this work published but have a few suggestions to improve the
manuscript.
1. Sentence 2 and 3 of the introduction would be more convincing if the listed studies provide a
more direct comparison of familiar and unfamiliar faces, that is by listing similar rather than
different factors for familiar faces (image quality and distortions) and unfamiliar faces
(expression and view). For example, there is evidence that image quality (pixelation) reduces
unfamiliar but not familiar face identification (e.g., Bindemann, Attard, Leach, & Johnston, 2013
versus Lander, Bruce & Hill, 2001), or that restriction of viewing time reduces unfamiliar face
identification much more than familiar face recognition (e.g., Ozbek & Bindemann, 2011 versus
Morrison, Bruce, & Burton, 2000).
2. Bindemann and Sandford (2011) also showed that provision of multiple images of an
unfamiliar “target” face improves matching to a simultaneous probe, and did so several years
prior to White et al. (18). Worth a mention on page 4.
3. Descriptives are provided for match and mismatch data in the results section, but not
inferential statistics. This omission is confusing. It also seems important as these conditions
provide notably different results. In addition, these conditions are not mentioned in the
Discussion.
4. More generally, I find the match/mismatch labelling misleading. In the literature these terms
are generally used to describe facial identification task in which memory factors are minimized
(comparisons of simultaneously presented faces). From my reading of the paper, this study is a
recognition task, so I think something like old / new labels would be more consistent with
already-published work.
label_end_comment
Decision letter (RSOS-172381.R0)
14-Mar-2018
Dear Dr Menon
On behalf of the Editors, I am pleased to inform you that your Manuscript RSOS-172381 entitled
"More than a sum of parts: Robust face recognition by integrating variation" has been accepted
for publication in Royal Society Open Science subject to minor revision in accordance with the
referee suggestions. Please find the referees' comments at the end of this email.
The reviewers and handling editors have recommended publication, but also suggest some minor
4
revisions to your manuscript. Therefore, I invite you to respond to the comments and revise your
manuscript.
• Ethics statement
If your study uses humans or animals please include details of the ethical approval received,
including the name of the committee that granted approval. For human studies please also detail
whether informed consent was obtained. For field studies on animals please include details of all
permissions, licences and/or approvals granted to carry out the fieldwork.
• Data accessibility
It is a condition of publication that all supporting data are made available either as
supplementary information or preferably in a suitable permanent repository. The data
accessibility section should state where the article's supporting data can be accessed. This section
should also include details, where possible of where to access other relevant research materials
such as statistical tools, protocols, software etc can be accessed. If the data has been deposited in
an external repository this section should list the database, accession number and link to the DOI
for all data from the article that has been made publicly available. Data sets that have been
deposited in an external repository and have a DOI should also be appropriately cited in the
manuscript and included in the reference list.
If you wish to submit your supporting data or code to Dryad (http://datadryad.org/), or modify
your current submission to dryad, please use the following link:
http://datadryad.org/submit?journalID=RSOS&manu=RSOS-172381
• Competing interests
Please declare any financial or non-financial competing interests, or state that you have no
competing interests.
• Authors’ contributions
All submissions, other than those with a single author, must include an Authors’ Contributions
section which individually lists the specific contribution of each author. The list of Authors
should meet all of the following criteria; 1) substantial contributions to conception and design, or
acquisition of data, or analysis and interpretation of data; 2) drafting the article or revising it
critically for important intellectual content; and 3) final approval of the version to be published.
All contributors who do not meet all of these criteria should be included in the
acknowledgements.
We suggest the following format:
AB carried out the molecular lab work, participated in data analysis, carried out sequence
alignments, participated in the design of the study and drafted the manuscript; CD carried out
the statistical analyses; EF collected field data; GH conceived of the study, designed the study,
coordinated the study and helped draft the manuscript. All authors gave final approval for
publication.
• Acknowledgements
Please acknowledge anyone who contributed to the study but did not meet the authorship
criteria.
• Funding statement
Please list the source of funding for each author.
Please note that we cannot publish your manuscript without these end statements included. We
5
have included a screenshot example of the end statements for reference. If you feel that a given
heading is not relevant to your paper, please nevertheless include the heading and explicitly state
that it is not relevant to your work.
Because the schedule for publication is very tight, it is a condition of publication that you submit
the revised version of your manuscript within 7 days (i.e. by the 23-Mar-2018). If you do not think
you will be able to meet this date please let me know immediately.
To revise your manuscript, log into https://mc.manuscriptcentral.com/rsos and enter your
Author Centre, where you will find your manuscript title listed under "Manuscripts with
Decisions". Under "Actions," click on "Create a Revision." You will be unable to make your
revisions on the originally submitted version of the manuscript. Instead, revise your manuscript
and upload a new version through your Author Centre.
When submitting your revised manuscript, you will be able to respond to the comments made by
the referees and upload a file "Response to Referees" in "Section 6 - File Upload". You can use this
to document any changes you make to the original manuscript. In order to expedite the
processing of the revised manuscript, please be as specific as possible in your response to the
referees. We strongly recommend uploading two versions of your revised manuscript:
1) Identifying all the changes that have been made (for instance, in coloured highlight, in bold
text, or tracked changes);
2) A 'clean' version of the new manuscript that incorporates the changes made, but does not
highlight them.
When uploading your revised files please make sure that you have:
1) A text file of the manuscript (tex, txt, rtf, docx or doc), references, tables (including captions)
and figure captions. Do not upload a PDF as your "Main Document";
2) A separate electronic file of each figure (EPS or print-quality PDF preferred (either format
should be produced directly from original creation package), or original software format);
3) Included a 100 word media summary of your paper when requested at submission. Please
ensure you have entered correct contact details (email, institution and telephone) in your user
account;
4) Included the raw data to support the claims made in your paper. You can either include your
data as electronic supplementary material or upload to a repository and include the relevant doi
within your manuscript. Make sure it is clear in your data accessibility statement how the data
can be accessed;
5) All supplementary materials accompanying an accepted article will be treated as in their final
form. Note that the Royal Society will neither edit nor typeset supplementary material and it will
be hosted as provided. Please ensure that the supplementary material includes the paper details
where possible (authors, article title, journal name).
Supplementary files will be published alongside the paper on the journal website and posted on
the online figshare repository (https://rs.figshare.com/). The heading and legend provided for
each supplementary file during the submission process will be used to create the figshare page,
so please ensure these are accurate and informative so that your files can be found in searches.
Files on figshare will be made available approximately one week before the accompanying article
so that the supplementary material can be attributed a unique DOI.
Please note that Royal Society Open Science will introduce article processing charges for all new
submissions received from 1 January 2018. Charges will also apply to papers transferred to Royal
Society Open Science from other Royal Society Publishing journals, as well as papers submitted
6
as part of our collaboration with the Royal Society of Chemistry
(http://rsos.royalsocietypublishing.org/chemistry).
If your manuscript is newly submitted and subsequently accepted for publication after 1 Jan 2018,
you will be asked to pay the article processing charge, unless you request a waiver and this is
approved by Royal Society Publishing. Manuscripts originally submitted prior to 1 Jan 2018 will
not subject to a charge, even if they are accepted in 2018. You can find out more about the charges
at http://rsos.royalsocietypublishing.org/page/charges. Should you have any queries, please
contact openscience@royalsociety.org.
Once again, thank you for submitting your manuscript to Royal Society Open Science and I look
forward to receiving your revision. If you have any questions at all, please do not hesitate to get
in touch.
Kind regards,
Alice Power
Editorial Coordinator
Royal Society Open Science
openscience@royalsociety.org
on behalf of Dr Isabelle Mareschal (Associate Editor) and Antonia Hamilton (Subject Editor)
openscience@royalsociety.org
Associate Editor Comments to Author (Dr Isabelle Mareschal):
Expert reviewers have read your paper and provide suggestions to improve it. Please provide a
point by point reply to their questions, notably addressing the issue raised by reviewer 1 about
differences in task difficulty.
Reviewer comments to Author:
Reviewer: 1
Comments to the Author(s)
This is a simple study comparing the effect of learning a face as one or two different people. In
this, it mirrors some work that we did several years ago and could not get coherent results.
Looking at this, I wondered why we didn't do something a simple as this and realised that the
reason is for the obvious confound that people are being asked to learn either about one person or
about two and so there is obviously a greater demand in the two ID case. Our approach was more
subtle and ... didn't work!
So, I'd like to see at least some discussion of the task demands: being presented with two clips
and two names causes the additional encoding work of trying to learn the difference. When then
asked if the probe could be either of them, the task is clearly harder than in the one ID case, as
they have two, weaker memory traces to compare to. The result found certainly isn't surprising;
I'd like to see some effect sizes in amongst the traditional stats.
I wasn't quite clear about the procedure: 'Immediately following this naming phase, the names
and clips were presented in the same sequence as before.' Is this a complete repeat of the training
phase? Was the naming phase repeated after it?
7
I also don't quite understand the numbers. There were 30 ids, split into three groups of 10. The
dataset shows each score as a multiple of 10%, implying 10 trials per condition. But there were TP
and TA trials, so should there not be only 5 per half condition? Or was each sequence shown
twice, once as TP and once as TA? That would seem to pose other confounds.
Peter Hancock
Reviewer: 2
Comments to the Author(s)
In this study, observers are exposed to a video clip or pair of different clips of the same person,
with pairs associated either with a single identity (Betty-Sue) or dual identities (Betty and Sue).
The results show that performance was best for paired-video single identities, providing evidence
for integration of identify information for face recognition.
This is a short but interesting study, with a neat design, that will make a nice addition to the
literature. I would like to see this work published but have a few suggestions to improve the
manuscript.
1. Sentence 2 and 3 of the introduction would be more convincing if the listed studies provide a
more direct comparison of familiar and unfamiliar faces, that is by listing similar rather than
different factors for familiar faces (image quality and distortions) and unfamiliar faces
(expression and view). For example, there is evidence that image quality (pixelation) reduces
unfamiliar but not familiar face identification (e.g., Bindemann, Attard, Leach, & Johnston, 2013
versus Lander, Bruce & Hill, 2001), or that restriction of viewing time reduces unfamiliar face
identification much more than familiar face recognition (e.g., Ozbek & Bindemann, 2011 versus
Morrison, Bruce, & Burton, 2000).
2. Bindemann and Sandford (2011) also showed that provision of multiple images of an
unfamiliar “target” face improves matching to a simultaneous probe, and did so several years
prior to White et al. (18). Worth a mention on page 4.
3. Descriptives are provided for match and mismatch data in the results section, but not
inferential statistics. This omission is confusing. It also seems important as these conditions
provide notably different results. In addition, these conditions are not mentioned in the
Discussion.
4. More generally, I find the match/mismatch labelling misleading. In the literature these terms
are generally used to describe facial identification task in which memory factors are minimized
(comparisons of simultaneously presented faces). From my reading of the paper, this study is a
recognition task, so I think something like old / new labels would be more consistent with
already-published work.
Author's Response to Decision Letter for (RSOS-172381.R0)
See Appendices A & B.
8
label_end_comment
Decision letter (RSOS-172381.R1)
24-Apr-2018
Dear Dr Menon,
I am pleased to inform you that your manuscript entitled "More than a sum of parts: Robust face
recognition by integrating variation" is now accepted for publication in Royal Society Open
Science.
You can expect to receive a proof of your article in the near future. Please contact the editorial
office (openscience_proofs@royalsociety.org and openscience@royalsociety.org) to let us know if
you are likely to be away from e-mail contact. Due to rapid publication and an extremely tight
schedule, if comments are not received, your paper may experience a delay in publication.
Royal Society Open Science operates under a continuous publication model
(http://bit.ly/cpFAQ). Your article will be published straight into the next open issue and this
will be the final version of the paper. As such, it can be cited immediately by other researchers.
As the issue version of your paper will be the only version to be published I would advise you to
check your proofs thoroughly as changes cannot be made once the paper is published.
In order to raise the profile of your paper once it is published, we can send through a PDF of your
paper to selected colleagues. If you wish to take advantage of this, please reply to this email with
the name and email addresses of up to 10 people who you feel would wish to read your article.
Please note that Royal Society Open Science will introduce article processing charges for all new
submissions received from 1 January 2018. Charges will also apply to papers transferred to Royal
Society Open Science from other Royal Society Publishing journals, as well as papers submitted
as part of our collaboration with the Royal Society of Chemistry
(http://rsos.royalsocietypublishing.org/chemistry). If your manuscript is submitted and
accepted for publication after 1 Jan 2018, you will be asked to pay the article processing charge,
unless you request a waiver and this is approved by Royal Society Publishing. You can find out
more about the charges at http://rsos.royalsocietypublishing.org/page/charges. Should you
have any queries, please contact openscience@royalsociety.org.
On behalf of the Editors of Royal Society Open Science, we look forward to your continued
contributions to the Journal.
Kind regards,
Andrew Dunn
Royal Society Open Science
openscience@royalsociety.org
on behalf of Dr Isabelle Mareschal (Associate Editor) and Antonia Hamilton (Subject Editor)
openscience@royalsociety.org
Appendix A
Nadia Menon
School of Psychology
University of New South Wales
Sydney, NSW, 2052
Email: n.menon@unswalumni.com
2 April 2018
Editorial Office
Royal Society Open Science
Dear Dr Power and Dr Mareschal
Please find enclosed our revised manuscript “More than a sum of parts: Robust face recognition by integrating
variation”. This paper has been revised in light of reviewer comments, and we have appended point by point
responses to these comments below.
Whenever appropriate, we have made changes in the manuscript to address the reviewers’ comments, as well as
changes to improve the clarity of writing. The authors have all approved these changes and agree that they have
substantially improved the paper,. We hope that it will be now suitable for publication in Royal Society Open
Science. I look forward to your reply.
Best Regards,
Nadia Menon
Author’s response to reviewers: RSOS-172381
Reviewer 1
1. I'd like to see at least some discussion of the task demands: being presented with two clips and two
names causes the additional encoding work of trying to learn the difference. When then asked if the probe
could be either of them, the task is clearly harder than in the one ID case, as they have two, weaker
memory traces to compare to. The result found certainly isn't surprising;
The reviewer suggests is that the 2ID condition is more difficult, as it would result in 2 weaker memory traces,
presumably compared to a single stronger memory trace in the 1ID condition. This is precisely the issue that our
study sought to clarify in this experiment. We hypothesised that participants would integrate the 2 clips in the
1ID condition into a single representation. However, another equally likely possibility is that participants in this
condition would continue to represent both clips separately, compare them separately to the probe image, and
make a decision based on a single clip just as they were expected to do in the 2ID condition. Therefore, the
improved performance in the 1ID compared to the 2ID condition refutes this possibility, and suggests that
information from both clips was integrated into a single representation. We have rewritten sections of the
introduction to make the distinction between the 2 explanations clearer:
Critically however, identification improvements resulting from exposure to a larger range of face images may
be due to the increased probability of obtaining a single image which is particularly useful for identification
(for example, showing similar pose or expression to the probe photograph). According to this explanation,
participants could represent each target instance separately, and only rely on information from this single
“best” image when making a decision, while disregarding the other target images. Therefore, previous studies
do not confirm that experience of variation per se. is driving face learning. This conclusion depends on
demonstrating that the advantage associated with experiencing a larger set of images of a target face is driven
by integrating the variance across images, and not simply by the increased likelihood of obtaining an
informative image. (Page 5, line 11).
Admittedly, there is a possibility that participants in the 1ID conditions could complete the task by only
choosing to attend to only one clip, which would make this condition easier. We have added the following
paragraph to my discussion to address this possibility:
There are other possible reasons why performance might have improved when the target clips were presented
as a single identity. For example, when participants knew that both clips would be of a single target, they could
potentially complete the task by choosing to attend to only one of the clips. On the other hand, when the two
clips were presented as two target identities participants would have to attend to both clips as they thought the
probe might be a match for either target. Therefore, the task may have been less demanding in the 1ID than the
2IDcondition. However, our data do not support this interpretation. Regardless of the identity attributions made
to the clips, accuracy improved when two target clips were presented compared to when only one target clip
was available, suggesting that both target clips were attended to in both conditions.. (Page 13, line 20).
2. I'd like to see some effect sizes in amongst the traditional stats.
Effect sizes have been added for all comparisons.
3. I wasn't quite clear about the procedure: 'Immediately following this naming phase, the names and
clips were presented in the same sequence as before.' Is this a complete repeat of the training phase? Was
the naming phase repeated after it?
There was an initial training phase, a naming phase, and then a third phase which was identical to the initial
training phase, after which the probe photograph was presented, and the participants made the matching
decision. We have rewritten sections of the procedure to make this clear.
Immediately following this naming phase, the names and clips were presented in the same sequence as they had
been presented in the initial training phase (Page 9, line 4).
4. I also don't quite understand the numbers. There were 30 ids, split into three groups of 10. The dataset
shows each score as a multiple of 10%, implying 10 trials per condition. But there were TP and TA trials,
so should there not be only 5 per half condition? Or was each sequence shown twice, once as TP and once
as TA? That would seem to pose other confounds.
There were 30 target identities, 10 of which appeared in each condition. Each target identity appeared in both a
match and a mismatch trial, so there were 20 trials (10 match trials and 10 mismatch trials) in each condition. To
deal with possible confounds the order of identities and trials within each condition was fully randomised for
each participant. We have rewritten sections of the procedure to make this clear:
There were sixty trials in total, separated into 20 trial blocks by condition (in each condition participants would
complete one match and one mismatch trial for each of 10 identities). The order of trials on each block was fully
randomised. (Page 9, line 13).
Reviewer 2
1. Sentence 2 and 3 of the introduction would be more convincing if the listed studies provide a more
direct comparison of familiar and unfamiliar faces, that is by listing similar rather than different factors
for familiar faces (image quality and distortions) and unfamiliar faces (expression and view). For
example, there is evidence that image quality (pixelation) reduces unfamiliar but not familiar face
identification (e.g., Bindemann, Attard, Leach, & Johnston, 2013 versus Lander, Bruce & Hill, 2001), or
that restriction of viewing time reduces unfamiliar face identification much more than familiar face
recognition (e.g., Ozbek & Bindemann, 2011 versus Morrison, Bruce, & Burton, 2000).
We have now rewritten the start of the article to focus in depth on two studies which directly compare
performance matching familiar and unfamiliar faces:
We are adept at recognising the faces of people we know, but this expertise does not extend to the faces of
people we are not familiar with. Instead, unfamiliar face recognition is greatly impaired under conditions in
which familiar face recognition remains robust. The gap between familiar and unfamiliar face recognition
performance was demonstrated in a study by Bruce (1), who asked participants to study a series of face
photographs, and then to recognise these faces in a subsequent test phase. In one condition, viewing angle and
facial expression was changed between the study and test images. For familiar faces, these changes did not
affect recognition accuracy, whilst for unfamiliar faces they resulted in a large drop in accuracy.
In another study (2) participants matched the identities in face photographs to poor quality CCTV images.
Participants who were personally familiar with the people in these images performed at around 90% accuracy,
while participants who were not familiar with them were significantly less accurate. Importantly, the set of
images being matched were the same for both groups of participants, and so the differences between their
performances cannot be attributed to differences in stimuli or task. Apparently, there is something about being
familiar with a face that transforms the nature of face identification, and this transformation results in
significant benefits to performance on identification tasks. (Page 3, line 2).
2. Bindemann and Sandford (2011) also showed that provision of multiple images of an unfamiliar
“target” face improves matching to a simultaneous probe, and did so several years prior to White et al.
(18). Worth a mention on page 4.
We now include a reference to this study (17). (Page 5, line 4).
3. Descriptives are provided for match and mismatch data in the results section, but not inferential
statistics. This omission is confusing. It also seems important as these conditions provide notably different
results. In addition, these conditions are not mentioned in the Discussion.
We have added an analysis match and mismatch data to the results section. These results support the analysis of
overall accuracy reported in the previous version, and further specify the locus of the effect:
We also analysed accuracy separately for match and mismatch trials (Table 1). For match trials, the main effect
of condition was significant, F (2,214) = 27.02, MSE = 215.4, p < .001, <U+03B7>_p^2 = .202. Planned comparisons for
match trial performance mirrored the pattern evident in overall accuracy, whereby participants were more
accurate in the 1ID condition compared to the 1CLIP condition F (1,107) = 50.88, MSE = 457.5, p < .001,
<U+03B7>_p^2 =.322, as well as in the 2ID condition compared to the 1CLIP condition, F (1,107) = 14.75, MSE =
422.2, p <. 001, <U+03B7>_p^2 = .121. Critically, accuracy was higher in the 1ID than the 2ID condition, F (1,107) =
13.14, MSE = 412.9, p <. 001, <U+03B7>_p^2 = .109, showing that identity attributions affected performance on match
trials.
For mismatch trials, the main effect of condition was not significant, F (2,214) = 2.332, MSE = 92.21, p = .100,
<U+03B7>_p^2 = .021. Planned comparisons revealed that mismatch accuracy was greater in the 2ID compared to the
1CLIP condition F (1,107) = 5.515, MSE = 151.1, p = .021, <U+03B7>_p^2 = .049, but not in the 1ID compared to the
1CLIP condition F (1,107) = 1.697, MSE = 211.1, p = .196, <U+03B7>_p^2 = .016. Mismatch accuracy was not affected
by identity attributions, with no differences evident between the 1ID and 2ID conditions, F (1,107) = .517, MSE
= 191.1, p = .474, <U+03B7>_p^2 = .005.. (Page 11, line 4).
In the discussion we now explain why the 1ID condition might have specifically benefitted match accuracy:
The benefit of presenting the two clips as a single rather than dual identity was evident on match trials, with
accuracy on mismatch trials unaffected by identity attributions. In other words, when the clips were presented
as one target identity, participants were specifically more likely to correctly decide that the probe image was the
same person as the target. This pattern suggests that the memory representations generated in the one-identity
condition were more tolerant of within-face variance in appearance, between the target clips and the probe
image. This interpretation is consistent with the conclusion that participants in this condition integrated the
variance across the clips into their representation of the target. It is also consistent with previous research
demonstrating that increased familiarity with a face engenders tolerance of variance across images of that face
(13). (Page 13, line 1).
4. More generally, I find the match/mismatch labelling misleading. In the literature these terms are
generally used to describe facial identification task in which memory factors are minimized (comparisons
of simultaneously presented faces). From my reading of the paper, this study is a recognition task, so I
think something like old / new labels would be more consistent with already-published work.
We have chosen to retain this labelling, to remain consistent with our previous publications (19, 23). In those
papers, tasks with a gap between the target and probe were described as sequential matching tasks, and so we
referred to “match” and “mismatch” trials. Maintaining consistency between these papers provides greater
clarity for readers of all papers.
FACE RECOGNTION BY INTEGRATING VARIATION
Appendix B
More than a sum of parts: Robust face recognition by integrating variation
Nadia Menon1, Richard I. Kemp1, David White1
1
School of Psychology, UNSW, Kensington, Sydney, Australia, 2052.
CORRESPONDING AUTHOR
Nadia Menon
n.menon@unswalumni.com
RUNNING HEAD: FACE RECOGNTION BY INTEGRATING VARIATION
1
FACE RECOGNTION BY INTEGRATING VARIATION
ABSTRACT
Familiarity incrementally improves our ability to identify faces. It has been hypothesised that
this improvement reflects the refinement of memory representations which incorporate
variation in appearance across encounters. Although it is established that exposure to
variation improves face identification accuracy, it is not clear how variation is assimilated
into internal face representations. To address this, we used a novel approach to isolate the
effect of integrating separate exposures into a single identity representation. Participants (n =
113) were exposed to either a single video clip or a pair of video clips of target identities.
Pairs of video clips were presented as either a single identity (associated with a single name
e.g. Betty-Sue), or dual identities (associated with two names, e.g. Betty, and Sue). Results
show that participants exposed to pairs of video clips showed better matching performance
compared with participants trained with a single clip. More importantly, identification
accuracy was higher for faces presented as single identities compared to faces presented as
dual identities. This provides the first direct evidence that the integration of information
across separate exposures benefits face matching, thereby establishing a mechanism that may
explain people’s impressive ability to recognise familiar faces.
KEYWORDS
Face recognition, Identification, Familiarity, Memory integration,
2
FACE RECOGNTION BY INTEGRATING VARIATION
INTRODUCTION
We are adept at recognising the faces of people we know, but this expertise does not extend
to the faces of people we are not familiar with. Instead, unfamiliar face recognition is greatly
impaired under conditions in which familiar face recognition remains robust. . Familiar face
recognition is robust The gap between familiar and unfamiliar face recognition performance
was demonstrated in a study by Bruce (1), who asked participants to study a series of face
photographs, and then to recognise these faces in a subsequent test phase. In one condition,
viewing angle and facial expression was changed between the study and test images. For
familiar faces, these changes did not affect recognition accuracy, whilst for unfamiliar faces
they resulted in a large drop in accuracy.
In another study (2) participants matched the identities in face photographs to poor quality
CCTV images. Participants who were personally familiar with the people in these images
performed at around 90% accuracy, while participants who were not familiar with them were
significantly less accurate. Importantly, the set of images being matched were the same for
both groups of participants, and so the differences between their performances cannot be
attributed to differences in stimuli or task. Apparently, there is something about being
familiar with a face that transforms the nature of face identification, and this transformation
results in significant benefits to performance on identification tasks.despite impoverished
information, such as when viewing poor quality images or images subjected to substantial
distortions in aspect ratio (1, 2). However, this visual expertise is not apparent with the faces
of unfamiliar individuals (3, 4); unfamiliar face identification is greatly affected by changes
in superficial aspects of the image, such as facial expression and viewing angle. (3)
3
FACE RECOGNTION BY INTEGRATING VARIATION
Research has repeatedly demonstrated thatSeveral other studies have had similar outcomes,
reporting marked increases in familiarity transforms our ability to identify faces, dramatically
increasing identification accuracy as a result of familiarity with a face (3, 4). (5-7). Brief,
laboratory-based familiarisation procedures, which involve exposing participants to videos or
photographs of previously unfamiliar faces, alsoalso improve subsequent identification
performance (5 - 78, 9). Indeed, face matching performance improves incrementally with
increased familiarity, leading to the suggestion that a viewer’s performance on matching
tasks can provide an index of their familiarity with a face (10 - 128-10).
The content of the memory representations which that result in this produce this
improvementd performance remains unclear. However, it is clear that such representations
must tolerate significant variation in appearance, as the face of a single individual can look
considerably different across encounters. Burton et al. (113) describe the sources of this
within-face variability as firstly reflecting changes in: (i) person-related factors, including
changes in hairstyle, facial expression, or changes related to age. Additionally, images of a
face may vary due to changes in and (ii) image-capture related conditionsfactors, such as
camera angle, camera to subject distance, or lighting conditions (113). A recent study has
demonstrated that components of this variation, for example the nature of changes in
expression, make-up, or facial hair, carry identity information that is specific to individual
identities (124). This latter finding suggests that variation in appearance itself may be
informative of facial identity, leading to the suggestion that person-specific variation may be
encoded in memory representations of familiar faces.
Supporting this suggestion, when faces are unfamiliar, variation due to changes in appearance
is often mistakenly interpreted as reflecting changes in identity. For example, when shown an
4
FACE RECOGNTION BY INTEGRATING VARIATION
array consisting of several photographs of multiple unfamiliar identities, participants
consistently overestimate the number of identities present in the array (135 – 157). This
tendency is reduced by previous exposure to the faces, suggesting that familiarity engenders
tolerance to variation across encounters with a face (135).
Further, face learning appears to be driven by exposure to variation. Studies have shown that
experience of within-face variation across instances of a face improves face-identification.
White et al. (168) found that if more than one image was provided of an unfamiliar “target”
face, participants were better able to decide if a simultaneously presented “probe” photo was
the same person as the target. Other studies have demonstrated that participants were better at
finding a target face in a gallery of photos when given multiple photos of the target, (171,
189) and also show improved recognition memory performance as a function of the number
of different photographs encountered during learning (146). Recently, Menon et al. (1920)
demonstrated that learning an unfamiliar face from a highly variable set of photographs,
rather than a less variable set, improved the ability to subsequently identify that face [see also
Ritchie & Burton (201)].
Critically however, identification improvements resulting from exposure to a larger range of
face images may be due to the increased probability of obtaining a single image which is
particularly useful for identification (for example, showing similar pose or expression to the
probe photograph). According to this explanation, participants could represent each target
instance separately, and only rely on information from this single “best” image when making
a decision, while disregarding the other target images. Therefore, previous studies do not
confirm that experience of variation per se. is driving face learning. per se. This conclusion Formatted: Font: Italic
depends on demonstrating that the advantage associated with experiencing a larger set of
5
FACE RECOGNTION BY INTEGRATING VARIATION
images of a target face is driven by integrating the variance across images, and not simply by Formatted: Font: Italic
the increased likelihood of obtaining an informative imageCritically however, identification
improvements resulting from exposure to a larger range of face images may be due to the
increased probability of obtaining a single photo which is particularly useful for identification
(for example, showing similar pose or expression to the probe photograph). Therefore,
previous studies do not confirm that experience of variation is driving face learning per se.
This conclusion depends on demonstrating that the advantage associated with experiencing a
larger set of images of a target face is driven by encoding the variance across images, and not
simply by the increased likelihood of obtaining a useful image.
Despite theoretical accounts emphasising the importance of representing within-face variance
(12, 212, 223, 14), very little work explicitly investigates whether information is aggregated
across multiple instances of a face. In the current experiment, we isolate the benefit of
experiencing variation across separate face instances, over and above any benefit inferred
from a particular instance, enabling us to determine whether experiencing variation is itself
critical in the process of familiarisation.
The current study is based onexplores the premise that multiple instances of a face contribute
to a single face representation if they are thought to be a single person, and are represented
separately if they are thought to be different people [see Menon et al. (234)]. We modelled
the process of familiarisation by exposing participants to pairs of video clips that incorporate
rigid (e.g. 3D rotation) and non-rigid (e.g. changes in expression or speech) facial motion.
Given the potential importance of variance in the process of familiarisation, the video clips
used in this study were not taken under controlled experimental conditions, but instead
6
FACE RECOGNTION BY INTEGRATING VARIATION
incorporate natural sources of variability in facial appearance, to better approximate our
encounters with faces in daily life [as suggested by Burton et al. (113)]. We then measured
the benefit of this familiarity to face matching performance, which provides an index of face
familiarity (810, 1012).
Critically, we also manipulated identity attributions to the exposures, by either presenting
pairs of videos of the same face as a single identity (associated with a single name; e.g. Betty-
Sue), or as two identities (associated with different names; e.g. Betty and Sue). If face
representations integrate variation across multiple encounters, as has been suggested by
recent theoretical accounts, (113, 124) then representations encoded as a single identity
should confer more robust familiarity in comparison to representations encoded as dual
identities, and should improve matching performance. In previous work, we have found that
this encoding manipulation affects performance when matching static images (234), and we
extend this investigation here using video clips.
METHOD
Participants
One-hundred and thirteen first year Psychology students (76 female, mean age = 18.86 years)
completed the experiment for course credit.
Stimuli
30 Europe-based minor celebrities were selected as target identities. These identities had been
verified in previous experiments as being unfamiliar to Australian participants. For each
target we collected two video clips of their face, taken from separate television appearances,
and two ‘probe’ photographs. Video clips were chosen by using the target’s name as a search
7
FACE RECOGNTION BY INTEGRATING VARIATION
term in YouTube, and extracting a 2.5 second segment of each clip. Suitable segments were
those in which the target was the only person visible, the face was unobscured, and engaged
in both rigid and non-rigid motion. All other factors were free to vary (e.g. age, hairstyle,
lighting angle, camera-to-subject distance, etc.). Video clips were cropped to a 2:3 aspect
ratio, resized to 724 x 484 pixels and saved without audio.
For each target we also downloaded one still image of their face via Google Image search, in
addition to one photo of a similar-looking ‘foil’ identity (selected by the experimenter from
our face database or Google Image). Photographs were free to vary on the same parameters as
video clips but we ensured that the face was pictured facing straight at the camera. All images
were then cropped to a 2:3 aspect ratio, and scaled to 200 x 300 pixels. Target photographs
appeared as probe images on match trials, and photos of foil identities were probe images on
mismatch trials.
We assigned a double-barrelled name (e.g. Betty-Sue) to each stimulus identity. In the dual-
identity (2ID) condition, the names were associated separately with each target clip (i.e. one
clip was “Betty” and the other “Sue”). In the single-clip (1CLIP) and single-identity (1ID)
conditions, the clips were always associated with the double-barrelled name. Stimulus
identities were randomly allocated to 3 groups. For each participant, each stimulus group
appeared in one of the three experimental conditions, this was counterbalanced across
participants.
Procedure
On each trial, participants initially learnt to associate names with target faces presented in
video clips, before deciding if a probe photograph matched the people shown in the videos.
8
FACE RECOGNTION BY INTEGRATING VARIATION
Clips, names and photos were all presented centrally on a computer monitor. In an initial
training phaseFirst, a target name was presented for 1 second. In 1CLIP and 1ID conditions
this was the full double barrel name (Betty-Sue) and in the 2ID condition this was a single
barrel name (either Betty or Sue). The name was then replaced by the first video clip of that
target and shown for 2.5 seconds. This was followed by a second name and video pairing. In
the 1CLIP condition, participants were presented with the same name and video clip as
before. In the 1ID and 2ID condition, a second video clip was presented, but in the 1ID
condition it was precedaccompanied by the same name as on the first presentation (e.g. Betty-
Sue) and in the 2ID condition it was preceded by the alternative single-barrel name (e.g. if
Betty was seen previously, then Sue).
Thus, the same clips appeared in the 1ID and 2ID conditions, however in the 2ID condition
they were (misleadingly) presented as two different people (Figure 1). The order of clip
presentation in the 1ID and 2ID conditions was counterbalanced across participants.
Immediately after the second clip was presented, the naming phase began. In the naming
phase, the first clip was shown again, followed by a text input box, into which participants
entered the name associated with that clip. They clicked a “continue” button when done. This
sequence was then repeated for the second clip.
Immediately following this naming phase, the names and clips were presented in the same
sequence as they had been presented in the initial training phase before. The probe image was
then presented and remained on screen until a response was made. In the 1CLIP and 1ID
condition, participants decided whether the probe photo was the same identity as the target
shown in the previous clips (i.e. is this Betty-Sue?). In the 2ID condition, they decided if the
9
FACE RECOGNTION BY INTEGRATING VARIATION
probe matched either target (i.e. is this either Betty or Sue?). Therefore, in all three
conditions participants were only required to make a “yes” or “no” response. Matching Formatted: Font: Italic
Formatted: Font: Italic
decisions were self-paced, with accuracy emphasised.
There were sixty trials in total, separated into 20 trial blocks by condition (in each condition
participants would complete one match and one mismatch trial for each of 10 identities). The
order of trials on each block was fully randomisedrial order was randomised within block. s,
Block and block order was also randomised for each participant. After the experiment, we
checked awareness of the identity manipulation, by asking participants if they had noticed
anything surprising or unusual during the experiment. Given its importance to the study, an a
priori decision was made to exclude participants who indicated they did not believe the 2ID
manipulation. All participants were then fully debriefed. The experiment took approximately
one hour to complete.
10
FACE RECOGNTION BY INTEGRATING VARIATION
RESULTS
Five participants were excluded prior to analysis based on awareness of the identity
manipulation, resulting in a sample of 108 participants (71 female, mean age 18.87 years).
Overall accuracy for the three conditions is shown in Figure 1. Table 1 shows accuracy in
each condition, separately for match and mismatch trials. Five participants were excluded
prior to analysis based on awareness of the identity manipulation, resulting in a sample of 108
participants (71 female, mean age 18.87 years). We analysed overall accuracy in a one-way
ANOVA with condition (1CLIP, 1ID, 2ID) as a within-subject factor. The main effect of
condition was significant, F (2, 214) = 26. 05, MSE = 72.12, p < .001, <U+07DF><U+0BE3><U+0B36> = .196. Planned
comparisons were conducted between each of the experimental conditions to confirm the
multiple clip benefit (1ID/2ID Vs 1CLIP), and to test for an effect of identity attribution (1ID
Vs 2ID).
11
FACE RECOGNTION BY INTEGRATING VARIATION
Figure 1. Illustration of the video-based face learning procedure (left) and mean overall
accuracy in each experimental condition (right). Error bars represent 1 SEM.
12
FACE RECOGNTION BY INTEGRATING VARIATION
Table 1: Mean (SEM) accuracy (%) for match and mismatch trials in each condition.
1CLIP 1ID 2ID
Match 57.4 (2.14) 72.1 (1.99) 65.0 (2.09)
Mismatch 86.3 (1.17) 88.1 (1.23) 89.1 (1.24)
First, we tested whether the benefit of multiple target instances shown in previous studies
(e.g. 168) was also replicated when using video clips rather than still photographs. It is
possible that a single video clip would provide sufficient information about the possible
variation in appearance of the target face, in which case the provision of a second clip would
afford no further benefit to performance. However, matching accuracy in both 1ID, F (1,
107) = 51.39, MSE = 143.1, p < .001, <U+07DF><U+0BE3><U+0B36> = .324, and 2ID conditions, F (1, 107) = 19.61, MSE
= 148.1, p < .001, <U+07DF><U+0BE3><U+0B36> = .155, was higher than the 1CLIP condition, confirming the benefit of
presenting multiple clips. More importantly for the aims of this experiment, the benefit was
modulated by identity attributions: accuracy was greater in the 1ID compared to the 2ID
condition, F (1, 107) = 7.174, MSE = 141.51, p = .009, <U+07DF><U+0BE3><U+0B36> = .063.
Table 1: Mean (SEM) accuracy (%) for match and mismatch trials in each condition.
1CLIP 1ID 2ID
Match 57.4 (2.14) 72.1 (1.99) 65.0 (2.09)
Mismatch 86.3 (1.17) 88.1 (1.23) 89.1 (1.24)
We also analysed accuracy separately for match and mismatch trials (Table 1). For match
Formatted: Font: Italic
trials, the main effect of condition was significant, F (2,214) = 27.02, MSE = 215.4, p < .001,
Formatted: Font: Italic
<U+07DF><U+0BE3><U+0B36> = .202. Planned comparisons for match trial performance mirrored the pattern evident in Formatted: Font: Italic
Formatted: Font: Italic
13
FACE RECOGNTION BY INTEGRATING VARIATION
overall accuracy, beingwhereby participants were more accurate in the 1ID condition
compared to the 1CLIP condition F (1,107) = 50.88, MSE = 457.5, p < .001, <U+07DF><U+0BE3><U+0B36> =.322, as Formatted: Font: Italic
Formatted: Font: Italic
well as in the 2ID condition compared to the 1CLIP condition, F (1,107) = 14.75, MSE =
422.2, p <. 001, <U+07DF><U+0BE3><U+0B36> = .121. Identity attributions affected performance on match
trialsCritically, accuracy was higher in the 1ID than the 2ID condition, F (1,107) = 13.14,
MSE = 412.9, p <. 001, <U+07DF><U+0BE3><U+0B36> = .109, showing that ientityidentity attributions affected
performance on match trials.
For mismatch trials, the main effect of condition was not significant, F (2,214) = 2.332, MSE
= 92.21, p = .100, <U+07DF><U+0BE3><U+0B36> = .021. Planned comparisons revealed that mismatch accuracy was
greater in the 2ID compared to the 1CLIP condition F (1,107) = 5.515, MSE = 151.1, p =
.021, <U+07DF><U+0BE3><U+0B36> = .049, but not in the 1ID compared to the 1CLIP condition F (1,107) = 1.697, MSE
= 211.1, p = .196, <U+07DF><U+0BE3><U+0B36> = .016. Mismatch accuracy was not affected by identity attributions,
with no significant differences evident between the 1ID and 2ID conditions, F (1,107) = .517,
MSE = 191.1, p = .474, <U+07DF><U+0BE3><U+0B36> = .005.
.
14
FACE RECOGNTION BY INTEGRATING VARIATION
DISCUSSION
These results clearly demonstrate that information is aggregatedaggregating information
across separate encounters with a face, improving improves the ability to subsequently
identify that face. Firstly, overall identification accuracy was superior after exposure to pairs
of video clips compared to a single clip, , which is consistent with studies using static
imagery (168, 234). Additionally, overall although identical clips and images appeared in the
single-identity and dual-identity conditions, accuracy was higher when naming information
encouraged the belief that these faces were of one person, despite the fact that the clips and
images presented to participants were identical across both conditions. Therefore, it appears
that integrating exposures into a unitary representation conferred an additional benefit,
compared to encoding precisely the same information, but as two separate identities.
The results of this experiment provide is is direct evidence that abstractive identity-level
representations combine variation across multiple exposures to support face matching.
Previous studies have shown that exposure to variation improves accuracy in face
identification tasks, (16-188, 19) with cumulative benefits observed with increasing exposure
(146). However, these earlier studies did not establish that the improvements depended on
integrating information across multiple encounters with the target face. Here we have
demonstrated that familiarity-based improvements in face identification confer benefits above
and beyond the individual exposures contributing to this familiarity.
The benefit of presenting the two clips as a single rather than dual identity, specifically
improved performancewas evident on match trials in particular, rather than on mismatch
trials;, with accuracy on mismatch performance wastrials unaffected by identity attributions..
15
FACE RECOGNTION BY INTEGRATING VARIATION
In other words, participants in the one-identity condition were more likely to correctly when
the clips were presented as one target identity, participants were specifically more likely to
decidecorrectly decide that the probe image was the same person as the target. This
patternresult is evidencesuggests that the memory representations generated in the one-
identity condition were more tolerant of within-face variance in appearance, between the
target clips and the probe image. This interpretation is consistent with the conclusion that
participants in this condition integrated the variance across the clips into their representation
of the target. It is also consistent with previous research demonstrating that increased
familiarity with a face engenders tolerance of variance across images of that face (13).
Recent work has shown that components of variation across different images of the same face
encode idiosyncratic parameters that are specific to particular faces (124). This mirrors work
aimed at improving automatic face recognition systems, where gains in performance have
been attained by building identity-specific subspaces that encode variation specific to an
individual (25). Our results show that integration of within-person variation also drives
accuracy in human subjects, supporting theoretical accounts of face representations that
model familiarity as a process of aggregating variation (113, 124).
There are other possible reasons why performance might have improved when the target clips
were presented as a single identity. For example, when participants knew that both clips
would be of a single target, they could potentially complete the task by choosing to attend to
only one of the clips. On the other hand, when the two clips were presented as two target
identities participants would have to attend to both clips as they thought the probe might be a
match for , in order to decide if the probe matched either target. Therefore, the task may have Formatted: Font: Italic
16
FACE RECOGNTION BY INTEGRATING VARIATION
been less demanding in the former1ID than the 2IDcondition. However, our data do not
support this interpretation. Regardless of the identity attributions made to the clips, accuracy
improved when two target clips were presented compared to when only one target clip was
available, suggesting that both target clips were attended to in both conditions.
Recent work has shown that components of variation across different images of the same face
encode idiosyncratic parameters that are specific to particular faces (14). This mirrors work
aimed at improving automatic face recognition systems, where gains in performance have
been attained by building identity-specific subspaces that encode variation specific to an
individual (25). Our results show that integration of within-person variation also drives
accuracy in human subjects, supporting theoretical accounts of face representations that
model familiarity as a process of aggregating variation (13, 14).
It is worth considering why single-identity encoding improved accuracy in the present study,
which employed video clips, but not in a previous study, which used static images (2423).
There is evidence that motion benefits face learning (86, 256, 267) possibly over the
contribution of multiple static images (278). Motion may improve face identification because
it facilitates the representation of 3D facial structure (278) or the facial motions which are
specific to a particular identity may themselves be incorporated into our representations of
that face (267, 289). In either case, representations based on multiple exposures to a moving
face should be richer and more informative than those based on two static images, and so
more likely to confer a clear advantage to matching.
An additional possibility is that motion itself is necessary for face, as opposed to image, Formatted: Indent: First line: 0"
representation. Thornton and Kourtzi (267) have proposed that, at least in working memory,
17
FACE RECOGNTION BY INTEGRATING VARIATION
our representations for dynamic and static faces are qualitatively different. Pike et al. (278)
make the point that, by their nature, photographs encourage pictorial representation, whilst
dynamic objects may force the extraction of “object-centred” representation, or identity-
specific representations, in the case of faces. By this logic, a face would need to be viewed in
motion, in order to engage the processes responsible for aggregated representation, as
described by Bruce and Young (221). In support of this, Wallis et al. (2930) demonstrated
that motion allows for temporal association of different views of an object (in this case a
face), which allows these views to be categorised as the same identity. Outside the laboratory,
the faces we encounter are generally moving. Therefore experiencing the target identities in
motion in this study might have allowed for the formation of a face representation that better
approximated our representations for familiar faces, and so provided a benefit to matching
performance. In future work, it will be important to examine the role of motion in the
formation of integrative representations across multiple encounters with a face.
In summary, we have demonstrated that identification accuracy is improved with increased
experience of a face. We designed a novel method to isolate the effect of integrating variation
across multiple face instances, and found that familiarity-based improvement was enhanced
by such integration. Notably, asymptotic performance was not observed in any condition and
so future research should explore the trajectory of face familiarity as it unfolds over wider
ranges of experience. We hope that the procedure used here to manipulate identity attribution
can be a useful technique for such research, and for the study of invariant representations in
other object domains.
Ethical Statement
18
FACE RECOGNTION BY INTEGRATING VARIATION
Ethics approval for this study was obtained from the UNSW Human Research Ethics
Advisory Panel (Psychology) HREAP #123-087. Informed consent was obtained from all
participants in the study.
Funding Statement
This research was supported by an Australian Postgraduate Award to Nadia Menon and an
Australian Research Council Linkage grant to Richard Kemp (LP110100448), in partnership
with the Department of Foreign Affairs and Trade (Australian Passport Office).
Data Accessibility
The dataset supporting this article has been uploaded as part of the Supplementary Material.
Competing Interests
We have no competing interests.’
Authors' Contributions
Nadia Menon participated in the conception and design of the study, prepared the stimuli for
the study, collected the data, conducted the data analysis and drafted the manuscript. Richard
Kemp participated in the conception and design of the study, and helped analyse and interpret
the data, and with drafting the manuscript. David White participated in the conception and
design of the study, and helped with the data analysis and interpretation, and with drafting the
manuscript. All authors gave final approval for publication.
Acknowledgements Formatted: Font: Bold
Other than the authors, there were no other contributors to this study.
19
FACE RECOGNTION BY INTEGRATING VARIATION
REFERENCES
1. Bruce V. 1982. Changing faces: Visual and non visual coding processes in face
recognition. Br J Psychol. 73(1):105-16. doi: 10.1111/j.2044-8295.1982.tb01795.x
2. Bruce V, Henderson Z, Newman C, Burton AM. 2001. Matching identities of familiar and
unfamiliar faces caught on CCTV images. J Exp Psychol Appl. 7(3):207-218. doi:
10.1037/1076-898X.7.3.207 Formatted: Font color: Auto, Do not check
spelling or grammar, Pattern: Clear
31. Burton AM, Wilson S, Cowan M, Bruce V. 1999. Face recognition in poor-quality video:
Evidence from security surveillance, Psychol Sci 10(3):243-8. doi: 10.1111/1467-
9280.00144
2. Hole GJ, George PA, Eaves K, Rasek A. 2002. Effects of geometric distortions on face-
recognition performance, Perception 31(10):1221-40. doi: 10.1068/p3252
3. Bruce V. 1982. Changing faces: Visual and non visual coding processes in face
recognition. Br J Psychol. 73(1):105-16. doi: 10.1111/j.2044-8295.1982.tb01795.x
4. Ellis HD, Shepherd JW, Davies GM. 1979. Identification of familiar and unfamiliar faces
from internal and external features: Some implications for theories of face
recognition. Perception 8(4):431-9. doi: 10.1068/p080431
5. Bruce V, Henderson Z, Newman C, Burton AM. 2001. Matching identities of familiar and
unfamiliar faces caught on CCTV images. J Exp Psychol Appl. 7(3):207-218. doi:
10.1037/1076-898X.7.3.207
46. Bruck M, Cavanagh P, Ceci SJ. 1991. Fortysomething: Recognizing faces at one’s 25th
reunion. Mem. Cogn. 19(3):221-8. doi: 10.3758/BF0321114
5. Megreya, A, & Burton, M. 2006. Unfamiliar faces are not faces: evidence from a matching
task. Mem. Cogn, 34(4): 865-876. doi: 10.3758/BF03193433
7, Hancock PJ, Bruce V, Burton AM. 2000. Recognition of unfamiliar faces. Trends Cogn
Sci. 4(9):330-7. doi: 10.1016/S1364-6613(00)01519-9
20
FACE RECOGNTION BY INTEGRATING VARIATION
68. Bonner L, Burton AM, Bruce V. 2003. Getting to know you: How we learn new faces.
Vis cogn.10(5):527-36. doi:10.1080/13506280244000168
79. Osborne CD, Stevenage SV. 2008. Internal feature saliency as a marker of familiarity and
configural processing. Vis cogn. 16(1):23-43. doi: 10.1080/13506280701238073
810. Clutterbuck R, Johnston RA. 2002. Exploring levels of face familiarity by using an
indirect face-matching measure. Perception. 31(8):985-94. doi: 10.1068/p3335
911. Clutterbuck R, Johnston RA. 2004.Matching as an index of face familiarity. Vis cogn.
11(7):857-69. doi: 10.1080/13506280444000021
102. Clutterbuck R, Johnston RA. 2005. Demonstrating how unfamiliar faces become
familiar using a face matching task. Eur J Cogn Psychol. 17(1):97-116. doi:
10.1080/09541440340000439
113. Burton AM, Jenkins R, Schweinberger SR. 2011. Mental representations of familiar
faces. Br J Psychol. 102(4):943-58. doi: 10.1111/j.2044-8295.2011.02039.x
124. Burton AM, Kramer RS, Ritchie KL, Jenkins R. 2016. Identity from variation:
Representations of faces derived from multiple instances. Cogn Sci. 40(1):202-23.
doi: 10.1111/cogs.12231
135. Jenkins R, White D, Van Montfort X, Burton AM. 2011. Variability in photos of the
same face. Cognition.121(3):313-23. doi: 10.1016/j.cognition.2011.08.001
146. Murphy J, Ipser A, Gaigg SB, Cook R. 2015. Exemplar variance supports robust
learning of facial identity. J Exp Psychol Hum Percept Perform. 41(3):577-581. doi:
10.1037/xhp0000049
157. Andrews S, Jenkins R, Cursiter H, Burton AM. 2015. Telling faces together: Learning
new faces through exposure to multiple instances. Q J Exp Psychol. 68(10):2041-50.
doi: 10.1080/17470218.2014.1003949
21
FACE RECOGNTION BY INTEGRATING VARIATION
168. White D, Burton AM, Jenkins R, Kemp RI. 2014. Redesigning photo-ID to improve
unfamiliar face matching performance. J Exp Psychol Appl. 20(2):166-173. doi:
10.1037/xap0000009
17. Bindemann M, Sandford A. 2011. Me, myself, and I: Different recognition rates for three
photo-IDs of the same person. Perception. 40(5):625-7. doi: 10.1068/p7008
189. Dowsett AJ, Sandford A, Burton AM. 2016. Face learning with multiple images leads to
fast acquisition of familiarity for specific individuals. Q J Exp Psychol. 69(1):1-10.
doi: 10.1080/17470218.2015.1017513
1920. Menon N, White D, Kemp RI. 2015. Variation in photos of the same face drives
improvements in identity verification. Perception. 44(11):1332-41. doi:
10.1177/0301006615599902
201. Ritchie KL, Burton AM. 2017. Learning faces from variability. Q J Exp Psychol.
70(5):897-905. doi: 10.1080/17470218.2015.1136656
212. Bruce V, Young A. 1986. Understanding face recognition. Br J Psychol. 77(3):305-27.
doi: 10.1111/j.2044-8295.1986.tb02199.x
223. Johnston RA, Edmonds AJ. 2009. Familiar and unfamiliar face recognition: A review.
Memory. 17(5):577-96. doi; 10.1080/09658210902976969
234. Menon N, White D, Kemp RI. 2015. Identity-level representations affect unfamiliar face
matching performance in sequential but not simultaneous tasks. Q J Exp Psychol.
68(9):1777-93. doi: 10.1080/17470218.2014.990468
245. Chiachia G, Falcao AX, Pinto N, Rocha A, Cox D. 2014. Learning person-specific
representations from faces in the wild. IEEE Trans. Inf. Forensics Security
9(12):2089-99. doi: 10.1109/TIFS.2014.2359543
256. Lander K, Bruce V. 2003. The role of motion in learning new faces. Vis cogn.
10(8):897-912. doi: 10.1080/13506280344000149
22
FACE RECOGNTION BY INTEGRATING VARIATION
267. Thornton IM, Kourtzi Z. 2002. A matching advantage for dynamic human faces.
Perception. 31(1):113-32. doi: 10.1068/p3300
278. Pike GE, Kemp RI, Towell NA, Phillips KC. 1997. Recognizing moving faces: The
relative contribution of motion and perspective view information. Vis cogn. 4(4):409-
38. doi: 10.1080/713756769
289, Pilz KS, Thornton IM, Bülthoff HH. 2006. A search advantage for faces learned in
motion. Exp Brain Res.171(4):436-47. doi: 10.1007/s00221-005-0283-8
2930. Wallis G, Backus BT, Langer M, Huebner G, Bülthoff H. 2009. Learning illumination-
and orientation-invariant representations of objects through temporal association. J
Vis. 9(7):6-.doi: 10.1167/9.7.6
23
FACE RECOGNTION BY INTEGRATING VARIATION
FIGURE AND TABLE CAPTIONS
Figure 1. Illustration of the video-based face learning procedure (left) and mean overall
accuracy in each experimental condition (right). Error bars represent 1 SEM.
Table 1: Mean (SEM) accuracy (%) for match and mismatch trials in each condition.
24
Society Open
