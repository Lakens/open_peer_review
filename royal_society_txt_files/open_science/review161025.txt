Experimental priming of independent and interdependent
activity does not affect culturally variable psychological
processes
Kesson Magid, Vera Sarkol and Alex Mesoudi
Article citation details
R. Soc. open sci. 4: 161025.
http://dx.doi.org/10.1098/rsos.161025
Review timeline
Original submission: 15 May 2016 Note: Reports are unedited and appear as
1st revised submission: 9 December 2016 submitted by the referee. The review history
2nd revised submission: 9 March 2017 appears in chronological order.
3rd revised submission: 11 April 2017
Final acceptance: 19 April 2017
Review History
label_version_1
RSOS-160338.R0 (Original submission)
label_author_1
Review form: Reviewer 1 (Thomas Talhelm)
Is the manuscript scientifically sound in its present form?
No
Are the interpretations and conclusions justified by the results?
No
Is the language acceptable?
Yes
Is it clear how to access all supporting data?
I don't remember seeing where it could be accessed. Maybe this could be added later.
Do you have any ethical concerns with this paper?
No
© 2017 The Authors. Published by the Royal Society under the terms of the Creative Commons
Attribution License http://creativecommons.org/licenses/by/4.0/, which permits unrestricted use,
provided the original author and source are credited
2
Have you any concerns about statistical analyses in this paper?
Yes
Recommendation?
label_recommendation_1
Major revision is needed (please make suggestions in comments)
Comments to the Author(s)
label_comment_1
See attached comments. (Appendix A.)
label_author_2
Review form: Reviewer 2 (Takahiko Masuda)
Is the manuscript scientifically sound in its present form?
Yes
Are the interpretations and conclusions justified by the results?
Yes
Is the language acceptable?
Yes
Is it clear how to access all supporting data?
Adequate and clear
Do you have any ethical concerns with this paper?
No
Have you any concerns about statistical analyses in this paper?
Yes
Recommendation?
label_recommendation_2
Accept with minor revision (please list in comments)
Comments to the Author(s)
label_comment_2
The manuscript “Experimental Priming of Independent and Interdependent Activity Does Not
Affect Culturally-Variable Psychological Processes” reported findings of two experiments that
attempt to test the ecocultural hypothesis by using a priming methodology. The results in general
did not support the hypothesis: They demonstrated that the participants’ performance on four
cognitive tasks and one self-report measure did not differ across experimental conditions. The
authors discussed three reasons which may explain the negative data: (1) theoretical limitation—
the explanatory power of the ecocultural hypothesis may be weak; (2) methodological
limitation—the priming tasks applied to the two studies were not effective; and (3) limitation of
participants sampled—participants who grew up in a Western monocultural environment
already internalize a set of culturally specific cognitive styles, and therefore their responses are
not at all malleable across the priming conditions.
Although the results are overall negative, the topic is interesting. In addition, the theoretical
framework and methodology are solid. The results are accurately reported with scientific rigor.
Given the fact that the Open Access Journal encourages the authors to report negative data, the
current manuscript has a great potential to be accepted with several major revisions. The detailed
explanations are as follows:
The Selection of Tasks
3
Although the authors reported five tasks in detail, and the descriptions meet the standard of the
scientific journal, the reasons of their choice were not justifiably discussed. For example, has the
pronoun selection task been used by many other researchers before, and the validity been
assured? Why did they not use the tasks reported by Talhelm et al., and Uskul et al.? Could the
results turn out to be positive if the authors used these tasks? These issues should be clearly
addressed in text.
The Absolute vs. Relative Condition
I think that the explanation of the absolute vs. relative manipulation is not satisfactory. Clearer
logic and a more elaborated description is needed.
The Manipulation Check
When using the priming methodology, it is mandatory for researchers to include several
manipulation check questions. Without these measures, researchers cannot assess whether the
priming is effective enough for randomly assigned participants to temporarily hold a specific
mindset (e.g., independent vs. interdependent mindset). Please report these values, and revise the
section of the methodological limitation in the general discussion section.
The Statistical Analyses.
In both Experiment 1 and 2, the authors reported the statistical analyses of the regression model
as well as the charts. The style is somewhat deviated from the convention of cross-cultural
studies. Why not simply report the values of ANOVA, which allows the audience to better
contrast the results of the current manuscript to those of previous cross-cultural literature?
The Limitation.
The authors did a good job addressing a variety of limitations of the current research design as
well as the current discourses in terms of the relationship between culture, ecology, and
psychological processes. To further facilitate the implications of the negative data, the authors
should elaborate their sections extensively and provide directions of future research. Such
extensive discussions will help the audience to further advance this topic.
label_author_3
Review form: Reviewer 3
Is the manuscript scientifically sound in its present form?
No
Are the interpretations and conclusions justified by the results?
No
Is the language acceptable?
Yes
Is it clear how to access all supporting data?
Yes
Do you have any ethical concerns with this paper?
No
Have you any concerns about statistical analyses in this paper?
No
Recommendation?
label_recommendation_3
Reject
4
Comments to the Author(s)
label_comment_3
I feel conflicted in my final recommendation. On the one hand the experiments are nicely
designed, the statistics appropriate, and the paper is well written. The literature could also use
more examples of null results. However, ultimately I was unconvinced that present studies
contribute to our understanding of the causes of cultural differences in
independence/interdependence. I see no obvious connection between cultural evolution of
interdependent and independent cognition, or related aspects of psychology, that occur on a
generational timescale and attempts to use these activities to move cognition in a lab setting.
Moreover, since these cultural differences seem to persist long after the particular historical
activities have been given up (i.e. people are no longer farming wheat or rice) and seem to persist
somewhat after migration to a new cultural context, they seem even less likely to be so easily
recreated in a lab. Unfortunately, I therefore have to recommend rejection on the basis that the
studies do not contribute to the debate over the ecocultural hypothesis.
I have two further critiques of the design. (1) The study also differs from many (though not all)
priming studies in this literature in that they are not moving existing individual variability in
independence and interdependence by reminding people via, for example, priming a context, but
instead trying to create this cognition. (2) The authors should do more to justify the sample size,
which is likely too low. Given what we know about p-hacking and underpowered studies, being
in the top 10% and 25% of the existing literature is not sufficient justification (nor are these
particularly high percentiles given the prevalence of the p-hacking and power problem).
label_end_comment
Decision letter (RSOS-160338)
20th October 2016
Dear Dr Mesoudi:
Manuscript ID RSOS-160338 entitled "Experimental priming of independent and interdependent
activity does not affect culturally-variable psychological processes" which you submitted to Royal
Society Open Science, has been reviewed. The comments from reviewers are included at the
bottom of this letter.
In view of the criticisms of the reviewers, the manuscript has been rejected in its current form.
However, a new manuscript may be submitted which takes into consideration these comments.
As you can see from the reviewer comments, reviewer 3 is dubious about the publication, whilst
reviewer 2 is happy to consider a revision that addresses the issues he/she raises. I would like the
authors to do that and also discuss some of the limitations that reviewer 3 brings up, even if they
cannot be addressed.
Please note that resubmitting your manuscript does not guarantee eventual acceptance, and that
your resubmission will be subject to peer review before a decision is made.
You will be unable to make your revisions on the originally submitted version of your
manuscript. Instead, revise your manuscript and upload the files via your author centre.
Once you have revised your manuscript, go to https://mc.manuscriptcentral.com/rsos and login
to your Author Center. Click on "Manuscripts with Decisions," and then click on "Create a
Resubmission" located next to the manuscript number. Then, follow the steps for resubmitting
your manuscript.
5
Your resubmitted manuscript should be submitted by 10th December 2016. If you are unable to
submit by this date please contact the Editorial Office.
We look forward to receiving your resubmission.
Sincerely,
Andrew Dunn
Senior Publishing Editor
Royal Society Open Science
on behalf of
Essi Viding, Royal Society Open Science
openscience@royalsociety.org
Reviewers' Comments to Author:
Reviewer: 1
Comments to the Author(s)
See attached comments.
Reviewer: 2
Comments to the Author(s)
The manuscript “Experimental Priming of Independent and Interdependent Activity Does Not
Affect Culturally-Variable Psychological Processes” reported findings of two experiments that
attempt to test the ecocultural hypothesis by using a priming methodology. The results in general
did not support the hypothesis: They demonstrated that the participants’ performance on four
cognitive tasks and one self-report measure did not differ across experimental conditions. The
authors discussed three reasons which may explain the negative data: (1) theoretical limitation—
the explanatory power of the ecocultural hypothesis may be weak; (2) methodological
limitation—the priming tasks applied to the two studies were not effective; and (3) limitation of
participants sampled—participants who grew up in a Western monocultural environment
already internalize a set of culturally specific cognitive styles, and therefore their responses are
not at all malleable across the priming conditions.
Although the results are overall negative, the topic is interesting. In addition, the theoretical
framework and methodology are solid. The results are accurately reported with scientific rigor.
Given the fact that the Open Access Journal encourages the authors to report negative data, the
current manuscript has a great potential to be accepted with several major revisions. The detailed
explanations are as follows:
The Selection of Tasks
Although the authors reported five tasks in detail, and the descriptions meet the standard of the
scientific journal, the reasons of their choice were not justifiably discussed. For example, has the
pronoun selection task been used by many other researchers before, and the validity been
assured? Why did they not use the tasks reported by Talhelm et al., and Uskul et al.? Could the
results turn out to be positive if the authors used these tasks? These issues should be clearly
addressed in text.
The Absolute vs. Relative Condition
I think that the explanation of the absolute vs. relative manipulation is not satisfactory. Clearer
logic and a more elaborated description is needed.
6
The Manipulation Check
When using the priming methodology, it is mandatory for researchers to include several
manipulation check questions. Without these measures, researchers cannot assess whether the
priming is effective enough for randomly assigned participants to temporarily hold a specific
mindset (e.g., independent vs. interdependent mindset). Please report these values, and revise the
section of the methodological limitation in the general discussion section.
The Statistical Analyses.
In both Experiment 1 and 2, the authors reported the statistical analyses of the regression model
as well as the charts. The style is somewhat deviated from the convention of cross-cultural
studies. Why not simply report the values of ANOVA, which allows the audience to better
contrast the results of the current manuscript to those of previous cross-cultural literature?
The Limitation.
The authors did a good job addressing a variety of limitations of the current research design as
well as the current discourses in terms of the relationship between culture, ecology, and
psychological processes. To further facilitate the implications of the negative data, the authors
should elaborate their sections extensively and provide directions of future research. Such
extensive discussions will help the audience to further advance this topic.
Reviewer: 3
Comments to the Author(s)
I feel conflicted in my final recommendation. On the one hand the experiments are nicely
designed, the statistics appropriate, and the paper is well written. The literature could also use
more examples of null results. However, ultimately I was unconvinced that present studies
contribute to our understanding of the causes of cultural differences in
independence/interdependence. I see no obvious connection between cultural evolution of
interdependent and independent cognition, or related aspects of psychology, that occur on a
generational timescale and attempts to use these activities to move cognition in a lab setting.
Moreover, since these cultural differences seem to persist long after the particular historical
activities have been given up (i.e. people are no longer farming wheat or rice) and seem to persist
somewhat after migration to a new cultural context, they seem even less likely to be so easily
recreated in a lab. Unfortunately, I therefore have to recommend rejection on the basis that the
studies do not contribute to the debate over the ecocultural hypothesis.
I have two further critiques of the design. (1) The study also differs from many (though not all)
priming studies in this literature in that they are not moving existing individual variability in
independence and interdependence by reminding people via, for example, priming a context, but
instead trying to create this cognition. (2) The authors should do more to justify the sample size,
which is likely too low. Given what we know about p-hacking and underpowered studies, being
in the top 10% and 25% of the existing literature is not sufficient justification (nor are these
particularly high percentiles given the prevalence of the p-hacking and power problem).
Author's Response to Decision Letter for (RSOS-160338)
See Appendix B.
7
label_version_2
RSOS-161025.R0 (Revision)
label_author_4
Review form: Reviewer 1 (Thomas Talhelm)
Is the manuscript scientifically sound in its present form?
No
Are the interpretations and conclusions justified by the results?
No
Is the language acceptable?
Yes
Is it clear how to access all supporting data?
Yes
Do you have any ethical concerns with this paper?
No
Have you any concerns about statistical analyses in this paper?
No
Recommendation?
label_recommendation_4
Accept with minor revision (please list in comments)
Comments to the Author(s)
label_comment_4
Review of: Ecocultural Priming
For Royal Society Open Science
Summary
This paper presents a test wherein participants complete solo or cooperative tasks. Participants
complete measures of individualism and analytic thought before and after. The activities lead to
no consistent effect.
I’ve reviewed a prior version of this paper, and I commend the authors for incorporating some of
the feedback. I felt the biggest flaw of the paper was that the authors were really confident that
the activities adequately represented rice farming and had almost no discussion of why folding
origami for 30 minutes may not be like farming paddy rice in a Chinese village in the 1500s. The
new paper has more recognition of the difficulty of that. For example, I appreciate the added
epistemological humbleness in the discussion here:
Yet we would certainly not advocate rejecting the ecocultural hypothesis on the basis of this
negative finding alone, without considering alternative reasons for our negative results.
Here are more plusses: I appreciate the benefits of experimental approaches. I also like that the
authors made the data and analysis script publicly available.
I think this paper is getting closer to being finished. In a broad scheme, I think two broad areas
need improvement:
(1) The discussion is now more humble about the equivalence between folding origami and rice
farming. Adding a few points there (described below) will be sufficient. But the intro is largely
unchanged. More needs to be done to discuss why these manipulations are and are not like actual
rice farming.
8
(2) There are a few details about how the results are analyzed and presented graphically that I
describe below. These should be fairly easy to address. Below I describe specific points.
Main Points
1. This study didn’t find evidence of eco-cultural priming. That’s fine. But I thought the
conclusion in the abstract was too extreme: “We suggest re-examination of this priming literature
particularly in light of the recent replication crisis.” Does one study call into question an entire
line of research with dozens of findings? I think of the Oyserman and Lee meta-analysis of 67
studies and 6,240 participants that found effects of priming individualism and collectivism on
cognitive style. One negative finding with a completely different prime doesn’t lead me to think
“replication crisis.”
2. I take issue with the statement “Rather than assuming that a certain means of subsistence
entails a certain pattern of activity…” In my own research, I don’t “assume” that rice farming
entails certain labor, coordination, and cooperation. I cite the evidence of people who have
studied and observed rice farmers personally. Specific sources can be found in Talhelm et al.,
2014.
3. I know I’m harping on the same point as my earlier review, but it’s easily the most important
point of the paper. The biggest theoretical issue with the paper is that choosing a task that
involves other people necessarily mimics rice. And that problem is still to confidently ignored in
certain parts of the paper. For example, that viewpoint is in this sentence on page 5:
…the key factor predicted to have generated cross-cultural differences in thinking styles
according to the ecocultural hypothesis: whether activity is solitary (as in herding) or collective
(as in rice farming).
There are lots of activities that involve other people that are not like rice farming at all. A couple
days ago, I watched the new Star Wars movie in a theater with 100 other people. But that was not
like farming rice.
Now, maybe I’m being a bit fatuous in that comparison, so let me use one that’s more
related to farming. Tsuruta studied labor sharing among shifting farmers in the Congo and rice
farmers in Japan. Both have labor sharing customs, but Tsuruta concludes they’re very different.
In the Congo, they’re more like beer parties. People help clear a field, and then the host gives
everyone beer brewed from bananas. In the Congo, the labor sharing is more like a festival.
In Japan, Tsuruta described labor sharing as a much more strict, reciprocal occasion. If one farmer
accepted help and then later couldn’t repay because of sickness or injury, they would go so far as
hiring labor to repay the debt. The labor sharing is more dyadic, long-term, and infused with
responsibilities and social debts. My point here is that the presence of other people, interacting
with them—even working directly with them—doesn’t adequately describe rice farming.
Now this account Tsuruta builds of the Congolese shifting farmers, doesn’t that sound a
bit more like the public goods game described in the paper? The Tsuruta account actually speaks
against choosing the public goods game to represent rice farming.
Here’s another reason why I would not choose a public goods game to represent rice farming:
individualistic cultures like those in northern Europe score among the highest on trust toward
strangers and support for welfare programs. In fact, in the 2014 rice theory paper, I specifically
used a task that measured the difference between how people treat friends and strangers. People
from the rice areas were much harsher toward strangers than people from wheat provinces. Thus,
I might actually predict the opposite result from a public goods game.
Ultimately, I’m making a tough criticism. How can I expect a lab experiment to adequately mimic
rice farming? And that’s fine. We have to be realistic in our expectations about what a single
study can do. Yet so should the authors in how they describe how well this priming mimics rice
farming.
9
So here’s specifically how the paper could be improved. Instead of saying that solitary versus
collective is the key feature, say that this is a feature of rice farming but also many other activities
that humans do. It’s also just one single part of rice farming, but doesn’t include lots of other
things about rice farming.
That more accurately describes what this study is testing. And that doesn’t mean this study has
no value. There’s plenty of value in it. But we should be clear about what we’re testing, what
we’re re-creating in the lab.
4. One thing I’ve learned from years of doing psychology experiments is that control conditions
aren’t always control conditions. In other words, tasks or conditions that are meant to be neutral
can still be priming certain feelings or modes of thinking. I worry that here with the Multi-Armed
Bandit game. I get that it’s solitary, but what else could that task be activating in people?
Calculation? Rationality concerns? (Economic tasks can make people aware that their rational
self-interest is being tested.) The paper could be improved by adding at least a couple sentences
about what the MAB game could be priming other than what we’re labeling it with.
5. I’ve studied rice for the last 8 years of my life. I’ve read books about traditional villages and
how they farm rice. I wrote an entire dissertation about rice farming. Yet after all these years, I’m
not sure what the key behavioral parts of rice were. Is it the coordination involved in irrigation?
Is it the commons problems of repairing irrigation dikes? Is it the tight, reciprocal labor
exchanges? Is it a combination of all of these? I don’t know. We just don’t have the data to be
sure.
All of this is to say, I think we need to be humble about what we’ve manipulated and
what the key element is. If I can study this for 8 years and say, “I still don’t know.” How are the
authors so sure that they’ve manipulated the right thing and that this specific way they’ve
manipulated it (in the lab, with strangers, for just 30 minutes) should lead to the outcomes we’re
talking about?
I’m not saying we should just throw up our hands and not try. I appreciate this attempt.
Attempts like this can build up our base of knowledge. But we must also keep in mind the bigger
picture and exactly what this study does and does not show. The new version of the discussion
has made progress in this regard. But the intro seems to be largely unchanged. The intro could be
made This could be done in the paper by:
1. Toning down some of the more extreme language, such as “rather than assuming…”
2. In the into or methods when the activities are describe, there should be more nuanced
discussion that recognizes how complicated this is, such as “rice farming involved lots of
behaviors, from commons dilemmas to long-term reciprocal labor exchanges. It is not clear
whether the tasks in this experiment accurately represent the key elements of rice farming or that
they should have an effect in this sort of context (one-off experiences for short periods of time [30
minutes] with strangers that will not be repeated).”
5. The DV tasks are pretty creative, but one problem with creativity is that we don’t know
whether they’re actually measuring what we want them to. For example, the portrait selection
task described on page 9: I’ve never seen any research showing that people in different cultures
actually actively express a preference for one or the other. The difference appears in cultural
behaviors and magazines, but do people actively endorse this as a preference? I don’t know.
Readers should be aware that this measure has not been used in this way before.
6. There’s a fairly profound issue with calling draw a horizon line higher or lower
“interdependent.” There’s one study documenting that as a cross-cultural difference, but I think
few cultural psychologists would call it “interdependent.” The paper would be improved if the
language were more carefully chosen and tasks more precisely connected to concepts.
10
Minor Points
1. Page 10 describes removing hierarchy items from the scale. Yet hierarchy differences are also
cultural differences and at least the horizontal-vertical scale of collectivism explicitly includes
hierarchy. I’m not saying these items cannot be removed. But I think either a better case needs to
be made or readers should (in a footnote) see what happens in the data if this decision were not
made.
2. I might have missed this, but was there an analysis of differences in the DVs between groups
before the manipulation? This would be a check of random assignment.
3. I’m curious about this statement on page 22:
“Experiment 1 suggested that participants understood the nature of the relationships as rule
based or relationship based.”
I know it’s not critical to the paper, but I would be interested to see the analysis used to reach
that conclusion. How about in a footnote or supplemental materials?
4. I may have missed this, but what was the order of the tasks after the prime? It’s possible that
the biggest effect would appear directly after the prime.
5. I wonder if there’s interesting information in the graphs hidden by the fact that the Y axis
includes the entire range of responses. Some of the means seem somewhat different, but no
matter how much I squint, I can’t be sure of what I’m seeing. I think it’s worth adding graphs
with smaller Y axes.
label_author_5
Review form: Reviewer 4 (Igor Grossmann)
Is the manuscript scientifically sound in its present form?
No
Are the interpretations and conclusions justified by the results?
No
Is the language acceptable?
Yes
Is it clear how to access all supporting data?
Yes
Do you have any ethical concerns with this paper?
No
Have you any concerns about statistical analyses in this paper?
Yes
Recommendation?
label_recommendation_5
Major revision is needed (please make suggestions in comments)
Comments to the Author(s)
label_comment_5
I enjoyed reading the RSOS-161025, which reports two studies examining effects of participation
in communal vs. solitary games on various tasks related to independence /interdependence
11
and/or holistic /analytic cognitive style. I found the paper well-articulated (especially in the
discussion), and the results quite provocative. As usual with negative results, it is hard to make
sense of them. The authors tried to account for several possibilities. There are some more, based
on the inherent flaws in the design used in the present manuscript. I will reflect on several such
issues below.
Major concerns:
• Given sufficient statistical power (more on this below), there seems no clear need for a
pre-test of the DV. In fact, the virtue of experimental design is that with a sufficiently-large
sample random assignment takes care of the distribution of the individual differences. The
authors make a big deal out of using pre-post design, and use this design in all of their studies.
This leads to unnecessary complications in design (incl. potential assimilation/contrast effects;
even though I appreciate authors’ attempt to take care of this issue by using slightly altered
stimuli in pre and post-sessions). Moreover, it appears that the authors results reveal failure of
random assignment in study 1 (with condition-specific sig different distributions on some DV-
tasks BEFORE the manipulation took place). That is highly problematic, as well.
• The selected DV tasks are a hotchpotch of measures, without a clear rationale for their
selection, different theoretical origins and apparent lack of coherence on the individual level (e.g.,
see Na et al., 2010). For instance, Singelis scale is notorious for poor reliability, with constant
arguments whether it represents 2 dimensions or one dimension (and hence, one would examine
difference/relative scores; see discussion by Oyserman et al., 2001; and Diener et al. in response).
Also see recent evidence that 2 dimensions may not be sufficient (at least, in a cross-cultural
context, see work by Vignoles et al., 2016, JEP: Genera)l. Categorization task comes from a
completely different theoretical orientation and has little to do with the notion of individualism
per se (rather, it has been conceptualized as measuring holistic – analytic thinking style). The
problem with the categorization task becomes salient in study 2, where the effect observed on this
task goes in the opposite from the predicted direction. Unless the authors can show that the
nomological network of this task (performed on the data in each study) indicates high
convergence between performance on this task and other holistic/analytic or
independence/interdependence tasks, it is hard to make sense out of this significant effect of
condition x prime on the categorization task. Other, drawing-based tasks have unknown
psychometric properties and have never been validated as measures of either individualism-
collectivism or holistic-analytic processing. Generally, the association between performance on
these difference tasks has not been discussed in the manuscript.
• AS the authors reflect in the discussion, it is an empirical question whether anonymous-
player PGG is suitable to induce interdependence when performed among strangers on separate
computer terminals in the lab. Given some evidence that PGG contributions tend to be selfish as a
function of greater time spent on a decision (see Rand et al., 2012, Nature), it is possible that some
participants became fairly self-centered as a function of taking part in a game, with a critical
moderator involving among of time spent on a task. Further, what were the actual contributions
in the PGG and how did they relate to the DVs?
• Overall, the anonymous nature in the PGG task in Study 1 appears to be a serious
weakness preventing one from having a clear interpretation of the meaning of the results.
• Both studies are underpowered, even though larger than the initial cultural priming
studies. It is a shared knowledge among many cross-cultural researchers that these priming
studies are subject to a huge file-drawer problem, which further diminishes estimates. For
instance, in Study 2 there are fewer than 35 ppl per between-subject cell, which is a problem for
the power in the studies. The average effect size in psychology is ~ d = .41, not considering the
possible file-drawer that was typical to the priming research in 90s-2000s. Thus, with four
between-subject cells, the sample in Study 2 appears underpowered.
• Is there a possible confound of mood/disappointment in Study 1? Given that PGG/
Bandit tasks preceded the DVs, it is possible that differential outcome/payment on the economic
game distorted the effects (this is not a concern if participants did not know about their actual
contributions until the very end of the study; but it is unclear from the write-up if it is what
actually happened). Did the authors control for this variable (both as a covariate and/or as a
moderator)? I suspect the small sample size may distort the effects here.
12
Additional/minor comments:
• I found the conclusion, as articulated in the abstract, dissatisfactory. The results indicate
that ecocultural theory is not supported in the lab. This can suggest either that priming is not
effective for introducing cultural differences AND/OR that ecocultural theory is simply wrong.
Both ought to be acknowledged as viable explanations. The authors do so later in the discussion,
but not in the abstract. Though I tend to agree that priming ineffectiveness is a more likely
explanation, it is still possible that the theory is plainly wrong.
• Some of the introduction, concerning review of prior literature, needs a more careful
read of the existing scholarship, including work by Bertram Malle and colleagues on the lack of
support for dispositional attributions in the US, as well as cross-cultural work on relative
differences to acknowledge situational vs. dispositional attributions beyond East vs. West
comparison (e.g., across differences social classes in the US, and when examining Eastern
Europeans; I have written on this topic in Grossmann & Varnum, 2011). Similarly, the cross-
cultural evidence for self-enhancement is more complex than presented in the MS (see works by
Sedikides and colleagues).
• I believe Ancient Greece reference as the origin of the western thought is used
figuratively and not literally. It is a metaphor. Same for Ancient China and farming... I doubt
there would be serious claims in the field these days that those ancient cultures have a direct
impact on the cross-cultural differences observed today.
• There is another approach to test ecocultural hypothesis, which involves cross-temporal
analysis of the psychological tendencies (e.g., IND/COL) and herding vs. farming development
over time (see relevant examples in related work; e.g., Grossmann & Varnum, 2015). Though not
taking a large-scale temporal frame inferred by the proponents of the ecocultural theory, this
approach has a larger temporal dimension than lab-experiments.
• Why did the authors use an original (24 item) Singelis scale, rather than the updated 30-
item scale? Why did the authors omit the analyses on hierarchy items of the Singelis scale rather
than including them as a separate group?
• Given some claims that men (vs. women) have a more independent self-construal (see
work by Susan Cross and Hazel Markus on this topic), I do not think that the gender x prime
interaction is atheoretical and not hypothesized. Quite the opposite, knowing the literature, it is
possible to predict that sex (or gender) would have an effect, especially pronounced after the
manipulation task. Of course, it is post-hoc, but the authors should acknowledge this viable
possibility.
label_end_comment
Decision letter (RSOS-161025)
9th Feb 2017
Dear Dr Mesoudi,
The Subject Editor assigned to your paper ("Experimental priming of independent and
interdependent activity does not affect culturally-variable psychological processes") has now
received comments from reviewers. We would like you to revise your paper in accordance with
the referee suggestions which can be found below (not including confidential reports to the
Editor). Please note this decision does not guarantee eventual acceptance.
Please submit a copy of your revised paper within three weeks (i.e. by the 2nd March 2017). If we
do not hear from you within this time then it will be assumed that the paper has been withdrawn.
In exceptional circumstances, extensions may be possible if agreed with the Editorial Office in
advance. We do not allow multiple rounds of revision so we urge you to make every effort to
fully address all of the comments at this stage. If deemed necessary by the Editors, your
13
manuscript will be sent back to one or more of the original reviewers for assessment. If the
original reviewers are not available we may invite new reviewers.
To revise your manuscript, log into http://mc.manuscriptcentral.com/rsos and enter your
Author Centre, where you will find your manuscript title listed under "Manuscripts with
Decisions." Under "Actions," click on "Create a Revision." Your manuscript number has been
appended to denote a revision. Revise your manuscript and upload a new version through your
Author Centre.
When submitting your revised manuscript, you must respond to the comments made by the
referees and upload a file "Response to Referees" in "Section 6 - File Upload". Please use this to
document how you have responded to each of the comments, and the adjustments you have
made. In order to expedite the processing of the revised manuscript, please be as specific as
possible in your response.
In addition to addressing all of the reviewers' and editor's comments please also ensure that your
revised manuscript contains the following sections before the reference list:
• Ethics statement
If your study uses humans or animals please include details of the ethical approval received,
including the name of the committee that granted approval. For human studies please also detail
whether informed consent was obtained. For field studies on animals please include details of all
permissions, licences and/or approvals granted to carry out the fieldwork.
• Data accessibility
It is a condition of publication that all supporting data are made available either as
supplementary information or preferably in a suitable permanent repository. The data
accessibility section should state where the article's supporting data can be accessed. This section
should also include details, where possible of where to access other relevant research materials
such as statistical tools, protocols, software etc can be accessed. If the data has been deposited in
an external repository this section should list the database, accession number and link to the DOI
for all data from the article that has been made publicly available. Data sets that have been
deposited in an external repository and have a DOI should also be appropriately cited in the
manuscript and included in the reference list.
If you wish to submit your supporting data or code to Dryad (http://datadryad.org/), or modify
your current submission to dryad, please use the following link:
http://datadryad.org/submit?journalID=RSOS&manu=RSOS-161025
• Competing interests
Please declare any financial or non-financial competing interests, or state that you have no
competing interests.
• Authors’ contributions
All submissions, other than those with a single author, must include an Authors’ Contributions
section which individually lists the specific contribution of each author. The list of Authors
should meet all of the following criteria; 1) substantial contributions to conception and design, or
acquisition of data, or analysis and interpretation of data; 2) drafting the article or revising it
critically for important intellectual content; and 3) final approval of the version to be published.
All contributors who do not meet all of these criteria should be included in the
acknowledgements.
14
We suggest the following format:
AB carried out the molecular lab work, participated in data analysis, carried out sequence
alignments, participated in the design of the study and drafted the manuscript; CD carried out
the statistical analyses; EF collected field data; GH conceived of the study, designed the study,
coordinated the study and helped draft the manuscript. All authors gave final approval for
publication.
• Acknowledgements
Please acknowledge anyone who contributed to the study but did not meet the authorship
criteria.
• Funding statement
Please list the source of funding for each author.
Once again, thank you for submitting your manuscript to Royal Society Open Science and I look
forward to receiving your revision. If you have any questions at all, please do not hesitate to get
in touch.
Kind regards,
Alice Power
Royal Society Open Science
openscience@royalsociety.org
Comments to Author:
Reviewers' Comments to Author:
Reviewer: 1
Comments to the Author(s)
Review of: Ecocultural Priming
For Royal Society Open Science
Summary
This paper presents a test wherein participants complete solo or cooperative tasks. Participants
complete measures of individualism and analytic thought before and after. The activities lead to
no consistent effect.
I’ve reviewed a prior version of this paper, and I commend the authors for incorporating some of
the feedback. I felt the biggest flaw of the paper was that the authors were really confident that
the activities adequately represented rice farming and had almost no discussion of why folding
origami for 30 minutes may not be like farming paddy rice in a Chinese village in the 1500s. The
new paper has more recognition of the difficulty of that. For example, I appreciate the added
epistemological humbleness in the discussion here:
Yet we would certainly not advocate rejecting the ecocultural hypothesis on the basis of this
negative finding alone, without considering alternative reasons for our negative results.
Here are more plusses: I appreciate the benefits of experimental approaches. I also like that the
authors made the data and analysis script publicly available.
I think this paper is getting closer to being finished. In a broad scheme, I think two broad areas
need improvement:
(1) The discussion is now more humble about the equivalence between folding origami and rice
farming. Adding a few points there (described below) will be sufficient. But the intro is largely
15
unchanged. More needs to be done to discuss why these manipulations are and are not like actual
rice farming.
(2) There are a few details about how the results are analyzed and presented graphically that I
describe below. These should be fairly easy to address. Below I describe specific points.
Main Points
1. This study didn’t find evidence of eco-cultural priming. That’s fine. But I thought the
conclusion in the abstract was too extreme: “We suggest re-examination of this priming literature
particularly in light of the recent replication crisis.” Does one study call into question an entire
line of research with dozens of findings? I think of the Oyserman and Lee meta-analysis of 67
studies and 6,240 participants that found effects of priming individualism and collectivism on
cognitive style. One negative finding with a completely different prime doesn’t lead me to think
“replication crisis.”
2. I take issue with the statement “Rather than assuming that a certain means of subsistence
entails a certain pattern of activity…” In my own research, I don’t “assume” that rice farming
entails certain labor, coordination, and cooperation. I cite the evidence of people who have
studied and observed rice farmers personally. Specific sources can be found in Talhelm et al.,
2014.
3. I know I’m harping on the same point as my earlier review, but it’s easily the most important
point of the paper. The biggest theoretical issue with the paper is that choosing a task that
involves other people necessarily mimics rice. And that problem is still to confidently ignored in
certain parts of the paper. For example, that viewpoint is in this sentence on page 5:
…the key factor predicted to have generated cross-cultural differences in thinking styles
according to the ecocultural hypothesis: whether activity is solitary (as in herding) or collective
(as in rice farming).
There are lots of activities that involve other people that are not like rice farming at all. A couple
days ago, I watched the new Star Wars movie in a theater with 100 other people. But that was not
like farming rice.
Now, maybe I’m being a bit fatuous in that comparison, so let me use one that’s more
related to farming. Tsuruta studied labor sharing among shifting farmers in the Congo and rice
farmers in Japan. Both have labor sharing customs, but Tsuruta concludes they’re very different.
In the Congo, they’re more like beer parties. People help clear a field, and then the host gives
everyone beer brewed from bananas. In the Congo, the labor sharing is more like a festival.
In Japan, Tsuruta described labor sharing as a much more strict, reciprocal occasion. If one farmer
accepted help and then later couldn’t repay because of sickness or injury, they would go so far as
hiring labor to repay the debt. The labor sharing is more dyadic, long-term, and infused with
responsibilities and social debts. My point here is that the presence of other people, interacting
with them—even working directly with them—doesn’t adequately describe rice farming.
Now this account Tsuruta builds of the Congolese shifting farmers, doesn’t that sound a
bit more like the public goods game described in the paper? The Tsuruta account actually speaks
against choosing the public goods game to represent rice farming.
Here’s another reason why I would not choose a public goods game to represent rice farming:
individualistic cultures like those in northern Europe score among the highest on trust toward
strangers and support for welfare programs. In fact, in the 2014 rice theory paper, I specifically
used a task that measured the difference between how people treat friends and strangers. People
from the rice areas were much harsher toward strangers than people from wheat provinces. Thus,
I might actually predict the opposite result from a public goods game.
Ultimately, I’m making a tough criticism. How can I expect a lab experiment to adequately mimic
rice farming? And that’s fine. We have to be realistic in our expectations about what a single
16
study can do. Yet so should the authors in how they describe how well this priming mimics rice
farming.
So here’s specifically how the paper could be improved. Instead of saying that solitary versus
collective is the key feature, say that this is a feature of rice farming but also many other activities
that humans do. It’s also just one single part of rice farming, but doesn’t include lots of other
things about rice farming.
That more accurately describes what this study is testing. And that doesn’t mean this study has
no value. There’s plenty of value in it. But we should be clear about what we’re testing, what
we’re re-creating in the lab.
4. One thing I’ve learned from years of doing psychology experiments is that control conditions
aren’t always control conditions. In other words, tasks or conditions that are meant to be neutral
can still be priming certain feelings or modes of thinking. I worry that here with the Multi-Armed
Bandit game. I get that it’s solitary, but what else could that task be activating in people?
Calculation? Rationality concerns? (Economic tasks can make people aware that their rational
self-interest is being tested.) The paper could be improved by adding at least a couple sentences
about what the MAB game could be priming other than what we’re labeling it with.
5. I’ve studied rice for the last 8 years of my life. I’ve read books about traditional villages and
how they farm rice. I wrote an entire dissertation about rice farming. Yet after all these years, I’m
not sure what the key behavioral parts of rice were. Is it the coordination involved in irrigation?
Is it the commons problems of repairing irrigation dikes? Is it the tight, reciprocal labor
exchanges? Is it a combination of all of these? I don’t know. We just don’t have the data to be
sure.
All of this is to say, I think we need to be humble about what we’ve manipulated and
what the key element is. If I can study this for 8 years and say, “I still don’t know.” How are the
authors so sure that they’ve manipulated the right thing and that this specific way they’ve
manipulated it (in the lab, with strangers, for just 30 minutes) should lead to the outcomes we’re
talking about?
I’m not saying we should just throw up our hands and not try. I appreciate this attempt.
Attempts like this can build up our base of knowledge. But we must also keep in mind the bigger
picture and exactly what this study does and does not show. The new version of the discussion
has made progress in this regard. But the intro seems to be largely unchanged. The intro could be
made This could be done in the paper by:
1. Toning down some of the more extreme language, such as “rather than assuming…”
2. In the into or methods when the activities are describe, there should be more nuanced
discussion that recognizes how complicated this is, such as “rice farming involved lots of
behaviors, from commons dilemmas to long-term reciprocal labor exchanges. It is not clear
whether the tasks in this experiment accurately represent the key elements of rice farming or that
they should have an effect in this sort of context (one-off experiences for short periods of time [30
minutes] with strangers that will not be repeated).”
5. The DV tasks are pretty creative, but one problem with creativity is that we don’t know
whether they’re actually measuring what we want them to. For example, the portrait selection
task described on page 9: I’ve never seen any research showing that people in different cultures
actually actively express a preference for one or the other. The difference appears in cultural
behaviors and magazines, but do people actively endorse this as a preference? I don’t know.
Readers should be aware that this measure has not been used in this way before.
6. There’s a fairly profound issue with calling draw a horizon line higher or lower
“interdependent.” There’s one study documenting that as a cross-cultural difference, but I think
few cultural psychologists would call it “interdependent.” The paper would be improved if the
language were more carefully chosen and tasks more precisely connected to concepts.
17
Minor Points
1. Page 10 describes removing hierarchy items from the scale. Yet hierarchy differences are also
cultural differences and at least the horizontal-vertical scale of collectivism explicitly includes
hierarchy. I’m not saying these items cannot be removed. But I think either a better case needs to
be made or readers should (in a footnote) see what happens in the data if this decision were not
made.
2. I might have missed this, but was there an analysis of differences in the DVs between groups
before the manipulation? This would be a check of random assignment.
3. I’m curious about this statement on page 22:
“Experiment 1 suggested that participants understood the nature of the relationships as rule
based or relationship based.”
I know it’s not critical to the paper, but I would be interested to see the analysis used to reach
that conclusion. How about in a footnote or supplemental materials?
4. I may have missed this, but what was the order of the tasks after the prime? It’s possible that
the biggest effect would appear directly after the prime.
5. I wonder if there’s interesting information in the graphs hidden by the fact that the Y axis
includes the entire range of responses. Some of the means seem somewhat different, but no
matter how much I squint, I can’t be sure of what I’m seeing. I think it’s worth adding graphs
with smaller Y axes.
Reviewer: 4
Comments to the Author(s)
I enjoyed reading the RSOS-161025, which reports two studies examining effects of participation
in communal vs. solitary games on various tasks related to independence /interdependence
and/or holistic /analytic cognitive style. I found the paper well-articulated (especially in the
discussion), and the results quite provocative. As usual with negative results, it is hard to make
sense of them. The authors tried to account for several possibilities. There are some more, based
on the inherent flaws in the design used in the present manuscript. I will reflect on several such
issues below.
Major concerns:
• Given sufficient statistical power (more on this below), there seems no clear need for a
pre-test of the DV. In fact, the virtue of experimental design is that with a sufficiently-large
sample random assignment takes care of the distribution of the individual differences. The
authors make a big deal out of using pre-post design, and use this design in all of their studies.
This leads to unnecessary complications in design (incl. potential assimilation/contrast effects;
even though I appreciate authors’ attempt to take care of this issue by using slightly altered
stimuli in pre and post-sessions). Moreover, it appears that the authors results reveal failure of
random assignment in study 1 (with condition-specific sig different distributions on some DV-
tasks BEFORE the manipulation took place). That is highly problematic, as well.
• The selected DV tasks are a hotchpotch of measures, without a clear rationale for their
selection, different theoretical origins and apparent lack of coherence on the individual level (e.g.,
see Na et al., 2010). For instance, Singelis scale is notorious for poor reliability, with constant
arguments whether it represents 2 dimensions or one dimension (and hence, one would examine
difference/relative scores; see discussion by Oyserman et al., 2001; and Diener et al. in response).
Also see recent evidence that 2 dimensions may not be sufficient (at least, in a cross-cultural
context, see work by Vignoles et al., 2016, JEP: Genera)l. Categorization task comes from a
completely different theoretical orientation and has little to do with the notion of individualism
18
per se (rather, it has been conceptualized as measuring holistic – analytic thinking style). The
problem with the categorization task becomes salient in study 2, where the effect observed on this
task goes in the opposite from the predicted direction. Unless the authors can show that the
nomological network of this task (performed on the data in each study) indicates high
convergence between performance on this task and other holistic/analytic or
independence/interdependence tasks, it is hard to make sense out of this significant effect of
condition x prime on the categorization task. Other, drawing-based tasks have unknown
psychometric properties and have never been validated as measures of either individualism-
collectivism or holistic-analytic processing. Generally, the association between performance on
these difference tasks has not been discussed in the manuscript.
• AS the authors reflect in the discussion, it is an empirical question whether anonymous-
player PGG is suitable to induce interdependence when performed among strangers on separate
computer terminals in the lab. Given some evidence that PGG contributions tend to be selfish as a
function of greater time spent on a decision (see Rand et al., 2012, Nature), it is possible that some
participants became fairly self-centered as a function of taking part in a game, with a critical
moderator involving among of time spent on a task. Further, what were the actual contributions
in the PGG and how did they relate to the DVs?
• Overall, the anonymous nature in the PGG task in Study 1 appears to be a serious
weakness preventing one from having a clear interpretation of the meaning of the results.
• Both studies are underpowered, even though larger than the initial cultural priming
studies. It is a shared knowledge among many cross-cultural researchers that these priming
studies are subject to a huge file-drawer problem, which further diminishes estimates. For
instance, in Study 2 there are fewer than 35 ppl per between-subject cell, which is a problem for
the power in the studies. The average effect size in psychology is ~ d = .41, not considering the
possible file-drawer that was typical to the priming research in 90s-2000s. Thus, with four
between-subject cells, the sample in Study 2 appears underpowered.
• Is there a possible confound of mood/disappointment in Study 1? Given that PGG/
Bandit tasks preceded the DVs, it is possible that differential outcome/payment on the economic
game distorted the effects (this is not a concern if participants did not know about their actual
contributions until the very end of the study; but it is unclear from the write-up if it is what
actually happened). Did the authors control for this variable (both as a covariate and/or as a
moderator)? I suspect the small sample size may distort the effects here.
Additional/minor comments:
• I found the conclusion, as articulated in the abstract, dissatisfactory. The results indicate
that ecocultural theory is not supported in the lab. This can suggest either that priming is not
effective for introducing cultural differences AND/OR that ecocultural theory is simply wrong.
Both ought to be acknowledged as viable explanations. The authors do so later in the discussion,
but not in the abstract. Though I tend to agree that priming ineffectiveness is a more likely
explanation, it is still possible that the theory is plainly wrong.
• Some of the introduction, concerning review of prior literature, needs a more careful
read of the existing scholarship, including work by Bertram Malle and colleagues on the lack of
support for dispositional attributions in the US, as well as cross-cultural work on relative
differences to acknowledge situational vs. dispositional attributions beyond East vs. West
comparison (e.g., across differences social classes in the US, and when examining Eastern
Europeans; I have written on this topic in Grossmann & Varnum, 2011). Similarly, the cross-
cultural evidence for self-enhancement is more complex than presented in the MS (see works by
Sedikides and colleagues).
• I believe Ancient Greece reference as the origin of the western thought is used
figuratively and not literally. It is a metaphor. Same for Ancient China and farming... I doubt
there would be serious claims in the field these days that those ancient cultures have a direct
impact on the cross-cultural differences observed today.
• There is another approach to test ecocultural hypothesis, which involves cross-temporal
analysis of the psychological tendencies (e.g., IND/COL) and herding vs. farming development
over time (see relevant examples in related work; e.g., Grossmann & Varnum, 2015). Though not
19
taking a large-scale temporal frame inferred by the proponents of the ecocultural theory, this
approach has a larger temporal dimension than lab-experiments.
• Why did the authors use an original (24 item) Singelis scale, rather than the updated 30-
item scale? Why did the authors omit the analyses on hierarchy items of the Singelis scale rather
than including them as a separate group?
• Given some claims that men (vs. women) have a more independent self-construal (see
work by Susan Cross and Hazel Markus on this topic), I do not think that the gender x prime
interaction is atheoretical and not hypothesized. Quite the opposite, knowing the literature, it is
possible to predict that sex (or gender) would have an effect, especially pronounced after the
manipulation task. Of course, it is post-hoc, but the authors should acknowledge this viable
possibility.
Author's Response to Decision Letter for (RSOS-161025)
See Appendix C.
label_version_3
RSOS-161025.R1 (Revision)
label_author_6
Review form: Reviewer 4 (Igor Grossmann)
Is the manuscript scientifically sound in its present form?
No
Are the interpretations and conclusions justified by the results?
No
Is the language acceptable?
Yes
Is it clear how to access all supporting data?
Yes
Do you have any ethical concerns with this paper?
No
Have you any concerns about statistical analyses in this paper?
Yes
Recommendation?
label_recommendation_6
Major revision is needed (please make suggestions in comments)
Comments to the Author(s)
label_comment_6
I enjoyed reading the RSOS-161025.R1. I was one of the prior reviewers (Reviewer 4). I greatly
appreciated authors clarifying some of the points I raised earlier. At the same time, drawing from
the revised manuscript and comments to my previous review, I believe there is a substantial need
for further revision of the manuscript. The authors ought to be a better job integrating prior
literature, providing a clear conceptual model of what they are measuring, more careful analyses,
and acknowledgements of the power-related limitations of their design.
20
Let me be perfectly frank. I like the manuscript and what the authors want to do. I am a strong
supporter of open science and believe in the value of replications, as well as theoretical and
methodological clarity. Thus, I appreciate the attention to detail the authors are devoted to. In the
revision, I would like to see more work done along these lines, focusing on the conceptual clarity
of DVs, psychometric analysis of the nomological network of tasks capturing interdependence
and cognitive style, study-wise correction for multiple testing (e.g., Bonferroni or other less
conservative tests), and a more detail-oriented evaluation of prior literature on cross-cultural
differences and priming research. I wish the authors best of luck in their revision.
Some of my concerns are the same I had earlier. Thus, before commenting on the updated text, I
would like to comment on authors responses on my previous queries.
1. I was a bit confused about the answer to the first point I raised in the prior version of the
manuscript. The issue I raised concerned the differences in performance on a pre-test as a
function of condition. Task-specific variations on the pre-experimental measurement as a
function of condition people will be assigned to later do not real concern the number of tasks run.
It also does not have anything to do with the direction of effects. If the assignment is truly
random, there should be no differences. Simply claiming that difference go in different directions
on different tasks does not help here. Maybe, what authors rather refer to is the notion of
multiple testing, so that some effects may be significant by chance? Such an interpretation is
certainly plausible. But if so, please used correction techniques for running multiple tests and
critically examine the effects. As it stands, the issue with failure of random assignment should be
acknowledged (and/or correction technique should be applied throughout the manuscript). To
me, the answer presented in the response letter remains unresolved.
2. As somebody who was majorly involved in the development and refinement of the
measures discussed in the present manuscript, and was one of the lead authors of the paper the
authors refer to support their claim (Varnum et al., 2010), I felt at odds with authors’
interpretation of the word by my colleagues and myself. Where exactly did my co-authors and I
wrote that holistic tasks such as those dealing with categorization should be sub-summed under
the rubric of interdependence? In our Current Directions piece, we have merely shown that
groups that score higher on social tasks measuring interdependence are also more likely to
engage in holistic processing of information. One of the points of the Varnum et al., 2010 was to
show that the families of tasks capturing interdependence and cognitive style are DISTINCT and
should not be put into the same basket together. The arguments in Varnum et al., 2010, do not
make any sense, unless one acknowledges that interdependence and cognitive style are separate
cosntructs. Together with the evidence from Na et al., 2010, which we published in the same year,
it seems implausible to treat the tasks as measuring a unitary construct on the individual level the
authors operate on in the present manuscript.
Further, I yet have to see a *single* psychometrics-oriented study validating ANY of the holistic
tasks reported in the present manuscript. If anything, Na et al., 2010, has shown that the
psychometric properties are likely to be pretty bad on the individual level of analysis. Masuda et
al. certainly did not publish anything concerning psychometric analysis of the tasks reported in
the 2008 paper. Merely showing cross-cultural differences on a measure and conceptually
replicating these differences with a different task does not have ANYTHING to with the question
of reliability and validity of the instrument. I can cook up a measure of using chopsticks vs. forks
as a test of holistic vs. analytic behaviour and I bet it would be a darn good measure for detecting
cross-cultural differences between East Asians and Europeans. Yet, it would still have zero
validity, in part because performance on this measure would be likely unrelated to performance
on established measures of holistic attention. Note, I am generally concerned about the use of
such measures for capturing individual differences and my criticism applies for this general
stream of research (including the work done by myself and my colleagues). But given the outlet
and the orientation on individual level of analysis, I believe this critique is particularly applicable
here. Overall, what I am trying to say in this point is that: (a) the authors should avoid
misrepresentation of the prior work; (b), they should be much more careful in their claims that
the tasks are well validated and psychometrically sound. They are absolutely not. The tasks were
developed ad-hoc, mainly for demonstrating cross-cultural differences (rather than reliable
estimating individual differences) and the tasks used in this paper have certainly not been
refined. My strong recommendation would be to conduct and report zero-order correlations as
21
well as PCA analyses of the tasks to show whether they indeed should be captured under the
same rubric on the individual level of analysis. Such an approach is only justified if the authors
can show positive correlations between different tasks and/or convergence of results on a single
principal component. Without it, any claims that the present results concern the concept of
“interdependence” appear methodologically and theoretically unwarranted.
3. Let me address the point about the perception of the priming literature. First, Reviewer 1
misremembers what Oyserman & Lee paper has actually shown. The paper included only a
handful of studies claiming effects of individualism/collectivism priming on strictly cognitive
tasks concerning decision-making, memory, or attentional patterns, all using different measures
(see Table A5 in the paper). Some other tasks claimed to show effects of interdependence prime
on cognition are misclassified (e.g., self-other differences or reports of personal attitudes are
clearly about social preferences and not cognitive processes) or are based on the fake data
produced by Stapel. This stands in contrast to the rather large number of tasks priming
individualism/collectivism and observing effects in the social domain of
individualism/collectivism. Second, as somebody who was in the same department as Oyserman
for a while, I can attest to the institutional knowledge of the huge file drawer problem. In this
sense, of course, i/we priming literature is no different from other bodies of research on priming.
So, I agree that being straightforward about non-replications is a great virtue in the current
climate.
4. The issue of power: The reason why I did not comment on your power analysis is
because it uses highly implausible d=.5. Even if one examined Oyserman & Lee (2008) and
focuses on cognition, the effect size varied from one cognitive task to another from d=.29 to d=.5
(see Table 4) or But there is a bigger question: Do you claim that priming effects are stronger than
most other studies in psychology? If you claim that expected d = .5, whereas in psychology it is d
= .41, you sort of do... However, given that you explicitly acknowledge the file drawer issue, you
should apriori lower your “real” d. That is precisely why I did not acknowledge your attempt to
post-hoc justify your design. The rationale does not seem sound to me. This section ought to be
re-written. Also note, your power is dependent on the number of factors in your model, not just
on the number of contrasts you are examining. Given that the design inherently involves a 2 X 2,
you ought to include the four cells in your model (even if just to control for them). This has an
implication for your power analysis, as well.
5. Minor comment (re: citing additional literature in the introduction): As I indicated in my
prior review, existing literature review is dissatisfactory. The authors merely dismissed this
point, indicating that they do not know why one should talk about the debate concerning cultural
differences in self-enhancement or dispositional vs. situational attributions. I may be wrong, but I
always thought that an introduction is supposed to provide a balanced overview of the state of
the field on the topic. The authors currently don’t provide such an overview. Instead, they seem
to be stuck in the 1990s in their knowledge of cultural differences. Merely writing along the lines
like in the statement here: “studies with Western participants suggested that people showed self
enhancement bias,[…] Yet subsequent studies with East Asian participants found this self-
enhancement bias to be much reduced, and often entirely absent” seems to suggest robust cross-
cultural difference on various related dimensions. By adding more recent citations (including
works by Sedikides or Malle) the authors can show that the differences may not be as large and
are probably more nuanced. This piece of information is important, as the paper ultimately tries
to test a theory drawing from cross-cultural research. Yet, the research the authors try to build on
is simply not as clear-cut as presented in the current manuscript. That is why I suggest providing
caveats about debates concerning self-enhancement and dispositional (vs situational) bias.
Igor Grossmann
22
label_end_comment
Decision letter (RSOS-161025.R1)
20th March 2017
Dear Dr Mesoudi:
Manuscript ID RSOS-161025.R1 entitled "Experimental priming of independent and
interdependent activity does not affect culturally-variable psychological processes" which you
submitted to Royal Society Open Science, has been reviewed. The comments of the reviewer(s)
are included at the bottom of this letter.
Please submit a copy of your revised paper within three weeks (i.e. by the 11th April 2017). If we
do not hear from you within this time then it will be assumed that the paper has been withdrawn.
In exceptional circumstances, extensions may be possible if agreed with the Editorial Office in
advance. We do not allow multiple rounds of revision so we urge you to make every effort to
fully address all of the comments at this stage. If deemed necessary by the Editors, your
manuscript will be sent back to one or more of the original reviewers for assessment. If the
original reviewers are not available we may invite new reviewers.
To revise your manuscript, log into http://mc.manuscriptcentral.com/rsos and enter your
Author Centre, where you will find your manuscript title listed under "Manuscripts with
Decisions." Under "Actions," click on "Create a Revision." Your manuscript number has been
appended to denote a revision. Revise your manuscript and upload a new version through your
Author Centre.
When submitting your revised manuscript, you must respond to the comments made by the
referees and upload a file "Response to Referees" in "Section 6 - File Upload". Please use this to
document how you have responded to the comments, and the adjustments you have made. In
order to expedite the processing of the revised manuscript, please be as specific as possible in
your response.
In addition to addressing all of the reviewers' and editor's comments please also ensure that your
revised manuscript contains the following sections before the reference list:
• Ethics statement
If your study uses humans or animals please include details of the ethical approval received,
including the name of the committee that granted approval. For human studies please also detail
whether informed consent was obtained. For field studies on animals please include details of all
permissions, licences and/or approvals granted to carry out the fieldwork.
• Data accessibility
It is a condition of publication that all supporting data are made available either as
supplementary information or preferably in a suitable permanent repository. The data
accessibility section should state where the article's supporting data can be accessed. This section
should also include details, where possible of where to access other relevant research materials
such as statistical tools, protocols, software etc can be accessed. If the data have been deposited in
an external repository this section should list the database, accession number and link to the DOI
for all data from the article that have been made publicly available. Data sets that have been
deposited in an external repository and have a DOI should also be appropriately cited in the
manuscript and included in the reference list.
• Competing interests
Please declare any financial or non-financial competing interests, or state that you have no
competing interests.
23
• Authors’ contributions
All submissions, other than those with a single author, must include an Authors’ Contributions
section which individually lists the specific contribution of each author. The list of Authors
should meet all of the following criteria; 1) substantial contributions to conception and design, or
acquisition of data, or analysis and interpretation of data; 2) drafting the article or revising it
critically for important intellectual content; and 3) final approval of the version to be published.
All contributors who do not meet all of these criteria should be included in the
acknowledgements.
We suggest the following format:
AB carried out the molecular lab work, participated in data analysis, carried out sequence
alignments, participated in the design of the study and drafted the manuscript; CD carried out
the statistical analyses; EF collected field data; GH conceived of the study, designed the study,
coordinated the study and helped draft the manuscript. All authors gave final approval for
publication.
• Acknowledgements
Please acknowledge anyone who contributed to the study but did not meet the authorship
criteria.
• Funding statement
Please list the source of funding for each author.
Once again, thank you for submitting your manuscript to Royal Society Open Science and I look
forward to receiving your revision. If you have any questions at all, please do not hesitate to get
in touch.
Sincerely,
Alice Power
Royal Society Open Science
openscience@royalsociety.org
on behalf of Essi Viding
Subject Editor, Royal Society Open Science
openscience@royalsociety.org
Comments to Author:
Reviewer: 4
Comments to the Author(s)
I enjoyed reading the RSOS-161025.R1. I was one of the prior reviewers (Reviewer 4). I greatly
appreciated authors clarifying some of the points I raised earlier. At the same time, drawing from
the revised manuscript and comments to my previous review, I believe there is a substantial need
for further revision of the manuscript. The authors ought to be a better job integrating prior
literature, providing a clear conceptual model of what they are measuring, more careful analyses,
and acknowledgements of the power-related limitations of their design.
Let me be perfectly frank. I like the manuscript and what the authors want to do. I am a strong
supporter of open science and believe in the value of replications, as well as theoretical and
methodological clarity. Thus, I appreciate the attention to detail the authors are devoted to. In the
revision, I would like to see more work done along these lines, focusing on the conceptual clarity
of DVs, psychometric analysis of the nomological network of tasks capturing interdependence
24
and cognitive style, study-wise correction for multiple testing (e.g., Bonferroni or other less
conservative tests), and a more detail-oriented evaluation of prior literature on cross-cultural
differences and priming research. I wish the authors best of luck in their revision.
Some of my concerns are the same I had earlier. Thus, before commenting on the updated text, I
would like to comment on authors responses on my previous queries.
1. I was a bit confused about the answer to the first point I raised in the prior version of the
manuscript. The issue I raised concerned the differences in performance on a pre-test as a
function of condition. Task-specific variations on the pre-experimental measurement as a
function of condition people will be assigned to later do not real concern the number of tasks run.
It also does not have anything to do with the direction of effects. If the assignment is truly
random, there should be no differences. Simply claiming that difference go in different directions
on different tasks does not help here. Maybe, what authors rather refer to is the notion of
multiple testing, so that some effects may be significant by chance? Such an interpretation is
certainly plausible. But if so, please used correction techniques for running multiple tests and
critically examine the effects. As it stands, the issue with failure of random assignment should be
acknowledged (and/or correction technique should be applied throughout the manuscript). To
me, the answer presented in the response letter remains unresolved.
2. As somebody who was majorly involved in the development and refinement of the
measures discussed in the present manuscript, and was one of the lead authors of the paper the
authors refer to support their claim (Varnum et al., 2010), I felt at odds with authors’
interpretation of the word by my colleagues and myself. Where exactly did my co-authors and I
wrote that holistic tasks such as those dealing with categorization should be sub-summed under
the rubric of interdependence? In our Current Directions piece, we have merely shown that
groups that score higher on social tasks measuring interdependence are also more likely to
engage in holistic processing of information. One of the points of the Varnum et al., 2010 was to
show that the families of tasks capturing interdependence and cognitive style are DISTINCT and
should not be put into the same basket together. The arguments in Varnum et al., 2010, do not
make any sense, unless one acknowledges that interdependence and cognitive style are separate
cosntructs. Together with the evidence from Na et al., 2010, which we published in the same year,
it seems implausible to treat the tasks as measuring a unitary construct on the individual level the
authors operate on in the present manuscript.
Further, I yet have to see a *single* psychometrics-oriented study validating ANY of the holistic
tasks reported in the present manuscript. If anything, Na et al., 2010, has shown that the
psychometric properties are likely to be pretty bad on the individual level of analysis. Masuda et
al. certainly did not publish anything concerning psychometric analysis of the tasks reported in
the 2008 paper. Merely showing cross-cultural differences on a measure and conceptually
replicating these differences with a different task does not have ANYTHING to with the question
of reliability and validity of the instrument. I can cook up a measure of using chopsticks vs. forks
as a test of holistic vs. analytic behaviour and I bet it would be a darn good measure for detecting
cross-cultural differences between East Asians and Europeans. Yet, it would still have zero
validity, in part because performance on this measure would be likely unrelated to performance
on established measures of holistic attention. Note, I am generally concerned about the use of
such measures for capturing individual differences and my criticism applies for this general
stream of research (including the work done by myself and my colleagues). But given the outlet
and the orientation on individual level of analysis, I believe this critique is particularly applicable
here. Overall, what I am trying to say in this point is that: (a) the authors should avoid
misrepresentation of the prior work; (b), they should be much more careful in their claims that
the tasks are well validated and psychometrically sound. They are absolutely not. The tasks were
developed ad-hoc, mainly for demonstrating cross-cultural differences (rather than reliable
estimating individual differences) and the tasks used in this paper have certainly not been
refined. My strong recommendation would be to conduct and report zero-order correlations as
well as PCA analyses of the tasks to show whether they indeed should be captured under the
same rubric on the individual level of analysis. Such an approach is only justified if the authors
can show positive correlations between different tasks and/or convergence of results on a single
principal component. Without it, any claims that the present results concern the concept of
“interdependence” appear methodologically and theoretically unwarranted.
25
3. Let me address the point about the perception of the priming literature. First, Reviewer 1
misremembers what Oyserman & Lee paper has actually shown. The paper included only a
handful of studies claiming effects of individualism/collectivism priming on strictly cognitive
tasks concerning decision-making, memory, or attentional patterns, all using different measures
(see Table A5 in the paper). Some other tasks claimed to show effects of interdependence prime
on cognition are misclassified (e.g., self-other differences or reports of personal attitudes are
clearly about social preferences and not cognitive processes) or are based on the fake data
produced by Stapel. This stands in contrast to the rather large number of tasks priming
individualism/collectivism and observing effects in the social domain of
individualism/collectivism. Second, as somebody who was in the same department as Oyserman
for a while, I can attest to the institutional knowledge of the huge file drawer problem. In this
sense, of course, i/we priming literature is no different from other bodies of research on priming.
So, I agree that being straightforward about non-replications is a great virtue in the current
climate.
4. The issue of power: The reason why I did not comment on your power analysis is
because it uses highly implausible d=.5. Even if one examined Oyserman & Lee (2008) and
focuses on cognition, the effect size varied from one cognitive task to another from d=.29 to d=.5
(see Table 4) or But there is a bigger question: Do you claim that priming effects are stronger than
most other studies in psychology? If you claim that expected d = .5, whereas in psychology it is d
= .41, you sort of do... However, given that you explicitly acknowledge the file drawer issue, you
should apriori lower your “real” d. That is precisely why I did not acknowledge your attempt to
post-hoc justify your design. The rationale does not seem sound to me. This section ought to be
re-written. Also note, your power is dependent on the number of factors in your model, not just
on the number of contrasts you are examining. Given that the design inherently involves a 2 X 2,
you ought to include the four cells in your model (even if just to control for them). This has an
implication for your power analysis, as well.
5. Minor comment (re: citing additional literature in the introduction): As I indicated in my
prior review, existing literature review is dissatisfactory. The authors merely dismissed this
point, indicating that they do not know why one should talk about the debate concerning cultural
differences in self-enhancement or dispositional vs. situational attributions. I may be wrong, but I
always thought that an introduction is supposed to provide a balanced overview of the state of
the field on the topic. The authors currently don’t provide such an overview. Instead, they seem
to be stuck in the 1990s in their knowledge of cultural differences. Merely writing along the lines
like in the statement here: “studies with Western participants suggested that people showed self
enhancement bias,[…] Yet subsequent studies with East Asian participants found this self-
enhancement bias to be much reduced, and often entirely absent” seems to suggest robust cross-
cultural difference on various related dimensions. By adding more recent citations (including
works by Sedikides or Malle) the authors can show that the differences may not be as large and
are probably more nuanced. This piece of information is important, as the paper ultimately tries
to test a theory drawing from cross-cultural research. Yet, the research the authors try to build on
is simply not as clear-cut as presented in the current manuscript. That is why I suggest providing
caveats about debates concerning self-enhancement and dispositional (vs situational) bias.
Igor Grossmann
Author's Response to Decision Letter for (RSOS-161025.R1)
See Appendix D.
26
label_version_4
RSOS-161025.R2 (Revision)
label_author_7
Review form: Reviewer 4 (Igor Grossmann)
Is the manuscript scientifically sound in its present form?
Yes
Are the interpretations and conclusions justified by the results?
Yes
Is the language acceptable?
Yes
Is it clear how to access all supporting data?
Yes
Do you have any ethical concerns with this paper?
No
Have you any concerns about statistical analyses in this paper?
No
Recommendation?
label_recommendation_7
Accept as is
Comments to the Author(s)
label_comment_7
I enjoyed reading the revision and appreciated seeing the additional analyses, explanations, and
revised interpretations of prior and current research. I recommend accepting the current version
for publication.
Apologies for being such a pain with my prior queries. The topic is close to my heart and I believe
one cannot be careful enough when working with non-replications (to bolster support against
possible critiques by scholars motivated to dismiss this work).
All the best,
IG
label_end_comment
Decision letter (RSOS-161025.R2)
18th April 2017
Dear Dr Mesoudi,
I am pleased to inform you that your manuscript entitled "Experimental priming of independent
and interdependent activity does not affect culturally-variable psychological processes" is now
accepted for publication in Royal Society Open Science.
You can expect to receive a proof of your article in the near future. Please contact the editorial
office (openscience_proofs@royalsociety.org and openscience@royalsociety.org) to let us know if
27
you are likely to be away from e-mail contact. Due to rapid publication and an extremely tight
schedule, if comments are not received, your paper may experience a delay in publication.
Royal Society Open Science operates under a continuous publication model
(http://bit.ly/cpFAQ). Your article will be published straight into the next open issue and this
will be the final version of the paper. As such, it can be cited immediately by other researchers.
As the issue version of your paper will be the only version to be published I would advise you to
check your proofs thoroughly as changes cannot be made once the paper is published.
In order to raise the profile of your paper once it is published, we can send through a PDF of your
paper to selected colleagues. If you wish to take advantage of this, please reply to this email with
the name and email addresses of up to 10 people who you feel would wish to read your article.
On behalf of the Editors of Royal Society Open Science, we look forward to your continued
contributions to the Journal.
Kind regards,
Alice Power
Editorial Coordinator
Royal Society Open Science
openscience@royalsociety.org
http://rsos.royalsocietypublishing.org/
Reviewer(s)' Comments to Author:
Reviewer: 4
Comments to the Author(s)
I enjoyed reading the revision and appreciated seeing the additional analyses, explanations, and
revised interpretations of prior and current research. I recommend accepting the current version
for publication.
Apologies for being such a pain with my prior queries. The topic is close to my heart and I believe
one cannot be careful enough when working with non-replications (to bolster support against
possible critiques by scholars motivated to dismiss this work).
All the best,
IG
Appendix A
Review of: Experimental priming of independent and interdependent activity does not
affect culturally-variable psychological processes
For Royal Society
Summary
This study had participants play collective or solitary games and then tested
them on cross-cultural measures. It found no effect of playing collective or solitary
games.
On the one hand, I commend the authors for putting negative findings out
there. We all too often just put these “failed” studies in the file drawer. But they
deserve to be seen!
On the other hand, the two biggest weaknesses I see are (1) loose selection of
cross-cultural tasks that suggests to me very little in-depth experience with these
measures and (2) a range of questions of how to interpret the findings. What can be
primed and what cannot be primed? I discuss these limitations more below.
Overall, I think this paper could be suitable for publication with several fairly
intense changes. First, the paper would benefit for a more nuanced discussion of
priming (described below). Second, the methods need to be explained and scrutinized
in more detail. I explain in detail ideas for improvements below.
Major Points
1. The biggest issue this paper raises in my mind is how possible is it to prime things?
Surely there are some simple things out there that are easy to prime. We’ve all had
experience thinking slowly and deliberately, and we’ve all had experience thinking
quickly and hotly. I’m not wary of priming that.
But let me talk about experience I’ve had priming something I was much more
wary about. Across several studies, my lab found that people who had moved more in
the past (residential mobility) are more individualistic and analytic thinking. That
makes sense. As people move around, they cut their ties with other people and focus
more on the self.
So then I tried three successive priming studies in the lab. I had people
imagine moving to a different city. Then I measured their thought style. I got the
opposite result three times! What gives?
We found that the mobility prime made people really nervous. “Would I know
anyone there? What would I do there? I wouldn’t have any friends!” That happens in
real life too, but it’s only the short-term effect of moving. In the long run, I think
people get used to it and start to rely on the self more.
This experience has made me fundamentally wary of the possibility of priming
large complex activities. I’m NOT saying “priming is worthless.” I’m NOT saying
“we shouldn’t try priming.” But if we find something in the real world but it doesn’t
work in priming, I am very hesitant to say, “well the real world finding is false.”
So where we do go from there? I think we need to think deeply about how
closely these primes stick to the real world. How closely can we replicate large,
complex real-world systems?
Finally, there’s the question: are we priming the actual thing or are we priming
people’s IDEA of that thing? For example, studies prime collectivism, and that’s OK
sometimes. But one problem is that people associate collectivism with being nice to
other people. At least in East Asia, collectivism is more about a friend/stranger
distinction, not about being nice to everyone. Are people getting that distinction when
we prime them with collectivism? I don’t know.
1A. Page 31 suggests essentially, “well maybe all those priming studies in the meta-
analysis are wrong.” That’s possible, but it seems unlikely to me. Instead, here’s a
much more nuanced and plausible explanation: some things can be primed and some
things can’t be primed. Some things are basic and common in everyday life; some
cultural systems are large and complex and not easily primed.
Or here’s another possibility. Some psychologists use bi-cultural samples (for
example, students living abroad) because they think these people have both concepts
readily available in mind. But if I grow up in the US or the UK, how easy is it for me
to really immerse myself in the
The authors can mention to reproducibility crisis. That’s fine. But if I have to
assign probability to different explanations, I’d assign a low probability to the idea
that a meta-analysis of dozens and dozens of studies showing an effect are all just
wrong. I’d assign a higher probability to the complications of priming I described
above.
1B. I agree with the argument that the origami game sounds more like rice farming (p.
22-23). But let me also bring us back to healthy skepticism. I presume the participants
are strangers. This is quite different from traditional rice farming. The paper should
also mention this limitation of priming.
2. The second biggest concern is the selection and analysis of cultural tasks. The
selection seems haphazard and needlessly deviant from prior research. The authors
cite prior studies but then change how they design the tasks. If we do that, how can
we be sure it’s still measuring the same thing? Below I point out specific issues.
2A. I noticed the categorization task is quite different from the version commonly
used in cross-cultural studies (e.g., Ji, Zhang, & Nisbett; Talhelm et al., 2014). First,
the individual items are different (dog and cat weren’t in the above studies). Readers
should be aware of this.
Second, having participants explain the reason for the pairing is different as
well. The authors provide a justification for it, and that’s fine, but if you haven’t
shown me that the two methods lead to similar results, how can I know that
verbalizing it like that won’t affect the result? For mood effects, many studies have
found that verbalizing it will completely wipe away the effect.
2B. I have a similar concern with the portrait task. Did the Masuda paper have people
pick photos? My understanding is they just looked at the pictures people naturally
took or drew. That does not necessarily mean that people will actively choose those
pictures. I suspect if you go around to different cultures and ask people, they’re not
aware of this difference. Why change the task from the original paper?
2C. Now the landscape drawing task was in the original Masuda study (if my memory
is correct). But I think it’s important to note explicitly whether the instructions were
the same as the original or not.
2D. Has the pronoun task been used like this before? I’ve never heard of it being used
in the lab like this. I only know of it being used as a prime (underling task) or in
large-scale datasets (such as English books use less “we” according to Google N gram
over time). My guess is you’d be unlikely to find differences unless the sample were
very large.
More importantly, why is the distinguishing feature singular versus plural
pronouns (p. 13)? As far as I’ve seen, the cultural difference is between group
pronouns (us, we) and self pronouns (I, me, mine). Singular versus plural is irrelevant.
There are singular pronouns (he, she) that are neither self-focused or we-focused. The
same is true for other plural pronouns (they, theirs).
2E. This study included the Singelis scale. I’m not 100% opposed to that, although I
wouldn’t use it in my study. Meta-analyses have found that these scales do NOT find
East-West differences. See the Heine “reference group effect” paper and the
Oyserman and Lee meta-analysis. Also see Kitayama et al., 2009, “a cultural task…”
The point here is that if these scales don’t find the expected cross-cultural
differences, why would we expect them to find differences here? At the very least, the
article needs to point out this documented problem with these scales.
3. The third big area of concern for me is potential deviations of these games from
actual rice farming. For one, as the paper points out, the collective goods game seems
to be anonymous. Page 10 says players “could not identify other players.” That seems
like a dangerous deviation from reality.
In reality, in-person shaming of identified individuals may have been a key
part of rice farming. Aoki in his book thinks it was. Now, the authors point this out
later. But I’d point it out in the earlier description of the game.
3B. Another dangerous deviation is that the product produced was communal. Rice
villages have collective irrigation infrastructure yes, but the goods are still kept at the
level of the farmer. They share labor, but I’ve never heard of any rice village splitting
up the crop or having communally farmed land.
The paper should discuss this at the very least in the discussion. I’d like at
least one paragraph in the intro too. The conclusion isn’t “so therefore this study is
meaningless.” Instead, the conclusion should be, “it’s hard to do this stuff!” There’s
imprecisions. Here are some we can think of. And that’s OK. Science is hard. Science
is imprecise.
4. The results (p. 12) describe categorical choices as “interdependent.” This confuses
thought style with social style. The descriptions should be changed throughout. The
same is true for the portrait task.
Page 3 says, “Eastern thought is characterised by interdependence.”
Interdependence is really more a social style, not a thought style. Instead, I’d say
Eastern thought is characterized by a focus on the connectedness between items
(something like that).
5. I wonder how seriously the authors take the potential problems with before-after
designs. Of course these have the benefit of high statistical power, but they also have
the problem of inertia. Let’s say I take the categorization task and get used to
choosing the relational items. Then I do the farming game and then go back to the
categorization task. I think, “Oh this task. All right, I’ll keep choosing these relational
items.”
Now if we sacrifice some statistical power and give the task only post-prime,
then we can be sure we’re not suffering from inertia. The prime can flex its full
muscle and get participants to think differently without prior influence.
I’m not saying I KNOW this problem contaminated the study. I have no idea.
But then, neither do the authors of this study. And what concerns me even more is the
paper doesn’t address this potential shortcoming.
6. Let me give a plea as a reader: Break up the long paragraphs! For example, a single
paragraph stretches from page 8 to page 10. As a reader, it’s tiring and muddled.
There are plenty of natural places to break up the flow into more manageable
chunks, such as: first talk about the categorization task, then the horizon task, and so
on. Your readers will thank you!
7. Page 4 is slightly incorrect. It says, “East Asian thought, on the other hand, arose
in Ancient China where the main means of subsistence was rice farming.” If we’re
talking about ancient China, I’d say “farming” not just “rice farming.” The true early
cradle of Chinese civilization (around the time of Confucius) was in the north and was
based mostly on millet. Rice came later and was mostly in the south.
This gets to an important point about the rice theory. The contrast between
East and West is rice and wheat, but it is also the intensity of farming. For example,
wheat farming in northern China was much more intensive than in Western Europe.
So it’s really a continuum from herding and hunting/gathering to farming, with
variations between wheat farming (less interdependent) and rice farming (more
interdependent).
8. Were the effects calculated as difference scores? Page 14 makes me think not. The
graphs seem to say yes. If no, why not? Don't we want to subtract their baseline
scores?
9. I can’t make heads or tails of Figure 1. It seems like every graph should have one
blue box and one red box, right?
Correction: I just figured it out. There is tiny font at the very bottom labeling
the different conditions. This should be made clearer.
10. I appreciate the comparison of sample sizes (p. 31), but how about calculating
power for the studies? Not post-hoc power, but with a 2x2 design and a sample size of
84, how big would the effect have to be to find an effect 80% of the time? 90% of the
time?
11. Lots of people think that collectivism should lead to lower competition. But when
I’ve discussed this with people who have lived in China or Japan, they have the exact
opposite feeling. And from an empirical perspective, I can say with certainty that
survey items measuring competitiveness (like on the horizontal-vertical individualism
scale) routinely show people from East Asia agree more with items assessing
competition.
This paper seems to presume the opposite based on…intuition? In fairness, the
paper does cite a study of herders, but that’s just one datapoint. At the very least, the
paper should acknowledge this complexity.
12. A final limitation that should definitely be mentioned in the paper is the fact that
there wasn’t a control condition. I wouldn’t say a control condition is 100% necessary,
but I think it would at the very least be interesting here. For example, we could see
whether there’s an effect of playing these sorts of games in the first place compared to
people who come and just complete the tasks.
Minor Points
1. P. 14 says there was no “reliable” shift. Was does that mean? Significant?
2. P. 17 says “much as real-life herders would be aware of other herders, although not
interact with them.” I wonder about that. Did herders usually see other herders as they
herded? It seems like a presumption to me. I’d delete it.
pendix B
neral response:
thank the editor for soliciting, and reviewers for providing, three carefully
nsidered and thoughtful reviews. All three reviewers found merit in the paper and
re enthusiastic about the publication, in particular of a negative finding relating to
opic of current interest within cultural psychology. All were largely happy with the
rature reviewed, analyses and presentation. The main concerns were over (i) our
oice of dependent measures, and (ii) the adequacy of the priming task. We have
ised the manuscript, particularly the Methods and Discussion sections, to take
se concerns into account. In particular, we provide greater justification and
ence of our choices of dependent measure tasks, and we are more explicit in the
cussion about the limitations of our priming methodology. Responses to specific
nts are below in bold. Page numbers refer to the revision with track changes.
viewers' Comments to Author:
viewer: 1
mments to the Author(s)
mmary
s study had participants play collective or solitary games and then tested them on
ss-cultural measures. It found no effect of playing collective or solitary games.
the one hand, I commend the authors for putting negative findings out there. We all
often just put these “failed” studies in the file drawer. But they deserve to be seen!
the other hand, the two biggest weaknesses I see are (1) loose selection of cross-
tural tasks that suggests to me very little in-depth experience with these measures and
a range of questions of how to interpret the findings. What can be primed and what
not be primed? I discuss these limitations more below.
erall, I think this paper could be suitable for publication with several fairly intense
nges. First, the paper would benefit for a more nuanced discussion of priming
scribed below). Second, the methods need to be explained and scrutinized in more
ail. I explain in detail ideas for improvements below.
jor Points1. The biggest issue this paper raises in my mind is how possible is it to
me things? Surely there are some simple things out there that are easy to prime. We’ve
had experience thinking slowly and deliberately, and we’ve all had experience thinking
ckly and hotly. I’m not wary of priming that.
t let me talk about experience I’ve had priming something I was much more wary
out. Across several studies, my lab found that people who had moved more in the past
idential mobility) are more individualistic and analytic thinking. That makes sense. As
ople move around, they cut their ties with other people and focus more on the self.So
n I tried three successive priming studies in the lab. I had people imagine moving to a
erent city. Then I measured their thought style. I got the opposite result three times!
at gives?
found that the mobility prime made people really nervous. “Would I know anyone
re? What would I do there? I wouldn’t have any friends!” That happens in real life too,
it’s only the short-term effect of moving. In the long run, I think people get used to it
d start to rely on the self more.
s experience has made me fundamentally wary of the possibility of priming large
mplex activities. I’m NOT saying “priming is worthless.” I’m NOT saying “we shouldn’t
priming.” But if we find something in the real world but it doesn’t work in priming, I am
y hesitant to say, “well the real world finding is false.”
where we do go from there? I think we need to think deeply about how closely these
mes stick to the real world. How closely can we replicate large, complex real-world
tems?
ally, there’s the question: are we priming the actual thing or are we priming people’s
A of that thing? For example, studies prime collectivism, and that’s OK sometimes.
t one problem is that people associate collectivism with being nice to other people. At
st in East Asia, collectivism is more about a friend/stranger distinction, not about being
e to everyone. Are people getting that distinction when we prime them with
lectivism? I don’t know.
e reviewer raises a series of questions and concerns about priming, many of
ich we share. At the heart is the comment: “if we find something in the real world
t it doesn’t work in priming, I am very hesitant to say, “well the real world finding
alse.””. We agree entirely, and indeed this is what we say in the Discussion. We
clear that while one explanation of our negative findings is that the ecocultural
pothesis is incorrect, it is equally possible that priming in our study was not
equate (because of the tasks or measures that we used), or priming in general is
possible to test this hypothesis (because independence-interdependence cannot
primed, and is instead developmentally fixed). We have now emphasised this in
Discussion: “Yet we would certainly not advocate rejecting the ecocultural
pothesis on the basis of this negative finding alone, without considering
ernative reasons for our negative results.” (line 626), as well as expanding our
cussion of the alternatives.
Page 31 suggests essentially, “well maybe all those priming studies in the meta-
lysis are wrong.” That’s possible, but it seems unlikely to me. Instead, here’s a much
re nuanced and plausible explanation: some things can be primed and some things
’t be primed. Some things are basic and common in everyday life; some cultural
tems are large and complex and not easily primed.
here’s another possibility. Some psychologists use bi-cultural samples (for example,
dents living abroad) because they think these people have both concepts readily
ilable in mind. But if I grow up in the US or the UK, how easy is it for me to really
merse myself in the
e authors can mention to reproducibility crisis. That’s fine. But if I have to assign
bability to different explanations, I’d assign a low probability to the idea that a meta-
lysis of dozens and dozens of studies showing an effect are all just wrong. I’d assign a
her probability to the complications of priming I described above.
have added this possibility, also raised by Reviewer 3: “Alternatively, it is
ssible that at least some previous priming effects are valid, but that priming only
rks by reminding people of contexts with which they are already familiar….” (line
9).
. I agree with the argument that the origami game sounds more like rice farming (p. 22-
. But let me also bring us back to healthy skepticism. I presume the participants are
angers. This is quite different from traditional rice farming. The paper should also
ntion this limitation of priming.
ded on line 655: “Finally, our collective groups were mostly composed of
angers. Perhaps long-term social relationships are required to foster
erdependence, and our experiments were too transient.”
The second biggest concern is the selection and analysis of cultural tasks. The
ection seems haphazard and needlessly deviant from prior research. The authors cite
or studies but then change how they design the tasks. If we do that, how can we be
e it’s still measuring the same thing? Below I point out specific issues.
r tasks were chosen with time constraints in mind: whereas previous studies
voted the entire study time to obtaining measures (e.g. Uskul et al.’s tasks took 40
ns), we had to use briefer versions of the tasks given the lengthy priming activity
between the measures. We also wanted to use a battery of measures rather than
t one, to tap self-report measures of social orientation as well as non-self-report
gnitive measures. Nevertheless we now acknowledge in the Discussion that
er, more targeted measures may have been a more powerful approach (line 655-
8). We also request that the editor and reviewers ask themselves: if we had found
ositive result in our measures, would they have made the same criticisms? We
nk perhaps not, and this underlines the need to publish negative findings.
I noticed the categorization task is quite different from the version commonly used in
ss-cultural studies (e.g., Ji, Zhang, & Nisbett; Talhelm et al., 2014). First, the individual
ms are different (dog and cat weren’t in the above studies). Readers should be aware of
.
s true that we used different items to those previous studies, but we disagree that
s is a major issue. We did not make up our word sets, we took our stimuli from
rkman and Hutchison (1984), who first established a tendency in Western
ldren to categorise based on superordinate category. The logic behind the items
identical: two of the triad can be grouped based on relations, and two based on
egory. If the cross-cultural effect of categorisation is robust, it should work for
y such word triads, not only those ones used in specific studies. We clearly cite
Markman and Hutchison paper, give examples of the items in the text, and
lude the full tasks in the Supplementary Information, so we think that this is quite
ar, transparent and replicable.
cond, having participants explain the reason for the pairing is different as well. The
hors provide a justification for it, and that’s fine, but if you haven’t shown me that the
methods lead to similar results, how can I know that verbalizing it like that won’t
ect the result? For mood effects, many studies have found that verbalizing it will
mpletely wipe away the effect.
act the reviewer reminded us that in Experiment 2 we removed the request to
vide a reason (as now added to the paper on lines 427-429) given that in
periment 1 the responses indicated that people’s explanations did indeed fit with
intended manipulation (i.e. they were either category-based or relationship-
sed). So, even if verbalisation did in fact wipe out the effect (which we are not
are of any evidence to support) in Experiment 1, this could not have been the
se in Experiment 2.
. I have a similar concern with the portrait task. Did the Masuda paper have people
k photos? My understanding is they just looked at the pictures people naturally took or
w. That does not necessarily mean that people will actively choose those pictures. I
pect if you go around to different cultures and ask people, they’re not aware of this
erence. Why change the task from the original paper?
s task was indeed identical to that used in Masuda et al.’s Study 3 (albeit with
erent photographs, as we did not have access to Masuda et al.’s photos and in
y case their photos were clearly based in the US, which would have been a
traction to our UK participants). In both our and their study, participants selected
e of four photographs of the same person but with the face-to-frame ratio
nipulated. We suspect the reviewer is thinking of Masuda et al.’s Study 2, in
ich participants took a photo of the experimenter.
. Now the landscape drawing task was in the original Masuda study (if my memory is
rect). But I think it’s important to note explicitly whether the instructions were the same
the original or not.
e instructions were indeed identical, except that because we asked participants
draw two landscapes (one before and one after the prime), we changed the items
one of the pictures. One, like Masuda et al., required “a house, a tree, a river, a
rson and a horizon”, the other required “a barn, a tree, a cow, a road and a
rizon”. This also partly addresses the reviewer’s point below that participants will
t give the same responses after the prime out of habit: by changing the items we
reased the chances that participants would not simply draw the same picture as
ore. Note that the experimental booklet is included as Supplementary
ormation, so readers can easily check this.
. Has the pronoun task been used like this before? I’ve never heard of it being used in
lab like this. I only know of it being used as a prime (underling task) or in large-scale
asets (such as English books use less “we” according to Google N gram over time).
guess is you’d be unlikely to find differences unless the sample were very large.More
portantly, why is the distinguishing feature singular versus plural pronouns (p. 13)? As
as I’ve seen, the cultural difference is between group pronouns (us, we) and self
nouns (I, me, mine). Singular versus plural is irrelevant. There are singular pronouns
, she) that are neither self-focused or we-focused. The same is true for other plural
nouns (they, theirs).
e reviewer is correct: singular vs plural was erroneously stated in the Methods,
d has now been changed to the correct ‘independent vs interdependent’
oughout.
This study included the Singelis scale. I’m not 100% opposed to that, although I
uldn’t use it in my study. Meta-analyses have found that these scales do NOT find
st-West differences. See the Heine “reference group effect” paper and the Oyserman
d Lee meta-analysis. Also see Kitayama et al., 2009, “a cultural task...”The point here is
t if these scales don’t find the expected cross-cultural differences, why would we
ect them to find differences here? At the very least, the article needs to point out this
cumented problem with these scales.
ded on line 674-675, with citation to Heine et al. (2002).
The third big area of concern for me is potential deviations of these games from actual
farming. For one, as the paper points out, the collective goods game seems to be
onymous. Page 10 says players “could not identify other players.” That seems like a
ngerous deviation from reality. In reality, in-person shaming of identified individuals may
e been a key part of rice farming. Aoki in his book thinks it was. Now, the authors
nt this out later. But I’d point it out in the earlier description of the game.
e anonymous nature of the task in Experiment 1 was explicitly stated when it was
t introduced (line 236: “participants...could not identify other players”). We agree
t this is a deviation from reality, which is why we made the second priming task
Experiment 2 a face-to-face, non-anonymous activity. This is clearly stated on
es 354-358:
Experiment 1, participants were anonymous, and even in the collective condition
at computer terminals and never interacted directly. In the real-life subsistence
ivities described and studied in [16,17], people are not anonymous, and in
lective situations will interact and communicate face-to-face in order to
ordinate and cooperate.”
. Another dangerous deviation is that the product produced was communal. Rice
ages have collective irrigation infrastructure yes, but the goods are still kept at the level
he farmer. They share labor, but I’ve never heard of any rice village splitting up the
p or having communally farmed land. The paper should discuss this at the very least in
discussion. I’d like at least one paragraph in the intro too. The conclusion isn’t “so
refore this study is meaningless.” Instead, the conclusion should be, “it’s hard to do
stuff!” There’s imprecisions. Here are some we can think of. And that’s OK. Science is
d. Science is imprecise.
e reviewer seems to slightly misunderstand the Public Goods Game here. While
re is a communal pot, each individual participant received their own final payoff,
ich is the amount of points they kept from the pot plus the shared output from
communal pot. Each participant therefore received their own unique payoff,
pending on how much they chose to invest. The pot here represents willingness
cooperate with others, e.g. in coordinating water use, or investing in irrigation
tems that affect every farmer. We think that this is roughly the situation faced in
e farming, although obviously with many simplifications.
The results (p. 12) describe categorical choices as “interdependent.” This confuses
ught style with social style. The descriptions should be changed throughout. The same
rue for the portrait task.Page 3 says, “Eastern thought is characterised by
erdependence.” Interdependence is really more a social style, not a thought style.
tead, I’d say Eastern thought is characterized by a focus on the connectedness
ween items (something like that).
noted on line 59, we used the terms ‘independence-interdependence’
oughout, to incorporate independent-interdependent social orientation,
ividualism-collectivism and analytic-holistic thought. As the reviewer
monstrates, it is difficult to come up with a single set of terms to capture all of the
ks, so we opted for independent-interdependence throughout for ease of
sentation, rather than having separate terms for different tasks which would be
nfusing and laborious.
wonder how seriously the authors take the potential problems with before-after
signs. Of course these have the benefit of high statistical power, but they also have the
blem of inertia. Let’s say I take the categorization task and get used to choosing the
tional items. Then I do the farming game and then go back to the categorization task. I
nk, “Oh this task. All right, I’ll keep choosing these relational items.”
w if we sacrifice some statistical power and give the task only post-prime, then we can
sure we’re not suffering from inertia. The prime can flex its full muscle and get
ticipants to think differently without prior influence.
not saying I KNOW this problem contaminated the study. I have no idea. But then,
ther do the authors of this study. And what concerns me even more is the paper
esn’t address this potential shortcoming.
e reviewer is correct that this is a trade-off. Within-participant designs have
her statistical power but may have affected responses in the way described. This
why we used different parts of the tasks before and after (e.g. different
dscapes were drawn before and after), counterbalanced across participants, to
courage fresh answers afterwards. But there may still be a carry-over effect. This
s been added to the Discussion (lines 680-689).
Let me give a plea as a reader: Break up the long paragraphs! For example, a single
agraph stretches from page 8 to page 10. As a reader, it’s tiring and muddled.There
plenty of natural places to break up the flow into more manageable chunks, such as:
t talk about the categorization task, then the horizon task, and so on. Your readers will
nk you!
ragraph broken up as suggested, here and throughout.
Page 4 is slightly incorrect. It says, “East Asian thought, on the other hand, arose in
cient China where the main means of subsistence was rice farming.” If we’re talking
out ancient China, I’d say “farming” not just “rice farming.” The true early cradle of
inese civilization (around the time of Confucius) was in the north and was based mostly
millet. Rice came later and was mostly in the south.
moved the word ‘rice’ on line 76.
s gets to an important point about the rice theory. The contrast between East and West
ice and wheat, but it is also the intensity of farming. For example, wheat farming in
thern China was much more intensive than in Western Europe. So it’s really a
ntinuum from herding and hunting/gathering to farming, with variations between wheat
ming (less interdependent) and rice farming (more interdependent).
s is a good suggestion, added the point about intensity of work on lines 651-654:
nother factor might be intensity of work. Rice farming was argued by Talhelm et
[17] to be more work-intensive than wheat farming, which necessitates it being
re collective, but perhaps it is work effort itself that is the crucial factor rather
n collective action. This could be varied independently of activity pattern.”
Were the effects calculated as difference scores? Page 14 makes me think not. The
phs seem to say yes. If no, why not? Don't we want to subtract their baseline scores?
e analyses did not use difference scores, as stated on line 166: we used before
d after values as within-participant measures. This allows us to include absolute
ormation about the scores, not just the relative differences. A predicted effect of
priming tasks would be indicated by a significant interaction between time
fore vs after) and condition (solitary vs collective), with solitary becoming more
ependent and collective becoming more interdependent. This has been added to
captions of Tables 2 and 3 for clarity. In the Figures however we show difference
ores for ease of presentation.
can’t make heads or tails of Figure 1. It seems like every graph should have one blue
x and one red box, right?Correction: I just figured it out. There is tiny font at the very
tom labeling the different conditions. This should be made clearer.
have made the figure text bigger. We have also made Figure 2 simpler by only
senting the main Activity comparison, omitting the Payoff condition. This makes
uch easier to follow. We have moved the original Figure 2 to the SI as Figure S1,
ich still shows both Activity and Payoff conditions together.
I appreciate the comparison of sample sizes (p. 31), but how about calculating power
the studies? Not post-hoc power, but with a 2x2 design and a sample size of 84, how
would the effect have to be to find an effect 80% of the time? 90% of the time?
wer analysis added to this section, line 697. For Study 2, a moderate effect would
e a power of 0.8 with our sample size.
Lots of people think that collectivism should lead to lower competition. But when I’ve
cussed this with people who have lived in China or Japan, they have the exact
posite feeling. And from an empirical perspective, I can say with certainty that survey
ms measuring competitiveness (like on the horizontal-vertical individualism scale)
tinely show people from East Asia agree more with items assessing competition.This
per seems to presume the opposite based on...intuition? In fairness, the paper does
a study of herders, but that’s just one datapoint. At the very least, the paper should
nowledge this complexity.
the reviewer notes, we cite a study [ref 32] supporting our prediction that more
ividualistic countries should be more competitive, at least at the individual level.
en that we found no effect of relative vs absolute payoffs, we feel it unwise to
eculate on other predictions unnecessarily, given that we found no effect of this
tor. Note that we had to make a decision either way in our experimental design,
d it is not clear to us whether the ecocultural hypothesis entails absolute or
ative payoffs. Hence our manipulation of this factor, to avoid the criticism that we
ployed the wrong payoff types.
A final limitation that should definitely be mentioned in the paper is the fact that there
sn’t a control condition. I wouldn’t say a control condition is 100% necessary, but I
nk it would at the very least be interesting here. For example, we could see whether
re’s an effect of playing these sorts of games in the first place compared to people
o come and just complete the tasks.
do not follow this point: for people just playing the games, on what measure
uld we assess the games’ effect? Our use of a before-after design was intended
capture responses before the prime, so this is equivalent to having people come
and just do the tasks.
nor Points
. 14 says there was no “reliable” shift. Was does that mean? Significant?
anged to ‘statistically significant’ (line 317).
. 17 says “much as real-life herders would be aware of other herders, although not
eract with them.” I wonder about that. Did herders usually see other herders as they
ded? It seems like a presumption to me. I’d delete it.
leted.
viewer: 2
mments to the Author(s)
e manuscript “Experimental Priming of Independent and Interdependent Activity Does
t Affect Culturally-Variable Psychological Processes” reported findings of two
eriments that attempt to test the ecocultural hypothesis by using a priming
thodology. The results in general did not support the hypothesis: They demonstrated
t the participants’ performance on four cognitive tasks and one self-report measure
not differ across experimental conditions. The authors discussed three reasons which
y explain the negative data: (1) theoretical limitation—the explanatory power of the
cultural hypothesis may be weak; (2) methodological limitation—the priming tasks
plied to the two studies were not effective; and (3) limitation of participants sampled—
ticipants who grew up in a Western monocultural environment already internalize a set
culturally specific cognitive styles, and therefore their responses are not at all malleable
oss the priming conditions.
hough the results are overall negative, the topic is interesting. In addition, the
oretical framework and methodology are solid. The results are accurately reported with
entific rigor. Given the fact that the Open Access Journal encourages the authors to
ort negative data, the current manuscript has a great potential to be accepted with
eral major revisions. The detailed explanations are as follows:
e Selection of Tasks
hough the authors reported five tasks in detail, and the descriptions meet the standard
he scientific journal, the reasons of their choice were not justifiably discussed. For
mple, has the pronoun selection task been used by many other researchers before,
d the validity been assured? Why did they not use the tasks reported by Talhelm et al.,
d Uskul et al.? Could the results turn out to be positive if the authors used these
ks? These issues should be clearly addressed in text.
s is a good point. A new paragraph has been added to the Discussion addressing
s (line 665-678). We justify our choices given the time constraints imposed by
luding a priming task. Uskul et al.’s measures, for example, took approx. 40 mins,
ich would have used up most of our 1 hour sessions. The Talhelm et al. study had
t been published when we ran the first set of experiments. Nevertheless, we
plicitly note that future studies might focus on one or two key measures in full.
e Absolute vs. Relative Condition
ink that the explanation of the absolute vs. relative manipulation is not satisfactory.
arer logic and a more elaborated description is needed.
think that the description on line 368 is clear, and already quite long (327 words).
he reviewer has specific queries then we would be happy to add clarification.
e Manipulation Check
en using the priming methodology, it is mandatory for researchers to include several
nipulation check questions. Without these measures, researchers cannot assess
ether the priming is effective enough for randomly assigned participants to temporarily
d a specific mindset (e.g., independent vs. interdependent mindset). Please report
se values, and revise the section of the methodological limitation in the general
cussion section.
s unclear to us what the reviewer has in mind here. The very point of our study
s to assess whether the priming tasks caused participants to temporarily hold a
ecific mindset (independence vs interdependence). We are not sure what such a
nipulation check would be, if not our dependent measures that assess
ependent vs interdependent thinking. As noted above in response to Reviewer 1,
have added a longer section in the Discussion on the possibility that our priming
ks were inadequate to elicit responses, which we fully acknowledge as a
ssibility.
e Statistical Analyses.
both Experiment 1 and 2, the authors reported the statistical analyses of the regression
del as well as the charts. The style is somewhat deviated from the convention of
ss-cultural studies. Why not simply report the values of ANOVA, which allows the
dience to better contrast the results of the current manuscript to those of previous
ss-cultural literature?
chose regression as this approach is more flexible than ANOVAs, is increasingly
ndard across fields (e.g. psychology, economics, biology), and is easier to
erpret (e.g. regression coefficients give the strength of the effect in a way that F
ues do not). ANOVAs are mathematically identical to regressions, so our findings
uld be unchanged. We include the full data as Supplementary Information so
er authors are welcome to run alternative analyses.
e Limitation.
e authors did a good job addressing a variety of limitations of the current research
sign as well as the current discourses in terms of the relationship between culture,
logy, and psychological processes. To further facilitate the implications of the negative
a, the authors should elaborate their sections extensively and provide directions of
ure research. Such extensive discussions will help the audience to further advance this
ic.
e reviewer does not provide any specific guidance as to what they would like to
e included, but given the expanded Discussion as a result of Reviewer 1’s
mments we hope that this is sufficient.
viewer: 3
mments to the Author(s)
el conflicted in my final recommendation. On the one hand the experiments are nicely
signed, the statistics appropriate, and the paper is well written. The literature could also
more examples of null results. However, ultimately I was unconvinced that present
dies contribute to our understanding of the causes of cultural differences in
ependence/interdependence. I see no obvious connection between cultural evolution
nterdependent and independent cognition, or related aspects of psychology, that
cur on a generational timescale and attempts to use these activities to move cognition
lab setting. Moreover, since these cultural differences seem to persist long after the
ticular historical activities have been given up (i.e. people are no longer farming wheat
ice) and seem to persist somewhat after migration to a new cultural context, they
m even less likely to be so easily recreated in a lab. Unfortunately, I therefore have to
ommend rejection on the basis that the studies do not contribute to the debate over
ecocultural hypothesis.
ile this may be the intuition of the reviewer, we think that many people –
rticularly within social and cultural psychology - would assume that cognition can
primed in this way. Indeed, this seems to be the opinion of Reviewer 1, with some
veats. We think that it is only by publishing negative findings such as ours that
s belief can be updated. At the very least, publishing our negative finding may
vent others from wasting time and effort pursuing this line of investigation (if the
viewer is indeed correct), although we feel that extensions of our study, as
tlined in the Discussion, will be worthwhile.
ve two further critiques of the design. (1) The study also differs from many (though not
priming studies in this literature in that they are not moving existing individual
iability in independence and interdependence by reminding people via, for example,
ming a context, but instead trying to create this cognition.
knowledged and discussed on line 729-739. We think that this is justified given
t the ecocultural hypothesis is all about collective vs solitary action rather than
mory of contexts, hence our use of behavioral rather than memory-based
mes.
The authors should do more to justify the sample size, which is likely too low. Given
at we know about p-hacking and underpowered studies, being in the top 10% and
% of the existing literature is not sufficient justification (nor are these particularly high
centiles given the prevalence of the p-hacking and power problem).
wer analysis added on line 697. We estimate a power, in Study 2 at least, of 0.8,
ich should be sufficient to detect an effect.
pendix C
viewers’ comments are reproduced in bold, with our responses in non-bold text. Line
mbers refer to the manuscript with track changes.
-----------
viewer: 1
mmary
s paper presents a test wherein participants complete solo or cooperative tasks.
ticipants complete measures of individualism and analytic thought before and after. The
ivities lead to no consistent effect.
e reviewed a prior version of this paper, and I commend the authors for incorporating
e of the feedback. I felt the biggest flaw of the paper was that the authors were really
fident that the activities adequately represented rice farming and had almost no discussion
why folding origami for 30 minutes may not be like farming paddy rice in a Chinese village
he 1500s. The new paper has more recognition of the difficulty of that. For example, I
reciate the added epistemological humbleness in the discussion here:
Yet we would certainly not advocate rejecting the ecocultural hypothesis
on the basis of this negative finding alone, without considering alternative
reasons for our negative results.
re are more plusses: I appreciate the benefits of experimental approaches. I also like that
authors made the data and analysis script publicly available.
ink this paper is getting closer to being finished.
appreciate the reviewer taking the time to review the paper again. We are very happy that
she acknowledges our attempts to qualify our findings and highlight their limitations, the
queness of our study in experimentally testing a hypothesis that has previously only been tested
ng natural contrasts, and our commitment to providing open data and scripts.
a broad scheme, I think two broad areas need improvement:
The discussion is now more humble about the equivalence between folding origami and
e farming. Adding a few points there (described below) will be sufficient. But the intro is
gely unchanged. More needs to be done to discuss why these manipulations are and are not
actual rice farming.
have revised the Introduction along these lines, to match the tone of the Discussion. See
ponses to specific comments below.
There are a few details about how the results are analyzed and presented graphically that
escribe below. These should be fairly easy to address. Below I describe specific points.
se have been addressed; see specific responses below.
in Points
his study didn’t find evidence of eco-cultural priming. That’s fine. But I thought the
clusion in the abstract was too extreme: “We suggest re-examination of this priming
rature particularly in light of the recent replication crisis.” Does one study call into
stion an entire line of research with dozens of findings? I think of the Oyserman and Lee
ta-analysis of 67 studies and 6,240 participants that found effects of priming individualism
collectivism on cognitive style. One negative finding with a completely different prime
sn’t lead me to think “replication crisis.”
appreciate this point, and have rewritten the abstract. The final sentence now reads:
“However, it may also be that our priming tasks are inappropriate or inadequate for
simulating subsistence-related behavioral practices, or that these measures are fixed early
in development and therefore not experimentally primable, despite many previous studies
that have purported to find such priming effects.”
have deleted the sentence “We suggest re-examination of this priming literature particularly in
t of the recent replication crisis”.
vertheless, we think that the reviewer may underestimate the magnitude of the replication crisis.
think, in the light of evidence of p-hacking, HARKing, and the file drawer effect, that it is
fectly plausible that the effect shown in Oyserman & Lee’s meta-analysis turns out to be
rious. Indeed, as Daniel Kahneman recently wrote regarding a book chapter he had previously
tten about social priming:
“the existence of a substantial file-drawer effect undermines the two main
tools that psychologists use to accumulate evidence for a broad hypotheses:
meta-analysis and conceptual replication. Clearly, the experimental
evidence for the ideas I presented [regarding social priming] in that chapter
was significantly weaker than I believed when I wrote it.”
(http://retractionwatch.com/2017/02/20/placed-much-faith-underpowered-
studies-nobel-prize-winner-admits-mistakes/)
agree with the reviewer that our single negative finding should not cause an overturning of the
ming literature, but it is only by publishing negative findings such as ours that this imbalance can
addressed.
take issue with the statement “Rather than assuming that a certain means of subsistence
ails a certain pattern of activity…” In my own research, I don’t “assume” that rice
ming entails certain labor, coordination, and cooperation. I cite the evidence of people who
e studied and observed rice farmers personally. Specific sources can be found in Talhelm
l., 2014.
at we meant here was that with naturally occurring populations, it is difficult to entirely rule out
founding factors and to know exactly what aspects of the activity (labor, coordination,
peration etc.) are causally responsible for certain psychological responses. Essentially it is the
e point made by the reviewer elsewhere: that rice farming is a hugely complex activity that is
icult to summarise and isolate into separate causal factors. We have rewritten this sentence to
following, also now highlighting the limitations of experiments (lower external validity):
“While obviously featuring lower external validity than comparisons of naturally-
occurring inter-group variation, lab experiments afford greater control over
confounding variables and greater power to systematically manipulate causal
factors – in this case relating to subsistence-related activity patterns.” (line 139-
143)
know I’m harping on the same point as my earlier review, but it’s easily the most
portant point of the paper. The biggest theoretical issue with the paper is that choosing a
k that involves other people necessarily mimics rice. And that problem is still to confidently
ored in certain parts of the paper. For example, that viewpoint is in this sentence on page
…the key factor predicted to have generated cross-cultural differences in thinking styles
according to the ecocultural hypothesis: whether activity is solitary (as in herding) or
collective (as in rice farming).
ere are lots of activities that involve other people that are not like rice farming at all. A
ple days ago, I watched the new Star Wars movie in a theater with 100 other people. But
t was not like farming rice.
Now, maybe I’m being a bit fatuous in that comparison, so let me use one that’s more
ated to farming. Tsuruta studied labor sharing among shifting farmers in the Congo and
e farmers in Japan. Both have labor sharing customs, but Tsuruta concludes they’re very
erent. In the Congo, they’re more like beer parties. People help clear a field, and then the
t gives everyone beer brewed from bananas. In the Congo, the labor sharing is more like a
ival. In Japan, Tsuruta described labor sharing as a much more strict, reciprocal occasion.
ne farmer accepted help and then later couldn’t repay because of sickness or injury, they
uld go so far as hiring labor to repay the debt. The labor sharing is more dyadic, long-
m, and infused with responsibilities and social debts. My point here is that the presence of
er people, interacting with them—even working directly with them—doesn’t adequately
cribe rice farming.
Now this account Tsuruta builds of the Congolese shifting farmers, doesn’t that sound
it more like the public goods game described in the paper? The Tsuruta account actually
aks against choosing the public goods game to represent rice farming. Here’s another
son why I would not choose a public goods game to represent rice farming: individualistic
tures like those in northern Europe score among the highest on trust toward strangers and
port for welfare programs. In fact, in the 2014 rice theory paper, I specifically used a task
t measured the difference between how people treat friends and strangers. People from the
e areas were much harsher toward strangers than people from wheat provinces. Thus, I
ht actually predict the opposite result from a public goods game.
imately, I’m making a tough criticism. How can I expect a lab experiment to adequately
mic rice farming? And that’s fine. We have to be realistic in our expectations about what a
gle study can do. Yet so should the authors in how they describe how well this priming
mics rice farming.
here’s specifically how the paper could be improved. Instead of saying that solitary versus
ective is the key feature, say that this is a feature of rice farming but also many other
ivities that humans do. It’s also just one single part of rice farming, but doesn’t include lots
ther things about rice farming.
at more accurately describes what this study is testing. And that doesn’t mean this study
no value. There’s plenty of value in it. But we should be clear about what we’re testing,
at we’re re-creating in the lab.
have addressed this “tough criticism” by rewriting several sections of the Introduction (lines 84-
111-126, 130-165), . Rather than confidently stating that it is solitary vs collective activity that
he key aspect of rice farming versus herding, we now follow the reviewer’s advice and highlight
rice farming entails many activities, any one of which could be responsible for increased
rdependence: solitary vs collective working, reliance on one’s own efforts vs the efforts of
ers, the presence of cooperation/collaboration, and overall intensity of working. We now quote
helm et al. at length, as well as Uskul et al., in order to illustrate this complexity and ambiguity
avoid charges of misinterpretation or simplification.
now acknowledge that many of these factors are also present (or at least ambiguous and not
y separated out) in our designs. We therefore present our experiment as an initial experimental
of the ecocultural hypothesis, which, if supportive, could then be used to go on to tease these
erent aspects of the hypothesis apart. The key section where we do this is below, with key
tence recognising limitations in bold:
“In both experiments, all participants completed various psychological measures of
independent-interdependent thinking both before and after a task. Half of the participants in
each experiment completed a solitary task where payoffs depend solely on one’s own
actions, with no coordination or cooperation with others, as is hypothesised to characterise
herding. The other half of the participants completed a collective task where payoffs depend
on one’s own and others’ actions, and involving some kind of coordination or cooperation,
as is hypothesised to characterise farming, and in particular rice farming. By necessity, our
tasks do not map precisely on to real-life subsistence practices, which are complex and
varied. But we think that our tasks capture at least some of the essential elements
described in verbal descriptions of the ecocultural hypothesis, including those quoted
above.” (line 150-165)
ally, regarding the ambiguity of the public goods game used in Experiment 1, we agree, which is
ctly why we abandoned it in Experiment 2 for a more clearly collaborative and collective task.
One thing I’ve learned from years of doing psychology experiments is that control
ditions aren’t always control conditions. In other words, tasks or conditions that are
ant to be neutral can still be priming certain feelings or modes of thinking. I worry that
e with the Multi-Armed Bandit game. I get that it’s solitary, but what else could that task
activating in people? Calculation? Rationality concerns? (Economic tasks can make people
are that their rational self-interest is being tested.) The paper could be improved by adding
east a couple sentences about what the MAB game could be priming other than what we’re
eling it with.
t, the reviewer misconstrues the MAB as a control condition, perhaps because of his background
rking with rice. It is not a control, it is a solitary prime, acting in opposition to the PGG, which
the collective prime. The MAB therefore does not need to be ‘neutral’, only as similar as
sible to the other condition except in our intended manipulation. We believe that we achieved
, contrary to the reviewer’s suggestion: both the MAB and the PGG were programmed in z-tree,
ured the same user interface, the same requirement for rationality (in the PGG just as much as
MAB, as people may try to calculate the rational response in the task presented), and the same
of performance-related monetary rewards as incentives. If the reviewer has some specific
rature on MAB tasks and what they might elicit then we would be happy to add this in a further
ision. Finally, note that we replaced the economic games in Experiment 2 with face-to-face
ami games, with no effect on the results.
’ve studied rice for the last 8 years of my life. I’ve read books about traditional villages
how they farm rice. I wrote an entire dissertation about rice farming. Yet after all these
rs, I’m not sure what the key behavioral parts of rice were. Is it the coordination involved
rrigation? Is it the commons problems of repairing irrigation dikes? Is it the tight,
iprocal labor exchanges? Is it a combination of all of these? I don’t know. We just don’t
e the data to be sure.
All of this is to say, I think we need to be humble about what we’ve manipulated and
at the key element is. If I can study this for 8 years and say, “I still don’t know.” How are
authors so sure that they’ve manipulated the right thing and that this specific way they’ve
nipulated it (in the lab, with strangers, for just 30 minutes) should lead to the outcomes
re talking about?
I’m not saying we should just throw up our hands and not try. I appreciate this
empt. Attempts like this can build up our base of knowledge. But we must also keep in
nd the bigger picture and exactly what this study does and does not show. The new version
he discussion has made progress in this regard. But the intro seems to be largely
hanged. The intro could be made This could be done in the paper by:
Toning down some of the more extreme language, such as “rather than assuming…”
eral parts of the Introduction have been rewritten to tone down the claims, including the
tence highlighted here (which was also raised above in point 2).
In the into or methods when the activities are describe, there should be more nuanced
cussion that recognizes how complicated this is, such as “rice farming involved lots of
aviors, from commons dilemmas to long-term reciprocal labor exchanges. It is not clear
ether the tasks in this experiment accurately represent the key elements of rice farming or
t they should have an effect in this sort of context (one-off experiences for short periods of
e [30 minutes] with strangers that will not be repeated).”
noted, we have rewritten the introduction to make the complexity of rice farming clear, for
mple:
“By necessity, our tasks do not map precisely on to real-life subsistence
practices, which are complex and varied.” (line 162)
have also now included two lengthy quotes from Uskul et al. and Talhelm et al. where they
cuss the characteristics of farming and herding, to provide direct information from the original
rces and avoid any suspicion of mischaracterisation or simplification of prior work.
erall though we think that the reviewer is being somewhat unreasonable here. The Discussion is
traditional place for comments regarding the limitations of the methods and suggestions for how
work can be improved in the future. To include a lengthy discussion of the limitations of the
hods in the Introduction, Methods and Discussion would be repetitive and redundant. There is
rit to being concise and readable, as well. We also note that the reviewer’s comments are overly
used on rice. Our study is intended to capture the broad herding vs farming distinction (as
oduced in the original Nisbett et al. 2001 paper), not only rice. This was a later qualification to
theory introduced by Talhelm et al. 2014. So while the reviewer is clearly focused on rice
ming, we do not wish our paper to become so.
he DV tasks are pretty creative, but one problem with creativity is that we don’t know
ether they’re actually measuring what we want them to. For example, the portrait selection
k described on page 9: I’ve never seen any research showing that people in different
tures actually actively express a preference for one or the other. The difference appears in
tural behaviors and magazines, but do people actively endorse this as a preference? I don’t
w. Readers should be aware that this measure has not been used in this way before.
we state in the paper, this portrait selection task was taken from Masuda et al.’s (2008) Study 4,
o found robust cross-cultural differences: Japanese participants selected portraits with smaller
jects and more background detail. As we make clear, all of our measures are taken from
lished literature showing cross-cultural variation in responses; none were created de novo for
study, so the reviewer can be reassured on this point.
here’s a fairly profound issue with calling draw a horizon line higher or lower
terdependent.” There’s one study documenting that as a cross-cultural difference, but I
nk few cultural psychologists would call it “interdependent.” The paper would be improved
he language were more carefully chosen and tasks more precisely connected to concepts.
state early on in the Introduction that we will use the terms ‘independent-interdependent’ as a
ch-all continuum for both social orientation (relationships between people) and holistic-analytic
nition (relationships between objects). As we note above, there is also a need for clarity and
ciseness; using the terms ‘analytic-holistic cognition’ and ‘independent-interdependent social
ntation and self-construal’ throughout the manuscript would be confusing and distracting. We
make these points explicitly and cite supporting evidence from Varnum et al. showing links
ween these dimensions:
“Although interdependence-independence of people and of objects are often characterised as
distinct, there is mounting evidence for their co-occurrence cross-culturally [13], and we
treat them here as part of a single interdependence-independence continuum.” (lines 66-68)
arding horizon height specifically, the original Masuda et al. paper argued that higher horizons
indicative of holistic cognition because it affords a broader perspective on a scene, allows more
ects to be drawn, with more interdependent relationships depicted between those objects. Hence
interdependence is between objects, rather than people, although this is argued to be linked
rnum et al. 2010).
nor Points
age 10 describes removing hierarchy items from the scale. Yet hierarchy differences are
o cultural differences and at least the horizontal-vertical scale of collectivism explicitly
ludes hierarchy. I’m not saying these items cannot be removed. But I think either a better
e needs to be made or readers should (in a footnote) see what happens in the data if this
ision were not made.
analysed the hierarchy (vertical-horizontal) dimension of the Singelis scale and found no effects
these measures either. Rather than add to an already dense Results section (in particular our
phs and tables), we leave this for interested readers to analyse using our enclosed dataset and R
e. We have, however, added vertical and horizontal measures to the data files to allow readers to
this, without having to calculate them from the questionnaire items.
might have missed this, but was there an analysis of differences in the DVs between
ups before the manipulation? This would be a check of random assignment.
rows labelled “Activity: solitary” in Tables 2 and 3 tell us whether there was a difference
ween conditions pre-test. Only one of the fourteen measures across both Experiments 1 and 2
significant at p<0.05: landscape horizon height in Experiment 1 at p<0.037, a weak effect that
lmost certainly spurious. Indeed, we would expect a type I error at p<0.05 to appear on average
ne in every twenty independent tests, therefore we suggest this difference would reasonably be
ected to arise by chance instead of a lack of randomization. Furthermore, seven of these pre-test
asures were in one direction (solitary higher than collective) and seven were in the opposite
ction (collective higher than solitary). This appears to us to be quite appropriately random.
ally, and most importantly, analysis of within participant variation in pre- and post-test measures
uces the influence of random individual differences upon our results.
’m curious about this statement on page 22:
xperiment 1 suggested that participants understood the nature of the relationships as rule
ed or relationship based.”
now it’s not critical to the paper, but I would be interested to see the analysis used to reach
t conclusion. How about in a footnote or supplemental materials?
s was based on an informal inspection of responses to the free-text explanation of choices in
eriment 1. All explanations matched the intended contrast, e.g. when cow and pig were selected,
reason was that cows produce milk. Unfortunately we did not conduct a formal analysis of these
cks, so cannot present them in the supplemental materials, but we think that this is a tangential
nt. The fact that these questions were included in Experiment 1 and omitted in Experiment 2,
hout an effect in either, suggests it was not crucial.
may have missed this, but what was the order of the tasks after the prime? It’s possible
t the biggest effect would appear directly after the prime.
stated on line 238, “All five measures were...counterbalanced in two randomly assigned versions
he questionnaires.” to avoid order effects.
wonder if there’s interesting information in the graphs hidden by the fact that the Y axis
ludes the entire range of responses. Some of the means seem somewhat different, but no
tter how much I squint, I can’t be sure of what I’m seeing. I think it’s worth adding graphs
h smaller Y axes.
en the high resolution of our images, the graphs can easily be zoomed in on to view the means
re precisely, and avoid squinting. But ultimately the exact position of the lines indicating the
ans matters little given the extensive overlap in variance between the conditions (as illustrated by
boxes), and statistical tests showing no reliable differences. Indeed, we would expect the mean
s to differ even if the data were entirely randomly generated, purely by chance. We have chosen
to alter the figures by truncating the y-axis as that would necessarily exclude some of the data,
ing a misleading picture of our data. Given that we have included our data and scripts as SI,
rested readers can easily reconstruct our graphs from our R code, and change the axis scales.
--------
viewer: 4
mments to the Author(s)
joyed reading the RSOS-161025, which reports two studies examining effects of
ticipation in communal vs. solitary games on various tasks related to independence
erdependence and/or holistic /analytic cognitive style. I found the paper well-articulated
pecially in the discussion), and the results quite provocative. As usual with negative results,
hard to make sense of them. The authors tried to account for several possibilities. There
some more, based on the inherent flaws in the design used in the present manuscript. I
l reflect on several such issues below.
jor concerns:
Given sufficient statistical power (more on this below), there seems no clear need for a
-test of the DV. In fact, the virtue of experimental design is that with a sufficiently-large
ple random assignment takes care of the distribution of the individual differences. The
hors make a big deal out of using pre-post design, and use this design in all of their studies.
s leads to unnecessary complications in design (incl. potential assimilation/contrast effects;
n though I appreciate authors’ attempt to take care of this issue by using slightly altered
muli in pre and post-sessions). Moreover, it appears that the authors results reveal failure
andom assignment in study 1 (with condition-specific sig different distributions on some
-tasks BEFORE the manipulation took place). That is highly problematic, as well.
noted above in the response to the previous reviewer, pre-test assignment was random: only one
4 measures showed a statistically significant difference between conditions pre-test at p<0.05,
this was a weak effect (p<0.037). Half of the measures showed a difference in one direction, the
er half showed a difference in the other direction. This seems pretty random to us. The
iewer’s assertion that there are problematic significant differences on DV tasks before
nipulation is therefore simply not true.
pite this, we used both pre and post test scores in our analysis for the simple reason that
oring pretest performance equates to throwing out useful information. Although there were no
tematic differences between conditions before the primes, there was certainly random individual
iation, as expected based on previous studies. Within-participant pre-post test designs are
licitly used to account for this individual variation. As the reviewer notes, the downside is the
ential for carry-over effects from pre to post test, but we are quite explicit in the manuscript
ut this limitation, and we employed various tactics to reduce it (e.g. using different items pre and
t test, counterbalancing the order of task presentation).
The selected DV tasks are a hotchpotch of measures, without a clear rationale for their
ction, different theoretical origins and apparent lack of coherence on the individual level
., see Na et al., 2010). For instance, Singelis scale is notorious for poor reliability, with
stant arguments whether it represents 2 dimensions or one dimension (and hence, one
uld examine difference/relative scores; see discussion by Oyserman et al., 2001; and Diener
l. in response). Also see recent evidence that 2 dimensions may not be sufficient (at least, in
ross-cultural context, see work by Vignoles et al., 2016, JEP: Genera)l. Categorization task
es from a completely different theoretical orientation and has little to do with the notion
ndividualism per se (rather, it has been conceptualized as measuring holistic – analytic
nking style). The problem with the categorization task becomes salient in study 2, where
effect observed on this task goes in the opposite from the predicted direction. Unless the
hors can show that the nomological network of this task (performed on the data in each
dy) indicates high convergence between performance on this task and other
istic/analytic or independence/interdependence tasks, it is hard to make sense out of this
nificant effect of condition x prime on the categorization task. Other, drawing-based tasks
e unknown psychometric properties and have never been validated as measures of either
ividualism-collectivism or holistic-analytic processing. Generally, the association between
formance on these difference tasks has not been discussed in the manuscript.
respectfully disagree with the reviewer’s points here, for the following reasons.
t, we chose a range of measures that have all been shown to vary cross-culturally (see specific
tions in the paper where the measures are first introduced), and that are all linked to
ependence/interdependence in either the social sense (dependence between people) or the object-
ception analytic-holistic sense (dependence between objects; see Varnum et al. 2010 for
dence that these two senses are linked). As the reviewer notes, each measure has its flaws and
itations: questionnaire items are more susceptible to self-report bias and reference-group effect,
example, as we acknowledge in the Discussion. This is exactly why we included a range of
erent measures: if we had used just one or two, and found a negative result, then it could easily
argued that we chose an inappropriate measure. We see the diversity of our measures as a
ngth, not a limitation.
ond, the reviewer notes a ‘problem’ with the categorization finding in Study 2, which goes in the
osite direction from that predicted. The reviewer treats this as a problem because he/she
umes that because it is statistically significant, it is a ‘true’ finding. We disagree, and explicitly
so in the paper: running several tests will inevitably lead to some findings that are below the
.05 cutoff, but are nevertheless due to chance. It would be different perhaps if it was a strong
ing (e.g. p<0.001), but it was actually quite weak (p<0.025). The overall pattern of findings, as
l as the weak effect for this measure, led us to argue that this finding is spurious. We therefore
not think that there is anything special about categorisation that requires any special explanation:
most plausible explanation is that the finding is spurious.
rd, it is simply not true that “drawing-based tasks have unknown psychometric properties and
e never been validated as measures of either individualism-collectivism or holistic-analytic
cessing”. The drawing tasks that we used in our study are taken from Masuda et al. (2008), who
only showed cross-cultural variation on this measure, but validated it with complementary
lyses of real-life artistic styles and other experimental measures. This reference is clearly cited
ur study (reference 28, on line 255 and 265). This study has been replicated several times,
uding with children, showing that differences only emerge at older ages indicative of cultural
uence (Senzaki et al. 2014). We have added a reference to that study to our manuscript to bolster
case (reference 29, on line 265).
AS the authors reflect in the discussion, it is an empirical question whether
nymous-player PGG is suitable to induce interdependence when performed among
angers on separate computer terminals in the lab. Given some evidence that PGG
tributions tend to be selfish as a function of greater time spent on a decision (see Rand et
2012, Nature), it is possible that some participants became fairly self-centered as a
ction of taking part in a game, with a critical moderator involving among of time spent on
sk. Further, what were the actual contributions in the PGG and how did they relate to the
s?
se are interesting points. First, all participants played the games (both the PGG and MAB) for
same amount of time, so time spent playing could not have differentially affected the results.
a on PGG contribution is included in our data file, as well as various other aspects of PGG play
uding earnings and punishment decisions. Of course, these cannot be used in the same main
lysis because they only apply to the half of our participants who played the PGG, not the MAB.
vertheless we can see whether these variables are significant predictors of performance on the
asures, within the collective condition only. Our analyses revealed no consistent or significant
cts of PGG performance on any measure. For brevity we do not include these results in our
er, but we do include our data set as SI so interested readers are welcome to conduct their own
lyses.
ally, note that we replaced the PGG with a face-to-face task in Experiment 2, so even if these
cerns were valid, they would not apply to our second Experiment.
Overall, the anonymous nature in the PGG task in Study 1 appears to be a serious
akness preventing one from having a clear interpretation of the meaning of the results.
agree entirely that this aspect of the PGG is a major limitation, which is why we ran Experiment
which we used a non-anonymous, face-to-face task.
Both studies are underpowered, even though larger than the initial cultural priming
dies. It is a shared knowledge among many cross-cultural researchers that these priming
dies are subject to a huge file-drawer problem, which further diminishes estimates. For
tance, in Study 2 there are fewer than 35 ppl per between-subject cell, which is a problem
the power in the studies. The average effect size in psychology is ~ d = .41, not considering
possible file-drawer that was typical to the priming research in 90s-2000s. Thus, with four
ween-subject cells, the sample in Study 2 appears underpowered.
t, we note that this directly conflicts with the comments of Reviewer 1 above who finds it
lausible that the Oyserman and Lee meta-analysis finding could be spurious. Clearly, the
sibility of extensive file-drawer effects is not shared amongst all researchers. Moreover, this
red knowledge seems to be private and unpublished. We think that this makes our case for
lication even stronger, to redress this file drawer effect.
ond, we explicitly acknowledge in the Discussion that our studies, while using a larger sample
than most other studies in the field, may still be too small to detect what may be weaker effects
n often assumed. Nevertheless, we present a power analysis in the Discussion (lines 751-753)
the reviewer strangely does not comment upon, where we show that assuming d=0.5, we would
e had power of 0.8 for finding an effect (note that the main contrast we tested was actually for
cells, contrasting solitary vs collective; the relative vs absolute payoff contrast was not central
ur hypothesis).
Is there a possible confound of mood/disappointment in Study 1? Given that PGG/
ndit tasks preceded the DVs, it is possible that differential outcome/payment on the
nomic game distorted the effects (this is not a concern if participants did not know about
ir actual contributions until the very end of the study; but it is unclear from the write-up if
what actually happened). Did the authors control for this variable (both as a covariate
/or as a moderator)? I suspect the small sample size may distort the effects here.
can calculate a standardised performance measure from both the PGG and MAB, representing
l earnings relative to earning of other participants. This is the variable ‘goodnessofparticipant’
he submitted data file. Including this variable in the regressions along with condition x time
ractions did not meaningfully alter the condition x time effects reported in the paper, nor was
ticipant performance ever a significant predictor itself. Participant performance, whether via
od or disappointment, therefore did not affect any of the dependent measures.
ditional/minor comments:
I found the conclusion, as articulated in the abstract, dissatisfactory. The results
icate that ecocultural theory is not supported in the lab. This can suggest either that
ming is not effective for introducing cultural differences AND/OR that ecocultural theory
imply wrong. Both ought to be acknowledged as viable explanations. The authors do so
r in the discussion, but not in the abstract. Though I tend to agree that priming
ffectiveness is a more likely explanation, it is still possible that the theory is plainly wrong.
s is a fair point, and we have rewritten the last lines of the abstract. See response to reviewer 1.
Some of the introduction, concerning review of prior literature, needs a more careful
d of the existing scholarship, including work by Bertram Malle and colleagues on the lack
upport for dispositional attributions in the US, as well as cross-cultural work on relative
erences to acknowledge situational vs. dispositional attributions beyond East vs. West
parison (e.g., across differences social classes in the US, and when examining Eastern
ropeans; I have written on this topic in Grossmann & Varnum, 2011). Similarly, the cross-
tural evidence for self-enhancement is more complex than presented in the MS (see works
Sedikides and colleagues).
have refrained from adding any additional literature to the Introduction. The reviewer does not
lain exactly how this additional literature might change our predictions or alter the implications
ur study. Simply presenting additional complexities for the sake of it reduces clarity and makes
paper harder to read. If, however, the reviewer wishes to point to specific places where citation
dditional literature would benefit the paper, we would be happy to consider it.
I believe Ancient Greece reference as the origin of the western thought is used
uratively and not literally. It is a metaphor. Same for Ancient China and farming... I doubt
re would be serious claims in the field these days that those ancient cultures have a direct
pact on the cross-cultural differences observed today.
r reading of the literature leads us to disagree with this claim. The study by Talhelm et al.
licitly argued that historical patterns of subsistence affect present-day cognition. They found
re interdependent thinking in regions of China which historically practiced rice farming,
pared to regions that historically practiced wheat farming. None of the participants in that study
e farmers of either variety. To quote Talhelm et al.:
“It is a safe bet that none of our thousand participants have actually farmed rice or
wheat for a living. Instead, the theory is that cultures that farm rice and wheat
over thousands of years pass on rice or wheat cultures, even after most people put
down their plows. Simply put, you do not need to farm rice yourself to inherit rice
culture.” (p.604-605)
ilarly, Nisbett et al. (2001) explicitly argued that differences in ancient Greece and China
pectively have been culturally inherited to shape contemporary psychological differences:
“[Our] research shows that, to a remarkable extent, the social and cognitive
differences that scholars have reported about ancient China and Greece find
their counterparts among contemporary peoples...[we] sketch an analysis of the
factors that might sustain "sociocognitive homeostatic systems" over
millennia” (p.292)
think it is clear that these authors are arguing that ancient patterns of subsistence and
losophy have persisted over “millenia”, and are not merely intended as metaphors.
There is another approach to test ecocultural hypothesis, which involves cross-
poral analysis of the psychological tendencies (e.g., IND/COL) and herding vs. farming
elopment over time (see relevant examples in related work; e.g., Grossmann & Varnum,
5). Though not taking a large-scale temporal frame inferred by the proponents of the
cultural theory, this approach has a larger temporal dimension than lab-experiments.
s study has been added to the list of alternative hypotheses (reference 40, line 685), in that it
ws an association between socio-economic status and individualism.
Why did the authors use an original (24 item) Singelis scale, rather than the updated
item scale? Why did the authors omit the analyses on hierarchy items of the Singelis scale
her than including them as a separate group?
comment above about hierarchy items, in response to Reviewer 1.
Given some claims that men (vs. women) have a more independent self-construal (see
rk by Susan Cross and Hazel Markus on this topic), I do not think that the gender x prime
eraction is atheoretical and not hypothesized. Quite the opposite, knowing the literature, it
ossible to predict that sex (or gender) would have an effect, especially pronounced after
manipulation task. Of course, it is post-hoc, but the authors should acknowledge this
ble possibility.
reviewer appears to misunderstand our results here. More independent self-construal would be
icated by a significant prime x condition interaction. An effect of the prime alone means that
re singular pronouns were used after the prime across both conditions (solitary and collective
s). What we found was a gender x prime interaction. In other words, men used more singular
nouns in both conditions than women. This is not what we predicted but nor does it indicate
re independent self-construal in men, which would need a gender x prime x condition
raction. The reviewer’s comment is therefore not valid and we have not added it to the
nuscript.
pendix D
mments to Author: Reviewer: 4
njoyed reading the RSOS-161025.R1. I was one of the prior reviewers (Reviewer
I greatly appreciated authors clarifying some of the points I raised earlier. At the
me time, drawing from the revised manuscript and comments to my previous
iew, I believe there is a substantial need for further revision of the manuscript.
e authors ought to be a better job integrating prior literature, providing a clear
nceptual model of what they are measuring, more careful analyses, and
knowledgements of the power-related limitations of their design.
me be perfectly frank. I like the manuscript and what the authors want to do. I
a strong supporter of open science and believe in the value of replications, as
ll as theoretical and methodological clarity. Thus, I appreciate the attention to
ail the authors are devoted to. In the revision, I would like to see more work done
ng these lines, focusing on the conceptual clarity of DVs, psychometric analysis
the nomological network of tasks capturing interdependence and cognitive style,
dy-wise correction for multiple testing (e.g., Bonferroni or other less
nservative tests), and a more detail-oriented evaluation of prior literature on
ss-cultural differences and priming research. I wish the authors best of luck in
ir revision.
thank Dr Grossman for taking the time to re-review our paper, for his thoughtful
mments, and for his positive overall evaluation of our study. We explain below how we
e addressed his remaining concerns. Line numbers refer to the track-changes version
he manuscript.
me of my concerns are the same I had earlier. Thus, before commenting on the
dated text, I would like to comment on authors responses on my previous
eries.
I was a bit confused about the answer to the first point I raised in the prior
sion of the manuscript. The issue I raised concerned the differences in
rformance on a pre-test as a function of condition. Task-specific variations on the
-experimental measurement as a function of condition people will be assigned to
er do not real concern the number of tasks run. It also does not have anything to
with the direction of effects. If the assignment is truly random, there should be
differences. Simply claiming that difference go in different directions on different
ks does not help here. Maybe, what authors rather refer to is the notion of
ltiple testing, so that some effects may be significant by chance? Such an
erpretation is certainly plausible. But if so, please used correction techniques for
ning multiple tests and critically examine the effects. As it stands, the issue with
ure of random assignment should be acknowledged (and/or correction technique
ould be applied throughout the manuscript). To me, the answer presented in the
ponse letter remains unresolved.
Grossman seems still unconvinced that there was random assignment and
nsequently no systematic differences between the groups in the pre-test measures. We
nk him for raising this concern again, which other readers may share. We now realise
should have addressed this explicitly in the manuscript, and we have now done so.
st please let us explain why we think that it is clear that there were no systematic
erences at pre-test between the participants who subsequently did the solitary priming
k and participants who subsequently did the collective priming task, contrary to Dr
ossman’s concern. As noted in our previous response, the relevant statistics are in the
s labelled “Activity: solitary” in Table 2 and Table 3 in the paper. These show the
gnitude of the difference between solitary and collective groups before any priming
ks in each of the 14 measures. We reproduce these rows below, for reference:
asure (Experiment 1) Value Std.Error t-value p-value
egorisation Activity: solitary 0.107 0.055 1.965 0.053
trait selection Activity: solitary -0.061 0.145 -0.42 0.676
dscape: Horizon height Activity: solitary 0.085 0.04 2.128 0.037*
dscape: Objects Activity: solitary -0.086 0.162 -0.531 0.597
nouns Activity: solitary -0.005 0.058 -0.087 0.931
ependent self-construal Activity: solitary -0.013 0.031 -0.419 0.676
rdependent self-construal Activity: solitary -0.067 0.037 -1.815 0.074
asure (Experiment 2) Value Std.Error t-value p-value
egorisation Activity: solitary -0.079 0.045 -1.738 0.085
trait selection Activity: solitary 0.026 0.036 0.722 0.472
dscape: Horizon height Activity: solitary -0.017 0.042 -0.405 0.686
dscape: Objects Activity: solitary 0.11 0.18 0.609 0.544
nouns Activity: solitary 0.033 0.047 0.7 0.485
ependent self-construal Activity: solitary 0.061 0.238 0.257 0.798
rdependent self-construal Activity: solitary 0.07 0.25 0.28 0.780
Grossman states that “if assignment is random, there should be no differences”. If by
he means that the mean values should be *identical* for each measure - in other
rds the “Value” column above should be exactly zero all the way down, indicating that
pretest means for the two groups are exactly identical - then we disagree.
ndomness does not produce identicality in finite samples. This would be like flipping a
n 100 times, repeating 14 times, and expecting on all 14 times to get exactly 50 heads
d 50 tails. Randomness does not work like this: sometimes we would get 49:51,
metimes 52:48, sometimes 45:55 and so on.
tead we should look at whether the differences in the table above are *statistically
erent* from zero. The p-values tell us that in only one case out of 14 can we reject the
l hypothesis that there is no difference between the groups (Landscape: Horizon
ight in Experiment 1, p=0.037). As we noted in our previous reviewer response, we
uld resist the temptation to see anything with p<0.05 as ‘true’. As we noted, we
uld also not be surprised to expect purely by chance that one of these 14 tests should
e p<0.05. We therefore dismiss this statistically ‘significant’ difference as spurious. The
t that 0.037 is so close to 0.05 also strongly indicates it is spurious.
Grossman suggests running Bonferroni corrections for multiple tests, which is indeed
at we were inferring when we said we would expect one in 20 tests to be spuriously
nificant. This is an excellent suggestion, and it was remiss of us not to have done this
he manuscript, which we now explicitly discuss. However, it would simply mean that
her than p<0.05 we should use p < 0.05/7=0.007, given the 7 models run for each set
participants. By this significance criteria, none of the comparisons in the table above
anywhere near significant, bolstering our claim even further that there are no
tematic differences between the groups at the pretest stage. We have added this point
ncerning multiple tests to the results section of Experiment 1 as follows:
“Note that this lack of statistical significance holds even without
controlling for multiple tests. Given that seven model were run, one for
each measure, on data from the same set of participants, we may
decide to use a Bonferroni-corrected a level of 0.05 / 7 = 0.007. Using
this more conservative criterion fails to change our conclusion that the
activity prime had no effect on any measure.” (line 437-441)
now explicitly state that pre-prime differences are small and inconsequential in
periment 1 here:
“there was a statistically significant effect of activity for the landscape
task horizon height measure (p = 0.037), and activity was
approaching significance for categorisation (p = 0.053), with higher
horizons and more holistic categorisation in the solitary condition.
Note that these comparisons test the difference between activity
conditions (solitary vs collective) at pre-prime, raising the possibility
that despite random group assignment, there were systematic pre-
prime differences in these measures. However, these effects were
weak and would fail to reach statistical significance if using a
Bonferroni-corrected a = 0.007.” (lines 444-451).
d here for Experiment 2, for the activity condition:
“As expected, there were no pre-prime differences between activity conditions for
any of the measures in Experiment 2. This is indicated by the “Activity: solitary”
rows in Table 3, none of which reached significance even without Bonferroni
correction.” (line 716-719)
e additional payoff condition in Experiment 2 was slightly more complicated, and we
nk Dr Grossman for drawing our attention to this. The relevant pre-prime comparisons
the payoff (absolute vs relative) condition are below:
asure (Experiment 2) Value Std.Error t-value p-value
egorisation Payment: relative 0.012 0.045 0.263 0.793
trait selection Payment: relative 0.012 0.036 0.332 0.740
dscape: Horizon height Payment: relative 0.081 0.041 1.964 0.052
dscape: Objects Payment: relative 0.518 0.177 2.92 0.004**
nouns Payment: relative 0.04 0.047 0.862 0.390
ependent self-construal Payment: relative 0.149 0.235 0.633 0.528
rdependent self-construal Payment: relative -0.336 0.247 -1.36 0.175
st of these comparisons are similarly unconvincing, except for Landscape: Objects.
re there were unanticipated pre-prime differences between absolute and relative payoff
nditions, despite random assignment. We cannot explain this large difference, beyond
ing that it is still possible to see such differences by chance (indeed, if we were to
nferroni correct for all of the 21 tests listed above and so use p<0.05/21=0.0023 as a
nificance cut-off, then this pre-prime difference would not be significant). Interestingly,
wever, this is also the only measure that showed a significant condition * time
eraction, as predicted by our hypothesis. Dr Grossman therefore may well be correct
t for this specific measure and this specific manipulation, pre-prime differences were
sent and affected our results.
nything, however, we think that this bolsters our conclusion that there was no overall
port for the ecocultural hypothesis, given that the only statistically significant
eraction in the predicted direction was due not to the hypothesis being supported, but
a fluke of random assignment at pre-prime. Our new Figure S1 shows that this looks
re like regression to the mean than a meaningful effect of the prime. We now explain
fully in the following paragraph:
“However, this may be because there was a large and
unanticipated difference at pre-prime between absolute and relative
groups in this measure, as indicated by the “Payment: relative” row
in Table 3, and as seen in Figure S1. We are unable to explain this
initial large difference which occurred despite random assignment,
but it may account for the significant interaction. From Figure S1
we can see that there were actually no differences between the
groups at post-prime, suggesting simple regression to the mean.”
(line 730-735)
As somebody who was majorly involved in the development and refinement of
measures discussed in the present manuscript, and was one of the lead authors
the paper the authors refer to support their claim (Varnum et al., 2010), I felt at
ds with authors’ interpretation of the word by my colleagues and myself. Where
actly did my co-authors and I wrote that holistic tasks such as those dealing with
egorization should be sub-summed under the rubric of interdependence? In our
rrent Directions piece, we have merely shown that groups that score higher on
cial tasks measuring interdependence are also more likely to engage in holistic
cessing of information. One of the points of the Varnum et al., 2010 was to show
t the families of tasks capturing interdependence and cognitive style are
TINCT and should not be put into the same basket together. The arguments in
num et al., 2010, do not make any sense, unless one acknowledges that
erdependence and cognitive style are separate cosntructs. Together with the
dence from Na et al., 2010, which we published in the same year, it seems
plausible to treat the tasks as measuring a unitary construct on the individual
el the authors operate on in the present manuscript.
rther, I yet have to see a *single* psychometrics-oriented study validating ANY of
holistic tasks reported in the present manuscript. If anything, Na et al., 2010, has
own that the psychometric properties are likely to be pretty bad on the individual
el of analysis. Masuda et al. certainly did not publish anything concerning
ychometric analysis of the tasks reported in the 2008 paper. Merely showing
ss-cultural differences on a measure and conceptually replicating these
erences with a different task does not have ANYTHING to with the question of
ability and validity of the instrument. I can cook up a measure of using
opsticks vs. forks as a test of holistic vs. analytic behaviour and I bet it would be
arn good measure for detecting cross-cultural differences between East Asians
d Europeans. Yet, it would still have zero validity, in part because performance on
s measure would be likely unrelated to performance on established measures of
istic attention. Note, I am generally concerned about the use of such measures
capturing individual differences and my criticism applies for this general stream
research (including the work done by myself and my colleagues). But given the
tlet and the orientation on individual level of analysis, I believe this critique is
rticularly applicable here. Overall, what I am trying to say in this point is that: (a)
authors should avoid misrepresentation of the prior work; (b), they should be
ch more careful in their claims that the tasks are well validated and
ychometrically sound. They are absolutely not. The tasks were developed ad-hoc,
inly for demonstrating cross-cultural differences (rather than reliable estimating
ividual differences) and the tasks used in this paper have certainly not been
ined. My strong recommendation would be to conduct and report zero-order
rrelations as well as PCA analyses of the tasks to show whether they indeed
ould be captured under the same rubric on the individual level of analysis. Such
approach is only justified if the authors can show positive correlations between
erent tasks and/or convergence of results on a single principal component.
thout it, any claims that the present results concern the concept of
terdependence” appear methodologically and theoretically unwarranted.
fully accept Dr Grossman’s point here and have made extensive changes throughout
manuscript to reflect these issues. We no longer subsume independent-
erdependent self-construal and analytic-holistic cognition under the same conceptual
ension, and we make clear that they are distinct. The following paragraph has been
ded to the Introduction to this effect:
“Given these findings, researchers have attempted to summarise this
variation in theoretically coherent dimensions or conceptual schemes.
One concerns self-construal [17] or social orientation [18], where people
with independent self-construal see themselves as separate and bounded
from others and value individual goals and personal autonomy, and
people with interdependent self-construal see themselves as overlapping
in identity with others and value collective goals and harmony with others.
Another dimension relates to cognition [11], where analytic cognition
involves a focus on single objects or entities independent of their wider
contexts, explaining events in terms of entities’ internal traits, and use of
formal logic, while holistic cognition involves a focus on relationships
between objects or entities within wider contexts, explaining events in
terms of contexts or situations, and use of relationship-based reasoning.
Countries and societies high in independent self-construal, such as the
United States, also tend to be high in analytic cognition, and countries
and societies high in interdependent self-construal, such as China, also
tend to be high in holistic cognition [19]. These dimensions extend
beyond a simplistic East-West dichotomy. For example, Russia is both
more interdependent and more cognitively holistic than the United States
[19], and Hokkaido is more independent than the rest of Japan [20].” (lines
77-91)
oughout the rest of the manuscript we have added references to analytic and holistic
gnition as appropriate. The new figures also refer to analytic – holistic cognition on their
s where appropriate.
also requested, we conducted scale validation using R package psy to determine
nvergent and discriminant validity, including zero-order correlations and a PCA. These
described in the manuscript for Experiment 1 as follows:
“To establish convergent and discriminant validity of subscales of our
measures, we applied a Multitrait Multimethod approach of scale
validation from R package psy [41]. The results suggested the different
DVs were measuring discriminant traits, with Pearson correlations below
0.3 between subscales (see File S3). In addition, a principle components
analysis of responses found six components with eigenvalues above 1.0
and nine with eigenvalues above 0.7 before the point of inflection on a
scree plot (File S3). This lack of correlation at the individual level between
measures of culturally variable psychological processes is to be expected
and is consistent with previous studies showing that group-level
differences do not translate into individual-level differences [23].” (lines
459-467)
d for Experiment 2 as follows:
“As for Experiment 1, we explored convergent and discriminant validity of
subscales of our measures using a Multitrait Multimethod approach from R
package psy [41]. As before, the results suggested the different DVs were
measuring discriminant traits, with Pearson correlations again below 0.3
between subscales (File S3). A principle components analysis found seven
components with eigenvalues above 1.0 and nine with eigenvalues above
0.7 before the point of inflection on a scree plot (File S3). This lack of
individual-level correlation between measures that show cross-cultural
variation is again to be expected [23].” (lines 755-761)
stated here, there is little evidence that our measures are tapping unitary constructs.
wever, given the findings of Na et al. (2009), we think that this is to be expected when
plying measures that have been found to vary cross-culturally to the individual level.
Let me address the point about the perception of the priming literature. First,
viewer 1 misremembers what Oyserman & Lee paper has actually shown. The
per included only a handful of studies claiming effects of individualism /
lectivism priming on strictly cognitive tasks concerning decision-making,
mory, or attentional patterns, all using different measures (see Table A5 in the
per). Some other tasks claimed to show effects of interdependence prime on
gnition are misclassified (e.g., self-other differences or reports of personal
itudes are clearly about social preferences and not cognitive processes) or are
sed on the fake data produced by Stapel. This stands in contrast to the rather
ge number of tasks priming individualism/collectivism and observing effects in
social domain of individualism/collectivism. Second, as somebody who was in
same department as Oyserman for a while, I can attest to the institutional
owledge of the huge file drawer problem. In this sense, of course, i/we priming
rature is no different from other bodies of research on priming. So, I agree that
ng straightforward about non-replications is a great virtue in the current climate.
thank Dr Grossman for sharing his unique insights on this point. We also agree that
ng straightforward about the possibility of file drawer effects is virtuous, and we hope
t our study can be one small contribution to the field’s self-examination. As we noted
ur previous response, not everyone has such insider knowledge (particularly students
w to the field who only have published literature to go by), so we think that publishing
l findings is of utmost importance.
The issue of power: The reason why I did not comment on your power
alysis is because it uses highly implausible d=.5. Even if one examined Oyserman
ee (2008) and focuses on cognition, the effect size varied from one cognitive task
another from d=.29 to d=.5 (see Table 4) or But there is a bigger question: Do you
im that priming effects are stronger than most other studies in psychology? If
u claim that expected d = .5, whereas in psychology it is d = .41, you sort of do...
wever, given that you explicitly acknowledge the file drawer issue, you should
riori lower your “real” d. That is precisely why I did not acknowledge your attempt
post-hoc justify your design. The rationale does not seem sound to me. This
ction ought to be re-written. Also note, your power is dependent on the number of
tors in your model, not just on the number of contrasts you are examining. Given
t the design inherently involves a 2 X 2, you ought to include the four cells in your
del (even if just to control for them). This has an implication for your power
alysis, as well.
thank Dr Grossman for his salient observations relating to our power analysis. Upon
nsidering further, we have removed the post-hoc power analysis from the Discussion.
explained in this blog post by Daniel Lakens, post-hoc power analysis simply
escribes the found p-values:
p://daniellakens.blogspot.co.uk/2014/12/observed-power-and-what-to-do-if-your.html
d so provides no new information. We are therefore more up front that our study may
underpowered, and recommend larger future studies:
“it is possible that our sample sizes were too small to detect the
possible effect of activity. We note that our sample size of n=84 in
Experiment 1 and n=135 in Experiment 2 is larger than 74% and 89%
respectively of the 104 experiments included in a meta-analysis of
independence-interdependence priming effects [19], making these two
of the largest studies of their kind to date. Nevertheless, given the
prevalence of under-powered studies in the literature, our sample size
was likely too small to detect anything but large effects, and we
recommend future studies use larger samples.” (lines 893-900)
Minor comment (re: citing additional literature in the introduction): As I
icated in my prior review, existing literature review is dissatisfactory. The authors
rely dismissed this point, indicating that they do not know why one should talk
out the debate concerning cultural differences in self-enhancement or
positional vs. situational attributions. I may be wrong, but I always thought that
introduction is supposed to provide a balanced overview of the state of the field
the topic. The authors currently don’t provide such an overview. Instead, they
em to be stuck in the 1990s in their knowledge of cultural differences. Merely
ting along the lines like in the statement here: “studies with Western participants
ggested that people showed self enhancement bias,[…] Yet subsequent studies
h East Asian participants found this self-enhancement bias to be much reduced,
d often entirely absent” seems to suggest robust cross-cultural difference on
ious related dimensions. By adding more recent citations (including works by
dikides or Malle) the authors can show that the differences may not be as large
d are probably more nuanced. This piece of information is important, as the paper
mately tries to test a theory drawing from cross-cultural research. Yet, the
earch the authors try to build on is simply not as clear-cut as presented in the
rrent manuscript. That is why I suggest providing caveats about debates
ncerning self-enhancement and dispositional (vs situational) bias.
o, below is Dr Grossman’s original comment from the previous review that he is
erring to]
me of the introduction, concerning review of prior literature, needs a more careful
d of the existing scholarship, including work by Bertram Malle and colleagues on
lack of support for dispositional attributions in the US, as well as cross-cultural
rk on relative differences to acknowledge situational vs. dispositional attributions
yond East vs. West comparison (e.g., across differences social classes in the US,
d when examining Eastern Europeans; I have written on this topic in Grossmann
arnum, 2011). Similarly, the cross-cultural evidence for self-enhancement is more
mplex than presented in the MS (see works by Sedikides and colleagues).
thank Dr Grossman for his continued insistence that we include more recent literature
he Introduction, and we apologise for dismissing this comment previously. We have
w addressed this. Specifically, we added the following text at lines 58-75, including
tions to Malle and Grossman on social attribution and Sedikides on self-enhancement:
“More recent research in cultural psychology has revealed both many
nuances in these cross-cultural differences, but also repeated
confirmation of the existence of meaningful cultural variation in
psychological processes. For example, the aforementioned findings
related to social attribution have been clarified by newer findings showing
that, for example, (i) while dispositional attribution seems to be cross-
culturally consistent, East Asians show greater endorsement of situational
factors than Westerners only when situational information is made salient
[13]; (ii) North Americans endorse dispositional explanations mostly for
ordinary actions and instead use mental state attributions for puzzling or
unusual actions [14]; and (ii) higher socio-economic status within both
Western and non-Western countries is associated with more dispositional
attribution [15]. Similarly, self-enhancement is thought by some to be a
culturally universal underlying motivation which manifests itself in different
ways cross-culturally: for example, Westerners self-enhance on traits
such as originality or self-reliance while East Asians self-enhance on traits
such as compromise and loyalty; modesty norms are stronger in East Asia
than the West, causing East Asian people to suppress self-enhancing
opinions in public settings [16]. In these and other cases, initial findings of
dramatic cross-cultural differences have given way to more nuanced
understandings of how cultural background interacts with contexts and
situations to create culturally variable psychological responses.”
her changes:
• In response to previous reviewers’ comments, we have redone all of the figures to
more clearly show the change in mean scores from pre- to post-prime, along with
individual-level data. Figures 1, 2 and S1 have all been completely reworked.
• The R scripts in File S3 have been cleaned up and are now much clearer, and
include code for creating the figures and for the discriminant trait analysis and
PCAs
Society Open
