Science podcasts: analysis of global production and output
from 2004 to 2018
Lewis E. MacKenzie
Article citation details
R. Soc. open sci. 5: 180932.
http://dx.doi.org/10.1098/rsos.180932
Review timeline
Original submission: 10 June 2018 Note: Reports are unedited and appear as
1st revised submission: 29 August 2018 submitted by the referee. The review history
2nd revised submission: 7 November 2018 appears in chronological order.
Final acceptance: 26 November 2018
Review History
label_version_1
RSOS-180932.R0 (Original submission)
label_author_1
Review form: Reviewer 1
Is the manuscript scientifically sound in its present form?
No
Are the interpretations and conclusions justified by the results?
Yes
Is the language acceptable?
Yes
Is it clear how to access all supporting data?
Yes
Do you have any ethical concerns with this paper?
No
Reports © 2018 The Reviewers; Decision Letters © 2018 The Reviewers and Editors;
Responses © 2018 The Reviewers, Editors and Authors. Published by the Royal Society under the
terms of the Creative Commons Attribution License http://creativecommons.org/licenses/by/4.0/,
which permits unrestricted use, provided the original author and source are credited
2
Have you any concerns about statistical analyses in this paper?
No
Recommendation?
label_recommendation_1
Major revision is needed (please make suggestions in comments)
Comments to the Author(s)
label_comment_1
This is an interesting study that argues well for its publication in terms of scarcity of research on
this topic. A substantial analysis of data has been conducted and I would support publication,
though not in the current form.
It is strange that the term 'science' is used in the article title, when topics such as mathematics,
statistics and engineering are included. Perhaps a broader term is needed? I suppose science is
used as the focus is 'natural sciences' on iTunes, but are there categories on iTunes for other
STEM disciplines?
It seems odd that computer science and engineering are included but technology is not. Perhaps
some clarification on the inclusion criteria would make this clearer.
Around line 11 on page 2: It seems advantages are being conflated. The advantages of podcasts
over conventional broadcast are stated ("can be listened to/watched at any time"), but the time
shift is not an advantage over print media. Perhaps a second sentence for print media is
warranted.
Around line 12 on page 2: Why is "global distribution" a requirement for "niche and highly
specialised". Please expand.
Around line 16 on page 3: Why were individuals not contacted? This is not a methodological
problem, but earlier in the paragraph reasons were given for methodological choices.
Around line 25 on page 3: I am not at all convinced that the reference to or citation of a blog post
by the author is needed here. It doesn't seem to relate to the later compilation of data.
Around line 30 on page 3: I need some convincing that the use of iTunes has not excluded non-
Apple users. For example, "many podcast apps run on the iTunes directory engine" - does this
include Android apps? More needs to be done to convey why iTunes is the right choice for data
source.
Around line 44 on page 3: reference to "podcasts spuriously listed" - I wanted some examples.
Perhaps say some examples are given in the next section.
Around line 12 on page 4: It is not clear why podcasts must be English language to be "valid" in
"this study of science podcasts". This suggests that non-English language podcasts are not valid
science. I think the framing of the study needs rewriting slightly to be clear that this is a "study of
English-language science podcasts".
Around line 17 on page 4: Why was video chosen over audio? Justify this choice.
Page 4: It might be good to hear at this point which topics were included as well as which were
excluded. This comes later, but it would be useful here, before the list around line 22.
Around line 30 on page 4: I find the bullet about no episodes available and the next one
3
confusing. What would happen if a podcast had 0 episodes available on its iTunes listing but 250
episodes available on its own website?
Around line 40 on page 4: It seems a strange choice to back-date the podcast start date of a
podcast that was previously a broadcast radio show. For example (a non-science example), very
old episodes of Desert Island Discs are available on iTunes, but it would not be sensible to
consider this a podcast that started in the 1970s. This has a big effect because back-dating podcast
start dates like this will effect the number of active podcasts in a given year and the podcast life
span statistics. Can this choice be reconsidered? If not, further justification is needed and these
issues should be discussed in the limitations.
Around line 8 on page 5: I wanted to know why two equations were proposed. Why not 1? Why
not more? This is touched on later, but more here would be helpful.
Around line 33 on page 6: On what basis is chemistry judged to be under-represented? For
example, psychology has 36 podcasts and chemistry has 30. Why is the former considered "well-
represented" (around line 23 page 12) and chemistry not? More needs to be done to set and justify
expectations. Why consider physics, chemistry and biology in particular? What about other
disciplines, e.g. maths and engineering? It is notable that the author feels his own discipline is
under-represented. Please argue more thoroughly for this key conclusion.
It would be useful to know if the comparisons between independent and affiliated podcasts at the
bottom of page 7 and the end of section 4 on page 8 are statistically significant.
The discussion of limitations is good, though the speculation about future technologies around
lines 14 and 24 on page 9 are not necessary, including the references 44 and 45.
Around line 50 on page 9: make it clear sooner that Morgan's definition of active podcasts differs,
rather than making this an after-thought.
Around line 1 on page 10: use of word "significantly". I do not believe any significance testing has
been done (though I think it should be).
Around line 55 on page 10: why is the comparison made with "high-quality scientific papers"?
More needs to be done to justify making this comparison in particular.
Figure 1D: does this include podcasts that are still 'alive'? In which case, this seems to be biased
towards producing an exponential graph. What does an equivalent graph of podcasts that have
lived out their lifespan look like?
Table 3:
Around line 15 on page 22: The criteria for inactive <1 year are not made clear. This should state
no episodes released in last 3 months.
Around line 19 on page 22: Lifespan appears to be defined differently elsewhere. Specifically, the
lifespace could start not with the release of a podcast episode, but with an original broadcast
which is later released as a podcast episode.
Around line 13 on page 23: Rename 'University' to reflect the fact that schools are also included.
Around line 26 on page 23: It seems very strange to combine radio and print media. One has as
traditional core business producing the sort of content that could be podcast, and the other does
not. Can this choice be reconsidered or justified?
Around line 37 on page 23: I was confused why 'Audio podcast' was listed under 'Non-audio
media'. Could this be explained in situ?
Aroud line 6 on page 24: It seems strange to combine advertising and sponsorship. If a podcast is
4
supported by, say, a charitable or research grant, this seems quantifiably different than being
supported by including commercial adverts in the podcast. Can this be reconsidered or justified?
Minor:
The summary refers to "mid-2000s", which I might take to mean around 2050. In fact, it means
2004.
A few minor typos and grammatical issues. One I noticed was the omission of % in 57% around
line 27 on page 12.
label_author_2
Review form: Reviewer 2
Is the manuscript scientifically sound in its present form?
No
Are the interpretations and conclusions justified by the results?
No
Is the language acceptable?
Yes
Is it clear how to access all supporting data?
Yes
Do you have any ethical concerns with this paper?
No
Have you any concerns about statistical analyses in this paper?
Yes
Recommendation?
label_recommendation_2
Major revision is needed (please make suggestions in comments)
Comments to the Author(s)
label_comment_2
I like many things about this and was happy to review it because it's information that I think is
useful baseline information for those who study science communication. That being said, I have
one primary concern related to methods as well as some minor comments.
With regards to methods, I really wish the author had used standard coding practices to both
exclude and categorize articles once the articles were selected. Books by people such as
Krippendorf discuss such methods.
Krippendorff, K. (2012). Content Analysis: An Introduction to its Methodology (3rd ed.).
Thousand Oaks, CA: Sage Publications.
The key idea, however, is whether the read can be confident that the the person who selected the
articles for coding and then coded the articles was reliable. Typically, this requires at least two
independent coders working from a common coding protocol on a blind an randomly
overlapped sub-sample of the content. Done properly, this allows for the reporting of chance-
adjusted intercoder reliability statistics for each categorization. In the absence of such coding
5
procedures and reporting, I would want a compelling argument that things that were coded for
were so clear as to be almost impossible to miscode. Some of the codes reported (e.g. those in the
podcast activity category) seem okay but a couple of key ones seem more iffy. Specifically, I
wonder about how the coder determined which topics were "non-science," the topic, and then
how audience was determined. I'm not sure there's enough information provided to replicate the
results. Related to this, it's not clear to me whether something could have multiple audiences,
topics, types of affiliations, and hosts, etc. For example, could something be both engineering and
climate change? Or statistics and medical?
You really also should have kept track of things that were excluded (and why).
On a related note, I'd love to see a more compelling argument for the focus on the natural
sciences sub-directory but not the full "science and medicine" directory. "Practical constraints" is
not a compelling argument. At minimum, I would want an argument for what that means for the
generalizabilty of f the results. The choice to add 18 additional podcasts, but not use the full
science and medicine directory seems particularly worthy of justification. I note that the list of
podcast includes many medical related-sounded podcasts. Curious to know how many of these
podcasts choose multiple categories. It may also be important to describe how a podcast gets
categorized.
Some more minor comments ...
- It would be nice to give some benchmark for why you think a subject is "under-represented"?
Perhaps you could use data related to PhDs awarded in the US/UK for specific topics or funding
for specific topics as a guide.
- You suggest that 952 podcasts is "representative" but this is a misuse of a the term in the absence
of a known population. In one sense, you start with a population and do a census (not including
those you exclude) and this means that any error you have comes from the fact that you only
looked at the "natural sciences" and the fact that some podcasts/vodcasts aren't listed on itunes.
In the absence of such knowledge, there's no statistical reason to use the term "representative" in
anything but a qualitative sense, and even then it's hard to know. I'd suggest just saying that it's a
population of one category (assuming the excludes are justifiable).
- The conclusions may re-hash already-reported data more than might be desirable, though it's
hard to know what else to say given that the discussion seems to cover what needs covering. I'd
also be a little more careful with how I word guesses. For example, rather than saying that
"science podcasts are being [used] as a decentralised and independent avenue of science
communication," it'd make more sense to me to simply present that as one possibility as we have
no data on this point. And the term independent doesn't really make sense given the affiliation
data suggests at least half of respondents have some explicit affiliation.
-Do people really care about the fit lines? How does this analysis help us make sense of the topic?
-The final comparison in results (p.11) about country only makes makes sense if you don't take
language into account, but language isn't mentioned.
label_author_3
Review form: Reviewer 3 (Sarah Davies)
Is the manuscript scientifically sound in its present form?
No
Are the interpretations and conclusions justified by the results?
No
6
Is the language acceptable?
Yes
Is it clear how to access all supporting data?
Yes
Do you have any ethical concerns with this paper?
No
Have you any concerns about statistical analyses in this paper?
I do not feel qualified to assess the statistics
Recommendation?
label_recommendation_3
Major revision is needed (please make suggestions in comments)
Comments to the Author(s)
label_comment_3
I find this article difficult to review in that it seeks to make a contribution to research on public
science communication but makes use neither of existing literature in this field nor the
methodological techniques and norms of social research. If this were in a more disciplinary
journal - such as Science Communication, Public Understanding of Science, or JCOM - I would
advise rejection. The findings, based on a somewhat superficial analysis of science podcasts, are
not particularly surprising, and there is little effort to discuss their implications. As a result it is
unclear why it is interesting or important to publish a finding that, say, 77% of podcasts are
targeted at public audiences.
It is true that there thus far is relatively little literature on science podcasts specifically, but there
is some that is not cited (e.g. Birch & Weitkamp 2010; Dantas-Querroz et al 2018; and associated
references), and there is far more on the form and nature of science communication in social and
digital media generally or on science radio, the obvious precursor to science podcasting, that a
quick literature search would have identified. I suggest that the author engages with this
literature in order to ground their discussion and, specifically, to show why this work meets
specific research needs, how it contributes to current debates around the use of new media for
science communication, and what its implications are (they write, in closing, that "these insights
will help inform public science communication policy" but it is not at all obvious to me how or
why this would be the case). This may entail carrying out further empirical research in order to
take the article beyond quantitative description of this emerging phenomenon to offer insight into
the patterns identified. For instance, carrying out interviews with producers of podcasts (as has
been done for other new media science communication formats; e.g. Ranger & Bultitude 2014;
Riesch & Mendel 2013) would offer a richer understanding of, for instance, topic choice or who
bears the cost of podcasting, and would thus avoid some of the speculation currently found in the
Discussion. It is worth noting that, in this field as a whole, it would not be usual to publish
description of a phenomenon without more in-depth analysis or theoretical testing.
There are also some methodological problems due to a lack of transparency as to how the
podcasts were coded. How exactly was the target audience identified? Or the focal topic (what
about interdisciplinary research)? How were hosts categorised, and based on what empirical
material (again, what about individuals or groups who fall into more than one category)? It is
also not clear how coding was carried out. Were web scraping tools used, or was all coding done
by hand? By how many people? More generally, it would be useful to have the methods
grounded within specific traditions or techniques; the work of Martin Bauer or Dietram Scheufele
and colleagues, who work in quantitative science communication research, might offer some
inspiration here.
7
A minor point: 'demographics' tends to refer to human populations (from 'demos', the people). If
it is to be applied to non-human cohorts then this needs to be explained and justified.
References
Birch, Hayley, and Emma Weitkamp. 2010. “Podologues: Conversations Created by Science
Podcasts: Podologues: Conversations Created by Science Podcasts.” New Media & Society 12 (6):
889–909. https://doi.org/10.1177/1461444809356333.
Dantas-Queiroz, Marcos V., Lia C. P. Wentzel, Luciano L. Queiroz. 2018. “Science
Communication Podcasting in Brazil: The Potential and Challenges Depicted by Two Podcasts.”
Anais Da Academia Brasileira de Ciências 90 (2): 1891–1901. https://doi.org/10.1590/0001-
3765201820170431.
Ranger, Mathieu, and Karen Bultitude. 2014. “‘The Kind of Mildly Curious Sort of Science
Interested Person like Me’: Science Bloggers’ Practices Relating to Audience Recruitment.” Public
Understanding of Science, October, 963662514555054.
https://doi.org/10.1177/0963662514555054.
Riesch, Hauke, and Jonathan Mendel. 2013. “Science Blogging: Networks, Boundaries and
Limitations.” Science as Culture 23 (1): 51–72. https://doi.org/10.1080/09505431.2013.801420.
label_end_comment
Decision letter (RSOS-180932.R0)
09-Aug-2018
Dear Dr MacKenzie,
The editors assigned to your paper ("Science Podcasts: Analysis of Global Production and Output
From 2004 to 2018") have now received comments from reviewers. We would like you to revise
your paper in accordance with the referee and Associate Editor suggestions which can be found
below (not including confidential reports to the Editor). Please note this decision does not
guarantee eventual acceptance.
Please submit a copy of your revised paper before 01-Sep-2018. Please note that the revision
deadline will expire at 00.00am on this date. If we do not hear from you within this time then it
will be assumed that the paper has been withdrawn. In exceptional circumstances, extensions
may be possible if agreed with the Editorial Office in advance. We do not allow multiple rounds
of revision so we urge you to make every effort to fully address all of the comments at this stage.
If deemed necessary by the Editors, your manuscript will be sent back to one or more of the
original reviewers for assessment. If the original reviewers are not available, we may invite new
reviewers.
To revise your manuscript, log into http://mc.manuscriptcentral.com/rsos and enter your
Author Centre, where you will find your manuscript title listed under "Manuscripts with
Decisions." Under "Actions," click on "Create a Revision." Your manuscript number has been
appended to denote a revision. Revise your manuscript and upload a new version through your
Author Centre.
When submitting your revised manuscript, you must respond to the comments made by the
8
referees and upload a file "Response to Referees" in "Section 6 - File Upload". Please use this to
document how you have responded to the comments, and the adjustments you have made. In
order to expedite the processing of the revised manuscript, please be as specific as possible in
your response.
In addition to addressing all of the reviewers' and editor's comments please also ensure that your
revised manuscript contains the following sections as appropriate before the reference list:
• Ethics statement (if applicable)
If your study uses humans or animals please include details of the ethical approval received,
including the name of the committee that granted approval. For human studies please also detail
whether informed consent was obtained. For field studies on animals please include details of all
permissions, licences and/or approvals granted to carry out the fieldwork.
• Data accessibility
It is a condition of publication that all supporting data are made available either as
supplementary information or preferably in a suitable permanent repository. The data
accessibility section should state where the article's supporting data can be accessed. This section
should also include details, where possible of where to access other relevant research materials
such as statistical tools, protocols, software etc can be accessed. If the data have been deposited in
an external repository this section should list the database, accession number and link to the DOI
for all data from the article that have been made publicly available. Data sets that have been
deposited in an external repository and have a DOI should also be appropriately cited in the
manuscript and included in the reference list.
If you wish to submit your supporting data or code to Dryad (http://datadryad.org/), or modify
your current submission to dryad, please use the following link:
http://datadryad.org/submit?journalID=RSOS&manu=RSOS-180932
• Competing interests
Please declare any financial or non-financial competing interests, or state that you have no
competing interests.
• Authors’ contributions
All submissions, other than those with a single author, must include an Authors’ Contributions
section which individually lists the specific contribution of each author. The list of Authors
should meet all of the following criteria; 1) substantial contributions to conception and design, or
acquisition of data, or analysis and interpretation of data; 2) drafting the article or revising it
critically for important intellectual content; and 3) final approval of the version to be published.
All contributors who do not meet all of these criteria should be included in the
acknowledgements.
We suggest the following format:
AB carried out the molecular lab work, participated in data analysis, carried out sequence
alignments, participated in the design of the study and drafted the manuscript; CD carried out
the statistical analyses; EF collected field data; GH conceived of the study, designed the study,
coordinated the study and helped draft the manuscript. All authors gave final approval for
publication.
• Acknowledgements
Please acknowledge anyone who contributed to the study but did not meet the authorship
criteria.
9
• Funding statement
Please list the source of funding for each author.
Please note that Royal Society Open Science charge article processing charges for all new
submissions that are accepted for publication. Charges will also apply to papers transferred to
Royal Society Open Science from other Royal Society Publishing journals, as well as papers
submitted as part of our collaboration with the Royal Society of Chemistry
(http://rsos.royalsocietypublishing.org/chemistry). If your manuscript is newly submitted and
subsequently accepted for publication, you will be asked to pay the article processing charge,
unless you request a waiver and this is approved by Royal Society Publishing. You can find out
more about the charges at http://rsos.royalsocietypublishing.org/page/charges. Should you
have any queries, please contact openscience@royalsociety.org.
Once again, thank you for submitting your manuscript to Royal Society Open Science and I look
forward to receiving your revision. If you have any questions at all, please do not hesitate to get
in touch.
Kind regards,
Royal Society Open Science Editorial Office
Royal Society Open Science
openscience@royalsociety.org
on behalf of Dr Hamed Haddadi (Associate Editor) and Prof. Marta Kwiatkowska (Subject
Editor)
openscience@royalsociety.org
Comments to Author:
Reviewers' Comments to Author:
Reviewer: 1
Comments to the Author(s)
This is an interesting study that argues well for its publication in terms of scarcity of research on
this topic. A substantial analysis of data has been conducted and I would support publication,
though not in the current form.
It is strange that the term 'science' is used in the article title, when topics such as mathematics,
statistics and engineering are included. Perhaps a broader term is needed? I suppose science is
used as the focus is 'natural sciences' on iTunes, but are there categories on iTunes for other
STEM disciplines?
It seems odd that computer science and engineering are included but technology is not. Perhaps
some clarification on the inclusion criteria would make this clearer.
Around line 11 on page 2: It seems advantages are being conflated. The advantages of podcasts
over conventional broadcast are stated ("can be listened to/watched at any time"), but the time
shift is not an advantage over print media. Perhaps a second sentence for print media is
warranted.
Around line 12 on page 2: Why is "global distribution" a requirement for "niche and highly
specialised". Please expand.
10
Around line 16 on page 3: Why were individuals not contacted? This is not a methodological
problem, but earlier in the paragraph reasons were given for methodological choices.
Around line 25 on page 3: I am not at all convinced that the reference to or citation of a blog post
by the author is needed here. It doesn't seem to relate to the later compilation of data.
Around line 30 on page 3: I need some convincing that the use of iTunes has not excluded non-
Apple users. For example, "many podcast apps run on the iTunes directory engine" - does this
include Android apps? More needs to be done to convey why iTunes is the right choice for data
source.
Around line 44 on page 3: reference to "podcasts spuriously listed" - I wanted some examples.
Perhaps say some examples are given in the next section.
Around line 12 on page 4: It is not clear why podcasts must be English language to be "valid" in
"this study of science podcasts". This suggests that non-English language podcasts are not valid
science. I think the framing of the study needs rewriting slightly to be clear that this is a "study of
English-language science podcasts".
Around line 17 on page 4: Why was video chosen over audio? Justify this choice.
Page 4: It might be good to hear at this point which topics were included as well as which were
excluded. This comes later, but it would be useful here, before the list around line 22.
Around line 30 on page 4: I find the bullet about no episodes available and the next one
confusing. What would happen if a podcast had 0 episodes available on its iTunes listing but 250
episodes available on its own website?
Around line 40 on page 4: It seems a strange choice to back-date the podcast start date of a
podcast that was previously a broadcast radio show. For example (a non-science example), very
old episodes of Desert Island Discs are available on iTunes, but it would not be sensible to
consider this a podcast that started in the 1970s. This has a big effect because back-dating podcast
start dates like this will effect the number of active podcasts in a given year and the podcast life
span statistics. Can this choice be reconsidered? If not, further justification is needed and these
issues should be discussed in the limitations.
Around line 8 on page 5: I wanted to know why two equations were proposed. Why not 1? Why
not more? This is touched on later, but more here would be helpful.
Around line 33 on page 6: On what basis is chemistry judged to be under-represented? For
example, psychology has 36 podcasts and chemistry has 30. Why is the former considered "well-
represented" (around line 23 page 12) and chemistry not? More needs to be done to set and justify
expectations. Why consider physics, chemistry and biology in particular? What about other
disciplines, e.g. maths and engineering? It is notable that the author feels his own discipline is
under-represented. Please argue more thoroughly for this key conclusion.
It would be useful to know if the comparisons between independent and affiliated podcasts at the
bottom of page 7 and the end of section 4 on page 8 are statistically significant.
The discussion of limitations is good, though the speculation about future technologies around
lines 14 and 24 on page 9 are not necessary, including the references 44 and 45.
11
Around line 50 on page 9: make it clear sooner that Morgan's definition of active podcasts differs,
rather than making this an after-thought.
Around line 1 on page 10: use of word "significantly". I do not believe any significance testing has
been done (though I think it should be).
Around line 55 on page 10: why is the comparison made with "high-quality scientific papers"?
More needs to be done to justify making this comparison in particular.
Figure 1D: does this include podcasts that are still 'alive'? In which case, this seems to be biased
towards producing an exponential graph. What does an equivalent graph of podcasts that have
lived out their lifespan look like?
Table 3:
Around line 15 on page 22: The criteria for inactive <1 year are not made clear. This should state
no episodes released in last 3 months.
Around line 19 on page 22: Lifespan appears to be defined differently elsewhere. Specifically, the
lifespace could start not with the release of a podcast episode, but with an original broadcast
which is later released as a podcast episode.
Around line 13 on page 23: Rename 'University' to reflect the fact that schools are also included.
Around line 26 on page 23: It seems very strange to combine radio and print media. One has as
traditional core business producing the sort of content that could be podcast, and the other does
not. Can this choice be reconsidered or justified?
Around line 37 on page 23: I was confused why 'Audio podcast' was listed under 'Non-audio
media'. Could this be explained in situ?
Aroud line 6 on page 24: It seems strange to combine advertising and sponsorship. If a podcast is
supported by, say, a charitable or research grant, this seems quantifiably different than being
supported by including commercial adverts in the podcast. Can this be reconsidered or justified?
Minor:
The summary refers to "mid-2000s", which I might take to mean around 2050. In fact, it means
2004.
A few minor typos and grammatical issues. One I noticed was the omission of % in 57% around
line 27 on page 12.
Reviewer: 2
Comments to the Author(s)
I like many things about this and was happy to review it because it's information that I think is
useful baseline information for those who study science communication. That being said, I have
one primary concern related to methods as well as some minor comments.
With regards to methods, I really wish the author had used standard coding practices to both
exclude and categorize articles once the articles were selected. Books by people such as
Krippendorf discuss such methods.
Krippendorff, K. (2012). Content Analysis: An Introduction to its Methodology (3rd ed.).
Thousand Oaks, CA: Sage Publications.
12
The key idea, however, is whether the read can be confident that the the person who selected the
articles for coding and then coded the articles was reliable. Typically, this requires at least two
independent coders working from a common coding protocol on a blind an randomly
overlapped sub-sample of the content. Done properly, this allows for the reporting of chance-
adjusted intercoder reliability statistics for each categorization. In the absence of such coding
procedures and reporting, I would want a compelling argument that things that were coded for
were so clear as to be almost impossible to miscode. Some of the codes reported (e.g. those in the
podcast activity category) seem okay but a couple of key ones seem more iffy. Specifically, I
wonder about how the coder determined which topics were "non-science," the topic, and then
how audience was determined. I'm not sure there's enough information provided to replicate the
results. Related to this, it's not clear to me whether something could have multiple audiences,
topics, types of affiliations, and hosts, etc. For example, could something be both engineering and
climate change? Or statistics and medical?
You really also should have kept track of things that were excluded (and why).
On a related note, I'd love to see a more compelling argument for the focus on the natural
sciences sub-directory but not the full "science and medicine" directory. "Practical constraints" is
not a compelling argument. At minimum, I would want an argument for what that means for the
generalizabilty of f the results. The choice to add 18 additional podcasts, but not use the full
science and medicine directory seems particularly worthy of justification. I note that the list of
podcast includes many medical related-sounded podcasts. Curious to know how many of these
podcasts choose multiple categories. It may also be important to describe how a podcast gets
categorized.
Some more minor comments ...
- It would be nice to give some benchmark for why you think a subject is "under-represented"?
Perhaps you could use data related to PhDs awarded in the US/UK for specific topics or funding
for specific topics as a guide.
- You suggest that 952 podcasts is "representative" but this is a misuse of a the term in the absence
of a known population. In one sense, you start with a population and do a census (not including
those you exclude) and this means that any error you have comes from the fact that you only
looked at the "natural sciences" and the fact that some podcasts/vodcasts aren't listed on itunes.
In the absence of such knowledge, there's no statistical reason to use the term "representative" in
anything but a qualitative sense, and even then it's hard to know. I'd suggest just saying that it's a
population of one category (assuming the excludes are justifiable).
- The conclusions may re-hash already-reported data more than might be desirable, though it's
hard to know what else to say given that the discussion seems to cover what needs covering. I'd
also be a little more careful with how I word guesses. For example, rather than saying that
"science podcasts are being [used] as a decentralised and independent avenue of science
communication," it'd make more sense to me to simply present that as one possibility as we have
no data on this point. And the term independent doesn't really make sense given the affiliation
data suggests at least half of respondents have some explicit affiliation.
-Do people really care about the fit lines? How does this analysis help us make sense of the topic?
-The final comparison in results (p.11) about country only makes makes sense if you don't take
language into account, but language isn't mentioned.
Reviewer: 3
Comments to the Author(s)
I find this article difficult to review in that it seeks to make a contribution to research on public
science communication but makes use neither of existing literature in this field nor the
13
methodological techniques and norms of social research. If this were in a more disciplinary
journal - such as Science Communication, Public Understanding of Science, or JCOM - I would
advise rejection. The findings, based on a somewhat superficial analysis of science podcasts, are
not particularly surprising, and there is little effort to discuss their implications. As a result it is
unclear why it is interesting or important to publish a finding that, say, 77% of podcasts are
targeted at public audiences.
It is true that there thus far is relatively little literature on science podcasts specifically, but there
is some that is not cited (e.g. Birch & Weitkamp 2010; Dantas-Querroz et al 2018; and associated
references), and there is far more on the form and nature of science communication in social and
digital media generally or on science radio, the obvious precursor to science podcasting, that a
quick literature search would have identified. I suggest that the author engages with this
literature in order to ground their discussion and, specifically, to show why this work meets
specific research needs, how it contributes to current debates around the use of new media for
science communication, and what its implications are (they write, in closing, that "these insights
will help inform public science communication policy" but it is not at all obvious to me how or
why this would be the case). This may entail carrying out further empirical research in order to
take the article beyond quantitative description of this emerging phenomenon to offer insight into
the patterns identified. For instance, carrying out interviews with producers of podcasts (as has
been done for other new media science communication formats; e.g. Ranger & Bultitude 2014;
Riesch & Mendel 2013) would offer a richer understanding of, for instance, topic choice or who
bears the cost of podcasting, and would thus avoid some of the speculation currently found in the
Discussion. It is worth noting that, in this field as a whole, it would not be usual to publish
description of a phenomenon without more in-depth analysis or theoretical testing.
There are also some methodological problems due to a lack of transparency as to how the
podcasts were coded. How exactly was the target audience identified? Or the focal topic (what
about interdisciplinary research)? How were hosts categorised, and based on what empirical
material (again, what about individuals or groups who fall into more than one category)? It is
also not clear how coding was carried out. Were web scraping tools used, or was all coding done
by hand? By how many people? More generally, it would be useful to have the methods
grounded within specific traditions or techniques; the work of Martin Bauer or Dietram Scheufele
and colleagues, who work in quantitative science communication research, might offer some
inspiration here.
A minor point: 'demographics' tends to refer to human populations (from 'demos', the people). If
it is to be applied to non-human cohorts then this needs to be explained and justified.
References
Birch, Hayley, and Emma Weitkamp. 2010. “Podologues: Conversations Created by Science
Podcasts: Podologues: Conversations Created by Science Podcasts.” New Media & Society 12 (6):
889–909. https://doi.org/10.1177/1461444809356333.
Dantas-Queiroz, Marcos V., Lia C. P. Wentzel, Luciano L. Queiroz. 2018. “Science
Communication Podcasting in Brazil: The Potential and Challenges Depicted by Two Podcasts.”
Anais Da Academia Brasileira de Ciências 90 (2): 1891–1901. https://doi.org/10.1590/0001-
3765201820170431.
Ranger, Mathieu, and Karen Bultitude. 2014. “‘The Kind of Mildly Curious Sort of Science
Interested Person like Me’: Science Bloggers’ Practices Relating to Audience Recruitment.” Public
Understanding of Science, October, 963662514555054.
https://doi.org/10.1177/0963662514555054.
14
Riesch, Hauke, and Jonathan Mendel. 2013. “Science Blogging: Networks, Boundaries and
Limitations.” Science as Culture 23 (1): 51–72. https://doi.org/10.1080/09505431.2013.801420.
Author's Response to Decision Letter for (RSOS-180932.R0)
See Appendix A.
label_version_2
RSOS-180932.R1 (Revision)
label_author_4
Review form: Reviewer 1
Is the manuscript scientifically sound in its present form?
Yes
Are the interpretations and conclusions justified by the results?
Yes
Is the language acceptable?
Yes
Is it clear how to access all supporting data?
Yes
Do you have any ethical concerns with this paper?
No
Have you any concerns about statistical analyses in this paper?
I do not feel qualified to assess the statistics
Recommendation?
label_recommendation_4
Accept as is
Comments to the Author(s)
label_comment_4
I thank the author for a very clear response to individual comments, which has made re-
reviewing this article straightforward. The author's modifications have substantially addressed
my previous comments and considerably improved the article. The study has some remaining
methodological issues, but these are now acknowledged in the article as limitations. Overall, the
novelty of the topic warrants inclusion. I recommend acceptance.
15
label_author_5
Review form: Reviewer 2
Is the manuscript scientifically sound in its present form?
No
Are the interpretations and conclusions justified by the results?
Yes
Is the language acceptable?
Yes
Is it clear how to access all supporting data?
Yes
Do you have any ethical concerns with this paper?
No
Have you any concerns about statistical analyses in this paper?
No
Recommendation?
label_recommendation_5
Major revision is needed (please make suggestions in comments)
Comments to the Author(s)
label_comment_5
I remain torn. I think the data is interesting and probably okay. And I appreciate the inclusion of
the limitations but, at the end of the day, I still wonder about what we can make of the data given
that it wasn't coded in a rigorous fashion using the long-developed methods of social science.
Saying that the day is freely available just isn't enough. I want to know that someone could
replicate the study and of the great things about training coders and working out a final
codebook is that it forces you to develop a rigorous coding schema and not just use your own
judgement. More generally, we also continue to be missing key coding information such as how
the target audiences and were identified. What's provided just tells us who the audiences were,
not how they were determined for coding. Similarly, how were the podcast hosts categorized?
Also, I might note that p. 10 seems to conflate qualitative data with latent quantitative data. In
quantitative content analysis, intercoder reliability is about being confident about latent variables
in the process of quantifying.
Another big question I started to wonder about as I reread the manuscript is the degree to which
it makes sense to analyze the active and inactive podcasts together. We don't really get at the key
question of what might make something survive/die.
Some other things ...
p. 2. Devices such as Alexa and Google Home are now also being used to listen to podcasts
p. 3. I'm not sure podcasts are inherently more dialogic; traditional media, inasmuch as most of it
appears in online formats, could have all the same features. It might just be that the norms of
podcasting are more dialogic
p. 6. I really don't understand the purpose of saying you're ranking podcast hosts; that doesn't
seem critical to the descriptive analysis
p. 10. I really didn't understand the discussion of why we would want to exclude podcasts that
have pseudo-scientific content from the analyses ... the idea of a "legitimate scientific podcast"
seems to conflate an effort to understand what content exists and what quality content exists. If
16
we're interested in trends in podcasts, wouldn't we want to know about all levels of quality?
p. 11. The paragraph on science vs. general podcasts needs some sort of transition or introductory
sentence.
p. 12. Is it appropriate in this kind of article to use hypothetical quotation marks? I prefer to keep
quotation marks for actual quotations.
Conclusion: This feels more like a summary than a conclusion.
label_end_comment
Decision letter (RSOS-180932.R1)
06-Nov-2018
Dear Dr MacKenzie:
Manuscript ID RSOS-180932.R1 entitled "Science Podcasts: Analysis of Global Production and
Output From 2004 to 2018" which you submitted to Royal Society Open Science, has been
reviewed. The comments of the reviewer(s) are included at the bottom of this letter.
Please submit a copy of your revised paper before 29-Nov-2018. Please note that the revision
deadline will expire at 00.00am on this date. If we do not hear from you within this time then it
will be assumed that the paper has been withdrawn. In exceptional circumstances, extensions
may be possible if agreed with the Editorial Office in advance. We do not allow multiple rounds
of revision so we urge you to make every effort to fully address all of the comments at this stage.
If deemed necessary by the Editors, your manuscript will be sent back to one or more of the
original reviewers for assessment. If the original reviewers are not available we may invite new
reviewers.
To revise your manuscript, log into http://mc.manuscriptcentral.com/rsos and enter your
Author Centre, where you will find your manuscript title listed under "Manuscripts with
Decisions." Under "Actions," click on "Create a Revision." Your manuscript number has been
appended to denote a revision. Revise your manuscript and upload a new version through your
Author Centre.
When submitting your revised manuscript, you must respond to the comments made by the
referees and upload a file "Response to Referees" in "Section 6 - File Upload". Please use this to
document how you have responded to the comments, and the adjustments you have made. In
order to expedite the processing of the revised manuscript, please be as specific as possible in
your response.
In addition to addressing all of the reviewers' and editor's comments please also ensure that your
revised manuscript contains the following sections before the reference list:
• Ethics statement
If your study uses humans or animals please include details of the ethical approval received,
including the name of the committee that granted approval. For human studies please also detail
whether informed consent was obtained. For field studies on animals please include details of all
permissions, licences and/or approvals granted to carry out the fieldwork.
• Data accessibility
It is a condition of publication that all supporting data are made available either as
supplementary information or preferably in a suitable permanent repository. The data
17
accessibility section should state where the article's supporting data can be accessed. This section
should also include details, where possible of where to access other relevant research materials
such as statistical tools, protocols, software etc can be accessed. If the data have been deposited in
an external repository this section should list the database, accession number and link to the DOI
for all data from the article that have been made publicly available. Data sets that have been
deposited in an external repository and have a DOI should also be appropriately cited in the
manuscript and included in the reference list.
• Competing interests
Please declare any financial or non-financial competing interests, or state that you have no
competing interests.
• Authors’ contributions
All submissions, other than those with a single author, must include an Authors’ Contributions
section which individually lists the specific contribution of each author. The list of Authors
should meet all of the following criteria; 1) substantial contributions to conception and design, or
acquisition of data, or analysis and interpretation of data; 2) drafting the article or revising it
critically for important intellectual content; and 3) final approval of the version to be published.
All contributors who do not meet all of these criteria should be included in the
acknowledgements.
We suggest the following format:
AB carried out the molecular lab work, participated in data analysis, carried out sequence
alignments, participated in the design of the study and drafted the manuscript; CD carried out
the statistical analyses; EF collected field data; GH conceived of the study, designed the study,
coordinated the study and helped draft the manuscript. All authors gave final approval for
publication.
• Acknowledgements
Please acknowledge anyone who contributed to the study but did not meet the authorship
criteria.
• Funding statement
Please list the source of funding for each author.
Please note that Royal Society Open Science charge article processing charges for all new
submissions that are accepted for publication. Charges will also apply to papers transferred to
Royal Society Open Science from other Royal Society Publishing journals, as well as papers
submitted as part of our collaboration with the Royal Society of Chemistry
(http://rsos.royalsocietypublishing.org/chemistry). If your manuscript is newly submitted and
subsequently accepted for publication, you will be asked to pay the article processing charge,
unless you request a waiver and this is approved by Royal Society Publishing. You can find out
more about the charges at http://rsos.royalsocietypublishing.org/page/charges. Should you
have any queries, please contact openscience@royalsociety.org.
Once again, thank you for submitting your manuscript to Royal Society Open Science and I look
forward to receiving your revision. If you have any questions at all, please do not hesitate to get
in touch.
Kind regards,
Royal Society Open Science Editorial Office
18
Royal Society Open Science
openscience@royalsociety.org
on behalf of Dr Hamed Haddadi (Associate Editor) and Prof. Marta Kwiatkowska (Subject
Editor)
openscience@royalsociety.org
Associate Editor Comments to Author (Dr Hamed Haddadi):
Please address the comments from the second reviewer before the article can be considered for
publication.
Reviewer comments to Author:
Reviewer: 1
Comments to the Author(s)
I thank the author for a very clear response to individual comments, which has made re-
reviewing this article straightforward. The author's modifications have substantially addressed
my previous comments and considerably improved the article. The study has some remaining
methodological issues, but these are now acknowledged in the article as limitations. Overall, the
novelty of the topic warrants inclusion. I recommend acceptance.
Reviewer: 2
Comments to the Author(s)
I remain torn. I think the data is interesting and probably okay. And I appreciate the inclusion of
the limitations but, at the end of the day, I still wonder about what we can make of the data given
that it wasn't coded in a rigorous fashion using the long-developed methods of social science.
Saying that the day is freely available just isn't enough. I want to know that someone could
replicate the study and of the great things about training coders and working out a final
codebook is that it forces you to develop a rigorous coding schema and not just use your own
judgement. More generally, we also continue to be missing key coding information such as how
the target audiences and were identified. What's provided just tells us who the audiences were,
not how they were determined for coding. Similarly, how were the podcast hosts categorized?
Also, I might note that p. 10 seems to conflate qualitative data with latent quantitative data. In
quantitative content analysis, intercoder reliability is about being confident about latent variables
in the process of quantifying.
Another big question I started to wonder about as I reread the manuscript is the degree to which
it makes sense to analyze the active and inactive podcasts together. We don't really get at the key
question of what might make something survive/die.
Some other things ...
p. 2. Devices such as Alexa and Google Home are now also being used to listen to podcasts
p. 3. I'm not sure podcasts are inherently more dialogic; traditional media, inasmuch as most of it
appears in online formats, could have all the same features. It might just be that the norms of
podcasting are more dialogic
p. 6. I really don't understand the purpose of saying you're ranking podcast hosts; that doesn't
seem critical to the descriptive analysis
p. 10. I really didn't understand the discussion of why we would want to exclude podcasts that
have pseudo-scientific content from the analyses ... the idea of a "legitimate scientific podcast"
19
seems to conflate an effort to understand what content exists and what quality content exists. If
we're interested in trends in podcasts, wouldn't we want to know about all levels of quality?
p. 11. The paragraph on science vs. general podcasts needs some sort of transition or introductory
sentence.
p. 12. Is it appropriate in this kind of article to use hypothetical quotation marks? I prefer to keep
quotation marks for actual quotations.
Conclusion: This feels more like a summary than a conclusion.
Author's Response to Decision Letter for (RSOS-180932.R1)
See Appendix B.
label_end_comment
Decision letter (RSOS-180932.R2)
26-Nov-2018
Dear Dr MacKenzie,
I am pleased to inform you that your manuscript entitled "Science Podcasts: Analysis of Global
Production and Output From 2004 to 2018" is now accepted for publication in Royal Society Open
Science.
You can expect to receive a proof of your article in the near future. Please contact the editorial
office (openscience_proofs@royalsociety.org and openscience@royalsociety.org) to let us know if
you are likely to be away from e-mail contact. Due to rapid publication and an extremely tight
schedule, if comments are not received, your paper may experience a delay in publication.
Royal Society Open Science operates under a continuous publication model
(http://bit.ly/cpFAQ). Your article will be published straight into the next open issue and this
will be the final version of the paper. As such, it can be cited immediately by other researchers.
As the issue version of your paper will be the only version to be published I would advise you to
check your proofs thoroughly as changes cannot be made once the paper is published.
On behalf of the Editors of Royal Society Open Science, we look forward to your continued
contributions to the Journal.
Kind regards,
Andrew Dunn
Senior Publishing Editor
Royal Society Open Science
openscience@royalsociety.org
on behalf of Dr Hamed Haddadi (Associate Editor) and Marta Kwiatkowska (Subject Editor)
openscience@royalsociety.org
20
Follow Royal Society Publishing on Twitter: @RSocPublishing
Follow Royal Society Publishing on Facebook:
https://www.facebook.com/RoyalSocietyPublishing.FanPage/
Read Royal Society Publishing's blog: https://blogs.royalsociety.org/publishing/
Appendix A
General response to Reviewer’s Comments.
I would like to thank the three-anonymous peer-reviewers for committing their
time and expertise to providing many insightful and detailed comments on my
manuscript. Each reviewer provided comments that were complimentary to the
other reviewer’s comments. Combined, their comments and suggestion have
enabled me to make many major revisions to this manuscript, which have greatly
improved the manuscript.
Additionally, some changes have been made that were not requested by the
reviewers. These were done to enhance the manuscript by improving context
and data clarity. These changes include: (1) the introduction and conclusions
being completely rewritten for style. (2) A new section was added to the
discussion title ‘Open Questions and Future Directions’. (3) Figures 7E and 7F,
along with table 2, have been deleted and replaced with a new Figure: Figure 8,
which shows the results more clearly. (4) A new sub-figure has been added to
Figure 4, showing the decline in new video podcast series vs. year.
Please find my detailed response to each one of the reviewer’s comments herein.
I hope the reviewers will find these extensive changes satisfactory.
Sincerely,
Dr Lewis MacKenzie
Comments to Author:
Reviewer: 1
R1 C1a. This is an interesting study that argues well for its publication in terms
of scarcity of research on this topic. A substantial analysis of data has been
conducted and I would support publication, though not in the current form.
R1 C1b It is strange that the term 'science' is used in the article title, when
topics such as mathematics, statistics and engineering are included. Perhaps a
broader term is needed? I suppose science is used as the focus is 'natural
sciences' on iTunes, but are there categories on iTunes for other STEM
disciplines?
R1 C1c It seems odd that computer science and engineering are included but
technology is not. Perhaps some clarification on the inclusion criteria would
make this clearer.
Response to comments R1 C1:
The reviewer raises a valid point of clarification. In Birch and Weitkamp (2010),
‘Podolouges: conversations created by science podcasts’, “science podcasts”
are defined as: “natural sciences and mathematics”. The definition used in this
study was broadly similar but I agree that this was unclear.
In the ‘Inclusion criteria section’ the bullet point reading:
<U+F0B7> “To be included in this study, the primary focus of a podcast must be on
covering scientific topics, for example, scientific research, science news,
scientific careers, scientific seminars, lectures, or similar.”
Has been replaced with the following:
<U+F0B7> For the purposes of this study, “science podcasts” are primarily defined
as podcast series covering topics in the natural sciences (i.e. physics,
chemistry, biosciences, geology, oceanography, climate change,
palaeontology, etc) and mathematics. NB: this primary definition is
functionally very similar to the definition used by Birch and Weitkamp
(2010).[15]
<U+F0B7> Under a secondary definition: podcast series covering the academic and
research aspects of computer science, engineering, pharmacology and
medicine were included. These podcast series account for 3% of the
podcasts included in the study.
Further, the examples of non-scientific topics that were excluded has been
reworded and moved further up the list. It now reads:
<U+F0B7> Podcast series focusing on non-science topics were excluded. NB:
examples of such topics include: consumer technology; business; gardening, bird-
watching (not ornithology); food/cooking; religion; life-coaching; weather;
sustainability; environmental activism; pseudo-science; occult and paranormal;
nerd culture, and podcasts primarily intended to review or sell commercial
products, e.g. relating to tropical fish keeping or telescopes.
With regards to the possibility of examining iTunes categories other than
‘Natural Sciences’, I refer the reviewer to my response to a similar (but more
substantial) comment by reviewer 2 (i.e. R2 C6), which will explain this
thoroughly.
R1 C2. Around line 11 on page 2: It seems advantages are being conflated. The
advantages of podcasts over conventional broadcast are stated ("can be
listened to/watched at any time"), but the time shift is not an advantage over
print media. Perhaps a second sentence for print media is warranted.
The reviewer is correct that this was unclear. The introduction has been
rewritten with a paragraph dedicated to advantages of audio media over print
media for science communication. The particular sentence under scrutiny has
been reworded for clarity within the first paragraph of the introduction and
references added:
“For audiences, podcasts are audio podcasts are particularly convenient because
they can be listened-to whilst undertaking other activities such as commuting or
housework without looking at a screen.[3–5]”
R1 C3. Around line 12 on page 2: Why is "global distribution" a requirement for
"niche and highly specialised". Please expand.
This has been clarified and relevant references are added. The relevant section
in paragraph 1 of the introduction now reads:
“Due to their online distribution, podcasts have the potential to reach
audiences around the globe, in a manner unconstrained by the demographic or
geographic restrictions associated with traditional regional or national media.[12]
This allows podcasts to potentially cater for niche audiences that are not a priority
for traditional media. One such example of a highly specialised podcast series is:
‘This Week in Virology’, which primarily serves the virology research community,
yet which also reportedly has a large proportion of public listeners.[9] Another
example of podcasts filling an under-served niche are podcasts that focus on
science for young children, one example of which is “Wow In The World”.[13]
Indeed, many more examples of such niche science podcasts could be provided
here if brevity was not a constraint. In summary, due to the large number of
science podcasts, their accessible nature, and their varied production, it could be
said that “there is a science podcast for everyone”.”
R1 C4. Around line 16 on page 3: Why were individuals not contacted? This is not
a methodological problem, but earlier in the paragraph reasons were given for
methodological choices.
I agree that this was vaguely worded. To clarify my reasoning for this I have firstly
tightened up the final paragraph of the introduction to be clearer with the aims
of this study. Emphasising that this is a large-scale quantitative study.
Additionally, individuals were not contacted in this study because this could
introduce bias into the data coding and reduce the dataset for valid comparison,
i.e. be effectively altering the methodology for podcasts where individuals
responded vs podcasts were individuals did not.
The final paragraph of the introduction now reads:
“Despite the rise of podcasts as a popular medium for science communication,
there have been no studies of the large-scale patterns in the production of science
podcasts; this represents a large and fundamental gap in our knowledge of science
communication. This study aimed to provide large-scale quantitative insight into
the overall global production and dissemination of science podcasts. This has been
achieved by analysing online textual and visual presence of 952 English language
science podcasts for key production variables, including: audio/visual format,
topic, target audiences, hosts, number of episodes released, lifespan of podcasts,
supplementary income, and, use of supplementary show notes. All data
associated with this study is available as a supplementary dataset in the form of a
Microsoft Excel spreadsheet.”
The paragraph 1 in ‘Materials and Methods’ now reads:
“All information used in this study was sourced from public websites that were
dedicated to the promotion of podcasts. Information was gleaned exclusively
from visual and textual “metadata” relating to each podcast series, including the
description of each podcast series on ‘iTunes’; the websites of podcasts; and the
social media content associated with podcast series on ‘Twitter’,[23]
‘Facebook’,[24] and ‘Patreon’.[25]. The audio and video content of podcasts
themselves was not utilized due to the impracticalities associated with listening
and transcribing the tens of thousands of hours of audio content that science
podcasts provide.[26] Producers and other individuals associated with the
production of podcast series were not contacted for information relating to this
study in order to avoid methodical disparity between podcast series where
producers would have responded and those where producers would not have. In
all cases, information was accessed between the 5th of January and 5th of February
2018. The associated supplementary database contains all the specific dates of
when each website URL was accessed. All data was manually coded by a single
individual (the author).”
Additionally, the reviewer may wish to note my response to Reviewer 3
Comment 6, where this matter of methodology is expanded upon in greater
detail.
R1 C5. Around line 25 on page 3: I am not at all convinced that the reference to
or citation of a blog post by the author is needed here. It doesn't seem to relate
to the later compilation of data.
I agree with the reviewer. This line has now been deleted.
R1 C6. Around line 30 on page 3: I need some convincing that the use of iTunes
has not excluded non-Apple users. For example, "many podcast apps run on the
iTunes directory engine" - does this include Android apps? More needs to be
done to convey why iTunes is the right choice for data source.
I agree that this point was not clear to those many readers who would be
unfamiliar with the workings of podcast apps. The iTunes podcast directory
search is cross-platform and is used by many podcast apps, including those
available on Android, or other operating systems. To clarify this, the first
paragraph of ‘Identifications of podcasts for study’ has been reworded and
references to Android podcast app descriptions that explicitly say they
incorporate iTunes search in their Google Play store descriptions. This paragraph
now reads:
“Due to the decentralised nature of the podcast medium, there is not a single
podcast database or website that lists all podcasts. However, the closest thing to a
“de-facto” centralised podcast database is the ‘iTunes’ podcast directory, which
as of 2015, was estimated to list over 200,000 podcasts.[24] The ‘iTunes’ podcast
directory is cross-platform, i.e. it can be used by podcast apps running on non-
Apple platforms, such as Android devices or other operating systems.[25,26]
Therefore if a podcast is not listed on the ‘iTunes’ podcast directory, then it is
considerably less likely to be found by listeners.[27] Other science podcasts have
utilised ‘iTunes’ as their primary podcast database.[3] Therefore, at the time of
writing, ‘iTunes’ podcast directory was deemed the best source to use as a basis
for this study.”
R1 C7. Around line 44 on page 3: reference to "podcasts spuriously listed" - I
wanted some examples. Perhaps say some examples are given in the next
section.
With regards to listing individual spurious podcasts, I appreciate the reviewer’s
point, but I politely disagree with the reviewer that specific examples of
individual podcasts are necessary. My rationale for this is that I am concerned
that specifically listing some non-scientific podcasts in this study risks lending
some sort of legitimacy to these non-scientific podcasts, which could be
damaging to the scientific discourse when it comes to the spread of pseudo-
science.
A list of topics that were excluded are already provided in the “inclusion
criteria” section. The relevant bullet point reads:
• “Podcast series focusing on non-science topics were excluded. NB:
examples of such topics include: consumer technology; business; gardening, bird-
watching (not ornithology); food/cooking; religion; life-coaching; weather;
sustainability; environmental activism; pseudo-science; occult and paranormal;
nerd culture, and podcasts primarily intended to review or sell commercial
products, e.g. relating to tropical fish keeping or telescopes. “
R1 C8. Around line 12 on page 4: It is not clear why podcasts must be English
language to be "valid" in "this study of science podcasts". This suggests that
non-English language podcasts are not valid science. I think the framing of the
study needs rewriting slightly to be clear that this is a "study of English-
language science podcasts".
This has been reworded to now read
<U+F0B7> “Only English language podcasts were included in this study. If a
podcast was available in multiple languages, then only the English
language podcast feed was analysed to avoid duplicating content.”
Additionally, the question of science podcasts in other languages has been
alluded to in the both the ‘Limitations’ and ‘Open Questions and Future
Directions’ subsections of the discussion.
R1 C9. Around line 17 on page 4: Why was video chosen over audio? Justify this
choice.
This was done simply to avoid data duplication. The line has been amended to
read:
• “If a podcast series was available as separate audio-only and video-feeds
covering the otherwise identical content, then only the video-feed was
included for analysis to avoid data duplication.”
R1 C10. Page 4: It might be good to hear at this point which topics were
included as well as which were excluded. This comes later, but it would be
useful here, before the list around line 22.
I believe this point has been answered in accordance to the response to
Reviewer 1, comment 7.
R1 C11. Around line 30 on page 4: I find the bullet about no episodes available
and the next one confusing. What would happen if a podcast had 0 episodes
available on its iTunes listing but 250 episodes available on its own website?
In the case the reviewer mentions, the podcast would be included. The
inclusion criteria have been updated to reflect that podcasts without an iTunes
site were included. The relevant points now read:
<U+F0B7> “Podcast series with no episodes available to stream or download via either
iTunes or another website related to the podcast were excluded.
<U+F0B7> To be included for analysis, episodes of a podcast series had to be freely available
for listeners to stream or download from a source at the time of sampling. For
example, if a podcast had 100 episodes available on ‘iTunes’, yet had 250 episodes available
to stream on their own website, then the 250 episodes were included for analysis.”
R1 C12. Around line 40 on page 4: It seems a strange choice to back-date the
podcast start date of a podcast that was previously a broadcast radio show. For
example (a non-science example), very old episodes of Desert Island Discs are
available on iTunes, but it would not be sensible to consider this a podcast that
started in the 1970s. This has a big effect because back-dating podcast start dates
like this will effect the number of active podcasts in a given year and the podcast
life span statistics. Can this choice be reconsidered? If not, further justification is
needed and these issues should be discussed in the limitations.
I generally agree with the comments of the reviewer. This was a bit of a
conundrum for experimental design. Fortunately, only 11 podcast series fit into
this category, 10 of which were ‘affiliated’ to an organisation or network and the
original broadcast dates of which range relatively uniformly from 1995 to 2003.
586 episodes were used in the data fitting for ‘Affiliated’ lifespans, so these 10
episodes make up < 2% of data points. Further, considering that we are looking
at a “half-life” of podcast lifespans – which is a rather robust measure - the
influence on the podcast lifetime results can be considered negligible for the
purposes of this study.
This section of the inclusion criteria has been updated to now read:
<U+F0B7> “If the content of a podcast series was originally available prior to 2004,
(e.g. as an internet or broadcast radio show), then the original air date of
the first show episode was used in-lieu of the upload date of the podcast
episode. This was a necessary compromise because the date that a collection of
podcasts was uploaded to iTunes is not known and so could not be used. Further,
it provides some context for long-running internet radio series that have embraced
the podcast format. However, this has some consequences for results (see the
“Methodology and associated limitations” sub-section).”
The following has been added to the “Methodology and associated limitations”
section of the D
“A notable limitation of this study is that the original podcast upload
date for radio shows broadcast pre-2004 are not known; instead the
original air-date episodes (as provided on iTunes or another relevant
website) is used as a compromise. This accounts for the 11 podcasts
available prior to 2004 (see supplementary database for full details). Of
these 11 podcasts, 10 are affiliated to an organisation. Considering that
586 ‘Affiliated’ podcasts were analysed and that the mean lifespan (<U+0442>) is
calculated from robust curve-fitting models, the influence of these 10
podcasts on the results of lifespan fitting calculations can be considered
negligible for the purposes of this study.”
R1 C13. Around line 8 on page 5: I wanted to know why two equations were
proposed. Why not 1? Why not more? This is touched on later, but more here
would be helpful.
This line has been clarified to read:
“To estimate mean lifespan of podcasts, single-term and two-term
exponential decays were fitted to podcast lifespan data by least-squares
regression. Two-term exponential fits were necessary because single-
term exponential decays was found to bit a poor fit to the data (R2 < 0.9)
(see Figure 8).”
R1 C14. Around line 33 on page 6: On what basis is chemistry judged to be
under-represented? For example, psychology has 36 podcasts and chemistry
has 30. Why is the former considered "well-represented" (around line 23 page
12) and chemistry not? More needs to be done to set and justify expectations.
Why consider physics, chemistry and biology in particular? What about other
disciplines, e.g. maths and engineering? It is notable that the author feels his
own discipline is under-represented. Please argue more thoroughly for this key
conclusion.
The reviewer is correct that this statement required further justification.
Therefore, this has been greatly expanded in the discussion with appropriate
references.
“Figure 2A shows that ‘chemistry’ as a subject only accounts for 3% of podcast
series: when compared to the two other primary science subjects typically taught
in schools, i.e. ‘biology' (13% of science podcast series), and ‘physics and
astronomy’ (18% of science podcasts), this gives the impression that chemistry is
under-represented by science podcasts. There are several potential explanations
as to why this may be. A 2011 editorial in the journal ‘Nature Chemistry’ suggested
that chemistry “is a central science”, meaning that aspects of chemistry are
incorporated into other disciplines (e.g. biochemistry and materials research);
therefore chemistry is often not distinctly represented in public-facing science
communication.[37] Similarly, Hartings and Fahly (2011) noted that popular
science involving chemistry may not be labelled as chemistry; that chemistry is
complex; and that chemistry lacks unifying themes and public narratives that may
be present in biology and physics.[38] Additionally, a review of chemistry
communication in 2016 noted that concepts in chemistry are well-served by
dynamic visual representations,[39] therefore chemistry may not be well-suited
to the primarily-audio format of podcasts. Indeed, chemistry content is very well
received in more visual internet mediums, e.g. the video series: ‘Periodic Videos’
on ‘YouTube’.[40] Velden and Lagoze (2009) note that chemistry has been slow to
adopt “new web-based models of scholarly communication” when compared to
physics and biology.[41] Whilst this may true for scholarly communications, it is
not clear if this is true for chemistry and digital science communication practices.
All these reasons are likely to play into the apparent lack of chemistry science
podcast series. This reinforces a 2016 recommendation from the ‘National
Academies of Science, Engineering, Medicine’ that science funding agencies support
digital media for chemistry communication as a priority.[42]”
R1 C15. It would be useful to know if the comparisons between independent
and affiliated podcasts at the bottom of page 7 and the end of section 4 on
page 8 are statistically significant.
A very astute point by the reviewer and one I am particularly grateful for.
Firstly, statistical analysis was conducted on number of episodes released
by affiliated and independent podcasts. A two-sampled t-test found that
the difference between number of episodes was statically significant (p
= 0.01) and that the greater number of episodes produced ‘Affiliated’
podcasts was also statically significant to a greater degree (p = 0.005).
This following have been updated to include this new information: the
abstract, results, discussion, conclusion, and Figure 7 caption.
The table containing this data has been removed and replaced with
Figure 8C and Figure 8D which show the best-estimate and 95%
confidence intervals of the mean lifespan data. This shows the data much
more clearly (see below).
A method for estimating whether or not an effect size (in this case,the
difference between two best-fit estimates) is statistically significant
when we only have 95% confidence intervals is described by Altman and
Bland (2011) “How to obtain the P value from a confidence interval” BMJ.
https://doi.org/10.1136/bmj.d2304. Applying this method to the
difference between the best-estimate of long-duration lifetimes gives a
p-value of < 0.02, so the difference in long-duration lifespans is statically
significant.
However, clearly the 95% confidence intervals for mean lifespan for the
short duration podcasts are non-normally distributed. This somewhat
complicates the statistical analysis. Fortunately, neither the the upper or
lower 95% confidence intervals can give a statically significant result.
Therefore, the difference in best-fit of the short-duration lifetimes in not
statically significant.
The ‘Data Analysis’ section of the Methods now has a new paragraph:
“Once the best-estimate and 95% confidence intervals bounds of <U+0422> were
known, the statistical significance of the difference between the best-fit
estimates of <U+0422> were estimated by the method described in Bland and
Altman (2011) which is based upon the 95% confidence intervals.[29] In
all cases (including the case of non-normally distributed 95% confidence
intervals), the larger confidence interval was used to assess statistical
significance.”
The results section on mean lifespans has also been adjusted
accordingly, with a large section of text deleted and the following
added:
“The best-fit and 95% confidence interval values for <U+0442> are shown in
Figure 8. For short-duration podcast series, the difference in the best-
estimates of <U+0442> for ‘Affiliated’ and ‘Independent’ podcast groupings were
not statically significant (p = 0.33 or greater). However, for long-duration
podcasts the difference in the best-estimates of <U+0442> or ‘Affiliated’ and
‘Independent’ podcast groupings was statistically significant (p < 0.02).”
The discussion section on mean lifespans has also been adjusted and
now reads:
“The statically significant greater best-estimate values for mean lifespan
of ‘affiliated’ podcast series (5.5 years) compared to ‘independent’
podcast series (4.3 years) (see Figure 8D) could be explained by the
hypothesis is that ‘independent’ podcast series may be more likely to be
produced by individuals or small groups, with limited time and
resources, whereas ‘affiliated’ podcast series are produced by
organisations with dedicated staff with defined duties. Such dedicated
staff could take-over podcasting duties, thus extending the overall
lifespan of the ‘Affiliated’ podcast series subset in comparison to the
‘independent’ podcast series subset. However, no firm conclusions with
regards to the causes of podcast series sustainability can be drawn from
this study, and it should be noted that there are exceptionally long-
running podcast series within both the ‘independent’ and ‘affiliated’
subsets. In their 2011 study titled “Why podcasters keep going”,
Markman found that creator-audience community, engagement (e.g.
emails, discussion forums, social media etc), audience appreciation, and
enjoyment were key drivers of podcast longevity. Markman notes that
further study is required into the phenomena of podcast longevity and
so-called ‘podfading’, where podcasts are no longer produced.[43]”
The relevant subsection of the conclusions has been altered and now
reads:
“Whether or not a science podcast series is independent or affiliated to
an organisation appears to make key differences in several production
outputs. Most notably, ‘independent’ podcast series produce fewer
episodes on average (median 16, average 48) than ‘affiliated’ podcast
series (median 24, average 90). Further, the long-term mean-lifespan of
‘independent’ podcasts (4.3 years) appears to be significantly less than
the long-term mean-lifespan of ‘affiliated’ podcasts (5.5 years).”
R1 C16. The discussion of limitations is good, though the speculation about
future technologies around lines 14 and 24 on page 9 are not necessary,
including the references 44 and 45.
These speculative components on machine learning have been deleted.
R1 C17. Around line 50 on page 9: make it clear sooner that Morgan's definition
of active podcasts differs, rather than making this an after-thought.
As per the reviewer request, the following has been moved up to the paragraph
before the one mentioned by the reviewer:
“Importantly, Morgan defined “active podcasts” as podcast series that had
released an episode within the 6 months prior to the sampling date [24]; this is
less stringent than the definition of activity used in the present study, which
defines “active podcasts” as podcast series that had released an episode within 3
months prior to the sampling date.”
R1 C18. Around line 1 on page 10: use of word "significantly". I do not believe
any significance testing has been done (though I think it should be).
The word “significant” has been removed.
R1 C19. Around line 55 on page 10: why is the comparison made with "high-
quality scientific papers"? More needs to be done to justify making this
comparison in particular.
I agree with the reviewer that this is not substantial enough to justify inclusion.
Therefore, this whole paragraph has been deleted and is not discussed
elsewhere in the manuscript.
R1 C20. Figure 1D: does this include podcasts that are still 'alive'? In which case,
this seems to be biased towards producing an exponential graph. What does an
equivalent graph of podcasts that have lived out their lifespan look like?
The reviewer raises an interesting question. I have analysed this data and it is
now shown the amended version of Figure 1, particularly sub-figures 1D and 1E
(see below). Further, some obsolete text from the results section has been
deleted and replaced with the following:
“A distinct difference in the lifespan of inactive and active podcast series can be
seen in Figure 1D and Figure 1E. This difference is most apparent for podcast
series with lifespans of less than one year: corresponding to ~40% of inactive
podcasts and ~22% of active (i.e. ongoing) podcast series.”
Figure 1: The growth and lifespan of science podcasts. (A) The total number of science
podcasts shows linear growth between 2004 and 2010, followed by exponential growth to
from 2010-2018. (B) The proportion of active/inactive science podcast series as of the
sampling period. (C) The total number of episodes released of all podcast series (NB: x-
axis is constrained to 350 episodes for clarity). (D) lifespan of inactive podcasts. (E)
Lifespan of active podcasts.
R1 C21
Table 3 (now Table 2)
<U+F0B7> Around line 15 on page 22: The criteria for inactive <1 year are not
made clear. This should state no episodes released in last 3 months.
I believe the text in the table of definitions is already consistent with
the reviewer’s request; it reads:
“A podcast series that has released at least one
episode in the period between twelve and three
months immediately prior to sampling date”
<U+F0B7> Around line 19 on page 22: Lifespan appears to be defined differently
elsewhere. Specifically, the lifespan could start not with the release of a
podcast episode, but with an original broadcast which is later released
as a podcast episode.
This has been updated to remove this discrepancy. It now reads:
“The time elapsed between the release dates of the first and last episode of a
podcast. If podcast release date is not known (e.g. in the case of internet radio
shows that have subsequently been released as podcasts), then this defaults to
the original air date of the first episode available to stream or download.”
<U+F0B7> Around line 13 on page 23: Rename 'University' to reflect the fact that
schools are also included.
This has been amended to now be: “Universities (and schools)”
<U+F0B7> Around line 26 on page 23: It seems very strange to combine radio and
print media. One has as traditional core business producing the sort of
content that could be podcast, and the other does not. Can this choice
be reconsidered or justified?
This is justified as a similar definition was used by Birch and Weitkam in
their 2010 study “podolouges: conversations created by science podcasts”,
published in New Media and Society. Quoting this study, they state:
“traditional media, such as newspapers and radio, are dominated by one-
way flow of information”. The definition in this study is functionally the
same, just “conventional” and “traditional” are swapped.
<U+F0B7> Around line 37 on page 23: I was confused why 'Audio podcast' was
listed under 'Non-audio media'. Could this be explained in situ?
The table section title has now been changed to: “Podcast media types”
<U+F0B7> Around line 6 on page 24: It seems strange to combine advertising and
sponsorship. If a podcast is supported by, say, a charitable or research
grant, this seems quantifiably different than being supported by
including commercial adverts in the podcast. Can this be reconsidered
or justified?
The reviewer raises a very astute point. The number of podcasts receiving
support from a research grant was not recorded. This was because, if the
research grants are acknowledged as a form of funding, then this is, in the
author’s opinion, very similar to sponsorship from other sources for the
purposes of this study. However, this distinction would be interesting to
make in future studies. To accommodate the reviewer’s point, the definition
now reads:
“Explicitly acknowledged sponsorship or advertisement from an
organisation other than the organisation the podcast is directly
affiliated with, including funding from research grants or charities.”
Minor:
The summary refers to "mid-2000s", which I might take to mean around 2050.
In fact, it means 2004.
<U+F0B7> “mid-2000s” has been changed to “2004”
A few minor typos and grammatical issues. One I noticed was the omission of %
in 57% around line 27 on page 12.
This has been corrected.
Reviewer 2
Comments to the Author(s)
I like many things about this and was happy to review it because it's
information that I think is useful baseline information for those who study
science communication. That being said, I have one primary concern related to
methods as well as some minor comments.
R2 C1:
With regards to methods, I really wish the author had used standard coding
practices to both exclude and categorize articles once the articles were
selected. Books by people such as Krippendorf discuss such methods.
R2 C2:
The key idea, however, is whether the read can be confident that the the
person who selected the articles for coding and then coded the articles was
reliable. Typically, this requires at least two independent coders working from a
common coding protocol on a blind an randomly overlapped sub-sample of the
content. Done properly, this allows for the reporting of chance-adjusted
intercoder reliability statistics for each categorization. In the absence of such
coding procedures and reporting, I would want a compelling argument that
things that were coded for were so clear as to be almost impossible to miscode.
R2 C3: Some of the codes reported (e.g. those in the podcast activity category)
seem okay but a couple of key ones seem more iffy. Specifically, I wonder about
how the coder determined which topics were "non-science," the topic, and
then how audience was determined. I'm not sure there's enough information
provided to replicate the results.
Response to comments 1-3:
The reviewer raises very astute points and I appreciate them raising them.
I agree with the reviewer that data reliability and transparency are fundamentally
important. That is why all data used in this study has been made freely and
publicly available in the supplementary dataset, which is an excel database. This
database contains all the results of data coding along with source URLs and the
date of retrieval that was used as a basis of all the data coding decisions.
However, I also agree I agree that the single-person coding is a shortcoming of
this study. This arose from the fact that the author is new to the methods of
qualitative research.
Therefore, I have added a new paragraph in the Discussion sub-section
“methodology and associated limitations” to ensure that reviewers are aware of
this limitation inherent to the study. This new paragraph reads:
“Secondly, it is important to note that the data generated in this study
was analysed (coded) by only a single person (the author). This is a
shortcoming of the study design because different individuals may
categorize qualitative data different. Best practice in such research
would have been to follow “multiple coding” procedures, i.e. for
multiple researchers to evaluate and analysing the data, subsequently
resolving any discrepancies arising, whilst also maximising robustness
in data coding. [30]”
R2 C4: Related to this, it's not clear to me whether something could have
multiple audiences, topics, types of affiliations, and hosts, etc. For example,
could something be both engineering and climate change? Or statistics and
medical?
The reviewer raises valid points that were not clearly addressed in the
manuscript.
A primary and secondary subject classification system was considered during the
design of the study. However, initial data analysis showed that it wasn’t
necessary because nearly all science podcasts were found to either clearly focus
on a single primary subject or they covered many different topics, i.e. they fitted
into the “general science” subject category. Similarly, ‘audiences’ were also
defined by this primary one-category system.
The question of podcast hosts is more difficult to handle, and I now realize this
was not explained properly in the first submission. The challenge of classifying
hosts arises from the fact that podcasts have multiple hosts (sometimes with
different backgrounds), that the hosts themselves can have multiple
backgrounds, and that there are often guest hosts. Firstly, if it were clear that
there are regular hosts, then only the regular hosts were assessed. These hosts
were then classified into ranked host categories: (5) Scientific
Researchers/Educators, (4) Media/Journalism Professionals, (3) Other
Professionals, (2) Amateurs, (1) Unclear. If there was a case where hosts had
clearly different expertise (e.g. some were scientists and somewhere journalists),
then the highest ranked host category was recorded, even if that host was in a
minority of hosts. This reflects the fact that having even a single scientist on a
podcast is elevates the scientific context of a science podcast. Clearly this is not
perfect classification system as information is lost, but it was a compromise as a
way of coding large amounts of data.
The above methodology was not described in the manuscript previously
submitted. This has been amended. The following has now been added to the
‘Categorical Definitions’ sub-section of the ‘Materials and Methods’ section:
“Science podcast series were typically found to be focused on either a
single distinct topic or to cover many different topics across a wide range
of scientific disciplines. Therefore, an exclusive single-category system
was used to classify the topics of podcast series; i.e. podcast series were
either classified as a single subject, or if they covered many topics, they
were classified as ‘general science’. Similarly, an exclusive one-category
classification system was deemed sufficient for organisational
affiliations, target audiences, and whether or not a podcast series was
video or audio format.
Three categories were devised for classifying supplementary income:
‘donations’, ‘merchandise’, and ‘advertising/sponsorship’. However,
this categorisation was not exclusive as individual podcast series may
employ some or all of these mechanisms in combination. ‘Country of
podcast production’ is the country primarily associated with a podcast
series and its hosts: this was an exclusive one-category classification
system, but if two or more countries were associated with a podcast
series, then it was classed as ‘multinational’.
Science podcast hosts were classified according to a ranked classification
system consisting of: ‘Scientific Researchers/Educators’ (Rank 5);
‘Media/Journalism Professionals’ (Rank 4); (3) ‘Other Professionals’
(Rank 3); ‘Amateurs’ (Rank 2); and ‘unclear’ (Rank 1), where the ranking
is related to general expertise/ scientific authority, i.e. the higher the rank
the higher the authority (see Table 2). In the case where podcasts had
multiple hosts (or a single host of different areas of expertise) then the
highest ranked category corresponding to one of the hosts was recorded,
even if that host was in an overall minority of hosts. The limitations of
this method are discussed in the ‘Methodology and associated
limitations” sub-section of the discussion. Podcast activity and podcast
lifespans were determined by objective definitions (also described in
Table 2).”
The following has now been added to the ‘Methodology and associated
limitations’ sub-section of the Discussion:
“Also relevant to data coding and interpretation of results is that a host
classification based on a notional ranking of scientific authority was used.
This system reflects that having even a single scientist in a grouping of hosts
will tend to elevate the scientific content of a podcast. However, this system
has several limitations: (1) it is based on a shallow analysis of textual and
visual data, (2) it may overly-simplify the data in a manner that over-
represents higher-ranked host classifications (i.e. scientists and media
professionals), and (3) it doesn’t consider the expertise of guests on podcasts.
For future studies, a classification system that better represents the myriad
possibilities of podcast host backgrounds should be implemented.”
R2 C5: You really also should have kept track of things that were excluded (and
why).
I agree with the reviewer that tracking all podcasts excluded would have been
good, but unfortunately it was not practicable to do so because the number of
excluded podcasts was similar in magnitude to the number of included podcasts,
i.e. several hundred. The ‘inclusion/exclusion criteria’ section is very clear about
why podcasts are included or excluded. However, the first line of the ‘Results’
section has been updated to now read:
“952 science podcast series met the inclusion criteria for this study (see
Section 3.3). A similar number, i.e. many hundreds of podcast series
were excluded as per the inclusion/exclusion criteria, but the details of
these individual excluded podcasts were not recorded.”
R2 C6: On a related note, I'd love to see a more compelling argument for the
focus on the natural sciences sub-directory but not the full "science and
medicine" directory. "Practical constraints" is not a compelling argument.
I agree with the reviewer that this should be more thoroughly justified. The
section of concern has been expanded upon and now reads:
“Thirdly, science podcasts were only identified by survey of a single
‘iTunes’ category: i.e. ‘Natural Sciences’.[2] The examination of ‘Natural
Sciences’ podcasts follows the methodology of a previous study by Birch
and Weitkamp, which defined science podcasts as “the natural sciences
and mathematics.[3] However, limiting this study to the ‘Natural Sciences’
category limits the podcasts examined for two reasons: (1) listing a
podcast on iTunes is not mandatory (although it is helpful for podcast
discovery); (2) the category a podcast is listed on ‘iTunes’ is self-selected
by the uploader, therefore, many podcasts meeting the inclusion criteria
may have been listed in other ‘iTunes’ categories. The most obvious
category that wasn’t analysed was the ‘Science and Medicine’
category.[31] However, although the ‘Science and Medicine’ may
contain many legitimate science podcasts, the author notes that it also
appears to contain a large number of podcasts that openly cover
dubious/harmful pseudo-medical practices and advice. Therefore, an
extremely stringent and in-depth inclusion/exclusion criteria strategy
would have to be developed applied, along with deep content analysis
(e.g. actually listening to the podcasts), to ensure that only legitimate
scientific podcasts are included in any such study. This was beyond the
scope of the current study.”
R2 C7: At minimum, I would want an argument for what that means for the
generalizability of the results.
I agree with the reviewer that this was not substantiated. Therefore, the
sentence claiming that the results are representative (shown below in purple)
has been deleted:
“Despite these limitations, is expected that the sample size of 952
science podcasts is sufficient to provide a representative sample of
science podcasts”
Further, the following has been added:
“Therefore, this study provides a lower-bound on the number of
science podcast series available during the sampling period.”
R2 C8: The choice to add 18 additional podcasts, but not use the full science
and medicine directory seems particularly worthy of justification. I note that
the list of podcast includes many medical related-sounded podcasts.
Podcasts in the pharmacology and medicine category were not excluded if they
were deemed to focus primarily on the academic and research aspects of
medicine and pharmacology, i.e. were not marketed towards providing medical
advice. The inclusion/exclusion criteria have been updated for clarity and now
include the following:
<U+F0B7> “Under a secondary definition: podcast series covering the academic and
research aspects of computer science, engineering, pharmacology and
medicine were included. These podcast series account for 3% of the
podcasts included in the study.”
R2 C9: Curious to know how many of these podcasts choose multiple
categories. It may also be important to describe how a podcast gets
categorized.
I believe this has been answered in comment 4? Details of the sources of
information are available in the ‘Information Sources’ sub-section of the
‘Material and Methods’ section.
Some more minor comments ...
R2 C10. It would be nice to give some benchmark for why you think a subject is
"under-represented"? Perhaps you could use data related to PhDs awarded in
the US/UK for specific topics or funding for specific topics as a guide.
Reviewer 1 also suggested that I justify why I think chemistry is under-
represented. Whilst I do not have a source for hard-numbers of, e.g. number of
PhDs awarded, I have expanded on my rationale in the discussion and I have
provided several highly relevant references. Please see my response to Reviewer
1, comment 14.
- R2 C11. You suggest that 952 podcasts is "representative" but this is a misuse
of a the term in the absence of a known population. In one sense, you start with
a population and do a census (not including those you exclude) and this means
that any error you have comes from the fact that you only looked at the
"natural sciences" and the fact that some podcasts/vodcasts aren't listed on
itunes. In the absence of such knowledge, there's no statistical reason to use
the term "representative" in anything but a qualitative sense, and even then it's
hard to know. I'd suggest just saying that it's a population of one category
(assuming the excludes are justifiable).
This has been answered: see response to reviewer 2 comment 7.
R2 C12. The conclusions may re-hash already-reported data more than might
be desirable, though it's hard to know what else to say given that the discussion
seems to cover what needs covering. I'd also be a little more careful with how I
word guesses. For example, rather than saying that "science podcasts are being
[used] as a decentralised and independent avenue of science communication,"
it'd make more sense to me to simply present that as one possibility as we have
no data on this point. And the term independent doesn't really make sense
given the affiliation data suggests at least half of respondents have some
explicit affiliation.
I agree with the reviewer’s comment. The entire ‘Conclusions’ section has been
rewritten accordingly.
R2 C13
-Do people really care about the fit lines? How does this analysis help us make
sense of the topic?
The fit lines and related results are rather important: without them, it would not
be clear that there are populations of short-duration and long-duration podcast
series. Reviewer 1 suggested adding some statistical tests, which have been
conducted and included (see Reviewer 1 Comment 15). That being said, figure
1D has been modified and the fit line removed to just clearly show the lifespans
of active and inactive podcast populations (see Reviewer 1 comment 20).
R2 C14
The final comparison in the discussion (p.11) about country only makes sense if
you don't take language into account, but language isn't mentioned.
After reviewer’s suggestions I think that this section was superfluous and not
substantiated. Therefore, it has been deleted from the revised manuscript.
Reviewer 3
Comments to the Author(s)
R3 C1: I find this article difficult to review in that it seeks to make a contribution
to research on public science communication but makes use neither of existing
literature in this field nor the methodological techniques and norms of social
research. If this were in a more disciplinary journal - such as Science
Communication, Public Understanding of Science, or JCOM - I would advise
rejection.
I am very appreciative of the reviewer’s helpful comments and suggestions that
have helped me improve the manuscript. See responses below.
R3 C2: The findings, based on a somewhat superficial analysis of science
podcasts, are not particularly surprising, and there is little effort to discuss their
implications. As a result it is unclear why it is interesting or important to publish
a finding that, say, 77% of podcasts are targeted at public audiences.
The study aims to be the first large-scale quantitative analysis of science
podcasts. Therefore, by design, the majority of new results will be very broad.
have endeavoured to clarify the aims of the study by changing the wording of the
final paragraph in the introduction, which now reads:
“Despite the rise of podcasts as a popular medium for science
communication, there have been no studies of the large-scale patterns in
the production of science podcasts; this represents a large and
fundamental gap in our knowledge of science communication. This
study aimed to provide large-scale quantitative insight into the overall
global production and dissemination of science podcasts. This has been
achieved by analysing online textual and visual presence of 952 English
language science podcasts for key production variables, including:
audio/visual format, topic, target audiences, hosts, number of episodes
released, lifespan of podcasts, supplementary income, and, use of
supplementary show notes. All data associated with this study is
available as a supplementary dataset in the form of a Microsoft Excel
spreadsheet.”
R3 C3: It is true that there thus far is relatively little literature on science
podcasts specifically, but there is some that is not cited (e.g. Birch & Weitkamp
2010; Dantas-Querroz et al 2018; and associated references)
I am very grateful to the reviewer for recommending these very helpful studies.
I have read these studies with enthusiasm and used them as a “jumping off point”
for a deeper dive into the background literature. As a result of the reviewer’s
recommendations, over 20 new references have been included and discussed
where appropriate – mainly in the introduction and discussion. This has greatly
improved the manuscript. It is impractical to list every incidence of the new
citations here, but please note that the new references are:
<U+F0B7> Dantas-Queroz and Quieroz, (2018). “Science communication
podcasting in Brazil: The potential and challenges depicted by two
podcasts”. Anais da Academia Brasileira de Ciencias
<U+F0B7> Birch and Weitkamp, (2010). “Podologues: Conversations created by
science podcasts”. New Media and Society.
<U+F0B7> Ranger and Bultitude (2014). “The kind of mildly curious sort of science
interested person like me’: Science bloggers’ practices relating to
audience recruitment” Public Understanding of Science.
<U+F0B7> Markman (2011). “Doing radio, making friends, and having fun:
Exploring the motivations of independent audio podcasters”. New
Media and Society.
<U+F0B7> Redfern, Illingworth, and Verran. (2016). “What does the UK public
want from academic science communication?” F1000
<U+F0B7> Markman and Sawyer (2014). “Why Pod? Further Explorations of the
Motivations for Independent Podcasting”. Journal of Radio and Audio
Media
<U+F0B7> Jham et al., (2008). “Joining the podcast revolution.” Journal of Dental
Education
<U+F0B7> Picardi and Regina (2008): “Science via Podcast”. Journal of Science
Communication.
<U+F0B7> Merzagora (2004). “Science On Air: the Role of Radio in Science
Communication” Journal of Science Communication.
<U+F0B7> Carpenter (2010). “A study of content diversity in online citizen
journalism and online newspaper articles” New Media and Society
<U+F0B7> Nisbett and Scheufelee (2009). “What's next for science
communication? promising directions and lingering distractions”
American Journal of Botany
<U+F0B7> Poliaikov and Webb (2007). “Scientists ’ Intentions to Participate in
Public Engagement of Science Activities ?” Science Communication.
<U+F0B7> Haran and Poliakove (2011). “The periodic table of videos”. Science
<U+F0B7> Welborn and Grant (2015. “Science communication on YouTube:
Factors that affect channel and video popularity”. Public
Understanding of Science.
<U+F0B7> Wynne (2006) “Public engagement as a means of restoring public trust
in science - Hitting the notes, but missing the music?” Community
Genetics
<U+F0B7> Prokop and Illingworth (2016). “Aiming for long-term, objective-driven
science communication in the UK”. F1000 research.
<U+F0B7> Miller et al., (2018). “The Development of Children's Gender-Science
Stereotypes: A Meta-analysis of 5 Decades of U.S. Draw-A-Scientist
Studies”. Child Development.
<U+F0B7> Maas (2018) “Opening the Audiobook” Comparative Literature
The following references have been added to specifically discuss the under-
representation of chemistry in podcasts (in accordance with Reviewer 1
Comment 14).
<U+F0B7> “Chemistry's year”. (2011). Editorial. Nature Chemistry
10.1038/nchem.933
<U+F0B7> Hartins and Fahy (2011) “Communicating chemistry for public
engagement.” Nature Chemistry,
<U+F0B7> National Academies of Sciences Engineering and Medicine. 2016
Evidence-Based Research on Learning and Communication. In Effective
Chemistry Communication in Informal Environments., Washington (DC):
National Academies Press (US).
<U+F0B7> National Academies of Sciences Engineering and Medicine. 2016
Communicating Chemistry: A Design Framework and Research
Agenda. In Effective Chemistry Communication in Informal
Environments., Washington (DC): National Academies Press (US).
R3 C4:, and there is far more on the form and nature of science communication
in social and digital media generally or on science radio, the obvious precursor
to science podcasting, that a quick literature search would have identified. I
suggest that the author engages with this literature in order to ground their
discussion and, specifically, to show why this work meets specific research
needs, how it contributes to current debates around the use of new media for
science communication
The introduction and discussion sections have been extensively rewritten to
include discussion of these points with appropriate references. Please see both
the updated manuscript and my response to comment 3.
R3 C5:, and what its implications are (they write, in closing, that "these insights
will help inform public science communication policy" but it is not at all obvious
to me how or why this would be the case).
I agree with the reviewer that this line was not substantiated. It has been
deleted from both the conclusions and the summary (i.e. the abstract).
R3 C6:, This may entail carrying out further empirical research in order to take
the article beyond quantitative description of this emerging phenomenon to
offer insight into the patterns identified. For instance, carrying out interviews
with producers of podcasts (as has been done for other new media science
communication formats; e.g. Ranger & Bultitude 2014; Riesch & Mendel 2013)
would offer a richer understanding of, for instance, topic choice or who bears
the cost of podcasting, and would thus avoid some of the speculation currently
found in the Discussion.
I agree that such qualitative studies would be interesting and very useful.
However, these are beyond the scope of this current study, which aims to be as
quantitative as possible. That being said, I have included a new sub-section in the
discussion, titled ‘Open Questions and Future Directions’. In this section I discuss
outstanding questions relating to science podcasts, and how methods from other
studies of new media could help answer these questions. I hope the reviewer will
find this a valuable addition to the manuscript.
R3 C7: It is worth noting that, in this field as a whole, it would not be usual to
publish description of a phenomenon without more in-depth analysis or
theoretical testing.
As per earlier responses, this study was designed to be as quantitative as possible
to give a snapshot of the overall view of the genre and medium of science
podcasting in 2018. As per earlier responses, I have included new references and
discussion to the literature throughout. I hope the reviewer finds this approach
sufficient for the limited scope of this study.
R3 C8: There are also some methodological problems due to a lack of
transparency as to how the podcasts were coded. How exactly was the target
audience identified? Or the focal topic (what about interdisciplinary research)?
How were hosts categorised, and based on what empirical material (again,
what about individuals or groups who fall into more than one category)? It is
also not clear how coding was carried out.
Reviewer 2 brought up a very similar point, which was an omission in the
explanation of the methodology used to categorise hosts and other important
categories. To avoid duplication, please refer to my response to reviewer 2,
comment 4.
R3 C9: Were web scraping tools used, or was all coding done by hand? By how
many people?
This comment is very similar to a comment by reviewer 2. To avoid duplication,
please see my response to Reviewer 2 comments 1-3. In addition, the following
has been added to the sub-section ‘Inflormation Sources’ in the ‘Materials and
Methods’:
“All data was manually coded by a single individual (the
author).”
R3 C10: More generally, it would be useful to have the methods grounded
within specific traditions or techniques; the work of Martin Bauer or Dietram
Scheufele and colleagues, who work in quantitative science communication
research, might offer some inspiration here.
I am grateful to the reviewer for their helpful suggestions to improve my
general methodology, but it is unclear to me what specific points I can address
from this comment.
R3 C11: A minor point: 'demographics' tends to refer to human populations
(from 'demos', the people). If it is to be applied to non-human cohorts then this
needs to be explained and justified.
The use of the word “demographics” has been removed and reworded where
appropriate throughout the manuscript.
Appendix B
Response to Reviewers’ comments
I would once again like to thank both Reviewers for again reviewing the revised form
of this paper and providing their feedback on the extensive changes that I made in
response to their comments. I am glad that Reviewer 1 suggests acceptance.
However, I must politely, yet firmly, disagree with most of the comments from Reviewer
2 because their comments are not actionable.
My reasoning is as follows:
1. Both Reviewer 1 and Reviewer 2 raised concerns about methodological
limitations. These limitations have been acknowledged, addressed, and
discussed throughout the manuscript where appropriate to ensure readers will
be aware of the limitations. This approach has satisfied Reviewer 1. Indeed,
an entire sub-section was added to the discussion to discuss limitations of the
study. Despite these major changes, Reviewer 2 implies that they want the
methodology of the whole study to be changed to multi-coder (i.e. multi-person
analysis). However, I am an early career researcher working independently,
therefore, I do not have access to the financial resources required to employ
another researcher within the time constrains required for revision (i.e. <1
month). For these reasons, I cannot feasibly comply with the implied request
from Reviewer 2.
2. Reviewer 2 asks that data collection and analysis is extended to cover pseudo-
scientific podcasts. Quote: “I really didn't understand the discussion of why we
would want to exclude podcasts that have pseudo-scientific content from the
analyses”. Firstly, it should be self-evident that pseudo-scientific content is not
fit for publishing in Royal Society Open Science, especially because there is a
considerable risk of lending false legitimacy to pseudo-scientific concepts.
Secondly, incorporating such pseudo-scientific content would require months
of data collection and analysis, and is also well beyond the clearly stated aims
of the study. Therefore, I cannot feasibly comply with this suggestion from
Reviewer 2.
3. Reviewer 2 also diffusely suggests that extensive open-ended analysis should
be conducted to analyse active vs. inactive podcasts. Whilst this is an
interesting vein to explore, it is well beyond the stated aim of this study.
Further, this study was not designed to answer such open-ended and complex
questions, which likely require a qualitative approach, which this study does
not take. Therefore, I cannot feasibly comply with this suggestion from Review
2.
4. Reviewer 2 asks for details of how podcast hosts were categorised. This has
been clearly explained in the ‘Categorical Definitions’ section as per the
requests of the other reviewers.
5. Reviewer 2 refers vaguely to data on p10, but does not state what they are
referring to, so I cannot act upon their comment.
6. Reviewer 2 states that “this conclusion feels more like a summary than a
conclusion” but does not suggest any actual means of improving the
conclusion section for this paper or indicate what they might find acceptable.
Therefore, I cannot act on this comment.
I hope the Editor will find this rebuttal of Reviewer #2’s comment satisfactory when
considering this revised paper for publication.
Sincerely,
Dr Lewis MacKenzie
Society Open
