Low statistical power in biomedical science: a review of
three human research domains
Estelle Dumas-Mallet, Katherine S. Button, Thomas Boraud, Francois Gonon and Marcus
R. Munafò
Article citation details
R. Soc. open sci. 4: 160254.
http://dx.doi.org/10.1098/rsos.160254
Review timeline
Original submission: 12 April 2016 Note: Reports are unedited and appear as
1st revised submission: 1 July 2016 submitted by the referee. The review history
2nd revised submission: 14 October 2016 appears in chronological order.
3rd revised submission: 2 December 2016
Final acceptance: 4 January 2017
Review History
label_version_1
RSOS-160254.R0 (Original submission)
label_author_1
Review form: Reviewer 1 (Malcolm Macleod)
Is the manuscript scientifically sound in its present form?
Yes
Are the interpretations and conclusions justified by the results?
Yes
Is the language acceptable?
Yes
Is it clear how to access all supporting data?
At this stage it is not clear. I would suggest they include details of all publications in a dataset
reposited e.g. at FigShare
Do you have any ethical concerns with this paper?
No
© 2017 The Authors. Published by the Royal Society under the terms of the Creative Commons
Attribution License http://creativecommons.org/licenses/by/4.0/, which permits unrestricted use,
provided the original author and source are credited
2
Have you any concerns about statistical analyses in this paper?
No
Recommendation?
label_recommendation_1
Accept with minor revision (please list in comments)
Comments to the Author(s)
label_comment_1
Thank you for the opportunity to review this important work, which builds on previous work
from this group. I have some comments:
1. Was the research guided by a protocol? Is a date-stamped version of the protocol available in
the public domain, that the reader might see whether their analysis is what they set out to do?
2. I think they could clarify the domains covered - Neurology is a little more than Alzheimer's
disease, epilepsy, Parkinson's disease and multiple sclerosis - and it is likely that there have been
meta-analyses for other conditions eg Huntington's disease, stroke etc. Sp it may be better to set
the question up with the specifics that would be identified by the search.
3. The authors might reflect on whether taking the fixed effect estimate is appropriate,
particularly as it may induce this behaviour in the future! I think there is sufficient evidence of
the prevalence of publication bias to conclude that this fixed effect will be inflated in many cases,
and so the problem is even worse than reported here. An alternative strategy (I don't propose
they adopt it here, but they may mention it in passing) is to establish a minimally biologically
important effect, and establish whether the literature is powered to detect this. That would be
difficult to generalise - we might be prepared to accept a 2-fold elevated risk of classroom
disruptiveness for kids taking chemotherapy for cancer, but only 1.01 for an effect of a commonly
available carbonated soft drink additive. But establishing the relationship between biological
imporatnce and power would be useful.
4. You might mention that - as well as being a waste of time and money, and polluting the
literature - the positive predictive value of studies powered at the levels reported here will be
much worse than had they been powered at 80%.
5. I get that for many of these studies there is a clear relationship between the number of subjects
and the power; but when sample size is very small and the variable is continuous, there can be
huge variation in the observed variance, which can then in turn have effects on power. I think
this is a bit circular, but the assumption that there is a direct mapping between n and observed
variance (rather than population variance) is not given. So in sampling from a population, there is
variation in the observed mean from the population mean; and variance in the observed variance
from the population variance. By taking the fixed effects (ie ~ population) mean with the
observed (rather than eg the median) sd may be complicating things. I dont know enough about
statistics to say whether this is a problem, and if so whether it is a big enough problem to
confound the findings reported, but the authors may wish to respond.
I sign my reviews: Malcolm Macleod, Edinburgh
label_author_2
Review form: Reviewer 2
Is the manuscript scientifically sound in its present form?
Yes
Are the interpretations and conclusions justified by the results?
No
Is the language acceptable?
Yes
3
Is it clear how to access all supporting data?
Yes
Do you have any ethical concerns with this paper?
No
Have you any concerns about statistical analyses in this paper?
No
Recommendation?
label_recommendation_2
Accept with minor revision (please list in comments)
Comments to the Author(s)
label_comment_2
Dumas-Mallet and colleagues conduct an extensive review of meta-analyses to estimate the
average statistical power across three biomedical disciplines (psychiatry, neurology, and somatic
medicine). Their results are very interesting and worthy of publication. Their methodology is also
sound and well-reported in the manuscript and supplementary materials, therefore my
comments relate only to some issues with their interpretations of their results.
- It is unclear to me why the authors have chosen initially to frame the purpose of the study
around
the idea of researchers dividing their resources into as many individual studies as possible (first
sentence of abstract). From my understanding, this is not the only cause of low power, nor does
evidence of low power in a discipline prove that this is occurring. The authors again speculate
about
this in the discussion (page 11, lines 6-20), which is a more appropriate place for this suggestion.
However, it still seems overly speculative to suggest that researchers conduct lots of small studies
followed by large confirmatory ones as an explanation for the bimodal distribution observed,
since there are lots of things that could account for this distribution, and certainly no evidence
that the same labs contribute to both peaks of the distribution.
- It would be useful to have some more statistics in the paper. The authors state (page 8, line 24)
that "the majority" of studies fell into the 0-20% decile, but it would be useful to know the exact
number overall (across all 3 disciplines). Equally, when cognitive studies are discussed as having
"relatively high" power, genetic studies "very low" (line 42), medians should be stated.
- I am not sure that it is justifiable to exclude cognitive/behavioural measures relating to ADHD,
depression, and schizophrenia (total N = 56) in the overall power estimates, given that these are
typical neuroscience techniques in psychiatry (page 8, line 10). Surely these studies should be
included when calculating the overall median statistical powers, in order to make the claim that
psychiatry, neurology, and somatic medicine all have "very similar median statistical power"?
- The authors claim that a strength of their study derives from excluding treatment studies, which
Button and colleagues included (page 10, lines 16-25). They go on to claim that this means studies
in
the present analysis are more representative of the wider neuroscience literature. This is a strange
claim to make as a large portion of their studies are not neuroscience studies by any definition
(i.e.,
glaucoma studies, breast cancer studies, etc.), and numerous non-psychiatry, non-neurology
neuroscience studies would be excluded entirely by their search terms. (Whether excluding
treatment studies is an advantage or a disadvantage of the paper could also be debated).
4
label_author_3
Review form: Reviewer 3 (Peter Bacchetti)
Is the manuscript scientifically sound in its present form?
No
Are the interpretations and conclusions justified by the results?
No
Is the language acceptable?
Yes
Is it clear how to access all supporting data?
Yes.
Do you have any ethical concerns with this paper?
No
Have you any concerns about statistical analyses in this paper?
Yes
Recommendation?
label_recommendation_3
Reject
Comments to the Author(s)
label_comment_3
Comments from:
Peter Bacchetti
Professor of Biostatistics
University of California, San Francisco, CA, USA
I have published arguments against conventional thinking about power, so I am obviously a
highly skeptical reviewer for this paper. While it appears to represent a lot of effort, with careful
implementation in many regards, I believe that there are a number of problems that preclude this
from making any meaningful positive contribution to understanding and improving biomedical
research. If this is to be published anyway, then I would hope that the problems noted below
would be addressed, or at least acknowledged.
1. A key assumption of this study is that power should always be evaluated at the true
effect size. This assumption is not justified in the paper and does not seem sensible. It would
imply that all studies of an association that turns out to be small or null are illegitimate due to
low power. Is there is no value to providing evidence that an association is small? Table S2 even
shows a number of cases where “power” was calculated with the alternative equal to the null,
which seems to me to clearly show that this approach cannot be sensible. This is very closely
related to the practice of post-hoc power calculation that is described in reference 6 as “deplored
in the statistical literature”: a study with an estimated effect at or near the null value will calculate
low post-hoc power and conclude that there is only weak evidence against an effect, regardless of
how narrow the CI around the nearly null estimate is. Here, a meta-analytic estimate at or near
the null tars all the studies that contributed to it, but this is essentially the same problem.
Excellent evidence against any substantial association can be interpreted as instead being a
collection of deeply flawed studies (because of their appallingly low power). (There are, on the
other hand, many people who consider it obvious that power should always be calculated using
the minimum important effect size. I believe that this also has severe problems.)
2. The issue in point 1 leads to an alternative explanation for the large number of studies in
the first two bins and the bimodality shown in Figure 1. Perhaps every study had an appropriate
sample size and Figure 1simply reflects the distribution of what turned out to be true about the
5
associations being studied: over half were very close to no association, a few were of intermediate
magnitudes, and some were large. As conducted, this study provides no way to distinguish this
scenario from most studies’ having “inappropriately small” sample sizes.
3. Previous studies claiming to document widespread “underpowering” have often
focused on randomized clinical trials, because those are where NHST is considered most
applicable. Why were treatment studies excluded, and why is a rationale for this absent from the
Methods? This exclusion seem likely to preferentially select studies where investigators had less
control over sample size and studies that are more exploratory and not intended to provide
conclusive evidence. Both of these make power less relevant, so this exclusion seems
counterproductive.
4. A major drawback of this and similar studies is the implicit acceptance of the NHST
paradigm and related poor approaches for interpreting research studies. A focus on power
reinforces the idea that the most important thing about a study is whether or not it finds P<0.05;
power is, after all, just the probability of this event. Although the Introduction attempts to
connect power to more general considerations, this seems quite strained to me: the influence of
sample size on precision is better shown and interpreted by use of confidence intervals and
standard errors than by the convoluted considerations mentioned and referenced, and the focus
on “a statistically significant finding” gets right back to reinforcing the excessive focus on
whether P<0.05.
5. The Abstract and Discussion also reinforce the requirement for a minimum of 80%
power. While I commend the authors for using the phrase “considered conventional” rather than
“adequate” or “acceptable”, I don’t think this is sufficient to avoid reinforcing a supposed
standard that has never been given any cogent scientific justification. (Colleagues and I have
argued in considerable detail that no such single standard can be justified.)
6. I agree that consideration of incentives and resource allocation are important issues.
Studies with low power, however, particularly as defined in this paper, may not have required
low resources. Indeed, high per person cost is a sensible reason for a smaller sample size and low
cost is a good reason for a larger sample size. The speculative explanation for the bimodal
distribution seems quite implausible to me. My experience (admittedly in a different country) is
that researchers do not themselves allocate the financial resources that they use, and they have
ample
incentives to make sample size as large as is reasonably feasible. They are constrained by cost
and feasibility. (The power-based paradigm for determining sample size is unrealistic in its
neglect of costs.)
label_end_comment
Decision letter (RSOS-160254)
26th May 2016
Dear Professor Munafo,
The editors assigned to your paper ("Low Statistical Power in Biomedical Science: A review of
three human research domains") has now received comments from reviewers. We would like
you to revise your paper in accordance with the referee and Subject Editor suggestions which can
be found below (not including confidential reports to the Editor). Please note this decision does
not guarantee eventual acceptance.
6
Please submit a copy of your revised paper within three weeks (i.e. by the 17th June 2016). If we
do not hear from you within this time then it will be assumed that the paper has been withdrawn.
In exceptional circumstances, extensions may be possible if agreed with the Editorial Office in
advance.We do not allow multiple rounds of revision so we urge you to make every effort to
fully address all of the comments at this stage. If deemed necessary by the Editors, your
manuscript will be sent back to one or more of the original reviewers for assessment. If the
original reviewers are not available we may invite new reviewers.
To revise your manuscript, log into http://mc.manuscriptcentral.com/rsos and enter your
Author Centre, where you will find your manuscript title listed under "Manuscripts with
Decisions." Under "Actions," click on "Create a Revision." Your manuscript number has been
appended to denote a revision. Revise your manuscript and upload a new version through your
Author Centre.
When submitting your revised manuscript, you must respond to the comments made by the
referees and upload a file "Response to Referees" in "Section 6 - File Upload". Please use this to
document how you have responded to the comments, and the adjustments you have made. In
order to expedite the processing of the revised manuscript, please be as specific as possible in
your response.
In addition to addressing all of the reviewers' and editor's comments please also ensure that your
revised manuscript contains the following sections as appropriate before the reference list:
• Ethics statement (if applicable)
If your study uses humans or animals please include details of the ethical approval received,
including the name of the committee that granted approval. For human studies please also detail
whether informed consent was obtained. For field studies on animals please include details of all
permissions, licences and/or approvals granted to carry out the fieldwork.
• Data accessibility
It is a condition of publication that all supporting data are made available either as
supplementary information or preferably in a suitable permanent repository. The data
accessibility section should state where the article's supporting data can be accessed. This section
should also include details, where possible of where to access other relevant research materials
such as statistical tools, protocols, software etc can be accessed. If the data has been deposited in
an external repository this section should list the database, accession number and link to the DOI
for all data from the article that has been made publicly available. Data sets that have been
deposited in an external repository and have a DOI should also be appropriately cited in the
manuscript and included in the reference list.
If you wish to submit your supporting data or code to Dryad (http://datadryad.org/), or modify
your current submission to dryad, please use the following link:
http://datadryad.org/submit?journalID=RSOS&manu=RSOS-160254
• Competing interests
Please declare any financial or non-financial competing interests, or state that you have no
competing interests.
• Authors’ contributions
All submissions, other than those with a single author, must include an Authors’ Contributions
section which individually lists the specific contribution of each author. The list of Authors
should meet all of the following criteria; 1) substantial contributions to conception and design, or
acquisition of data, or analysis and interpretation of data; 2) drafting the article or revising it
critically for important intellectual content; and 3) final approval of the version to be published.
7
All contributors who do not meet all of these criteria should be included in the
acknowledgements.
We suggest the following format:
AB carried out the molecular lab work, participated in data analysis, carried out sequence
alignments, participated in the design of the study and drafted the manuscript; CD carried out
the statistical analyses; EF collected field data; GH conceived of the study, designed the study,
coordinated the study and helped draft the manuscript. All authors gave final approval for
publication.
• Acknowledgements
Please acknowledge anyone who contributed to the study but did not meet the authorship
criteria.
• Funding statement
Please list the source of funding for each author.
Once again, thank you for submitting your manuscript to Royal Society Open Science and I look
forward to receiving your revision. If you have any questions at all, please do not hesitate to get
in touch.
Yours sincerely,
Matthew Allinson,
Editorial Coordinator, Royal Society Open Science
on behalf of Essi Viding
Subject Editor, Royal Society Open Science
openscience@royalsociety.org
Associate Editor's comments (Dr Jonathan Roiser):
Associate Editor: 1
Comments to the Author:
Editor's note: Two of the reviewers were generally positive in their comments, but the third was
quite skeptical. Nonetheless, we would be willing to consider a revised version in which the
comments of all of the reviewers are addressed thoroughly.
Comments to Author:
Reviewers' Comments to Author:
Reviewer: 1
Comments to the Author(s)
Thank you for the opportunity to review this important work, which builds on previous work
from this group. I have some comments:
1. Was the research guided by a protocol? Is a date-stamped version of the protocol available in
the public domain, that the reader might see whether their analysis is what they set out to do?
2. I think they could clarify the domains covered - Neurology is a little more than Alzheimer's
disease, epilepsy, Parkinson's disease and multiple sclerosis - and it is likely that there have been
meta-analyses for other conditions eg Huntington's disease, stroke etc. Sp it may be better to set
the question up with the specifics that would be identified by the search.
3. The authors might reflect on whether taking the fixed effect estimate is appropriate,
particularly as it may induce this behaviour in the future! I think there is sufficient evidence of
the prevalence of publication bias to conclude that this fixed effect will be inflated in many cases,
and so the problem is even worse than reported here. An alternative strategy (I don't propose
they adopt it here, but they may mention it in passing) is to establish a minimally biologically
important effect, and establish whether the literature is powered to detect this. That would be
8
difficult to generalise - we might be prepared to accept a 2-fold elevated risk of classroom
disruptiveness for kids taking chemotherapy for cancer, but only 1.01 for an effect of a commonly
available carbonated soft drink additive. But establishing the relationship between biological
imporatnce and power would be useful.
4. You might mention that - as well as being a waste of time and money, and polluting the
literature - the positive predictive value of studies powered at the levels reported here will be
much worse than had they been powered at 80%.
5. I get that for many of these studies there is a clear relationship between the number of subjects
and the power; but when sample size is very small and the variable is continuous, there can be
huge variation in the observed variance, which can then in turn have effects on power. I think
this is a bit circular, but the assumption that there is a direct mapping between n and observed
variance (rather than population variance) is not given. So in sampling from a population, there is
variation in the observed mean from the population mean; and variance in the observed variance
from the population variance. By taking the fixed effects (ie ~ population) mean with the
observed (rather than eg the median) sd may be complicating things. I dont know enough about
statistics to say whether this is a problem, and if so whether it is a big enough problem to
confound the findings reported, but the authors may wish to respond.
I sign my reviews: Malcolm Macleod, Edinburgh
Reviewer: 2
Comments to the Author(s)
Dumas-Mallet and colleagues conduct an extensive review of meta-analyses to estimate the
average statistical power across three biomedical disciplines (psychiatry, neurology, and somatic
medicine). Their results are very interesting and worthy of publication. Their methodology is also
sound and well-reported in the manuscript and supplementary materials, therefore my
comments relate only to some issues with their interpretations of their results.
- It is unclear to me why the authors have chosen initially to frame the purpose of the study
around
the idea of researchers dividing their resources into as many individual studies as possible (first
sentence of abstract). From my understanding, this is not the only cause of low power, nor does
evidence of low power in a discipline prove that this is occurring. The authors again speculate
about
this in the discussion (page 11, lines 6-20), which is a more appropriate place for this suggestion.
However, it still seems overly speculative to suggest that researchers conduct lots of small studies
followed by large confirmatory ones as an explanation for the bimodal distribution observed,
since there are lots of things that could account for this distribution, and certainly no evidence
that the same labs contribute to both peaks of the distribution.
- It would be useful to have some more statistics in the paper. The authors state (page 8, line 24)
that "the majority" of studies fell into the 0-20% decile, but it would be useful to know the exact
number overall (across all 3 disciplines). Equally, when cognitive studies are discussed as having
"relatively high" power, genetic studies "very low" (line 42), medians should be stated.
- I am not sure that it is justifiable to exclude cognitive/behavioural measures relating to ADHD,
depression, and schizophrenia (total N = 56) in the overall power estimates, given that these are
typical neuroscience techniques in psychiatry (page 8, line 10). Surely these studies should be
included when calculating the overall median statistical powers, in order to make the claim that
psychiatry, neurology, and somatic medicine all have "very similar median statistical power"?
- The authors claim that a strength of their study derives from excluding treatment studies, which
Button and colleagues included (page 10, lines 16-25). They go on to claim that this means studies
in
the present analysis are more representative of the wider neuroscience literature. This is a strange
9
claim to make as a large portion of their studies are not neuroscience studies by any definition
(i.e.,
glaucoma studies, breast cancer studies, etc.), and numerous non-psychiatry, non-neurology
neuroscience studies would be excluded entirely by their search terms. (Whether excluding
treatment studies is an advantage or a disadvantage of the paper could also be debated).
Reviewer: 3
Comments to the Author(s)
Comments from:
Peter Bacchetti
Professor of Biostatistics
University of California, San Francisco, CA, USA
I have published arguments against conventional thinking about power, so I am obviously a
highly skeptical reviewer for this paper. While it appears to represent a lot of effort, with careful
implementation in many regards, I believe that there are a number of problems that preclude this
from making any meaningful positive contribution to understanding and improving biomedical
research. If this is to be published anyway, then I would hope that the problems noted below
would be addressed, or at least acknowledged.
1. A key assumption of this study is that power should always be evaluated at the true
effect size. This assumption is not justified in the paper and does not seem sensible. It would
imply that all studies of an association that turns out to be small or null are illegitimate due to
low power. Is there is no value to providing evidence that an association is small? Table S2 even
shows a number of cases where “power” was calculated with the alternative equal to the null,
which seems to me to clearly show that this approach cannot be sensible. This is very closely
related to the practice of post-hoc power calculation that is described in reference 6 as “deplored
in the statistical literature”: a study with an estimated effect at or near the null value will calculate
low post-hoc power and conclude that there is only weak evidence against an effect, regardless of
how narrow the CI around the nearly null estimate is. Here, a meta-analytic estimate at or near
the null tars all the studies that contributed to it, but this is essentially the same problem.
Excellent evidence against any substantial association can be interpreted as instead being a
collection of deeply flawed studies (because of their appallingly low power). (There are, on the
other hand, many people who consider it obvious that power should always be calculated using
the minimum important effect size. I believe that this also has severe problems.)
2. The issue in point 1 leads to an alternative explanation for the large number of studies in
the first two bins and the bimodality shown in Figure 1. Perhaps every study had an appropriate
sample size and Figure 1simply reflects the distribution of what turned out to be true about the
associations being studied: over half were very close to no association, a few were of intermediate
magnitudes, and some were large. As conducted, this study provides no way to distinguish this
scenario from most studies’ having “inappropriately small” sample sizes.
3. Previous studies claiming to document widespread “underpowering” have often
focused on randomized clinical trials, because those are where NHST is considered most
applicable. Why were treatment studies excluded, and why is a rationale for this absent from the
Methods? This exclusion seem likely to preferentially select studies where investigators had less
control over sample size and studies that are more exploratory and not intended to provide
conclusive evidence. Both of these make power less relevant, so this exclusion seems
counterproductive.
4. A major drawback of this and similar studies is the implicit acceptance of the NHST
paradigm and related poor approaches for interpreting research studies. A focus on power
reinforces the idea that the most important thing about a study is whether or not it finds P<0.05;
power is, after all, just the probability of this event. Although the Introduction attempts to
10
connect power to more general considerations, this seems quite strained to me: the influence of
sample size on precision is better shown and interpreted by use of confidence intervals and
standard errors than by the convoluted considerations mentioned and referenced, and the focus
on “a statistically significant finding” gets right back to reinforcing the excessive focus on
whether P<0.05.
5. The Abstract and Discussion also reinforce the requirement for a minimum of 80%
power. While I commend the authors for using the phrase “considered conventional” rather than
“adequate” or “acceptable”, I don’t think this is sufficient to avoid reinforcing a supposed
standard that has never been given any cogent scientific justification. (Colleagues and I have
argued in considerable detail that no such single standard can be justified.)
6. I agree that consideration of incentives and resource allocation are important issues.
Studies with low power, however, particularly as defined in this paper, may not have required
low resources. Indeed, high per person cost is a sensible reason for a smaller sample size and low
cost is a good reason for a larger sample size. The speculative explanation for the bimodal
distribution seems quite implausible to me. My experience (admittedly in a different country) is
that researchers do not themselves allocate the financial resources that they use, and they have
ample
incentives to make sample size as large as is reasonably feasible. They are constrained by cost
and feasibility. (The power-based paradigm for determining sample size is unrealistic in its
neglect of costs.)
Author's Response to Decision Letter for (RSOS-160254)
See Appendix A.
label_version_2
RSOS-160254.R1 (Revision)
label_author_4
Review form: Reviewer 1 (Malcolm Macleod)
Is the manuscript scientifically sound in its present form?
Yes
Are the interpretations and conclusions justified by the results?
Yes
Is the language acceptable?
Yes
Is it clear how to access all supporting data?
Yes
Do you have any ethical concerns with this paper?
No
Have you any concerns about statistical analyses in this paper?
No
11
Recommendation?
label_recommendation_4
Accept as is
Comments to the Author(s)
label_comment_4
Thank you.
Malcolm Macleod
label_author_5
Review form: Reviewer 2
Is the manuscript scientifically sound in its present form?
Yes
Are the interpretations and conclusions justified by the results?
Yes
Is the language acceptable?
Yes
Is it clear how to access all supporting data?
Yes
Do you have any ethical concerns with this paper?
No
Have you any concerns about statistical analyses in this paper?
No
Recommendation?
label_recommendation_5
Accept as is
Comments to the Author(s)
label_comment_5
The authors have fully addressed the issues arising with their first version of the paper. The
paper is now in good standing and will make an important contribution to the field of statistical
power in the biomedical sciences. I have a couple minor recommendations for the authors:
1. I query their distinction between "severe" and "less severe" psychiatric disorders. As the
authors are probably aware, the category "severe mental illness" includes two disorders:
schizophrenia, and bipolar disorder. Their categorization of schizophrenia and autism as "severe"
and depression and ADHD as "less severe" seems a bit misleading. Clearly some cases of autism
are more severe than some cases of depression, but the opposite is also true: many patients with
MDD would certainly be characterized as more severe than the large population of people with
autism spectrum disorders for whom the disorder does not impact on their life in any major way.
There is no evidence that the meta-analyses included only covered severe cases of autism;
therefore, I think this distinction should either be clarified/justified (e.g. by citing somewhere
where it has been made before), or removed.
2. Typo on pg 9, line 21 "a more comparison"
3. pg 10, line 7, it seems odd to describe the power of cognitive/behavioural studies (93%) as
"relatively high" in comparison to 8%, 27%, and 39%. Surely this is just high power?
4. Typo on pg 13, line 28 "would also results"
12
5. pg 14, Might be better to describe their estimates as "underestimates" rather than low (ln 11)
and "overestimates" rather than high (line 17).
6. Typo pg 15, missing comma in line 9.
label_author_6
Review form: Reviewer 3 (Peter Bacchetti)
Is the manuscript scientifically sound in its present form?
No
Are the interpretations and conclusions justified by the results?
No
Is the language acceptable?
Yes
Is it clear how to access all supporting data?
Yes.
Do you have any ethical concerns with this paper?
No
Have you any concerns about statistical analyses in this paper?
No
Recommendation?
label_recommendation_6
Major revision is needed (please make suggestions in comments)
Comments to the Author(s)
label_comment_6
I've marked my further comments in the attached. (Appendix B)
label_author_7
Review form: Reviewer 4 (Philip Quinlan)
Is the manuscript scientifically sound in its present form?
No
Are the interpretations and conclusions justified by the results?
No
Is the language acceptable?
Yes
Is it clear how to access all supporting data?
All fine
Do you have any ethical concerns with this paper?
No
Have you any concerns about statistical analyses in this paper?
Yes
13
Recommendation?
label_recommendation_7
Major revision is needed (please make suggestions in comments)
Comments to the Author(s)
label_comment_7
See attached (Appendix C).
label_author_8
Review form: Reviewer 5 (Dietrich Schwarzkopf)
Is the manuscript scientifically sound in its present form?
Yes
Are the interpretations and conclusions justified by the results?
Yes
Is the language acceptable?
Yes
Is it clear how to access all supporting data?
Yes
Do you have any ethical concerns with this paper?
No
Have you any concerns about statistical analyses in this paper?
No
Recommendation?
label_recommendation_8
Accept with minor revision (please list in comments)
Comments to the Author(s)
label_comment_8
In recent years the problems with underpowered experiments for the replicability of scientific
findings have received substantial attention. This manuscript describes an investigation of
statistical power in the biological sciences with a particular focus on various disorders and
clinical conditions. The authors find that power is generally very low although this also varies
considerably between subfields. By and large, this is an interesting study but I feel the following
issues should be discussed in more detail (I’m aware that the authors already touch upon some of
these issues but they could be emphasized more):
1. I am unsure that in the context of this study the meta-analytical effect sizes used by the
authors are necessarily a good estimate of the true population effect. Many meta-analyses
presumably test effects that are simply false (i.e. for which the null hypothesis is true), even when
the meta-analysis suggests a small effect. In fact, that seems to be the conclusion of many recent
meta-analyses in the social sciences. Quantifying statistical power on a true effect of zero is
meaningless.
On the other hand, some meta-analyses could be skewed by low quality studies dragging down
the effect size estimate. This may not always be readily apparent. Consequently, the whole
estimate of statistical power based on this meta-analytical effect size would be deflated. This
problem may be further exacerbated because the authors focus on fixed-effect analyses only. I
understand the authors’ reasoning for doing so but it is also a great limitation. The assumption of
homogeneity in effect sizes perhaps only holds when the individual studies are fairly similar.
14
2. Following from the previous point, the main problem is essentially that experimenters
cannot predict the future (unless you believe the authors of one recent meta-analysis). Therefore,
they cannot plan the statistical power based on studies that have not yet been done. The
suggestion to come up with meaningful minimally relevant effect sizes when planning a study is
certainly sensible – but it is not always practically feasible. Many “minimal” effects will require
very large samples to detect and resources are limited, even in multi-lab collaborations.
Instead I think it would be wiser to encourage clearer predictions of effect sizes a priori and
planning statistical power at the outset – for instance by preregistration of the experimental
design. Nobody should be prevented from publishing a small effect size with an underpowered
design (and such findings can still make important contributions to scientific knowledge) but it
should be more obvious when this is the case.
3. Increasing sample sizes is not the only way to boost statistical power. Relative effect sizes
are dependent on the signal to noise ratio of the experiment. Thus another way to enhance power
is to improve the sensitivity of the experimental procedures, for example by reducing
measurement error. This is not always feasible but in many situations it is, for instance by
collecting more data on each participant rather than collecting more participants. The
consequence is a greater relative effect size and thus greater statistical power. The use of designs
with variable sensitivity can also contribute to the heterogeneity of effect sizes. This issue seems
to be frequently overlooked. In my view, in situations where moving to ever larger samples is
impractical such methodological improvements should be of much higher priority.
4. I was surprised that the authors do not mention Bayesian hypothesis testing as an
alternative to frequentist inference. The manuscript highlights the importance of making
predicting effect sizes a priori and being able to tell compelling from inconclusive results.
Bayesian statistics provide both. Moreover, unlike picking a single effect size and calculating
power a priori, the choice of the Bayesian prior inherently accounts for the uncertainty in the
predictions. Whether or not the authors want to advocate Bayesian statistics is beside the point
but it seems sufficiently relevant to be discussed.
Sam Schwarzkopf
UCL
label_end_comment
Decision letter (RSOS-160254.R1)
26th September 2016
Dear Professor Munafo:
Manuscript ID RSOS-160254.R1 entitled "Low Statistical Power in Biomedical Science: A review
of three human research domains" which you submitted to Royal Society Open Science, has been
reviewed. The comments of the reviewer(s) are included at the bottom of this letter.
Please submit a copy of your revised paper within three weeks (i.e. by the 18th October 2016). If
we do not hear from you within this time then it will be assumed that the paper has been
withdrawn. In exceptional circumstances, extensions may be possible if agreed with the Editorial
Office in advance. We do not allow multiple rounds of revision so we urge you to make every
effort to fully address all of the comments at this stage. If deemed necessary by the Editors, your
manuscript will be sent back to one or more of the original reviewers for assessment. If the
original reviewers are not available we may invite new reviewers.
15
To revise your manuscript, log into http://mc.manuscriptcentral.com/rsos and enter your
Author Centre, where you will find your manuscript title listed under "Manuscripts with
Decisions." Under "Actions," click on "Create a Revision." Your manuscript number has been
appended to denote a revision. Revise your manuscript and upload a new version through your
Author Centre.
When submitting your revised manuscript, you must respond to the comments made by the
referees and upload a file "Response to Referees" in "Section 6 - File Upload". Please use this to
document how you have responded to the comments, and the adjustments you have made. In
order to expedite the processing of the revised manuscript, please be as specific as possible in
your response.
In addition to addressing all of the reviewers' and editor's comments please also ensure that your
revised manuscript contains the following sections before the reference list:
• Ethics statement
If your study uses humans or animals please include details of the ethical approval received,
including the name of the committee that granted approval. For human studies please also detail
whether informed consent was obtained. For field studies on animals please include details of all
permissions, licences and/or approvals granted to carry out the fieldwork.
• Data accessibility
It is a condition of publication that all supporting data are made available either as
supplementary information or preferably in a suitable permanent repository. The data
accessibility section should state where the article's supporting data can be accessed. This section
should also include details, where possible of where to access other relevant research materials
such as statistical tools, protocols, software etc can be accessed. If the data have been deposited in
an external repository this section should list the database, accession number and link to the DOI
for all data from the article that have been made publicly available. Data sets that have been
deposited in an external repository and have a DOI should also be appropriately cited in the
manuscript and included in the reference list.
• Competing interests
Please declare any financial or non-financial competing interests, or state that you have no
competing interests.
• Authors’ contributions
All submissions, other than those with a single author, must include an Authors’ Contributions
section which individually lists the specific contribution of each author. The list of Authors
should meet all of the following criteria; 1) substantial contributions to conception and design, or
acquisition of data, or analysis and interpretation of data; 2) drafting the article or revising it
critically for important intellectual content; and 3) final approval of the version to be published.
All contributors who do not meet all of these criteria should be included in the
acknowledgements.
We suggest the following format:
AB carried out the molecular lab work, participated in data analysis, carried out sequence
alignments, participated in the design of the study and drafted the manuscript; CD carried out
the statistical analyses; EF collected field data; GH conceived of the study, designed the study,
coordinated the study and helped draft the manuscript. All authors gave final approval for
publication.
• Acknowledgements
Please acknowledge anyone who contributed to the study but did not meet the authorship
criteria.
16
• Funding statement
Please list the source of funding for each author.
Once again, thank you for submitting your manuscript to Royal Society Open Science and I look
forward to receiving your revision. If you have any questions at all, please do not hesitate to get
in touch.
Sincerely,
Alice Power
Editorial Coordinator
Royal Society Open Science
openscience@royalsociety.org
on behalf of Essi Viding
Subject Editor, Royal Society Open Science
openscience@royalsociety.org
Associate Editor Comments to Author:
Associate Editor: 1
Comments to the Author:
Editor's note: Your manuscript has been re-reviewed by the three original reviewers, and
reviewed for the first time by two new reviewers. I apologise for the delay in returning these
reviews to you.
Two of the original reviewers are basically happy with the manuscript in its current form, though
one has a suggestion for one minor change relating to the categorisation of mental disorders.
One of the original reviewers feels that some of the changes have been counter-productive, and
has made further suggestions for changes to the manuscript. These largely comprise further
discussion of the limitations of the Frequentist statistical framework. These concerns are also
shared by one of the new reviewers.
The other new reviewer has several substantive points, and also shares the view of the reviewers
in the above paragraph relating to Frequentist inference.
We are willing to consider a revised version of the manuscript that addresses the above points.
The biggest priority, now raised by three reviewers, is to include further discussion of the relative
merits of Frequentist versus other statistical frameworks.
Comments to Author:
Reviewer: 3
Comments to the Author(s)
I've marked my further comments in the attached.
Reviewer: 4
Comments to the Author(s)
see attached
Reviewer: 5
17
Comments to the Author(s)
In recent years the problems with underpowered experiments for the replicability of scientific
findings have received substantial attention. This manuscript describes an investigation of
statistical power in the biological sciences with a particular focus on various disorders and
clinical conditions. The authors find that power is generally very low although this also varies
considerably between subfields. By and large, this is an interesting study but I feel the following
issues should be discussed in more detail (I’m aware that the authors already touch upon some of
these issues but they could be emphasized more):
1. I am unsure that in the context of this study the meta-analytical effect sizes used by the
authors are necessarily a good estimate of the true population effect. Many meta-analyses
presumably test effects that are simply false (i.e. for which the null hypothesis is true), even when
the meta-analysis suggests a small effect. In fact, that seems to be the conclusion of many recent
meta-analyses in the social sciences. Quantifying statistical power on a true effect of zero is
meaningless.
On the other hand, some meta-analyses could be skewed by low quality studies dragging down
the effect size estimate. This may not always be readily apparent. Consequently, the whole
estimate of statistical power based on this meta-analytical effect size would be deflated. This
problem may be further exacerbated because the authors focus on fixed-effect analyses only. I
understand the authors’ reasoning for doing so but it is also a great limitation. The assumption of
homogeneity in effect sizes perhaps only holds when the individual studies are fairly similar.
2. Following from the previous point, the main problem is essentially that experimenters
cannot predict the future (unless you believe the authors of one recent meta-analysis). Therefore,
they cannot plan the statistical power based on studies that have not yet been done. The
suggestion to come up with meaningful minimally relevant effect sizes when planning a study is
certainly sensible – but it is not always practically feasible. Many “minimal” effects will require
very large samples to detect and resources are limited, even in multi-lab collaborations.
Instead I think it would be wiser to encourage clearer predictions of effect sizes a priori and
planning statistical power at the outset – for instance by preregistration of the experimental
design. Nobody should be prevented from publishing a small effect size with an underpowered
design (and such findings can still make important contributions to scientific knowledge) but it
should be more obvious when this is the case.
3. Increasing sample sizes is not the only way to boost statistical power. Relative effect sizes
are dependent on the signal to noise ratio of the experiment. Thus another way to enhance power
is to improve the sensitivity of the experimental procedures, for example by reducing
measurement error. This is not always feasible but in many situations it is, for instance by
collecting more data on each participant rather than collecting more participants. The
consequence is a greater relative effect size and thus greater statistical power. The use of designs
with variable sensitivity can also contribute to the heterogeneity of effect sizes. This issue seems
to be frequently overlooked. In my view, in situations where moving to ever larger samples is
impractical such methodological improvements should be of much higher priority.
4. I was surprised that the authors do not mention Bayesian hypothesis testing as an
alternative to frequentist inference. The manuscript highlights the importance of making
predicting effect sizes a priori and being able to tell compelling from inconclusive results.
Bayesian statistics provide both. Moreover, unlike picking a single effect size and calculating
power a priori, the choice of the Bayesian prior inherently accounts for the uncertainty in the
predictions. Whether or not the authors want to advocate Bayesian statistics is beside the point
but it seems sufficiently relevant to be discussed.
Sam Schwarzkopf
UCL
18
Reviewer: 1
Comments to the Author(s)
Thank you.
Malcolm Macleod
Reviewer: 2
Comments to the Author(s)
The authors have fully addressed the issues arising with their first version of the paper. The
paper is now in good standing and will make an important contribution to the field of statistical
power in the biomedical sciences. I have a couple minor recommendations for the authors:
1. I query their distinction between "severe" and "less severe" psychiatric disorders. As the
authors are probably aware, the category "severe mental illness" includes two disorders:
schizophrenia, and bipolar disorder. Their categorization of schizophrenia and autism as "severe"
and depression and ADHD as "less severe" seems a bit misleading. Clearly some cases of autism
are more severe than some cases of depression, but the opposite is also true: many patients with
MDD would certainly be characterized as more severe than the large population of people with
autism spectrum disorders for whom the disorder does not impact on their life in any major way.
There is no evidence that the meta-analyses included only covered severe cases of autism;
therefore, I think this distinction should either be clarified/justified (e.g. by citing somewhere
where it has been made before), or removed.
2. Typo on pg 9, line 21 "a more comparison"
3. pg 10, line 7, it seems odd to describe the power of cognitive/behavioural studies (93%) as
"relatively high" in comparison to 8%, 27%, and 39%. Surely this is just high power?
4. Typo on pg 13, line 28 "would also results"
5. pg 14, Might be better to describe their estimates as "underestimates" rather than low (ln 11)
and "overestimates" rather than high (line 17).
6. Typo pg 15, missing comma in line 9.
Author's Response to Decision Letter for (RSOS-160254.R1)
See Appendix D.
label_version_3
RSOS-160254.R2 (Revision)
label_author_9
Review form: Reviewer 3 (Peter Bacchetti)
Is the manuscript scientifically sound in its present form?
No
Are the interpretations and conclusions justified by the results?
No
Is the language acceptable?
No
19
Is it clear how to access all supporting data?
Yes
Do you have any ethical concerns with this paper?
No
Have you any concerns about statistical analyses in this paper?
No
Recommendation?
label_recommendation_9
Major revision is needed (please make suggestions in comments)
Comments to the Author(s)
label_comment_9
In attached file. (Appendix E)
label_author_10
Review form: Reviewer 4 (Philip Quinlan)
Is the manuscript scientifically sound in its present form?
Yes
Are the interpretations and conclusions justified by the results?
No
Is the language acceptable?
Yes
Is it clear how to access all supporting data?
Yes
Do you have any ethical concerns with this paper?
No
Have you any concerns about statistical analyses in this paper?
Yes
Recommendation?
label_recommendation_10
Reject
Comments to the Author(s)
label_comment_10
see attached (Appendix F).
label_end_comment
Decision letter (RSOS-160254.R2)
7th November 2016
Dear Professor Munafo:
Manuscript ID RSOS-160254.R2 entitled "Low Statistical Power in Biomedical Science: A review
of three human research domains" which you submitted to Royal Society Open Science, has been
reviewed. The comments of the reviewer(s) are included at the bottom of this letter.
20
Please submit a copy of your revised paper within three weeks (i.e. by the 28th November 2016).
If we do not hear from you within this time then it will be assumed that the paper has been
withdrawn. In exceptional circumstances, extensions may be possible if agreed with the Editorial
Office in advance. We do not allow multiple rounds of revision so we urge you to make every
effort to fully address all of the comments at this stage. If deemed necessary by the Editors, your
manuscript will be sent back to one or more of the original reviewers for assessment. If the
original reviewers are not available we may invite new reviewers.
To revise your manuscript, log into http://mc.manuscriptcentral.com/rsos and enter your
Author Centre, where you will find your manuscript title listed under "Manuscripts with
Decisions." Under "Actions," click on "Create a Revision." Your manuscript number has been
appended to denote a revision. Revise your manuscript and upload a new version through your
Author Centre.
When submitting your revised manuscript, you must respond to the comments made by the
referees and upload a file "Response to Referees" in "Section 6 - File Upload". Please use this to
document how you have responded to the comments, and the adjustments you have made. In
order to expedite the processing of the revised manuscript, please be as specific as possible in
your response.
In addition to addressing all of the reviewers' and editor's comments please also ensure that your
revised manuscript contains the following sections before the reference list:
• Ethics statement
If your study uses humans or animals please include details of the ethical approval received,
including the name of the committee that granted approval. For human studies please also detail
whether informed consent was obtained. For field studies on animals please include details of all
permissions, licences and/or approvals granted to carry out the fieldwork.
• Data accessibility
It is a condition of publication that all supporting data are made available either as
supplementary information or preferably in a suitable permanent repository. The data
accessibility section should state where the article's supporting data can be accessed. This section
should also include details, where possible of where to access other relevant research materials
such as statistical tools, protocols, software etc can be accessed. If the data have been deposited in
an external repository this section should list the database, accession number and link to the DOI
for all data from the article that have been made publicly available. Data sets that have been
deposited in an external repository and have a DOI should also be appropriately cited in the
manuscript and included in the reference list.
• Competing interests
Please declare any financial or non-financial competing interests, or state that you have no
competing interests.
• Authors’ contributions
All submissions, other than those with a single author, must include an Authors’ Contributions
section which individually lists the specific contribution of each author. The list of Authors
should meet all of the following criteria; 1) substantial contributions to conception and design, or
acquisition of data, or analysis and interpretation of data; 2) drafting the article or revising it
critically for important intellectual content; and 3) final approval of the version to be published.
All contributors who do not meet all of these criteria should be included in the
acknowledgements.
21
We suggest the following format:
AB carried out the molecular lab work, participated in data analysis, carried out sequence
alignments, participated in the design of the study and drafted the manuscript; CD carried out
the statistical analyses; EF collected field data; GH conceived of the study, designed the study,
coordinated the study and helped draft the manuscript. All authors gave final approval for
publication.
• Acknowledgements
Please acknowledge anyone who contributed to the study but did not meet the authorship
criteria.
• Funding statement
Please list the source of funding for each author.
Once again, thank you for submitting your manuscript to Royal Society Open Science and I look
forward to receiving your revision. If you have any questions at all, please do not hesitate to get
in touch.
Sincerely,
Alice Power
Royal Society Open Science
openscience@royalsociety.org
on behalf of Essi Viding
Subject Editor, Royal Society Open Science
openscience@royalsociety.org
Associate Editor Comments to Author:
Associate Editor: 1
Comments to the Author:
Editor's note: One of the reviewers still has substantial concerns that need to be addressed. The
other does not support publication of the manuscript, but given that there are 3 reviewers who
have appraised the work positively this in itself will not be a barrier to publication. So please try
to revise the manuscript in accordance with the suggestions of Reviewer 3.
Comments to Author:
Reviewer: 3
Comments to the Author(s)
In attached file.
Reviewer: 4
Comments to the Author(s)
see attached
Author's Response to Decision Letter for (RSOS-160254.R2)
See Appendix G.
22
label_version_4
RSOS-160254.R3 (Revision)
label_author_11
Review form: Reviewer 3 (Peter Bacchetti)
Is the manuscript scientifically sound in its present form?
No
Are the interpretations and conclusions justified by the results?
No
Is the language acceptable?
Yes
Is it clear how to access all supporting data?
Yes
Do you have any ethical concerns with this paper?
No
Have you any concerns about statistical analyses in this paper?
No
Recommendation?
label_recommendation_11
Accept as is
Comments to the Author(s)
label_comment_11
Reviewer 3 (Peter Bacchetti, University of California, San Francisco):
I appreciate the authors’ efforts to address my concerns about reinforcing the NHST framework
and related misconceptions. Although the premise of the paper does not permit complete
avoidance of this problem, I think the latest changes are a substantial improvement, and I do not
think that any further delay in publication for additional revisions is warranted.
One additional way to mitigate the potential reinforcement of misconceptions would be for the
authors to opt to publish the review process documentation alongside the publication.
label_end_comment
Decision letter (RSOS-160254.R3)
3rd January 2017
Dear Professor Munafo,
I am pleased to inform you that your manuscript entitled "Low Statistical Power in Biomedical
Science: A review of three human research domains" is now accepted for publication in Royal
Society Open Science.
You can expect to receive a proof of your article in the near future. Please contact the editorial
office (openscience_proofs@royalsociety.org and openscience@royalsociety.org) to let us know if
you are likely to be away from e-mail contact. Due to rapid publication and an extremely tight
schedule, if comments are not received, your paper may experience a delay in publication.
23
Royal Society Open Science operates under a continuous publication model
(http://bit.ly/cpFAQ). Your article will be published straight into the next open issue and this
will be the final version of the paper. As such, it can be cited immediately by other researchers.
As the issue version of your paper will be the only version to be published I would advise you to
check your proofs thoroughly as changes cannot be made once the paper is published.
In order to raise the profile of your paper once it is published, we can send through a PDF of your
paper to selected colleagues. If you wish to take advantage of this, please reply to this email with
the name and email addresses of up to 10 people who you feel would wish to read your article.
On behalf of the Editors of Royal Society Open Science, we look forward to your continued
contributions to the Journal.
Kind regards,
Alice Power
Editorial Coordinator
Royal Society Open Science
openscience@royalsociety.org
http://rsos.royalsocietypublishing.org/
Reviewer(s)' Comments to Author:
Reviewer: 3
Comments to the Author(s)
Reviewer 3 (Peter Bacchetti, University of California, San Francisco):
I appreciate the authors’ efforts to address my concerns about reinforcing the NHST framework
and related misconceptions. Although the premise of the paper does not permit complete
avoidance of this problem, I think the latest changes are a substantial improvement, and I do not
think that any further delay in publication for additional revisions is warranted.
One additional way to mitigate the potential reinforcement of misconceptions would be for the
authors to opt to publish the review process documentation alongside the publication.
Appendix A
Reviewer 1 (Malcolm Macleod):
Was the research guided by a protocol? Is a date-stamped version of the protocol
available in the public domain, that the reader might see whether their analysis is
what they set out to do?
Unfortunately we did not pre-register a protocol prior to conducting the study. We
now note this as a limitation.
I think they could clarify the domains covered - Neurology is a little more than
Alzheimer's disease, epilepsy, Parkinson's disease and multiple sclerosis - and it is
likely that there have been meta-analyses for other conditions eg Huntington's
disease, stroke etc. So it may be better to set the question up with the specifics that
would be identified by the search.
We have attempted to provide more detail about the domains covered, and why we
chose the specific disease categories that we did.
The authors might reflect on whether taking the fixed effect estimate is appropriate,
particularly as it may induce this behaviour in the future! I think there is sufficient
evidence of the prevalence of publication bias to conclude that this fixed effect will be
inflated in many cases, and so the problem is even worse than reported here. An
alternative strategy (I don't propose they adopt it here, but they may mention it in
passing) is to establish a minimally biologically important effect, and establish
whether the literature is powered to detect this. That would be difficult to generalise -
we might be prepared to accept a 2-fold elevated risk of classroom disruptiveness for
kids taking chemotherapy for cancer, but only 1.01 for an effect of a commonly
available carbonated soft drink additive. But establishing the relationship between
biological importance and power would be useful.
This is the approach suggested by Ioannidis, who developed the excess of
significance test, on the basis that the random effects estimate weight smaller
studies more equally, whereas the fixed effects estimate places greater weight on the
largest (and therefore most precise) studies. We now highlight this.
We agree that focusing on a minimally biologically important effect would be an
interesting and informative strategy, albeit one beyond the scope of the present
study, and now discus this as a possible future direction.
You might mention that - as well as being a waste of time and money, and polluting
the literature - the positive predictive value of studies powered at the levels reported
here will be much worse than had they been powered at 80%.
We now include this important point in our discussion.
I get that for many of these studies there is a clear relationship between the number
of subjects and the power; but when sample size is very small and the variable is
continuous, there can be huge variation in the observed variance, which can then in
turn have effects on power. I think this is a bit circular, but the assumption that there
is a direct mapping between n and observed variance (rather than population
variance) is not given. So in sampling from a population, there is variation in the
observed mean from the population mean; and variance in the observed variance
from the population variance. By taking the fixed effects (ie ~ population) mean with
the observed (rather than eg the median) sd may be complicating things. I dont know
enough about statistics to say whether this is a problem, and if so whether it is a big
enough problem to confound the findings reported, but the authors may wish to
respond.
Fixed effects meta-analysis assumes that all studies are drawn from the same
underlying population and attempt to estimate the effect size within that population.
Random effects meta-analysis, on the other hand, does not make this assumption,
allowing studies to be drawn from multiple underlying populations, and therefore
attempting to estimate the range of effect sizes that may exist across these
populations. The assumptions of our analysis therefore align best with those of fixed
effects meta-analysis. We now include a discussion of this point, and the extent to
which it constitutes a limitation of our approach.
Reviewer 2:
It is unclear to me why the authors have chosen initially to frame the purpose of the
study around the idea of researchers dividing their resources into as many individual
studies as possible (first sentence of abstract). From my understanding, this is not
the only cause of low power, nor does evidence of low power in a discipline prove
that this is occurring. The authors again speculate about this in the discussion (page
11, lines 6-20), which is a more appropriate place for this suggestion. However, it still
seems overly speculative to suggest that researchers conduct lots of small studies
followed by large confirmatory ones as an explanation for the bimodal distribution
observed, since there are lots of things that could account for this distribution, and
certainly no evidence that the same labs contribute to both peaks of the distribution.
We agree that this is a speculative proposal, and have therefore removed it from the
introduction. We also agree that there are many other potential explanations for this
observed distribution, and have therefore removed this point from the discussion.
It would be useful to have some more statistics in the paper. The authors state (page
8, line 24) that "the majority" of studies fell into the 0-20% decile, but it would be
useful to know the exact number overall (across all 3 disciplines). Equally, when
cognitive studies are discussed as having "relatively high" power, genetic studies
"very low" (line 42), medians should be stated.
We now provide this information.
I am not sure that it is justifiable to exclude cognitive/behavioural measures relating
to ADHD, depression, and schizophrenia (total N = 56) in the overall power
estimates, given that these are typical neuroscience techniques in psychiatry (page
8, line 10). Surely these studies should be included when calculating the overall
median statistical powers, in order to make the claim that psychiatry, neurology, and
somatic medicine all have "very similar median statistical power"?
This is a point we discussed extensively ourselves. On the one hand, these
measures are the backbone of much psychiatry research. On the other, they are not
directly comparable to the measures used in the other disciplines we use for
comparison purposes. Therefore, we have reported our results both with and without
these studies, for completeness. We now note this as a limitation.
The authors claim that a strength of their study derives from excluding treatment
studies, which Button and colleagues included (page 10, lines 16-25). They go on to
claim that this means studies in the present analysis are more representative of the
wider neuroscience literature. This is a strange claim to make as a large portion of
their studies are not neuroscience studies by any definition (i.e., glaucoma studies,
breast cancer studies, etc.), and numerous non-psychiatry, non-neurology
neuroscience studies would be excluded entirely by their search terms. (Whether
excluding treatment studies is an advantage or a disadvantage of the paper could
also be debated).
We agree that the term “neuroscience” is perhaps unhelpful here (indeed, what is
indexed as “neuroscience” by PubMed includes many studies that would not typically
be described as such). We have attempted to rephrase this section accordingly.
Reviewer 3 (Peter Bacchetti):
A key assumption of this study is that power should always be evaluated at the true
effect size. This assumption is not justified in the paper and does not seem sensible.
It would imply that all studies of an association that turns out to be small or null are
illegitimate due to low power. Is there is no value to providing evidence that an
association is small? Table S2 even shows a number of cases where “power” was
calculated with the alternative equal to the null, which seems to me to clearly show
that this approach cannot be sensible. This is very closely related to the practice of
post-hoc power calculation that is described in reference 6 as “deplored in the
statistical literature”: a study with an estimated effect at or near the null value will
calculate low post-hoc power and conclude that there is only weak evidence against
an effect, regardless of how narrow the CI around the nearly null estimate is. Here, a
meta-analytic estimate at or near the null tars all the studies that contributed to it, but
this is essentially the same problem. Excellent evidence against any substantial
association can be interpreted as instead being a collection of deeply flawed studies
(because of their appallingly low power). (There are, on the other hand, many people
who consider it obvious that power should always be calculated using the minimum
important effect size. I believe that this also has severe problems.)
We believe that studies should be large enough to be informative. If there is a real
effect, they should be large enough to detect it (and effects are often smaller in
reality than researcher hope). If there is no real effect, studies should be large
enough to confidently lead to this conclusion (e.g., by providing a confidence interval
that excludes a theoretically, biologically or clinically meaningful difference). In our
opinion a large body of very small studies providing inconclusive evidence is indeed
problematic. However, while we clearly take a different view to Dr Bacchetti, we
appreciate that there are different perspectives on this point, and now attempt to
capture this in our discussion.
The issue in point 1 leads to an alternative explanation for the large number of
studies in the first two bins and the bimodality shown in Figure 1. Perhaps every
study had an appropriate sample size and Figure 1 simply reflects the distribution of
what turned out to be true about the associations being studied: over half were very
close to no association, a few were of intermediate magnitudes, and some were
large. As conducted, this study provides no way to distinguish this scenario from
most studies’ having “inappropriately small” sample sizes.
We now include a discussion of an alternative approach, suggested by Reviewer 1,
whereby the ability of studies to identify effects of biological / theoretical interest is
the focus.
Previous studies claiming to document widespread “underpowering” have often
focused on randomized clinical trials, because those are where NHST is considered
most applicable. Why were treatment studies excluded, and why is a rationale for this
absent from the Methods? This exclusion seem likely to preferentially select studies
where investigators had less control over sample size and studies that are more
exploratory and not intended to provide conclusive evidence. Both of these make
power less relevant, so this exclusion seems counterproductive.
See above – we have attempted to clarify this point. Our primary motivation was to
obtain a more comparable group of studies focused on the association of various
biomarkers and other biological parameters with disease outcomes. We see no
reason why should lead to the bias described, but we now discuss this possibility.
A major drawback of this and similar studies is the implicit acceptance of the NHST
paradigm and related poor approaches for interpreting research studies. A focus on
power reinforces the idea that the most important thing about a study is whether or
not it finds P<0.05; power is, after all, just the probability of this event. Although the
Introduction attempts to connect power to more general considerations, this seems
quite strained to me: the influence of sample size on precision is better shown and
interpreted by use of confidence intervals and standard errors than by the convoluted
considerations mentioned and referenced, and the focus on “a statistically significant
finding” gets right back to reinforcing the excessive focus on whether P<0.05.
We agree that this is a potential problem of our approach. We now include a
discussion of the need to focus on estimation and precision, rather than P-values.
The Abstract and Discussion also reinforce the requirement for a minimum of 80%
power. While I commend the authors for using the phrase “considered conventional”
rather than “adequate” or “acceptable”, I don’t think this is sufficient to avoid
reinforcing a supposed standard that has never been given any cogent scientific
justification. (Colleagues and I have argued in considerable detail that no such single
standard can be justified.)
We agree and have replaced this terminology to emphasise that studies should be
large (and therefore precise) enough to be informative.
I agree that consideration of incentives and resource allocation are important issues.
Studies with low power, however, particularly as defined in this paper, may not have
required low resources. Indeed, high per person cost is a sensible reason for a
smaller sample size and low cost is a good reason for a larger sample size. The
speculative explanation for the bimodal distribution seems quite implausible to me.
My experience (admittedly in a different country) is that researchers do not
themselves allocate the financial resources that they use, and they have ample
incentives to make sample size as large as is reasonably feasible. They are
constrained by cost and feasibility. (The power-based paradigm for determining
sample size is unrealistic in its neglect of costs.)
This opinion contrasts markedly with both our own experience and that of many other
authors (from many countries) who have published on this topic. We now
acknowledge that differences between countries and funding systems may be
important. We have also removed our speculation regarding the bimodal distribution.
Reviewer 3 (Peter Bacchetti):
I appreciate a number of additions and modifications that attempt to respond
to my concerns. Unfortunately, I believe that the revisions have exacerbated
some of the problems I raised. In addition, I think the revised paper has not
been clear enough about the key limitation I pointed out. The latter could be
easily remedied (see below). On the other hand, I do not believe there is any
way to focus on power without embracing the NHST paradigm, which causes
the attempts to do so in this revision to worsen the problems I raised. It may
work better to simply acknowledge the drawbacks of the NHST paradigm as a
limitation on the relevance of this study.
A key assumption of this study is that power should always be evaluated at the true
effect size. This assumption is not justified in the paper and does not seem sensible.
It would imply that all studies of an association that turns out to be small or null are
illegitimate due to low power. Is there is no value to providing evidence that an
association is small? Table S2 even shows a number of cases where “power” was
calculated with the alternative equal to the null, which seems to me to clearly show
that this approach cannot be sensible. This is very closely related to the practice of
post-hoc power calculation that is described in reference 6 as “deplored in the
statistical literature”: a study with an estimated effect at or near the null value will
calculate low post-hoc power and conclude that there is only weak evidence against
an effect, regardless of how narrow the CI around the nearly null estimate is. Here, a
meta-analytic estimate at or near the null tars all the studies that contributed to it, but
this is essentially the same problem. Excellent evidence against any substantial
association can be interpreted as instead being a collection of deeply flawed studies
(because of their appallingly low power). (There are, on the other hand, many people
who consider it obvious that power should always be calculated using the minimum
important effect size. I believe that this also has severe problems.)
We believe that studies should be large enough to be informative. If there is a real
effect, they should be large enough to detect it (and effects are often smaller in
reality than researcher hope). If there is no real effect, studies should be large
enough to confidently lead to this conclusion (e.g., by providing a confidence interval
that excludes a theoretically, biologically or clinically meaningful difference). In our
opinion a large body of very small studies providing inconclusive evidence is indeed
problematic. However, while we clearly take a different view to Dr Bacchetti, we
appreciate that there are different perspectives on this point, and now attempt to
capture this in our discussion.
An explicit acknowledgement of this key assumption and when it is not valid has
been added, but I believe that this should be moved to the Limitations section and
should also explicitly acknowledge implications for the interpretation of this study.
For example, an added sentence like “' may be prohibitive). We have not
attempted to exclude such cases and therefore cannot address what proportion of
the studies in the bolus with power below 20% had low power because the true
effects were small or null, rather than because of any defects in their sample sizes.”
In addition, the new opening sentence of the Abstract is false, because of this
problem. When null or small values are “plausible effects”, precision can be
excellent even though power is low.
The issue in point 1 leads to an alternative explanation for the large number of
studies in the first two bins and the bimodality shown in Figure 1. Perhaps every
study had an appropriate sample size and Figure 1 simply reflects the distribution of
what turned out to be true about the associations being studied: over half were very
close to no association, a few were of intermediate magnitudes, and some were
large. As conducted, this study provides no way to distinguish this scenario from
most studies’ having “inappropriately small” sample sizes.
We now include a discussion of an alternative approach, suggested by Reviewer 1,
whereby the ability of studies to identify effects of biological / theoretical interest is
the focus.
As above, I believe that a more explicit acknowledgement of the implications of this
problem is warranted.
Previous studies claiming to document widespread “underpowering” have often
focused on randomized clinical trials, because those are where NHST is considered
most applicable. Why were treatment studies excluded, and why is a rationale for this
absent from the Methods? This exclusion seem likely to preferentially select studies
where investigators had less control over sample size and studies that are more
exploratory and not intended to provide conclusive evidence. Both of these make
power less relevant, so this exclusion seems counterproductive.
See above – we have attempted to clarify this point. Our primary motivation was to
obtain a more comparable group of studies focused on the association of various
biomarkers and other biological parameters with disease outcomes. We see no
reason why should lead to the bias described, but we now discuss this possibility.
I appreciate the explicit acknowledgement of some of these concerns. (I think the
word “may” is missing somewhere in the added text.)
A major drawback of this and similar studies is the implicit acceptance of the NHST
paradigm and related poor approaches for interpreting research studies. A focus on
power reinforces the idea that the most important thing about a study is whether or
not it finds P<0.05; power is, after all, just the probability of this event. Although the
Introduction attempts to connect power to more general considerations, this seems
quite strained to me: the influence of sample size on precision is better shown and
interpreted by use of confidence intervals and standard errors than by the convoluted
considerations mentioned and referenced, and the focus on “a statistically significant
finding” gets right back to reinforcing the excessive focus on whether P<0.05.
We agree that this is a potential problem of our approach. We now include a
discussion of the need to focus on estimation and precision, rather than P-values.
Despite the explicit acknowledgement that has been added, some revisions seem to
me to have worsened this problem. Notably, the concept of positive predictive value
depends entirely on acceptance of NHST, but it is now mentioned in several places
as if it were completely unproblematic. Royall explained three decades ago
(https://www.jstor.org/stable/2684616) that this supposed drawback of smaller
sample size arises only if one focuses exclusively on whether P<0.05—as soon as
we pay attention to the actual, undichotomized P-value, the picture is reversed.
Mentioning PPV in this way seems to me to legitimize an exclusive focus on whether
or not P<0.05. In addition, many added uses of “detect” will be read as meaning “find
P<0.05”, and that does appear to be the intended meaning. This presumes the
NHST approach and equates P<0.05 with the meaning of “detect” in ordinary
language, which implies certainty.
The Abstract and Discussion also reinforce the requirement for a minimum of 80%
power. While I commend the authors for using the phrase “considered conventional”
rather than “adequate” or “acceptable”, I don’t think this is sufficient to avoid
reinforcing a supposed standard that has never been given any cogent scientific
justification. (Colleagues and I have argued in considerable detail that no such single
standard can be justified.)
We agree and have replaced this terminology to emphasise that studies should be
large (and therefore precise) enough to be informative.
I believe that the revision has actually regressed on this issue, contrary to the
authors’ intention. The Abstract seems to state that having power below the arbitrary
80% convention implies that the estimates will be “highly inaccurate and imprecise”,
and a similar assertion is given in the Introduction. In reality, the precision of
estimates parallels the square root of sample size and nothing special happens at
80% power, so this supporting reasoning for the convention is invalid. In addition,
many phrases have been added that appear to reflect an implicit acceptance of what
I have called the “threshold myth” (http://www.biomedcentral.com/content/pdf/1741-7015-
8-17.pdf), an assumption that there is a meaningful demarcation between “adequate”
and “inadequate” sample size. These phrases include “too small to reliably detect”,
“large enough to be informative”, “large enough to detect it”, “a large enough study to
detect this”. These beg the questions of what is “too small” or “large enough”, and
what exactly qualifies as “reliably”. I believe that readers will assume that these all
refer to the 80% power convention, particularly given the assertions made in the
Abstract and Introduction.
I agree that consideration of incentives and resource allocation are important issues.
Studies with low power, however, particularly as defined in this paper, may not have
required low resources. Indeed, high per person cost is a sensible reason for a
smaller sample size and low cost is a good reason for a larger sample size. The
speculative explanation for the bimodal distribution seems quite implausible to me.
My experience (admittedly in a different country) is that researchers do not
themselves allocate the financial resources that they use, and they have ample
incentives to make sample size as large as is reasonably feasible. They are
constrained by cost and feasibility. (The power-based paradigm for determining
sample size is unrealistic in its neglect of costs.)
This opinion contrasts markedly with both our own experience and that of many other
authors (from many countries) who have published on this topic. We now
acknowledge that differences between countries and funding systems may be
important. We have also removed our speculation regarding the bimodal distribution.
OK.
If the authors are to be believed then theirs is a truly bleak appraisal of the
extant scientific literature. However, despite being troubled by their conclusions,
I am also somewhat puzzled. They expose the shaky beliefs that are apparently
prevalent (pp. 3-4 ). But they themselves are apparently unaware of the power
fallacy that seemingly underpins their own set of beliefs. The fallacy is to assume
that “high-power experiments always yield more informative data then do low-
power experiments” (Wagenmakers et al, 2015, p. 913).
Wagenmakers et al. have clearly shown that the most appropriate way to assess
the worth of any dataset is to compute the corresponding likelihood ratios and
Bayes factors. It is these that “quantify the evidence that a particular data set
provides for or against the null or any hypothesis.” (op. cit. p. 913). On these
grounds, the conclusions that the authors draw merely on their own estimates of
statistical power are perhaps not as solid as we might otherwise think.
Now of course it may not be possible to retrospectively compute likelihood
ratios and Bayes factors for the meta-analyzed studies, but without these
statistics the basic issues remain unresolved.
Philip Quinlan.
Wagenmakers, E-J et al. (1915). A power fallacy. Behavior Research Methods, 47,
319-317.
Appendix D
Reviewer: 1
No comments.
Reviewer: 2
I query their distinction between "severe" and "less severe" psychiatric disorders. I
think this distinction should either be clarified/justified (e.g. by citing somewhere
where it has been made before), or removed.
We have removed this distinction.
Typo on pg 9, line 21 "a more comparison"
We have corrected this typo.
pg 10, line 7, it seems odd to describe the power of cognitive/behavioural studies
(93%) as "relatively high" in comparison to 8%, 27%, and 39%. Surely this is just
high power?
We agree and have changed this accordingly.
Typo on pg 13, line 28 "would also results"
We have corrected this typo.
pg 14, Might be better to describe their estimates as "underestimates" rather than
low (ln 11) and "overestimates" rather than high (line 17).
We have changed this accordingly.
Typo pg 15, missing comma in line 9.
We have corrected this typo.
Reviewer: 3
I appreciate a number of additions and modifications that attempt to respond to my
concerns. Unfortunately, I believe that the revisions have exacerbated some of the
problems I raised. In addition, I think the revised paper has not been clear enough
about the key limitation I pointed out. The latter could be easily remedied (see
below). On the other hand, I do not believe there is any way to focus on power
without embracing the NHST paradigm, which causes the attempts to do so in this
revision to worsen the problems I raised. It may work better to simply acknowledge
the drawbacks.
We have reworked our manuscript to make it clear that our approach rests on the
NHST framework and now include a discussion of the limitations of this framework
(and implications for our analysis).
An explicit acknowledgement of this key assumption and when it is not valid has
been added, but I believe that this should be moved to the Limitations section and
should also explicitly acknowledge implications for the interpretation of this study. For
example, an added sentence like “' may be prohibitive). We have not attempted to
exclude such cases and therefore cannot address what proportion of the studies in
the bolus with power below 20% had low power because the true effects were small
or null, rather than because of any defects in their sample sizes.” In addition, the new
opening sentence of the Abstract is false, because of this problem. When null or
small values are “plausible effects”, precision can be excellent even though power is
low. As above, I believe that a more explicit acknowledgement of the implications of
this problem is warranted.
We have moved this to the limitations section, as suggested, and expanded our
discussion of this point. In addition, we now include an analysis of only those meta-
analyses that reported a statistically significant pooled effect size estimate, in an
attempt to address this issue somewhat. We have also modified the abstract, as
suggested.
I appreciate the explicit acknowledgement of some of these concerns. (I think the
word “may” is missing somewhere in the added text.)
We have corrected this typo.
Despite the explicit acknowledgement that has been added, some revisions seem to
me to have worsened this problem. Notably, the concept of positive predictive value
depends entirely on acceptance of NHST, but it is now mentioned in several places
as if it were completely unproblematic. Royall explained three decades ago
(https://www.jstor.org/stable/2684616) that this supposed drawback of smaller
sample size arises only if one focuses exclusively on whether P<0.05—as soon as
we pay attention to the actual, undichotomized P-value, the picture is reversed.
Mentioning PPV in this way seems to me to legitimize an exclusive focus on whether
or not P<0.05. In addition, many added uses of “detect” will be read as meaning “find
P<0.05”, and that does appear to be the intended meaning. This presumes the NHST
approach and equates P<0.05 with the meaning of “detect” in ordinary language,
which implies certainty.
We now include an explicit discussions of the limitations of the PPV concept, which
we include in the limitations section. We have also modified our language in an
attempt to avoid the potential misconceptions highlighted.
I believe that the revision has actually regressed on this issue, contrary to the
authors’ intention. The Abstract seems to state that having power below the arbitrary
80% convention implies that the estimates will be “highly inaccurate and imprecise”,
and a similar assertion is given in the Introduction. In reality, the precision of
estimates parallels the square root of sample size and nothing special happens at
80% power, so this supporting reasoning for the convention is invalid. In addition,
many phrases have been added that appear to reflect an implicit acceptance of what
I have called the “threshold myth” (http://www.biomedcentral.com/content/pdf/1741
7015- 8-17.pdf), an assumption that there is a meaningful demarcation between
“adequate” and “inadequate” sample size. These phrases include “too small to
reliably detect”, “large enough to be informative”, “large enough to detect it”, “a large
enough study to detect this”. These beg the questions of what is “too small” or “large
enough”, and what exactly qualifies as “reliably”. I believe that readers will assume
that these all refer to the 80% power convention, particularly given the assertions
made in the Abstract and Introduction.
This was not our intention. We have reworded these sections to make it clear that
our approach relies on the NHST framework, which in turn relies on the use of
(arbitrary) thresholds (e.g., alpha of 5%, beta of 20% etc.). We also now include a
section of alternative approaches (estimation, and Bayesian inference).
Reviewer: 4
If the authors are to be believed then theirs is a truly bleak appraisal of the extant
scientific literature. However, despite being troubled by their conclusions, I am also
somewhat puzzled. They expose the shaky beliefs that are apparently prevalent (pp.
3-4 ). But they themselves are apparently unaware of the power fallacy that
seemingly underpins their own set of beliefs. The fallacy is to assume that “high-
power experiments always yield more informative data then do low power
experiments” (Wagenmakers et al, 2015, p. 913).
We do not feel that we have succumbed to the power fallacy because we have not
attempted to infer anything about individual cases, but rather have attempted to
describe the situation on average within the fields in question. We have attempted to
clarify this in the text by discussing (as Wagenmakers does) how power is a pre-
study concept. We note that the Wagenmakers article cited in fact cites our previous
work using the same methodology that we apply here (Button et al., 2013) to support
the statement that “It is well known that psychology has a power problem” (first
paragraph). However, we have reworded our manuscript extensively to avoid
potential confusion, and now discuss alternative (e.g., Bayesian) approaches.
Wagenmakers et al. have clearly shown that the most appropriate way to assess the
worth of any dataset is to compute the corresponding likelihood ratios and Bayes
factors. It is these that “quantify the evidence that a particular data set provides for or
against the null or any hypothesis.” (op. cit. p. 913). On these grounds, the
conclusions that the authors draw merely on their own estimates of statistical power
are perhaps not as solid as we might otherwise think. Now of course it may not be
possible to retrospectively compute likelihood ratios and Bayes factors for the meta-
analyzed studies, but without these statistics the basic issues remain unresolved.
We now acknowledge more clearly that our approach rests on the NHST framework,
and discuss the limitations associated with this. We also discuss the role of
alternative statistical frameworks, although it is worth reiterating that the NHST
framework (rightly or wrongly) remains the dominant one within the biomedical
sciences. We agree that the analyses proposed would be interesting, but feel that
these fall beyond the scope of the present article.
Reviewer: 5
I am unsure that in the context of this study the meta-analytical effect sizes used by
the authors are necessarily a good estimate of the true population effect. Many meta-
analyses presumably test effects that are simply false (i.e. for which the null
hypothesis is true), even when the meta-analysis suggests a small effect. In fact, that
seems to be the conclusion of many recent meta-analyses in the social sciences.
Quantifying statistical power on a true effect of zero is meaningless.
See above – we now include an analysis of only those meta-analyses that indicated
a nominally significant pooled effect size estimate, to account for the possibility that
some of the meta-analyses that indicated a non-significant estimate include cases
where the true effect size is zero and the calculation of power inappropriate. In our
opinion this provides an upper and a lower bound to our estimate to average power.
It is also worth noting that one of the primary motivations behind meta-analysis is to
provide a more precise estimate of the population effect size. Therefore, within the
limitations of the NHST framework that our approach rests on we feel that using
meta-analytic effect sizes in this way is appropriate.
Moreover, there are cases where meta-analyses have provided effect size estimates
that have turned out to be reasonably precise, even if the pooled effect size estimate
is non-significant (e.g., genetic association studies), because even the combined N
within the meta-analysis is inadequate to detect these real but very small effects,
On the other hand, some meta-analyses could be skewed by low quality studies
dragging down the effect size estimate. This may not always be readily apparent.
Consequently, the whole estimate of statistical power based on this meta-analytical
effect size would be deflated. This problem may be further exacerbated because the
authors focus on fixed-effect analyses only. I understand the authors’ reasoning for
doing so but it is also a great limitation. The assumption of homogeneity in effect
sizes perhaps only holds when the individual studies are fairly similar.
Low quality studies typically report inflated (rather than deflated) effect size
estimates, so in our opinion there is no reason to assume that any distorting effect
will systematically operate in one direction. Nevertheless, we take the point regarding
the limitations of using meta-analytic effect sizes and now discuss these.
I think it would be wiser to encourage clearer predictions of effect sizes a priori and
planning statistical power at the outset – for instance by preregistration of the
experimental design.
We agree and now include this recommendation in our conclusions.
Increasing sample sizes is not the only way to boost statistical power. Relative effect
sizes are dependent on the signal to noise ratio of the experiment. Thus another way
to enhance power is to improve the sensitivity of the experimental procedures, for
example by reducing measurement error. This is not always feasible but in many
situations it is, for instance by collecting more data on each participant rather than
collecting more participants. The consequence is a greater relative effect size and
thus greater statistical power. The use of designs with variable sensitivity can also
contribute to the heterogeneity of effect sizes. This issue seems to be frequently
overlooked. In my view, in situations where moving to ever larger samples is
impractical such methodological improvements should be of much higher priority.
We agree and now include a discussion of other means by which power can be
increased.
I was surprised that the authors do not mention Bayesian hypothesis testing as an
alternative to frequentist inference. The manuscript highlights the importance of
making predicting effect sizes a priori and being able to tell compelling from
inconclusive results. Bayesian statistics provide both. Moreover, unlike picking a
single effect size and calculating power a priori, the choice of the Bayesian prior
inherently accounts for the uncertainty in the predictions. Whether or not the authors
want to advocate Bayesian statistics is beside the point but it seems sufficiently
relevant to be discussed.
We now make it clear that our analysis applies to one particular statistical framework
(i.e., NHST) and that other approaches exist that may offer advantages.
Appendix E
Reviewer 3 (Peter Bacchetti):
As I mentioned at the outset, I am a highly skeptical reviewer for this paper, and my
goal has been to try to mitigate its harm. While I appreciate their efforts, I believe
that the authors’ responses to my concerns have not mitigated them very effectively
and in some cases have worsened them. I therefore cannot in good conscience
recommend publication of this manuscript in its current form, even though it has
already undergone two rounds of revision.
Notably, the concept of PPV was absent from the original submission, and its
subsequent addition, retention despite my objection in the last review, and major
emphasis (opening lines of Abstract and Conclusion) seem to me to be a major step
backward. The concept of PPV relies entirely on ignoring the actual P-value
obtained (and all other study results) in favor of using only the information that
P<0.05. This is an indefensible practice, and reference 19 includes a quotation
calling this “an egregious mistake.” I do not see how this prominent role for PPV can
be justified, in particular by the claim that NHST is the dominant paradigm in
biomedical research. Flaws in this reasoning include:
1) A paper that criticizes existing research practices and seeks to improve them
should itself be held to high standards. It therefore should not treat such an
obviously poor practice as acceptable, even if it is common.
2) PPV has no role in NHST. Instead, it is a Bayesian posterior probability, but
one that is calculated using gratuitously coarsened information (which is the
part that parallels NHST).
3) While P<0.05 features prominently in biomedical research, I don’t think this
means that NHST is dominant. There are many widespread practices that
are not part of NHST. Guidelines mandate reporting of the actual P-values
obtained, estimated effects, and confidence intervals, and most papers do
this. Meta-analyses play a prominent role (as in this paper), and they make
no use at all of the original studies’ P-values. And many researchers are
aware that whether or not P<0.05 is not the only basis for reaching
conclusions or making decisions. This is reflected in Principle 3 of the recent
statement about P-values from the American Statistical Association:
“Scientific conclusions and business or policy decisions should not be based
only on whether a p-value passes a specific threshold.”
I think it would be much simpler to remove PPV from the paper entirely. If it must be
included, then I think the authors should more clearly acknowledge its drawback
(next point) and offer some justification for nevertheless claiming that it is relevant.
In addition, caveats should be as prominent as its use, for example, by adding text
like “although the concept of PPV is problematic because it neglects much of the
information that a study provides” to the headline statements.
The second limitation (Page 13, lines 4-12) acknowledges a drawback to PPV, but
has several problems. First, the approach in this paper is unrelated to PPV, so the
idea that it “rests in part” on it is confusing. My understanding is that PPV is
mentioned only to help make a case for the unacceptability of low power (and this is
not valid for the reasons noted above). Second, it seems needlessly unclear on the
major drawback of PPV: it could replace “when statistical evidence passes an arbitrary
threshold (typically P < 0.05)” with a clearer statement like “based only on whether the P-
value is below an arbitrary threshold (usually P<0.05), without any consideration of the
exact P-value or the other evidence that a study provides.” Third, the assertion “which in
turn implies” is incorrect. The impossibility of justifying a demarcation between adequate
and inadequate sample size follows from diminishing marginal returns, which hold even if
we fully accept the P-value threshold. So the P-value threshold and the power threshold
are two different issues.
The acknowledgement of one of my major concerns about this paper’s methodology
appears now on Page 12, lines 52-57:
“Moreover, we used meta-analytic estimates of effect size as a best estimate of
any true effect size, which assumes that power should always be evaluated at the
true effect size. This is invalid if the true effect size is zero (in which case power
calculations are meaningless).”
This is misleading, because the assumption is invalid in much wider circumstances,
at the very least whenever the true effect size is too small to be important. The
sensitivity analysis intended to address this problem relies on using whether P<0.05
as a surrogate for the magnitude of the effect. This is notoriously invalid, as
highlighted by Principle 5 of the ASA statement on P-values: “A p-value, or statistical
significance, does not measure the size of an effect or the importance of a result.”
The authors have calculated effect sizes (Figure 4), so I am unclear why they did not
use an effect size cutoff for the sensitivity analysis. This would still be problematic,
because any given log odds ratio may be trivial in some contexts and substantial in
others, but at least this would not reinforce the misconception that P-values
somehow measure the magnitude or importance of an effect. Picking an effect size
cutoff will be more obviously arbitrary, because there is no universal convention like
P<0.05, but I see that as an advantage rather than a problem, because the arbitrary
nature of the cutoff should be made clear to readers.
The opening of the Conclusion continues to read: “Low statistical power is one factor
that contributes to poor robustness and reliability of research findings, by expending
valuable resources on studies with a low PPV. This is wasteful in the long run and will
contribute to publication bias.” In addition to the use of PPV, the invocation of
resource efficiency and publication bias seem to me to be unjustifiable. Colleagues
and I have carefully documented the fact of diminishing marginal returns and its
implications for cost efficiency (reference 17, and for more detail: Bacchetti P,
McCulloch CE, Segal MR. Simple, defensible sample sizes based on cost efficiency
(with Discussion and Rejoinder). Biometrics, 64: 577–594, 2008). The falsity of the
threshold myth implies that power alone cannot assess cost-efficiency:
unsurprisingly, costs must also be considered. In addition, Button et al. (2013) made
a similar claim about waste of research animals, and I posted a mathematical proof
that their assertion is false and notified the senior author (see
https://www.ctspedia.org/wiki/pub/CTSpedia/EthicsSampleSize/Wasted_animals.pdf
). While I understand that the mathematical details may seem obscure, I see no
justification for discounting these fundamental facts about cost-efficiency. As for
publication bias, it is not caused by low power or PPV or sample size. It is caused by
selective publication. Claiming otherwise seems to me to make the basic mistake of
equating correlation with causation.
There is still some wording used that reinforces misconceptions either about the
P<0.05 threshold or sample size thresholds. I believe that rephrasing these would
avoid harmful tacit endorsement of prevalent misconceptions. The word “detect”
(page 4 line 55, page 13 line 4) could be replaced with “find P<0.05 for”. As I
mentioned previously, the phrase “large enough to be informative” (page 14 line 57)
seems to reinforce the threshold myth about sample size. In reality, the information
provided by a study is a continuous function of sample size with a concave shape
that reflects diminishing marginal returns. This holds not only for power in the NHST
paradigm, but also for any of the other perspectives mentioned in the next sentence
(see Biometrics reference noted above for details). So there is no “large enough”
sample size that converts a study from uninformative to informative.
Appendix F
Given the authors’ previous apocalyptic warnings and, indeed, the recent writings of
Ioannidis (http://dx.doi.org/10.1101/071530) this particular message about statistical power
is now established as part of the current zeitgeist. Perhaps my previous comments were a
little too cryptic? The importance that the authors attach to statistical power alone is
unfounded. The alternative is that other indicators of strength of evidence are critical.
There is very little I can add. The authors have, essentially, ignored my concerns (although
grudging reference is now made to the ‘potentially more informative’ Bayesian estimates)
and dismissed my other suggestions as being ‘beyond the scope of the present article’.
Philp Quinlan.
As I mentioned at the outset, I am a highly skeptical reviewer for this paper, and my goal has been to
try to mitigate its harm. While I appreciate their efforts, I believe that the authors’ responses to my
concerns have not mitigated them very effectively and in some cases have worsened them. I therefore
cannot in good conscience recommend publication of this manuscript in its current form, even though it
has already undergone two rounds of revision.
Response: We appreciate Dr Bacchetti’s openness throughout this review process regarding his
skepticism. We have attempted to address his comments throughout the previous round of review, and
done so in good faith, but clearly we have not satisfied him. Given the skepticism he admits to it’s not
clear whether we would ever be able to satisfy him entirely, but here we make one last attempt to reach
some middle ground.
Notably, the concept of PPV was absent from the original submission, and its subsequent addition,
retention despite my objection in the last review, and major emphasis (opening lines of Abstract and
Conclusion) seem to me to be a major step backward.
Response: As our major revision to this manuscript we have removed reference to the PPV (which, as is
noted, was absent from our original submission).
The authors have calculated effect sizes (Figure 4), so I am unclear why they did not use an effect size
cutoff for the sensitivity analysis. This would still be problematic, because any given log odds ratio may
be trivial in some contexts and substantial in others, but at least this would not reinforce the
misconception that P-values somehow measure the magnitude or importance of an effect. Picking an
effect size cutoff will be more obviously arbitrary, because there is no universal convention like P<0.05,
but I see that as an advantage rather than a problem, because the arbitrary nature of the cutoff should
be made clear to readers.
Response: We agree that some effect sizes will be trivial and others meaningful (either clinically or
biologically). However, this will be subject-specific, and given the broad range of studies included in our
analysis it would not be sensible to apply a single cut-off. On the other hand, identifying an appropriate
cut-off for each of the over 200 meta-analyses would be challenging (and potentially impossible, since
it’s not clear whether universally agreed cut-offs for a biologically or clinically meaningful effect size
apply in each of these cases). We therefore believe that using a convention of P < 0.05 is the only
practical solution, and one that is at least transparent. We have added a discussion of this limitation, but
would be happy to remove this additional analysis if this is preferable (i.e., revert to our original
analysis).
The opening of the Conclusion continues to read: “Low statistical power is one factor that contributes to
poor robustness and reliability of research findings, by expending valuable resources on studies with a
low PPV. This is wasteful in the long run and will contribute to publication bias.”
Response: We have removed this sentence and no longer refer to resource issues.
As for publication bias, it is not caused by low power or PPV or sample size. It is caused by selective
publication. Claiming otherwise seems to me to make the basic mistake of equating correlation with
causation.
Response: We do not say that publication bias is caused by low power / small sample size, but it is true
that it is a factor (hence tests of small study bias, such as the funnel plot and Egger test that are used to
infer the potential presence of publication bias). Nevertheless, we have removed reference to publication
bias.
There is still some wording used that reinforces misconceptions either about the P<0.05 threshold or
sample size thresholds. I believe that rephrasing these would avoid harmful tacit endorsement of
prevalent misconceptions. The word “detect” (page 4 line 55, page 13 line 4) could be replaced with
“find P<0.05 for”. As I mentioned previously, the phrase “large enough to be informative” (page 14 line
57) seems to reinforce the threshold myth about sample size. In reality, the information provided by a
study is a continuous function of sample size with a concave shape that reflects diminishing marginal
returns. This holds not only for power in the NHST paradigm, but also for any of the other perspectives
mentioned in the next sentence (see Biometrics reference noted above for details). So there is no “large
Response: We have attempted to rephrase our manuscript to avoid these potential misconceptions,
although our optimism that we have done so successfully is tempered by the unintended consequences
of our well-intentioned revisions to date.
Society Open
