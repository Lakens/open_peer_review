Intra-individual variation in performance on novel variants
of similar tasks influences single factor explanations of
general cognitive processes
Jayden O. van Horik, Ellis J. G. Langley, Mark A. Whiteside, Philippa R. Laker and Joah R.
Madden
Article citation details
R. Soc. open sci. 5: 171919.
http://dx.doi.org/10.1098/rsos.171919
Review timeline
Original submission: 15 November 2017 Note: Reports are unedited and appear as
1st revised submission: 4 April 2018 submitted by the referee. The review history
2nd revised submission: 1 June 2018 appears in chronological order.
Final acceptance: 6 June 2018
Review History
label_version_1
RSOS-171919.R0 (Original submission)
label_author_1
Review form: Reviewer 1
Is the manuscript scientifically sound in its present form?
No
Are the interpretations and conclusions justified by the results?
No
Is the language acceptable?
Yes
Is it clear how to access all supporting data?
No
© 2018 The Authors. Published by the Royal Society under the terms of the Creative Commons
Attribution License http://creativecommons.org/licenses/by/4.0/, which permits unrestricted use,
provided the original author and source are credited
2
Do you have any ethical concerns with this paper?
No
Have you any concerns about statistical analyses in this paper?
Yes
Recommendation?
label_recommendation_1
Major revision is needed (please make suggestions in comments)
Comments to the Author(s)
label_comment_1
The authors assess a general factor and domain factors that contribute to the performance of
pheasant chicks on nine nominally cognitive tasks that represent four domains of ability. No
general influence performance was identified, and when random combinations of the tasks were
subjected to factor analysis, 14 combinations generated significant main factors. The authors
conclude that there is no domain-specific or domain-general influence on their animals’
performance. Based on this analysis, the authors argue that previous identifications of general
influences on the cognitive performance of non-human animals may be an artifact of the
composition of the test battery.
This is a timely study and it represents a fairly extensive analysis. However, there are some real
problems with the discussion/interpretation of the results as well as the manner in which the
data was collected and analyzed. I cannot recommend publication of the present manuscript,
although it might be acceptable following a major revision.
1. My first concern is a general one regarding the authors’ conclusion that previous
identifications of general influences on the cognitive performance of non-human animals may be
an artifact of the composition of the test battery. Of course this is true, as it is true for human test
battery performance. For instance, the WAIS is not comprised of a random selection of tasks.
The test was originally conceived and subsequently revised to include only tasks that load on a
general factor. This is based on the assumption that a general influence exists and the test battery
should be sensitive to it. In fact, independent analyses that include many other tasks (not
included in the WAIS itself) have rarely found tasks that DO NOT load on a principal factor. But
of course depending on how the task is administered and what the task tests, it is possible to find
tasks that do not load on the principal factor. But so what? What is impressive about tests like
the WAIS is that so many tasks that nominally have nothing in common DO load on a principal
factor. This result demands the existence of a general factor.
The present study uses tasks that in some instances have only minimal cognitive requirements
(e.g., the robo worm, which is primarily a test of coordination). Why should we expect such tasks
to load on a general COGNITIVE factor? Generally, I think that the authors’ conclusion is not
warranted by the actual data that they present.
2. IF a general factor was identified in this study, it would be impossible to conclude that it is a
general influence on COGNITIVE ability. ALL tasks that are used in this study are motivated by
the same food reinforcer. To help interpret the general factor, tasks should have been developed
and included which were motivated by other drives. With the present composition of the test
battery, it would be impossible to distinguish a general cognitive factor from a general MEAL
WORM MOTIVATION factor. The later might be interesting, but not given the intent of this
study.
3. The study began with 200 animals, but only 31 completed all tests in the battery. This is
because animals could self-select (“volunteer”) for inclusion in every test. This is problematic for
3
a bunch of reasons, not the least of which is that the final 31 are NOT representative of the
population. If the range of individuals is restricted (by some influence on the likelihood of
participating), then there would be insufficient variation in the sample to ever detect a general (or
domain-specific) influence. Also, the animals were not individually housed (apparently being
housed in groups as large as 50), and animals’ position in dominance hierarchies are known to
impact cognitive performance.
The authors suggest in their rationale for this experiment that many prior studies have used
inadequate samples of only 20 subjects (although many have used far more). Is 31 really that
much better than 20? It is also noted that in the present study, the ratio of variables to sample
size is considered too small to produce a stable factor analysis (which is seemingly true of the
present study, given the number of factors identified with random combinations of tasks). I think
a separate analysis is warranted in which a larger sample of birds is included based on their
participation in more tasks. Only 31 completed ALL task, but I would assume that many more
completed a subset of tasks. The authors should sacrifice the inclusion of some tasks in order to
obtain a larger sample on SOME tasks. If this analysis is consistent with what the authors
presently report, I would be more inclined to recommend publication. Alternatively, it might
suggest a different interpretation of the data. This sub-analysis is imperative if this paper is to be
published.
3. The tasks that are described as being representative of spatial memory make no cognitive
demads on spatial processing. They can obviously be solved with no reliance on spatial cues
(and in fact, are probably solved by simple place or route learning).
I think that this paper is a good start. However, given the limitations on the subject pool and the
composition of the test battery, I’m not sure that the data is meaningful or correctly interpreted.
This experiment was obviously a lot of work. Nevertheless, the selection/inclusion of subjects
seriously clouds the interpretation of the data. This might be resolved by some additional
analyses (as described above).
label_author_2
Review form: Reviewer 2 (Rachael Shaw)
Is the manuscript scientifically sound in its present form?
Yes
Are the interpretations and conclusions justified by the results?
Yes
Is the language acceptable?
Yes
Is it clear how to access all supporting data?
No
Do you have any ethical concerns with this paper?
No
Have you any concerns about statistical analyses in this paper?
No
4
Recommendation?
label_recommendation_2
Accept with minor revision (please list in comments)
Comments to the Author(s)
label_comment_2
The authors present the results of a test battery of nine tasks for 31 pheasant chicks. Overall, an
initial Principal Axis Factor (PAF) analysis found little support for a general intelligence factor
underlying test battery performance in all nine tasks. However, further PAF analyses of all
possible combinations of a subset of six out of the nine test battery tasks revealed that some
combinations produce single factor solutions that are analogous to those that have previously
been taken as evidence for a general intelligence factor in other non-human test battery studies.
This paper raises important concerns in terms of the design, analysis and interpretation of test
battery data and how such factors may affect our current understanding of the structure of
intelligence in non-human species. As such, it makes both a timely and important contribution to
this rapidly growing field.
Overall the paper is well written and clearly presents the rationale, design and interpretation of
the results, citing relevant and recent literature. However, I do have a number of suggestions and
clarifications which I hope will help to increase the impact and relevance of the paper for other
researchers in this area.
My main issue with the study is that it substantially differs from previous non-human test battery
research in terms of both the learning criteria used for each task and the approach used for the
factor analyses. As such, it is at times difficult to ascertain how directly comparable the results of
this study are to the previous work in this field. From my reading of the paper, I understand that
not all tasks had a specified learning criterion, but instead they measure the individual’s
accuracy/latency (e.g. the paper puncture, roboworm, spatial memory and detour reach tasks), or
the (predicted?) error rate in the tasks with a fixed number of trials (e.g. colour
discrimination/reversal and spatial learning tasks). It would be interesting to know whether
these task performance measures would ultimately correlate with performance that is measured
up to a specified learning criteria in each task (although this is unlikely to be possible in the
current study). At the very least, in the discussion the authors should attempt to address how
such differences in methodology may have impacted the PAF results (e.g. could this account for
the large intra-individual variation in task performances?), or limit comparisons with previous
research.
I also think it would be very useful if the authors could elaborate on why they chose to use PAF,
rather than Principal Component Analysis (PCA). PCA is the approach most commonly used in
other non-human studies (e.g. see Table 1 in Shaw & Schmelz, 2017), thus it would increase the
contribution of the current paper if the authors could make a case for why their chosen approach
is preferable for the sample sizes and/or types of tasks typically used in non-human studies. If
PAF is indeed preferable to PCA, then it would be useful to strongly advocate for its adoption by
other non-human test battery researchers.
Minor comments:
In the title “intra-individual variation” is ambiguous. Please clarify that this refers to variation in
task performance.
P. 4, lines 35-40: “test may be excluded...” is ambiguous. Are the authors suggesting here that
previous studies may have purposefully excluded tasks from a test battery if they did not support
a single factor solution? To me, while it is certainly plausible, this seems like a serious accusation
of data ‘cherry picking’, which does not have any direct support in the paper. If this was not the
authors’ intention, I would suggest re-phrasing this sentence.
5
P. 4, line 55: The term “biomechanically plausible” is not immediately clear and I would suggest
re-phrasing.
P. 5, lines 9-12: It is arguable that other non-human studies have explored how test battery
composition affects estimates of ‘g’, but not as the main focus of the paper. For example, both
Shaw et al. 2015 and Herrman & Call 2012 present factor analyses for subsets of the tasks used in
their study. I would therefore suggest re-phrasing or removing this statement.
P. 5, lines 28-35: This summarises sample sizes in previous studies, please also state the sample
size in the current study for comparison (this could be included in the following sentence on line
37).
In the methods it would be useful to state the cognitive skills being assessed by the ‘paper
puncture’ and ‘robo-worm’ tasks.
It appears that methods for the two spatial tasks have been moved in front of the colour
learning/reversal task descriptions at some point during revisions on the paper, without
appropriate edits being made. As such, they are now lacking in sufficient detail, as they refer the
reader to the ‘above’ colour discrimination problems (line 5, p. 9). Please add relevant details, or
consider revising the order of the methods.
P. 10, lines 36-41: The authors argue that giving individuals a fixed number of trials is an
ecologically valid approach for the discrimination task before the reversal trials, as ephemeral
fluctuations in food in the wild mean that pheasants may experience natural, unpredictable
switches in reinforcement as food resources fluctuate. However, as it is currently presented this is
not a comparable analogy to the discrimination and reversal tasks, as it seems more likely that for
most natural food resources the cue associated with the food would also disappear as the source
is depleted (rather than birds frequently encountering a cue associated with food that remains
present even once the food is no longer available). Please consider refining this analogy, or
removing it.
Please ensure that the performance measure is clearly presented for each task. For example, in
tasks where logistic curves were used to obtain a ‘predicted trial number’ at which the bird was
performing with 80% accuracy, was this how all birds performance scores were obtained, or did
some birds reach this criteria within the 50 trials of each of the four discrimination problems,
hence their score is not a predicted value?
P.12, Table 1: Please include a description in the table heading of what is meant by the ‘9+18’ and
‘4+1’ in the table.
P.12, lines 49-51: Please clarify what is meant by an ‘ANOVA with improvement’
P. 13, lines 7-12: It is stated that only detour reach, robo-worm and spatial memory were
inversely transformed, as smaller values were associated with better performance. Perhaps I am
missing something, but isn’t this true for all other task performance values (except ‘paper
puncture’?) as well, where fewer predicted trials to reach the 80% success rate (e.g.
discrimination, reversal and spatial memory tasks) is also associated with better performance in
the task? Apologies if I am misinterpreting something here, but I would again suggest that the
authors ensure that performance measures for each task are clearly stated throughout the
methods.
6
P. 17, lines 48-52: Please give a justification for why six tasks was chosen as the number of tasks to
include in the subset analyses.
P. 21, line 37: “explanation cognitive processes” - is a word missing here?
P. 27, line 3: “ensure” should replace “insure”
P. 27, line 26: Delete “do” from “we cannot conclude”
Data supplement:
Please make it clear in the methods or results that data are available in the supplementary
material.
In the supplementary data file, the data column headers are unclear. Also, all data relevant to the
analyses described in the paper has not been made available (as is required by this journal). It
appears that only the transformed overall performance scores for each individual in each task
have been provided.
Review provided by Dr Rachael Shaw, School of Biological Sciences, Victoria University of
Wellington
label_end_comment
Decision letter (RSOS-171919.R0)
28-Feb-2018
Dear Dr van Horik,
The editors assigned to your paper ("Test battery composition and intra-individual variation
influence single factor explanations of cognitive processes") have now received comments from
reviewers. We would like you to revise your paper in accordance with the referee and Associate
Editor suggestions which can be found below (not including confidential reports to the Editor).
Please note this decision does not guarantee eventual acceptance.
Please submit a copy of your revised paper within three weeks (i.e. by the 23-Mar-2018). If we do
not hear from you within this time then it will be assumed that the paper has been withdrawn. In
exceptional circumstances, extensions may be possible if agreed with the Editorial Office in
advance.We do not allow multiple rounds of revision so we urge you to make every effort to
fully address all of the comments at this stage. If deemed necessary by the Editors, your
manuscript will be sent back to one or more of the original reviewers for assessment. If the
original reviewers are not available, we may invite new reviewers.
To revise your manuscript, log into http://mc.manuscriptcentral.com/rsos and enter your
Author Centre, where you will find your manuscript title listed under "Manuscripts with
Decisions." Under "Actions," click on "Create a Revision." Your manuscript number has been
appended to denote a revision. Revise your manuscript and upload a new version through your
Author Centre.
When submitting your revised manuscript, you must respond to the comments made by the
7
referees and upload a file "Response to Referees" in "Section 6 - File Upload". Please use this to
document how you have responded to the comments, and the adjustments you have made. In
order to expedite the processing of the revised manuscript, please be as specific as possible in
your response.
In addition to addressing all of the reviewers' and editor's comments please also ensure that your
revised manuscript contains the following sections as appropriate before the reference list:
• Ethics statement (if applicable)
If your study uses humans or animals please include details of the ethical approval received,
including the name of the committee that granted approval. For human studies please also detail
whether informed consent was obtained. For field studies on animals please include details of all
permissions, licences and/or approvals granted to carry out the fieldwork.
• Data accessibility
It is a condition of publication that all supporting data are made available either as
supplementary information or preferably in a suitable permanent repository. The data
accessibility section should state where the article's supporting data can be accessed. This section
should also include details, where possible of where to access other relevant research materials
such as statistical tools, protocols, software etc can be accessed. If the data have been deposited in
an external repository this section should list the database, accession number and link to the DOI
for all data from the article that have been made publicly available. Data sets that have been
deposited in an external repository and have a DOI should also be appropriately cited in the
manuscript and included in the reference list.
If you wish to submit your supporting data or code to Dryad (http://datadryad.org/), or modify
your current submission to dryad, please use the following link:
http://datadryad.org/submit?journalID=RSOS&manu=RSOS-171919
• Competing interests
Please declare any financial or non-financial competing interests, or state that you have no
competing interests.
• Authors’ contributions
All submissions, other than those with a single author, must include an Authors’ Contributions
section which individually lists the specific contribution of each author. The list of Authors
should meet all of the following criteria; 1) substantial contributions to conception and design, or
acquisition of data, or analysis and interpretation of data; 2) drafting the article or revising it
critically for important intellectual content; and 3) final approval of the version to be published.
All contributors who do not meet all of these criteria should be included in the
acknowledgements.
We suggest the following format:
AB carried out the molecular lab work, participated in data analysis, carried out sequence
alignments, participated in the design of the study and drafted the manuscript; CD carried out
the statistical analyses; EF collected field data; GH conceived of the study, designed the study,
coordinated the study and helped draft the manuscript. All authors gave final approval for
publication.
• Acknowledgements
Please acknowledge anyone who contributed to the study but did not meet the authorship
criteria.
8
• Funding statement
Please list the source of funding for each author.
Please note that Royal Society Open Science will introduce article processing charges for all new
submissions received from 1 January 2018. Charges will also apply to papers transferred to Royal
Society Open Science from other Royal Society Publishing journals, as well as papers submitted
as part of our collaboration with the Royal Society of Chemistry
(http://rsos.royalsocietypublishing.org/chemistry). If your manuscript is submitted and
accepted for publication after 1 Jan 2018, you will be asked to pay the article processing charge,
unless you request a waiver and this is approved by Royal Society Publishing. You can find out
more about the charges at http://rsos.royalsocietypublishing.org/page/charges. Should you
have any queries, please contact openscience@royalsociety.org.
Once again, thank you for submitting your manuscript to Royal Society Open Science and I look
forward to receiving your revision. If you have any questions at all, please do not hesitate to get
in touch.
Kind regards,
Alice Power
Editorial Coordinator
Royal Society Open Science
openscience@royalsociety.org
on behalf of Antonia Hamilton (Subject Editor)
openscience@royalsociety.org
Associate Editor's comments:
I am particularly concerned about the issue of sample size with only 31 complete datasets. If it is
possible to collect more data, I strongly recommend doing so. Other helpful comments are
detailed by the reviewers below.
Comments to Author:
Reviewers' Comments to Author:
Reviewer: 1
Comments to the Author(s)
The authors assess a general factor and domain factors that contribute to the performance of
pheasant chicks on nine nominally cognitive tasks that represent four domains of ability. No
general influence performance was identified, and when random combinations of the tasks were
subjected to factor analysis, 14 combinations generated significant main factors. The authors
conclude that there is no domain-specific or domain-general influence on their animals’
performance. Based on this analysis, the authors argue that previous identifications of general
influences on the cognitive performance of non-human animals may be an artifact of the
composition of the test battery.
This is a timely study and it represents a fairly extensive analysis. However, there are some real
problems with the discussion/interpretation of the results as well as the manner in which the
data was collected and analyzed. I cannot recommend publication of the present manuscript,
although it might be acceptable following a major revision.
9
1. My first concern is a general one regarding the authors’ conclusion that previous
identifications of general influences on the cognitive performance of non-human animals may be
an artifact of the composition of the test battery. Of course this is true, as it is true for human test
battery performance. For instance, the WAIS is not comprised of a random selection of tasks.
The test was originally conceived and subsequently revised to include only tasks that load on a
general factor. This is based on the assumption that a general influence exists and the test battery
should be sensitive to it. In fact, independent analyses that include many other tasks (not
included in the WAIS itself) have rarely found tasks that DO NOT load on a principal factor. But
of course depending on how the task is administered and what the task tests, it is possible to find
tasks that do not load on the principal factor. But so what? What is impressive about tests like
the WAIS is that so many tasks that nominally have nothing in common DO load on a principal
factor. This result demands the existence of a general factor.
The present study uses tasks that in some instances have only minimal cognitive requirements
(e.g., the robo worm, which is primarily a test of coordination). Why should we expect such tasks
to load on a general COGNITIVE factor? Generally, I think that the authors’ conclusion is not
warranted by the actual data that they present.
2. IF a general factor was identified in this study, it would be impossible to conclude that it is a
general influence on COGNITIVE ability. ALL tasks that are used in this study are motivated by
the same food reinforcer. To help interpret the general factor, tasks should have been developed
and included which were motivated by other drives. With the present composition of the test
battery, it would be impossible to distinguish a general cognitive factor from a general MEAL
WORM MOTIVATION factor. The later might be interesting, but not given the intent of this
study.
3. The study began with 200 animals, but only 31 completed all tests in the battery. This is
because animals could self-select (“volunteer”) for inclusion in every test. This is problematic for
a bunch of reasons, not the least of which is that the final 31 are NOT representative of the
population. If the range of individuals is restricted (by some influence on the likelihood of
participating), then there would be insufficient variation in the sample to ever detect a general (or
domain-specific) influence. Also, the animals were not individually housed (apparently being
housed in groups as large as 50), and animals’ position in dominance hierarchies are known to
impact cognitive performance.
The authors suggest in their rationale for this experiment that many prior studies have used
inadequate samples of only 20 subjects (although many have used far more). Is 31 really that
much better than 20? It is also noted that in the present study, the ratio of variables to sample
size is considered too small to produce a stable factor analysis (which is seemingly true of the
present study, given the number of factors identified with random combinations of tasks). I think
a separate analysis is warranted in which a larger sample of birds is included based on their
participation in more tasks. Only 31 completed ALL task, but I would assume that many more
completed a subset of tasks. The authors should sacrifice the inclusion of some tasks in order to
obtain a larger sample on SOME tasks. If this analysis is consistent with what the authors
presently report, I would be more inclined to recommend publication. Alternatively, it might
suggest a different interpretation of the data. This sub-analysis is imperative if this paper is to be
published.
3. The tasks that are described as being representative of spatial memory make no cognitive
demads on spatial processing. They can obviously be solved with no reliance on spatial cues
(and in fact, are probably solved by simple place or route learning).
10
I think that this paper is a good start. However, given the limitations on the subject pool and the
composition of the test battery, I’m not sure that the data is meaningful or correctly interpreted.
This experiment was obviously a lot of work. Nevertheless, the selection/inclusion of subjects
seriously clouds the interpretation of the data. This might be resolved by some additional
analyses (as described above).
Reviewer: 2
Comments to the Author(s)
The authors present the results of a test battery of nine tasks for 31 pheasant chicks. Overall, an
initial Principal Axis Factor (PAF) analysis found little support for a general intelligence factor
underlying test battery performance in all nine tasks. However, further PAF analyses of all
possible combinations of a subset of six out of the nine test battery tasks revealed that some
combinations produce single factor solutions that are analogous to those that have previously
been taken as evidence for a general intelligence factor in other non-human test battery studies.
This paper raises important concerns in terms of the design, analysis and interpretation of test
battery data and how such factors may affect our current understanding of the structure of
intelligence in non-human species. As such, it makes both a timely and important contribution to
this rapidly growing field.
Overall the paper is well written and clearly presents the rationale, design and interpretation of
the results, citing relevant and recent literature. However, I do have a number of suggestions and
clarifications which I hope will help to increase the impact and relevance of the paper for other
researchers in this area.
My main issue with the study is that it substantially differs from previous non-human test battery
research in terms of both the learning criteria used for each task and the approach used for the
factor analyses. As such, it is at times difficult to ascertain how directly comparable the results of
this study are to the previous work in this field. From my reading of the paper, I understand that
not all tasks had a specified learning criterion, but instead they measure the individual’s
accuracy/latency (e.g. the paper puncture, roboworm, spatial memory and detour reach tasks), or
the (predicted?) error rate in the tasks with a fixed number of trials (e.g. colour
discrimination/reversal and spatial learning tasks). It would be interesting to know whether
these task performance measures would ultimately correlate with performance that is measured
up to a specified learning criteria in each task (although this is unlikely to be possible in the
current study). At the very least, in the discussion the authors should attempt to address how
such differences in methodology may have impacted the PAF results (e.g. could this account for
the large intra-individual variation in task performances?), or limit comparisons with previous
research.
I also think it would be very useful if the authors could elaborate on why they chose to use PAF,
rather than Principal Component Analysis (PCA). PCA is the approach most commonly used in
other non-human studies (e.g. see Table 1 in Shaw & Schmelz, 2017), thus it would increase the
contribution of the current paper if the authors could make a case for why their chosen approach
is preferable for the sample sizes and/or types of tasks typically used in non-human studies. If
PAF is indeed preferable to PCA, then it would be useful to strongly advocate for its adoption by
other non-human test battery researchers.
11
Minor comments:
In the title “intra-individual variation” is ambiguous. Please clarify that this refers to variation in
task performance.
P. 4, lines 35-40: “test may be excluded...” is ambiguous. Are the authors suggesting here that
previous studies may have purposefully excluded tasks from a test battery if they did not support
a single factor solution? To me, while it is certainly plausible, this seems like a serious accusation
of data ‘cherry picking’, which does not have any direct support in the paper. If this was not the
authors’ intention, I would suggest re-phrasing this sentence.
P. 4, line 55: The term “biomechanically plausible” is not immediately clear and I would suggest
re-phrasing.
P. 5, lines 9-12: It is arguable that other non-human studies have explored how test battery
composition affects estimates of ‘g’, but not as the main focus of the paper. For example, both
Shaw et al. 2015 and Herrman & Call 2012 present factor analyses for subsets of the tasks used in
their study. I would therefore suggest re-phrasing or removing this statement.
P. 5, lines 28-35: This summarises sample sizes in previous studies, please also state the sample
size in the current study for comparison (this could be included in the following sentence on line
37).
In the methods it would be useful to state the cognitive skills being assessed by the ‘paper
puncture’ and ‘robo-worm’ tasks.
It appears that methods for the two spatial tasks have been moved in front of the colour
learning/reversal task descriptions at some point during revisions on the paper, without
appropriate edits being made. As such, they are now lacking in sufficient detail, as they refer the
reader to the ‘above’ colour discrimination problems (line 5, p. 9). Please add relevant details, or
consider revising the order of the methods.
P. 10, lines 36-41: The authors argue that giving individuals a fixed number of trials is an
ecologically valid approach for the discrimination task before the reversal trials, as ephemeral
fluctuations in food in the wild mean that pheasants may experience natural, unpredictable
switches in reinforcement as food resources fluctuate. However, as it is currently presented this is
not a comparable analogy to the discrimination and reversal tasks, as it seems more likely that for
most natural food resources the cue associated with the food would also disappear as the source
is depleted (rather than birds frequently encountering a cue associated with food that remains
present even once the food is no longer available). Please consider refining this analogy, or
removing it.
Please ensure that the performance measure is clearly presented for each task. For example, in
tasks where logistic curves were used to obtain a ‘predicted trial number’ at which the bird was
performing with 80% accuracy, was this how all birds performance scores were obtained, or did
some birds reach this criteria within the 50 trials of each of the four discrimination problems,
hence their score is not a predicted value?
P.12, Table 1: Please include a description in the table heading of what is meant by the ‘9+18’ and
‘4+1’ in the table.
P.12, lines 49-51: Please clarify what is meant by an ‘ANOVA with improvement’
12
P. 13, lines 7-12: It is stated that only detour reach, robo-worm and spatial memory were
inversely transformed, as smaller values were associated with better performance. Perhaps I am
missing something, but isn’t this true for all other task performance values (except ‘paper
puncture’?) as well, where fewer predicted trials to reach the 80% success rate (e.g.
discrimination, reversal and spatial memory tasks) is also associated with better performance in
the task? Apologies if I am misinterpreting something here, but I would again suggest that the
authors ensure that performance measures for each task are clearly stated throughout the
methods.
P. 17, lines 48-52: Please give a justification for why six tasks was chosen as the number of tasks to
include in the subset analyses.
P. 21, line 37: “explanation cognitive processes” - is a word missing here?
P. 27, line 3: “ensure” should replace “insure”
P. 27, line 26: Delete “do” from “we cannot conclude”
Data supplement:
Please make it clear in the methods or results that data are available in the supplementary
material.
In the supplementary data file, the data column headers are unclear. Also, all data relevant to the
analyses described in the paper has not been made available (as is required by this journal). It
appears that only the transformed overall performance scores for each individual in each task
have been provided.
Review provided by Dr Rachael Shaw, School of Biological Sciences, Victoria University of
Wellington
Author's Response to Decision Letter for (RSOS-171919.R0)
See Appendix A.
label_version_2
RSOS-171919.R1 (Revision)
label_author_3
Review form: Reviewer 1
Is the manuscript scientifically sound in its present form?
Yes
Are the interpretations and conclusions justified by the results?
Yes
13
Is the language acceptable?
Yes
Is it clear how to access all supporting data?
Yes
Do you have any ethical concerns with this paper?
No
Have you any concerns about statistical analyses in this paper?
No
Recommendation?
label_recommendation_3
Accept with minor revision (please list in comments)
Comments to the Author(s)
label_comment_3
Relative to the prior submission, this revised manuscript is greatly improved, and in many
instances, the authors have been responsive to the prior comments by the editor and reviewer. In
particular, the statistics are now more clearly described and succinct, and the overall discussion is
more clear (and the results are more clearly communicated). Nevertheless, a number of concerns
still exist that lessen my overall enthusiasm and confidence in the conclusions. However, the
authors are sufficiently cautious in their interpretation and discussion of the limitations of their
data, and thus I generally support publication of the manuscript. My remaining concerns are
summarized below.
The general tenor of the authors’ responses is that this is a large and complex analysis (which it
is), and thus it was necessary to compromise the procedures and analyses in order to complete
the experiment. They often note that “other people do it too”. While I’m sympathetic, sometimes
the virtue of laboratory studies is that conclusive answers are at least possible when compromises
are minimized. Here are the problems:
1. The n is simply too small. While hundreds of birds started the study, only 31 completed all 9
tests. The authors argue that many comparable studies use fewer tests, making it easier to test
more subjects. Similarly, the authors note that many studies are published with a number of
subjects comparable to what they report. I don’t find the argument that other people have
published studies based on inadequate numbers very compelling. In the present case, 31 subjects
and nine tests yields a three-to-one ratio of subjects-to-variables, which is universally considered
too small to support a stable factor analysis. Even more importantly is the reason that the final n
is so small. In this study, the subjects could self-select for inclusion. This demands that the final
subject pool is somehow different that the larger population. This restricted sample may not
contain enough variability to ever reveal a general factor. The authors respond that they have
minimized any discussion of implications of their data for the existence of a general factor, but
even in the abstract, the absence of a general factor appears as a focus. This needs work.
2. The authors say that tasks were chosen “because they were available”. Since some of the tasks
have no obvious dependence on any cognitive process, it is not surprising that they don’t all load
on a cognitive factor. Again, the authors argue that others doing similar work sometimes
“incorporate tasks available regardless of how well they capture the full breadth of general
intelligence”. Again, this is a weak rationale, and I don’t think tasks’ availability makes them
appropriate for assessing COGNITIVE ability.
3. All tasks are motivated by food. The authors say that since they are not claiming the existence
14
of a cognitive factor, “we do not consider this to be an issue”. First, this is a circular argument,
and second, the birds may have self-selected for inclusion based on their motivation for food
rewards.
4. Indeed it is unnatural to singly-house social animals. But, when they are singly housed they
are ALL being exposed to the same conditions. This is NOT the case when they are group
housed such that dominance hierarchies (and related differential stress responses) arise. If there
are problems with either method, we should err on the side of treating all animals similarly,
particularly if it is our intention to study individual differences. Again, dominant animals may be
the ones that self-select for inclusion, and this may underlie the absence of sufficient variability to
reveal a general factor.
5. I still maintain that many of the tasks are inaccurately described. For instance, the “spatial”
tasks have no reliance on spatial cues and are most likely solved through egocentric navigation.
In defense, the authors state that “there is a precedent” for calling them spatial tasks, and
furthermore, that the animals improved across trials, “suggesting that learning was involved”.
I’m not disputing that these are learning tasks, and it may well be that others have incorrectly
described them. This does not justify their being described as spatial in nature. We’d all be better
off if we spoke a common language.
6. I still think that it would have been useful to analyze a subset of animals that completed more
tasks. The rationale for not doing so is not persuasive.
Again, I think that there is enough data here to warrant publication. I would hope that the
authors would do one more revision of the presentation of their data in order to address some of
the above concerns.
label_end_comment
Decision letter (RSOS-171919.R1)
01-May-2018
Dear Dr van Horik:
On behalf of the Editors, I am pleased to inform you that your Manuscript RSOS-171919.R1
entitled "Intra-individual variation in performance on novel variants of similar tasks influences
single factor explanations of general cognitive processes" has been accepted for publication in
Royal Society Open Science subject to minor revision in accordance with the referee suggestions.
Please find the referees' comments at the end of this email.
The reviewers and Subject Editor have recommended publication, but also suggest some minor
revisions to your manuscript. Therefore, I invite you to respond to the comments and revise your
manuscript.
• Ethics statement
If your study uses humans or animals please include details of the ethical approval received,
including the name of the committee that granted approval. For human studies please also detail
whether informed consent was obtained. For field studies on animals please include details of all
permissions, licences and/or approvals granted to carry out the fieldwork.
15
• Data accessibility
It is a condition of publication that all supporting data are made available either as
supplementary information or preferably in a suitable permanent repository. The data
accessibility section should state where the article's supporting data can be accessed. This section
should also include details, where possible of where to access other relevant research materials
such as statistical tools, protocols, software etc can be accessed. If the data has been deposited in
an external repository this section should list the database, accession number and link to the DOI
for all data from the article that has been made publicly available. Data sets that have been
deposited in an external repository and have a DOI should also be appropriately cited in the
manuscript and included in the reference list.
If you wish to submit your supporting data or code to Dryad (http://datadryad.org/), or modify
your current submission to dryad, please use the following link:
http://datadryad.org/submit?journalID=RSOS&manu=RSOS-171919.R1
• Competing interests
Please declare any financial or non-financial competing interests, or state that you have no
competing interests.
• Authors’ contributions
All submissions, other than those with a single author, must include an Authors’ Contributions
section which individually lists the specific contribution of each author. The list of Authors
should meet all of the following criteria; 1) substantial contributions to conception and design, or
acquisition of data, or analysis and interpretation of data; 2) drafting the article or revising it
critically for important intellectual content; and 3) final approval of the version to be published.
All contributors who do not meet all of these criteria should be included in the
acknowledgements.
We suggest the following format:
AB carried out the molecular lab work, participated in data analysis, carried out sequence
alignments, participated in the design of the study and drafted the manuscript; CD carried out
the statistical analyses; EF collected field data; GH conceived of the study, designed the study,
coordinated the study and helped draft the manuscript. All authors gave final approval for
publication.
• Acknowledgements
Please acknowledge anyone who contributed to the study but did not meet the authorship
criteria.
• Funding statement
Please list the source of funding for each author.
Please note that we cannot publish your manuscript without these end statements included. We
have included a screenshot example of the end statements for reference. If you feel that a given
heading is not relevant to your paper, please nevertheless include the heading and explicitly state
that it is not relevant to your work.
Because the schedule for publication is very tight, it is a condition of publication that you submit
the revised version of your manuscript within 7 days (i.e. by the 10-May-2018). If you do not
think you will be able to meet this date please let me know immediately.
To revise your manuscript, log into https://mc.manuscriptcentral.com/rsos and enter your
16
Author Centre, where you will find your manuscript title listed under "Manuscripts with
Decisions". Under "Actions," click on "Create a Revision." You will be unable to make your
revisions on the originally submitted version of the manuscript. Instead, revise your manuscript
and upload a new version through your Author Centre.
When submitting your revised manuscript, you will be able to respond to the comments made by
the referees and upload a file "Response to Referees" in "Section 6 - File Upload". You can use this
to document any changes you make to the original manuscript. In order to expedite the
processing of the revised manuscript, please be as specific as possible in your response to the
referees.
When uploading your revised files please make sure that you have:
1) A text file of the manuscript (tex, txt, rtf, docx or doc), references, tables (including captions)
and figure captions. Do not upload a PDF as your "Main Document".
2) A separate electronic file of each figure (EPS or print-quality PDF preferred (either format
should be produced directly from original creation package), or original software format)
3) Included a 100 word media summary of your paper when requested at submission. Please
ensure you have entered correct contact details (email, institution and telephone) in your user
account
4) Included the raw data to support the claims made in your paper. You can either include your
data as electronic supplementary material or upload to a repository and include the relevant doi
within your manuscript
5) All supplementary materials accompanying an accepted article will be treated as in their final
form. Note that the Royal Society will neither edit nor typeset supplementary material and it will
be hosted as provided. Please ensure that the supplementary material includes the paper details
where possible (authors, article title, journal name).
Supplementary files will be published alongside the paper on the journal website and posted on
the online figshare repository (https://figshare.com). The heading and legend provided for each
supplementary file during the submission process will be used to create the figshare page, so
please ensure these are accurate and informative so that your files can be found in searches. Files
on figshare will be made available approximately one week before the accompanying article so
that the supplementary material can be attributed a unique DOI.
Please note that Royal Society Open Science will introduce article processing charges for all new
submissions received from 1 January 2018. Charges will also apply to papers transferred to Royal
Society Open Science from other Royal Society Publishing journals, as well as papers submitted
as part of our collaboration with the Royal Society of Chemistry
(http://rsos.royalsocietypublishing.org/chemistry). If your manuscript is submitted and
accepted for publication after 1 Jan 2018, you will be asked to pay the article processing charge,
unless you request a waiver and this is approved by Royal Society Publishing. You can find out
more about the charges at http://rsos.royalsocietypublishing.org/page/charges. Should you
have any queries, please contact openscience@royalsociety.org.
Once again, thank you for submitting your manuscript to Royal Society Open Science and I look
forward to receiving your revision. If you have any questions at all, please do not hesitate to get
in touch.
Kind regards,
Andrew Dunn
Royal Society Open Science
openscience@royalsociety.org
17
on behalf of Dr Antonia Hamilton (Subject Editor)
openscience@royalsociety.org
Reviewer comments to Author:
Reviewer: 1
Comments to the Author(s)
Relative to the prior submission, this revised manuscript is greatly improved, and in many
instances, the authors have been responsive to the prior comments by the editor and reviewer. In
particular, the statistics are now more clearly described and succinct, and the overall discussion is
more clear (and the results are more clearly communicated). Nevertheless, a number of concerns
still exist that lessen my overall enthusiasm and confidence in the conclusions. However, the
authors are sufficiently cautious in their interpretation and discussion of the limitations of their
data, and thus I generally support publication of the manuscript. My remaining concerns are
summarized below.
The general tenor of the authors’ responses is that this is a large and complex analysis (which it
is), and thus it was necessary to compromise the procedures and analyses in order to complete
the experiment. They often note that “other people do it too”. While I’m sympathetic, sometimes
the virtue of laboratory studies is that conclusive answers are at least possible when compromises
are minimized. Here are the problems:
1. The n is simply too small. While hundreds of birds started the study, only 31 completed all 9
tests. The authors argue that many comparable studies use fewer tests, making it easier to test
more subjects. Similarly, the authors note that many studies are published with a number of
subjects comparable to what they report. I don’t find the argument that other people have
published studies based on inadequate numbers very compelling. In the present case, 31 subjects
and nine tests yields a three-to-one ratio of subjects-to-variables, which is universally considered
too small to support a stable factor analysis. Even more importantly is the reason that the final n
is so small. In this study, the subjects could self-select for inclusion. This demands that the final
subject pool is somehow different that the larger population. This restricted sample may not
contain enough variability to ever reveal a general factor. The authors respond that they have
minimized any discussion of implications of their data for the existence of a general factor, but
even in the abstract, the absence of a general factor appears as a focus. This needs work.
2. The authors say that tasks were chosen “because they were available”. Since some of the tasks
have no obvious dependence on any cognitive process, it is not surprising that they don’t all load
on a cognitive factor. Again, the authors argue that others doing similar work sometimes
“incorporate tasks available regardless of how well they capture the full breadth of general
intelligence”. Again, this is a weak rationale, and I don’t think tasks’ availability makes them
appropriate for assessing COGNITIVE ability.
3. All tasks are motivated by food. The authors say that since they are not claiming the existence
of a cognitive factor, “we do not consider this to be an issue”. First, this is a circular argument,
and second, the birds may have self-selected for inclusion based on their motivation for food
rewards.
4. Indeed it is unnatural to singly-house social animals. But, when they are singly housed they
are ALL being exposed to the same conditions. This is NOT the case when they are group
housed such that dominance hierarchies (and related differential stress responses) arise. If there
are problems with either method, we should err on the side of treating all animals similarly,
18
particularly if it is our intention to study individual differences. Again, dominant animals may be
the ones that self-select for inclusion, and this may underlie the absence of sufficient variability to
reveal a general factor.
5. I still maintain that many of the tasks are inaccurately described. For instance, the “spatial”
tasks have no reliance on spatial cues and are most likely solved through egocentric navigation.
In defense, the authors state that “there is a precedent” for calling them spatial tasks, and
furthermore, that the animals improved across trials, “suggesting that learning was involved”.
I’m not disputing that these are learning tasks, and it may well be that others have incorrectly
described them. This does not justify their being described as spatial in nature. We’d all be better
off if we spoke a common language.
6. I still think that it would have been useful to analyze a subset of animals that completed more
tasks. The rationale for not doing so is not persuasive.
Again, I think that there is enough data here to warrant publication. I would hope that the
authors would do one more revision of the presentation of their data in order to address some of
the above concerns.
Author's Response to Decision Letter for (RSOS-171919.R1)
See Appendix B.
label_end_comment
Decision letter (RSOS-171919.R2)
06-Jun-2018
Dear Dr van Horik,
I am pleased to inform you that your manuscript entitled "Intra-individual variation in
performance on novel variants of similar tasks influences single factor explanations of general
cognitive processes" is now accepted for publication in Royal Society Open Science.
You can expect to receive a proof of your article in the near future. Please contact the editorial
office (openscience_proofs@royalsociety.org and openscience@royalsociety.org) to let us know if
you are likely to be away from e-mail contact. Due to rapid publication and an extremely tight
schedule, if comments are not received, your paper may experience a delay in publication.
Royal Society Open Science operates under a continuous publication model
(http://bit.ly/cpFAQ). Your article will be published straight into the next open issue and this
will be the final version of the paper. As such, it can be cited immediately by other researchers.
As the issue version of your paper will be the only version to be published I would advise you to
check your proofs thoroughly as changes cannot be made once the paper is published.
19
In order to raise the profile of your paper once it is published, we can send through a PDF of your
paper to selected colleagues. If you wish to take advantage of this, please reply to this email with
the name and email addresses of up to 10 people who you feel would wish to read your article.
Please note that Royal Society Open Science will introduce article processing charges for all new
submissions received from 1 January 2018. Charges will also apply to papers transferred to Royal
Society Open Science from other Royal Society Publishing journals, as well as papers submitted
as part of our collaboration with the Royal Society of Chemistry
(http://rsos.royalsocietypublishing.org/chemistry).
If your manuscript is newly submitted and subsequently accepted for publication after 1 Jan 2018,
you will be asked to pay the article processing charge, unless you request a waiver and this is
approved by Royal Society Publishing. Manuscripts originally submitted prior to 1 Jan 2018 will
not subject to a charge, even if they are accepted in 2018. You can find out more about the charges
at http://rsos.royalsocietypublishing.org/page/charges. Should you have any queries, please
contact openscience@royalsociety.org.
On behalf of the Editors of Royal Society Open Science, we look forward to your continued
contributions to the Journal.
Kind regards,
Thadcha Retneswaran
Royal Society Open Science
openscience@royalsociety.org
on behalf of Prof. Antonia Hamilton (Subject Editor)
openscience@royalsociety.org
Appendix A
Dear Editor,
Thank you for the helpful reviews of our manuscript by yourself and the two referees. We
have now had a chance to address all of them and are pleased to resubmit our revised MS
for your consideration. Our responses are detailed below after each comment. We look
forward to your decision.
Yours truly,
Jayden van Horik and co-authors.
28-Feb-2018
Due 23 March
Associate Editor's comments:
I am particularly concerned about the issue of sample size with only 31 complete datasets. If
it is possible to collect more data, I strongly recommend doing so. Other helpful comments
are detailed by the reviewers below.
JvH: We agree that it seems a shame that data on so many individuals had to be discarded.
However, this is probably an inevitable consequence of working with animals that engage with
tests of their own volition, and which have to be tested at a certain stage in their lives. All our
chicks were tested at exactly the same ages and after exactly the same prior experiences. This
constriction does not apply to studies that collect their data in a much less controlled manner –
for example testing primates in zoos at any time, over any duration with no constraints on
prior experiences, needs to enforce repeated testing, or even testing order. Such studies face
their own problems in interpretation and thus should be treated with some caution. In a similar
way, our study faces issues of its own and we agree that our findings should also be treated
with caution because of this. We now make this explicit in our Discussion.
In terms of comparable sample sizes, we draw your attention to other recent bird studies that
deployed multiple tests to explore “g”. As you can see, none attempted as many different tests
as us – we used 9 – and all had comparable sample sizes to ours. Isden et al. 2013 tested 14
individual (males) with 6 tasks; Shaw et al. 2015 tested 16 individuals with 6 tasks; Ashton et al
2017 tested 42 individuals with 4 tests; Boogert et al. 2011 tested 22 (males) with 4 tasks;
Dubois et al. (2018) tested 49 (males) with 5 tests. Therefore, we hope that you and readers can
comprehend the challenges faced when deploying cognition tests on free-acting birds and the
inevitable attrition caused by a single rejection of one trial in the test series. To set it in context,
our birds were expected to engage >200 times with a test apparatus. Any single non-
participation was sufficient for us to exclude them.
Crucially, if our exclusions had led to a strong “g” factor emerging, then perhaps you could
have rightly been sceptical that we were only testing ‘super’ individuals who were highly
motivated and had a conspicuous general intelligence. We agree that in such cases, our tight
exclusion criterion might be suspect and we could justify relaxing our criteria or striving to gain
a more representative sample. However, we did not find such a striking g factor, despite our
highly conservative approach. Therefore, we do not believe that our (relatively) small sample
size detracts from our findings, but rather that our rigorous consideration of data to be
included adds strength to our study and confidence in our results.
Comments to Author:
Reviewers' Comments to Author:
Reviewer: 1
Comments to the Author(s)
The authors assess a general factor and domain factors that contribute to the performance
of pheasant chicks on nine nominally cognitive tasks that represent four domains of
ability. No general influence performance was identified, and when random combinations of
the tasks were subjected to factor analysis, 14 combinations generated significant main
factors. The authors conclude that there is no domain-specific or domain-general influence
on their animals’ performance. Based on this analysis, the authors argue that previous
identifications of general influences on the cognitive performance of non-human animals
may be an artifact of the composition of the test battery.
This is a timely study and it represents a fairly extensive analysis. However, there are some
real problems with the discussion/interpretation of the results as well as the manner in which
the data was collected and analyzed. I cannot recommend publication of the present
manuscript, although it might be acceptable following a major revision.
1. My first concern is a general one regarding the authors’ conclusion that previous
identifications of general influences on the cognitive performance of non-human animals may
be an artefact of the composition of the test battery. Of course this is true, as it is true for
human test battery performance. For instance, the WAIS is not comprised of a random
selection of tasks. The test was originally conceived and subsequently revised to include only
tasks that load on a general factor. This is based on the assumption that a general influence
exists and the test battery should be sensitive to it. In fact, independent analyses that include
many other tasks (not included in the WAIS itself) have rarely found tasks that DO NOT load
on a principal factor. But of course depending on how the task is administered and what the
task tests, it is possible to find tasks that do not load on the principal factor. But so
what? What is impressive about tests like the WAIS is that so many tasks that nominally have
nothing in common DO load on a principal factor. This result demands the existence of a
general factor.
The present study uses tasks that in some instances have only minimal cognitive requirements
(e.g., the robo worm, which is primarily a test of coordination). Why should we expect such
tasks to load on a general COGNITIVE factor? Generally, I think that the authors’ conclusion is
not warranted by the actual data that they present.
JvH: The authors agree with the reviewer’s point, that the array of cognitive tasks included in
human test batteries (i.e. WAIS) are included because they load positively on a single factor, and
hence support the notion of “g”. However, tasks available for testing cognition in animals are
much more limited compared with those available in human studies. Moreover, the few studies
that adopt test batteries in the animal cognition literature tend to incorporate all tasks available
regardless of how well they capture the full breadth of ‘general’ intelligence. In our, admittedly
exploratory, test battery, we include tasks that have previously been used to assess cognitive
processes in other species of birds. Crucially, we highlight that there is little consistency in
individual performances across novel variants of similar tasks. These inconsistencies therefore
have important implications on whether or not a significant single factor can be identified.
To address the reviewer’s concerns, we have revised our claims that previous identifications of
general influences on the cognitive performance of non-human animals may be an artefact of
the composition of the test battery. We instead adopt a more cautious approach by stating that
inter-individual variation in performance on similar tasks may result in an under representation
of ‘g’. Hence future studies may benefit by presenting multiple task exemplars probing the same
cognitive domain to bolster their ability to capture ‘g’ in animals.
2. IF a general factor was identified in this study, it would be impossible to conclude that it is
a general influence on COGNITIVE ability. ALL tasks that are used in this study are motivated
by the same food reinforcer. To help interpret the general factor, tasks should have been
developed and included which were motivated by other drives. With the present composition
of the test battery, it would be impossible to distinguish a general cognitive factor from a
general MEAL WORM MOTIVATION factor. The later might be interesting, but not given the
intent of this study.
JvH: The authors agree with the reviewer’s comments, however, given that we are not claiming
that ‘g’ underlies pheasants cognitive performance, we do not consider this to be an issue.
Moreover, there is a precedent for using appetitive tasks with food rewards in the animal
cognition literature. All previous studies of g in non-human animals use food rewards apart from
two studies of bowerbirds in which access to decorations (a unique situation amongst animals)
was used as stimuli in one component of a test battery. We also note that in humans, there is
not a range of rewards for test batteries. Indeed, many such studies offer no tangible reward,
such that these results, according to the reviewer, would tell us little about cognition. We agree
that careful consideration should be given to how task performance is motivated and encourage
future work to explore this – for example offering different reward types for performance in
identical tasks.
In acknowledgement of the reviewer’s concerns, we have added the following section in the
discussion: A further limitation to this study is that all tasks were appetitive in nature. Hence, if
domain general processes were revealed, it may be difficult to dissociate them from a general
motivation to acquire mealworms. Future studies may therefore benefit from including tasks in
which performance was mediated by other reinforcers, such as navigating an arena to access
social rewards.
3. The study began with 200 animals, but only 31 completed all tests in the battery. This is
because animals could self-select (“volunteer”) for inclusion in every test. This is problematic
for a bunch of reasons, not the least of which is that the final 31 are NOT representative of the
population. If the range of individuals is restricted (by some influence on the likelihood of
participating), then there would be insufficient variation in the sample to ever detect a general
(or domain-specific) influence. Also, the animals were not individually housed (apparently
being housed in groups as large as 50), and animals’ position in dominance hierarchies are
known to impact cognitive performance.
The authors suggest in their rationale for this experiment that many prior studies have used
inadequate samples of only 20 subjects (although many have used far more). Is 31 really that
much better than 20? It is also noted that in the present study, the ratio of variables to sample
size is considered too small to produce a stable factor analysis (which is seemingly true of the
present study, given the number of factors identified with random combinations of tasks). I
think a separate analysis is warranted in which a larger sample of birds is included based on
their participation in more [JvH: fewer?] tasks. Only 31 completed ALL task, but I would assume
that many more completed a subset of tasks. The authors should sacrifice the inclusion of
some tasks in order to obtain a larger sample on SOME tasks. If this analysis is consistent with
what the authors presently report, I would be more inclined to recommend
publication. Alternatively, it might suggest a different interpretation of the data. This sub-
analysis is imperative if this paper is to be published.
JvH: Please see our response to the Editor above. As we explain, we agree that the level of attrition
is unsatisfactory, but we do not consider it critical to the integrity of the interpretation of the
results as explained above.
The reviewer’s claim that dominance hierarchies may also influence cognitive performance is
also difficult to avoid, as these constraints presumably interfere with interpretations of
performances of animals housed in any captive situation. Solitary confinement or indeed any
unnatural housing conditions during development – inevitable in captive populations - may also
be unnatural and present difficulties in interpreting performance. The alternative, to test only
wild-living animals, brings its own set of problems such as differential parental effects,
environmental confounds and differential, uncontrolled prior experiences that affect cognitive
biases and performances.
We decline the reviewer’s suggestion to reanalyse the dataset, as to improve the sample size at
the cost of reporting fewer tasks. The reason for this is that the intention of this MS is not to show
whether or not pheasants possess capacities for “g”, but rather that interpretations of “g” are
influenced by intra-individual variation in performance on novel variants of the same cognitive
task. This is a fundamental issue in animal cognition as cognitive tasks are rarely repeated within
individuals and hence the stability of particular cognitive abilities is unclear. We have amended
the title of the MS to clarify this point.
3. The tasks that are described as being representative of spatial memory make no cognitive
demads on spatial processing. They can obviously be solved with no reliance on spatial cues
(and in fact, are probably solved by simple place or route learning).
JvH: We justify the use of these tasks as there is already a precedent for them being used as an
indication of spatial ability in the literature. We also discuss the limitations of interpretations of
performance on tasks included in this study. Performances on the spatial memory tasks improved
across trials, suggesting processes of learning were involved. We accept that our task cannot
confirm the precise cognitive mechanisms by which the task is solved and agree that this may
involve place or route learning rather than spatial cues per se, but these are still elements
involved in the comprehension and utilisation of spatial information.
I think that this paper is a good start. However, given the limitations on the subject pool and
the composition of the test battery, I’m not sure that the data is meaningful or correctly
interpreted. This experiment was obviously a lot of work. Nevertheless, the selection/inclusion
of subjects seriously clouds the interpretation of the data. This might be resolved by some
additional analyses (as described above).
Reviewer: 2
Comments to the Author(s)
The authors present the results of a test battery of nine tasks for 31 pheasant chicks. Overall,
an initial Principal Axis Factor (PAF) analysis found little support for a general intelligence factor
underlying test battery performance in all nine tasks. However, further PAF analyses of all
possible combinations of a subset of six out of the nine test battery tasks revealed that some
combinations produce single factor solutions that are analogous to those that have previously
been taken as evidence for a general intelligence factor in other non-human test battery
studies. This paper raises important concerns in terms of the design, analysis and interpretation
of test battery data and how such factors may affect our current understanding of the structure
of intelligence in non-human species. As such, it makes both a timely and important
contribution to this rapidly growing field.
Overall the paper is well written and clearly presents the rationale, design and interpretation
of the results, citing relevant and recent literature. However, I do have a number of suggestions
and clarifications which I hope will help to increase the impact and relevance of the paper for
other researchers in this area.
My main issue with the study is that it substantially differs from previous non-human test
battery research in terms of both the learning criteria used for each task and the approach
used for the factor analyses. As such, it is at times difficult to ascertain how directly comparable
the results of this study are to the previous work in this field. From my reading of the paper, I
understand that not all tasks had a specified learning criterion, but instead they measure the
individual’s accuracy/latency (e.g. the paper puncture, roboworm, spatial memory and detour
reach tasks), or the (predicted?) error rate in the tasks with a fixed number of trials (e.g. colour
discrimination/reversal and spatial learning tasks). It would be interesting to know whether
these task performance measures would ultimately correlate with performance that is
measured up to a specified learning criteria in each task (although this is unlikely to be possible
in the current study). At the very least, in the discussion the authors should attempt to address
how such differences in methodology may have impacted the PAF results (e.g. could this
account for the large intra-individual variation in task performances?), or limit comparisons
with previous research.
JvH: We used learning curves rather than training to criterion, due to practical reasons as this
procedure was more appropriate for assessing performances in a particularly large number of
individuals on a broad variety of tasks in a consistent manner. (If one trains to criteria then by
necessity every individual experiences a different set of training events – some may only engage
with the test apparatus a few times while others may engage many hundreds of times. This is
problematic when individuals are to be tested on multiple tasks in which case their performances
may be shaped by their prior experiences. One solution to this is that all individuals may be
made to reach criteria, with those reaching it continuing to the tested to ensure constant
experience. However, this approach risks overtraining and if a single individual ‘fails’ to meet the
criteria then testing persists indefinitely. There is an additional risk in training to criteria when
studying large groups of individuals. Accepting criteria of 7 consecutive correct choices in a
binary (50/50) task – a common criteria – means that there is a 1/128 chance of an individual
getting it correct by chance in the first 7 encounters. In a test sample of 200, then we expect that
at least one individual will exhibit ‘perfect’ learning purely by chance. It would be impossible to
separate such chance events from ‘real’ learning. This makes interpreting criteria based learning
tasks deployed on large samples problematic.
We used factor analysis because it standardises individual variation in performance within each
task so that performances are comparable across tasks. Therefore, even though e.g. Roboworm
and Learning tasks produce very different forms of data, PAF controls for variation across tasks
to make individual differences relative to one another. Please see our specific response about use
of PAF below.
I also think it would be very useful if the authors could elaborate on why they chose to use
PAF, rather than Principal Component Analysis (PCA). PCA is the approach most commonly
used in other non-human studies (e.g. see Table 1 in Shaw & Schmelz, 2017), thus it would
increase the contribution of the current paper if the authors could make a case for why their
chosen approach is preferable for the sample sizes and/or types of tasks typically used in non-
human studies. If PAF is indeed preferable to PCA, then it would be useful to strongly advocate
for its adoption by other non-human test battery researchers.
JvH: the authors have included the following justification of PAF over PCA in the Statistical
Analysis section:
“The decision to use Principal Axis Factoring (PAF), rather than Principal Components Analysis
(PCA) is because PAF is more appropriate for investigating the latent structure of cognitive
abilities (see Bryant & Yarnold, 1995). PCA is a descriptive technique that can be used to simplify
interpretations of relationships between a large number of variables (performances on different
tasks) of unknown relationships. PCA analyses all the variance from each variable and adopts a
two-directional approach to predict variables by components and vice versa. By contrast, PAF is
an exploratory technique in which causal relationships between variables are assumed, which
should hence load on to the same factor. PAF is a modelling method that analyses only shared
variance between variables (i.e. leaves out unique variance), and hence latent factors steer
observed variables one-directionally. PAF can therefore be used to illuminate whether the same
cognitive ability underlies performance on different tasks that are presumed to be governed by
the same cognitive process. For example, the same cognitive ability may be considered to
underlie performance on multiple variants of tasks that use different colours assess
discrimination learning. Conversely, PCA could be used instead to reduce/simplify performances
obtained from a number of different colour discrimination tasks, so that one variable represents
an ability to discriminate between different colours. “
Bryant, F. B., & Yarnold, P. R. (1995). Principal-Components Analysis and Exploratory and
Confirmatory Factor Analysis. In L. G. Grimm & P. R. Yarnold (Eds.), Reading and
Understanding Multivariate Statistics (pp. 99–136). Washington, DC, US: American
Psychology Association.
Minor comments:
In the title “intra-individual variation” is ambiguous. Please clarify that this refers to variation
in task performance.
JvH: the authors have amended the title accordingly
P. 4, lines 35-40: “test may be excluded...” is ambiguous. Are the authors suggesting here that
previous studies may have purposefully excluded tasks from a test battery if they did not
support a single factor solution? To me, while it is certainly plausible, this seems like a serious
accusation of data ‘cherry picking’, which does not have any direct support in the paper. If this
was not the authors’ intention, I would suggest re-phrasing this sentence.
JvH: We agree that this was worded somewhat ambiguously. First, it referred specifically to
human tests – we are not implying that animal researchers (or human researchers) are cherry-
picking data AFTER the event, but rather that human studies presume that a ‘general factor’
exists, and only include tasks that are sensitive to this. By contrast, animal studies have a limited
selection of tasks available to use and tend to include all different tasks when investigating “g”.
We have amended the text according to the reviewer’s suggestions as follows:
“In humans, the construction of test batteries may assume the existence of a general factor, and
hence exclusively include tests that are sensitive to it (Wechsler, 1997). However, if all tests
included in a battery use the same test paradigm, or rely on a single underlying process, such as
associative learning, then the presence of a ‘g’ may be overstated or miss-ascribed to domain
specific processes”.
P. 4, line 55: The term “biomechanically plausible” is not immediately clear and I would suggest
re-phrasing.
JvH: the authors have elaborated for clarity: “(e.g. a rat or pigeon can press or peck a lever
respectively, whereas a jellyfish may have more difficulty in performing the same action).”
P. 5, lines 9-12: It is arguable that other non-human studies have explored how test battery
composition affects estimates of ‘g’, but not as the main focus of the paper. For example, both
Shaw et al. 2015 and Herrman & Call 2012 present factor analyses for subsets of the tasks used
in their study. I would therefore suggest re-phrasing or removing this statement.
JvH: Thanks. We have elaborated on this section according to the reviewers suggestions: “The
relationship between ‘g’ and test battery composition has been explored in some non-human
animals, for example in North Island robins (Shaw et al., 2015) and apes (Herrmann & Call,
2012), but requires further investigation.”
P. 5, lines 28-35: This summarises sample sizes in previous studies, please also state the sample
size in the current study for comparison (this could be included in the following sentence on
line 37).
JvH: the following sentence has been included according to the reviewer’s suggestions: “In the
current study, we assess performances of 31 pheasants.” We have also included two new studies
that have been published since our first submission.
In the methods it would be useful to state the cognitive skills being assessed by the ‘paper
puncture’ and ‘robo-worm’ tasks.
JvH: We have elaborated on the cognitive underpinnings considered associated with performance
on these tasks as suggested by the reviewer as follows:
Paper Puncture: “… and New Zealand robins (Shaw et al., 2015), [to capture performance on a
task previously used to assess motor-cognition], by determining how proficient individuals were
when extracting a concealed mealworm food reward…”
Robo-Worm: “ We further assessed motor-cognition by attaching a freshly killed mealworm to
the second-hand…”
It appears that methods for the two spatial tasks have been moved in front of the colour
learning/reversal task descriptions at some point during revisions on the paper, without
appropriate edits being made. As such, they are now lacking in sufficient detail, as they refer
the reader to the ‘above’ colour discrimination problems (line 5, p. 9). Please add relevant
details, or consider revising the order of the methods.
JvH: Thanks for alerting us to that. We have now relocated the methods for the two spatial tasks
so that they appear after the methods for the colour discrimination and reversal tasks.
P. 10, lines 36-41: The authors argue that giving individuals a fixed number of trials is an
ecologically valid approach for the discrimination task before the reversal trials, as ephemeral
fluctuations in food in the wild mean that pheasants may experience natural, unpredictable
switches in reinforcement as food resources fluctuate. However, as it is currently presented
this is not a comparable analogy to the discrimination and reversal tasks, as it seems more
likely that for most natural food resources the cue associated with the food would also
disappear as the source is depleted (rather than birds frequently encountering a cue associated
with food that remains present even once the food is no longer available). Please consider
refining this analogy, or removing it.
JvH: the authors have removed the analogy according to the reviewer’s suggestions. Please see
our response above about our concerns over using a criteria based rather than fixed trials testing
schedules.
Please ensure that the performance measure is clearly presented for each task. For example,
in tasks where logistic curves were used to obtain a ‘predicted trial number’ at which the bird
was performing with 80% accuracy, was this how all birds performance scores were obtained,
or did some birds reach this criteria within the 50 trials of each of the four discrimination
problems, hence their score is not a predicted value?
JvH: To confirm – all predicted trial values were exactly that and no bird was stopped after
reaching a criteria. The methods for each task have been revised according to the reviewer’s
suggestions so that the response measures for each task are more clear.
P.12, Table 1: Please include a description in the table heading of what is meant by the ‘9+18’
and ‘4+1’ in the table.
JvH: We have elaborated on the figure caption according to the reviewer’s suggestions as follows:
“The number of trials that each bird participated in were used to determine their performance
measures (Trials). Trials in parentheses are training trials that preceded test trials and were not
used to determine performance“.
P.12, lines 49-51: Please clarify what is meant by an ‘ANOVA with improvement’
JvH: We have amended this section for clarity as follows: “All statistical analyses were conducted
in SPSS (IBM Corp, 2013). We used Repeated Measures ANOVA to determine if subjects’
performances improved across trials and hence learned each task. Improvement was considered
the proportion of correct choices in the first 10-trial session and the final 10-trial session of a
given task. Improvement and Task (Colour learning, Reversal Learning and Spatial Learning
tasks) were included as factors in the analysis.”
P. 13, lines 7-12: It is stated that only detour reach, robo-worm and spatial memory were
inversely transformed, as smaller values were associated with better performance. Perhaps I
am missing something, but isn’t this true for all other task performance values (except ‘paper
puncture’?) as well, where fewer predicted trials to reach the 80% success rate (e.g.
discrimination, reversal and spatial memory tasks) is also associated with better performance
in the task? Apologies if I am misinterpreting something here, but I would again suggest that
the authors ensure that performance measures for each task are clearly stated throughout the
methods.
JvH: Thanks for that suggestion. We have amended the Statistical Analysis section for clarity
according to the reviewer’s suggestions.
P. 17, lines 48-52: Please give a justification for why six tasks was chosen as the number of
tasks to include in the subset analyses.
JvH: We have included the following justification as suggested by the reviewer:” We chose to
investigate the relationships between six tasks as this provided the largest possible sample size
while retaining a reasonably broad selection of different cognitive abilities.”.
P. 21, line 37: “explanation cognitive processes” - is a word missing here?
JvH: amended accordingly
P. 27, line 3: “ensure” should replace “insure”
JvH: amended accordingly
P. 27, line 26: Delete “do” from “we cannot conclude”
JvH: amended accordingly
Data supplement:
Please make it clear in the methods or results that data are available in the supplementary
material.
In the supplementary data file, the data column headers are unclear. Also, all data relevant to
the analyses described in the paper has not been made available (as is required by this journal).
It appears that only the transformed overall performance scores for each individual in each
task have been provided.
JvH: We report that the data is available as supplementary materials in the Data Availability
Section following the Discussion. The data headers have now been amended for clarity as
suggested by the reviewer.
Review provided by Dr Rachael Shaw, School of Biological Sciences, Victoria University of
Wellington
Appendix B
Reviewer comments to Author:
Reviewer: 1
Comments to the Author(s)
Relative to the prior submission, this revised manuscript is greatly improved, and in many instances,
the authors have been responsive to the prior comments by the editor and reviewer. In particular,
the statistics are now more clearly described and succinct, and the overall discussion is more clear
(and the results are more clearly communicated). Nevertheless, a number of concerns still exist that
lessen my overall enthusiasm and confidence in the conclusions. However, the authors are
sufficiently cautious in their interpretation and discussion of the limitations of their data, and thus I
generally support publication of the manuscript. My remaining concerns are summarized below.
The general tenor of the authors’ responses is that this is a large and complex analysis (which it is),
and thus it was necessary to compromise the procedures and analyses in order to complete the
experiment. They often note that “other people do it too”. While I’m sympathetic, sometimes the
virtue of laboratory studies is that conclusive answers are at least possible when compromises are
minimized. Here are the problems:
1. The n is simply too small. While hundreds of birds started the study, only 31 completed all 9
tests. The authors argue that many comparable studies use fewer tests, making it easier to test
more subjects. Similarly, the authors note that many studies are published with a number of
subjects comparable to what they report. I don’t find the argument that other people have
published studies based on inadequate numbers very compelling. In the present case, 31 subjects
and nine tests yields a three-to-one ratio of subjects-to-variables, which is universally considered too
small to support a stable factor analysis.
JvH: We agree that the study is constrained by a small sample size and that the ratio of subjects to
tasks is low. Indeed, we highlight these issues in the Discussion:
“We report findings based on analyses of 31 subjects on nine tasks, providing a ratio of under four. A
more representative sample of 45-90 subjects, yielding a ratio of 5:1-10:1 would be more
appropriate for such analyses.”
We also emphasise this limitation in our Discussion and suggest the findings should be interpreted
with caution.
To address the reviewer’s concern, we have amended the Discussion to clarify that analyses on the 6
tasks provide a ratio greater than 5:1, which is at the minimal requirements for factor analyses:
“…findings based on analyses of 31 subjects on nine tasks, providing a ratio of under four, are likely
to be constrained by low power. A more representative sample of 45-90 subjects, yielding a ratio of
5:1-10:1 would be more appropriate for such analyses. However, analyses of performances on the 6
tasks provide a ratio greater than 5, which remains acceptable.”
Even more importantly is the reason that the final n is so small. In this study, the subjects could self-
select for inclusion. This demands that the final subject pool is somehow different that the larger
population. This restricted sample may not contain enough variability to ever reveal a general
factor. The authors respond that they have minimized any discussion of implications of their data
for the existence of a general factor, but even in the abstract, the absence of a general factor
appears as a focus. This needs work.
JvH: we have acknowledged the reviewers concerns in the seventh paragraph of the Discussion,
highlighting the issue that a small sample size may comprise a biased subsample of the population.
We disagree with the reviewer that the absence of a general factor is a focus of the abstract. The
absence of a general factor is mentioned in a single sentence: “…we found no evidence to suggest
that a single factor encompassed a diverse range of cognitive abilities in pheasants”. Conversely, we
argue that the focus of the abstract is on intra-individual variation in performance on similar tasks,
as is reflected in the title of the MS.
2. The authors say that tasks were chosen “because they were available”. Since some of the tasks
have no obvious dependence on any cognitive process, it is not surprising that they don’t all load on
a cognitive factor. Again, the authors argue that others doing similar work sometimes “incorporate
tasks available regardless of how well they capture the full breadth of general intelligence”. Again,
this is a weak rationale, and I don’t think tasks’ availability makes them appropriate for assessing
COGNITIVE ability.
JvH: We agree with the reviewer and acknowledge the limitations of tasks available, particularly
when assessing cognition in birds and when testing such a large number of individuals.
We have attempted to alleviate some of the reviewers concerns in the Discussion by clarifying that
the cognitive underpinnings of some tasks may be unclear:
“Although, we show improvements in performances on all tests with multiple trials at the population
level, the tests we used may, for example, have lacked ecological validity, been too difficult, or the
cognitive underpinnings unclear. Moreover, our findings may also be due to a relatively high
contribution of non-cognitive influences (or simply noise) that we did not or cannot identify.”
3. All tasks are motivated by food. The authors say that since they are not claiming the existence of
a cognitive factor, “we do not consider this to be an issue”. First, this is a circular argument, and
second, the birds may have self-selected for inclusion based on their motivation for food rewards.
JvH: We agree with the reviewer, although cognitive performance on non-appetitive tasks are also
constrained by extrinsic factors that vary between and within individuals, such as stress, or fear
responses. We have attempted to address the reviewers concerns by amending the Discussion as
follows:
“The alternative to our use of voluntary participation with its attendant attrition of subjects is to
obtain performance measures from subjects’ forced participation on non-appetitive tasks. However,
while such tasks may illuminate whether performance is related to food motivation, such procedures
may also confound results, as stress (e.g. induced by forcing participation) can have detrimental
influences on cognitive performance (60). Forced participation may also be difficult to achieve in
studies conducted in the wild, making comparisons between forced and appetitive procedures
difficult to interpret.”
4. Indeed it is unnatural to singly-house social animals. But, when they are singly housed they are
ALL being exposed to the same conditions. This is NOT the case when they are group housed such
that dominance hierarchies (and related differential stress responses) arise. If there are problems
with either method, we should err on the side of treating all animals similarly, particularly if it is our
intention to study individual differences. Again, dominant animals may be the ones that self-select
for inclusion, and this may underlie the absence of sufficient variability to reveal a general factor.
JvH: We understand the reviewers concerns and have amended the Discussion to acknowledge this
issue:
“While all birds had equal opportunity, free from competition, to enter the testing chamber and
engage with each task, such high attrition suggests that birds that participated may differ from those
that failed to participate. Consequently, our study may have comprised a biased subsample of the
population. Indeed, we have found in previous studies on pheasants that a number of non-cognitive,
motivational, traits can influence participation on cognitive tasks (57). Hence, more exploratory
pheasants, or those that were particularly food motivated, may result in a self-selecting sample, by
voluntarily participating in the tasks. These individuals may then represent either end of the
extremes of the distribution of general cognitive ability. At the one end, individuals that are more
exploratory may show greater cognitive ability. Exploratory behaviours in mice, for example, covary
with general learning ability (19). Object exploration has also been considered an important trait
associated with cognitive flexibility in birds, such as the kea (58). At the other end of the spectrum,
individuals that are highly food motivated may show poor cognitive performance, as has been
demonstrated in other species of birds, such as the North Island robin (59). As such, pheasants that
are more exploratory and food motivated may be more likely to participate in appetitive food-
related tasks. Yet, exploratory and highly food motivated individuals may inadvertently represent
subjects from both tails of a cognitive distribution, hence ensuring that no general factor could be
detected. However, we consider this explanation unlikely as we have found in a previous study that
pheasants which rapidly acquired a freely available meal-worm (Baseline Worm), were more likely to
participate in a cognitive test (41). Moreover, while we might expect differences in growth rates in
response to an individual’s sex or body condition to influence their food motivation, we found no
effect of sex or body condition on task performance in the current study.” [change in main text]
5. I still maintain that many of the tasks are inaccurately described. For instance, the “spatial” tasks
have no reliance on spatial cues and are most likely solved through egocentric navigation. In
defense, the authors state that “there is a precedent” for calling them spatial tasks, and
furthermore, that the animals improved across trials, “suggesting that learning was involved”. I’m
not disputing that these are learning tasks, and it may well be that others have incorrectly described
them. This does not justify their being described as spatial in nature. We’d all be better off if we
spoke a common language.
JvH: We acknowledge the reviewers concerns and have revised the MS for clarity so that all tasks
previously referred to as Spatial tasks are now referred to as Positional tasks (i.e. Positional Learning
and Positional Memory).
6. I still think that it would have been useful to analyze a subset of animals that completed more
[fewer] tasks. The rationale for not doing so is not persuasive.
JvH: Our procedures constrain our findings in that we must choose to either include fewer
individuals on a greater variety of tasks, or more individuals on a fewer tasks. We have previously
analysed our dataset so that we could include a greater sample size using fewer tasks and this did
not influence our interpretations of the results. For example, we can obtain a sample size of 45 on 5
tasks (colour learning; positional learning, positional memory; robo-worm & detour reach). These
findings produce a single factor of 29.29%. However, the aim of this MS was not necessarily to reveal
a general intelligence factor in pheasants, but instead to highlight how inconsistent performances of
individuals across novel variants of similar cognitive tasks may influence interpretations of
performances on different test batteries. We therefore argue that reanalysis of the dataset would
fail to add to the MS. An alternative to cutting the number of tasks is to replace missing data with
mean data from across the population. We conducted such analyses in an earlier iteration of our
work and found qualitatively similar results. We were concerned that our patterns of inconsistency
were being driven by excesses of ‘fake’ data (the mean values that we had inserted), hence we
decided to take the more conservative approach of excluding those birds for which data we were
missing, with the inevitable decline in sample sizes.
Society Open
