A more principled use of the p-value? Not so fast: a critique
of Colquhoun’s argument
Ognjen Arandjelovic
Article citation details
R. Soc. open sci. 6: 181519.
http://dx.doi.org/10.1098/rsos.181519
Review timeline
Original submission: 10 January 2018 Note: Reports are unedited and appear as
1st revised submission: 15 October 2018 submitted by the referee. The review history
2nd revised submission: 25 March 2019 appears in chronological order.
Final acceptance: 16 April 2019
Review History
label_version_1
RSOS-180026.R0 (Original submission)
label_author_1
Review form: Reviewer 1 (David Colquhoun)
Is the manuscript scientifically sound in its present form?
Yes
Are the interpretations and conclusions justified by the results?
No
Is the language acceptable?
No
Is it clear how to access all supporting data?
Not Applicable
Do you have any ethical concerns with this paper?
No
Reports © 2019 The Reviewers; Decision Letters © 2019 The Reviewers and Editors;
Responses © 2019 The Reviewers, Editors and Authors. Published by the Royal Society under the
terms of the Creative Commons Attribution License http://creativecommons.org/licenses/by/4.0/,
which permits unrestricted use, provided the original author and source are credited
2
Have you any concerns about statistical analyses in this paper?
Yes
Recommendation?
label_recommendation_1
Major revision is needed (please make suggestions in comments)
Comments to the Author(s)
label_comment_1
File attached (See Appendix A).
David Colquhoun
label_author_2
Review form: Reviewer 2 (Stephen Senn)
Is the manuscript scientifically sound in its present form?
No
Are the interpretations and conclusions justified by the results?
No
Is the language acceptable?
No
Is it clear how to access all supporting data?
Not Applicable
Do you have any ethical concerns with this paper?
No
Have you any concerns about statistical analyses in this paper?
Yes
Recommendation?
label_recommendation_2
Reject
Comments to the Author(s)
label_comment_2
Major point
1) The problem of versions of the treatment is not unknown to statisticians and is one aspect of
Rubin's famous SUTVA conditions, See for example, his comment on Paul Holland's ground
breaking work on causes of effects and effects of causes
http://fitelson.org/woodward/rubin.pdf
2) In fact it has often been discussed by trialists also, that whereas one may usually assume
(reasonably) that pills are similar enough, it is not true of surgical interventions, where the
surgeon plausibly matters a great deal. However as Rubin points out the versions of the
treatment objection does not matter to what he calls the Fisherian null
3) In fact, in drug regulation the batch to batch variability to which the author refers is a practical
concern that is addressed by a number of guidelines. Its philosophical relevance to a point null is
debatable.
4) Furthermore, bioequivalence studies are required to show that generic forms are equivalent to
brand name ones. Thus the possibility of differences in things one might suppose, naively, to be
similar is taken seriously. There is an enormous literature on this topic. It is, however, a mistake
3
to assume that all problems of equivalence are automatically solved by Bayesian approaches but
again they are not really relevant when the task is to reject equivalence.
5) I admit that Colquhoun makes an implicit assumption that a pill is a pill. Although I disagree
with his view of hypothesis testing I would say that it's not the most promising ground to fight
him on. For example, as far as I can see the author makes an implicit assumption that there are
only two sorts of Bayesian statistics, despite the fact that Jack Good (a pioneer and colleague of
Turing's) has a famous article claiming there are (potentially) 46,656 types of Bayesian. I note also
that the author has no compunction about searching PubMed for P-values. Has he checked that
they are all the same sort of thing?Perhaps different things are meant? Epidemiologists, for
example, regularly use 'multivariate' and 'quintile' in a way that statisticians (usually) don't and
regard as being incorrect. (For example for statisticians there are 4 quintiles dividing the
distribution into fifths. Epidemiologists use 'quintile' to mean 'fifth', leaving themselves with not
word for the four divisions.) Of course, we could get into an argument about what statisticians
are etc but is this really the way we want to go? I am unconvinced that the linguistic debate is
important
Minor point:
1) Colquhoun's name is often written incorrectly in the paper as 'Coloquin'
References
Good IJ. Good Thinking: The Foundations of Probability and its Applications. Minneapolis:
University of Minnesota Press 1983. (Chapter 3)
Holland PW. Statistics and Causal Inference. Journal of the American Statistical Association
1986;81(396):945-60.
Senn SJ. Statistical issues in bioequivalence. Statistics in Medicine 2001;20(17-18):2785-99.
Stephen Senn, Luxembourg Institute of Health
label_author_3
Review form: Reviewer 3 (David Robert Grimes)
Is the manuscript scientifically sound in its present form?
Yes
Are the interpretations and conclusions justified by the results?
Yes
Is the language acceptable?
No
Is it clear how to access all supporting data?
Not Applicable
Do you have any ethical concerns with this paper?
No
Have you any concerns about statistical analyses in this paper?
No
Recommendation?
label_recommendation_3
Major revision is needed (please make suggestions in comments)
4
Comments to the Author(s)
label_comment_3
Robust scientific debate is essential, and as such as I welcome both this comment and the paper it
refers to. I do however have some concerns about the strength of language, and the conclusions
inferred. I will attempt to outline these here - as a comment of course, there is more freedom to
state opinion that a research article would allow, but I would like to see scientific discourse kept
as constructive as possible. My comments are listed below:
(1) There is something of a potential misrepresentation of Prof Colquhoun's argument here - I do
not think anyone would read his 2017 work and think it was an attempt to vindicate p-value as a
metric, especially in light of his 2014 work in this journal. Specifically, Prof. Colquhoun is
interested in false positive rates, which we all agree are a serious issue in biomedical science. The
abstract to my reading presents a very simplified and unfair strawman summation of what was
actually argued, and this is unfortunate as it jars with some of the arguments made later which
reflect a level of nuance.
(2) It is important to note that false positives and p-values are not synonymous - a p value is a
statistical metric, but where are numerous other ways to arrive at a false positive which are
experimental or theoretical in origin. This isn't a major point, but in the minute details of this
argument it should be borne in mind.
(3) I would dispute the claim that researcher en masse have ignored the warnings doled out by
the statistical community. The work of people like John Ioannidis is mentioned in this
manuscript, but you might also consider recent works by Smaldino and McElreath (
http://rsos.royalsocietypublishing.org/content/3/9/160384 ) or Grimes, Bauch and Ioannidis (
http://rsos.royalsocietypublishing.org/content/5/1/171511 ) or even the works being discussed
by Prof Colquhoun as examples of researchers touching on these problems. Researchers are even
debunking the statistical manipulations of others (See this popular piece on SPRITE and James's
Heathers work - https://hackernoon.com/introducing-sprite-and-the-case-of-the-carthorse-
child-58683c2bfeb ). I do agree there is still a huge gap in understanding (especially in
applications of Bayes Theorem) but there is certainly an awareness that statistics are frequently
abused by researchers (see Fanelli 2009 -
http://journals.plos.org/plosone/article?id=10.1371/journal.pone.0005738 or introduction to
Grimes, Bauch Ioannidis for measures of awareness on this factor) . This is a minor point, but it's
far to say there's growing awareness of dubious manipulations across all of biomedical science.
(4) I have a number of issues with section 2 - the 'same pill' argument employed is rather
nebulous. In a thought experiment, I could conceive of a perfect manufacturing process where
deviations in the final product are negligible. in which case 'same pill' is perfectly acceptable.
Indeed, in physics we have particles that are indistinguishable, and in an experiment we would
be justified in saying we use the 'same' particles. The philosophical argument on unknown
unknowns does not render Prof Colquhoun wrong, nor his argument sloppy. I agree with the
author that if you had a PDF for manufacturing uncertainty you could factor this in to perpetuate
your errors and confidence levels - indeed this would be accounted for in an ideal experimental
design. But as deployed here it seems the author refers to completely hypothetical and un-
measurable errors. If this is the case, then the hypothesis is not testable - and by Popper's maxim
it is not a 'scientific' question in the first instance. I have no issues with the author robustly
defending a Bayesian approach, but I think the example chosen here doesn't rebut Prof
Colquhoun's argument above a semantic level.
(5) By contrast, I agree with many of the observations in section 3 - mechanistic understanding is
absolutely vital to avoid confounding influence, and this is an important aspect. I also agree that
'spike' is a vague term, though probably well-understood. I do have a query about something
5
here however - the author states that every possible outcome is consistent with an effect size of
zero. For example, that 1000 heads up is entirely consistent with a fair coin toss - but how so? The
probability of this (0.5)^1000, or ~ 9.33 x 10E-302, which is extraordinarily low, but non-zero. But
effect size (and there are to my knowledge numerous slightly different definitions and metrics)
quantifies the difference between two groups. In this case, you'd compare the 1000 heads coin to a
theoretically unbiased coin, and would get a non-zero effect size. For example, if I used the
definition of effect size as (mean(sample) - mean(control) )/ std(control) in this case, I'd get an
effect size of 0.9995, not zero. I really think this has to be better explained, as otherwise it jars with
the following points.
(6) In general, I'd like to see a slightly longer but more cautious comment - referring to other
statistical discussions is fine, but as Open Science is interdisciplinary, I think it would be
worthwhile to actually expand the discussion and outline what is meant, rather than delving into
technical arguments without any introduction. If this was done, I think this comment could be a
very valuable contribution to the literature. It might be worth framing the comment in a less
combative way too, and presume less knowledge on the reader's behalf - I feel this would yield a
much more powerful comment.
label_end_comment
Decision letter (RSOS-180026.R0)
07-Mar-2018
Dear Professor Arandjelovic:
Manuscript ID RSOS-180026 entitled "Partial Vindication of the p-Value? Not so Fast: A Critique
of Colquhoun's Argument" which you submitted to Royal Society Open Science, has been
reviewed. The comments from reviewers are included at the bottom of this letter.
In view of the criticisms of the reviewers, the manuscript has been rejected in its current form.
However, a new manuscript may be submitted which takes into consideration these comments.
Please note that resubmitting your manuscript does not guarantee eventual acceptance, and that
your resubmission will be subject to peer review before a decision is made.
You will be unable to make your revisions on the originally submitted version of your
manuscript. Instead, revise your manuscript and upload the files via your author centre.
Once you have revised your manuscript, go to https://mc.manuscriptcentral.com/rsos and login
to your Author Center. Click on "Manuscripts with Decisions," and then click on "Create a
Resubmission" located next to the manuscript number. Then, follow the steps for resubmitting
your manuscript.
Your resubmitted manuscript should be submitted by 04-Sep-2018. If you are unable to submit by
this date please contact the Editorial Office.
Please note that Royal Society Open Science will introduce article processing charges for all new
submissions received from 1 January 2018. Charges will also apply to papers transferred to Royal
Society Open Science from other Royal Society Publishing journals, as well as papers submitted
6
as part of our collaboration with the Royal Society of Chemistry
(http://rsos.royalsocietypublishing.org/chemistry). If your manuscript is submitted and
accepted for publication after 1 Jan 2018, you will be asked to pay the article processing charge,
unless you request a waiver and this is approved by Royal Society Publishing. You can find out
more about the charges at http://rsos.royalsocietypublishing.org/page/charges. Should you
have any queries, please contact openscience@royalsociety.org.
We look forward to receiving your resubmission.
Kind regards,
Alice Power
Editorial Coordinator
Royal Society Open Science
openscience@royalsociety.org
on behalf of Mark Chaplain (Subject Editor)
openscience@royalsociety.org
Reviewers' Comments to Author:
Reviewer: 1
Comments to the Author(s)
File attached
David Colquhoun
Reviewer: 2
Comments to the Author(s)
Major point
1) The problem of versions of the treatment is not unknown to statisticians and is one aspect of
Rubin's famous SUTVA conditions, See for example, his comment on Paul Holland's ground
breaking work on causes of effects and effects of causes
http://fitelson.org/woodward/rubin.pdf
2) In fact it has often been discussed by trialists also, that whereas one may usually assume
(reasonably) that pills are similar enough, it is not true of surgical interventions, where the
surgeon plausibly matters a great deal. However as Rubin points out the versions of the
treatment objection does not matter to what he calls the Fisherian null
3) In fact, in drug regulation the batch to batch variability to which the author refers is a practical
concern that is addressed by a number of guidelines. Its philosophical relevance to a point null is
debatable .
4) Furthermore, bioequivalence studies are required to show that generic forms are equivalent to
brand name ones. Thus the possibility of differences in things one might suppose, naively, to be
similar is taken seriously. There is an enormous literature on this topic. It is, however, a mistake
to assume that all problems of equivalence are automatically solved by Bayesian approaches but
again they are not really relevant when the task is to reject equivalence.
5) I admit that Colquhoun makes an implicit assumption that a pill is a pill. Although I disagree
with his view of hypothesis testing I would say that it's not the most promising ground to fight
him on. For example, as far as I can see the author makes an implicit assumption that there are
only two sorts of Bayesian statistics, despite the fact that Jack Good (a pioneer and colleague of
Turing's) has a famous article claiming there are (potentially) 46,656 types of Bayesian. I note also
that the author has no compunction about searching PubMed for P-values. Has he checked that
7
they are all the same sort of thing?Perhaps different things are meant? Epidemiologists, for
example, regularly use 'multivariate' and 'quintile' in a way that statisticians (usually) don't and
regard as being incorrect. (For example for statisticians there are 4 quintiles dividing the
distribution into fifths. Epidemiologists use 'quintile' to mean 'fifth', leaving themselves with not
word for the four divisions.) Of course, we could get into an argument about what statisticians
are etc but is this really the way we want to go? I am unconvinced that the linguistic debate is
important
Minor point:
1) Colquhoun's name is often written incorrectly in the paper as 'Coloquin'
References
Good IJ. Good Thinking: The Foundations of Probability and its Applications. Minneapolis:
University of Minnesota Press 1983. (Chapter 3)
Holland PW. Statistics and Causal Inference. Journal of the American Statistical Association
1986;81(396):945-60.
Senn SJ. Statistical issues in bioequivalence. Statistics in Medicine 2001;20(17-18):2785-99.
Stephen Senn, Luxembourg Institute of Health
Reviewer: 3
Comments to the Author(s)
Robust scientific debate is essential, and as such as I welcome both this comment and the paper it
refers to. I do however have some concerns about the strength of language, and the conclusions
inferred. I will attempt to outline these here - as a comment of course, there is more freedom to
state opinion that a research article would allow, but I would like to see scientific discourse kept
as constructive as possible. My comments are listed below:
(1) There is something of a potential misrepresentation of Prof Colquhoun's argument here - I do
not think anyone would read his 2017 work and think it was an attempt to vindicate p-value as a
metric, especially in light of his 2014 work in this journal. Specifically, Prof. Colquhoun is
interested in false positive rates, which we all agree are a serious issue in biomedical science. The
abstract to my reading presents a very simplified and unfair strawman summation of what was
actually argued, and this is unfortunate as it jars with some of the arguments made later which
reflect a level of nuance.
(2) It is important to note that false positives and p-values are not synonymous - a p value is a
statistical metric, but where are numerous other ways to arrive at a false positive which are
experimental or theoretical in origin. This isn't a major point, but in the minute details of this
argument it should be borne in mind.
(3) I would dispute the claim that researcher en masse have ignored the warnings doled out by
the statistical community. The work of people like John Ioannidis is mentioned in this
manuscript, but you might also consider recent works by Smaldino and McElreath (
http://rsos.royalsocietypublishing.org/content/3/9/160384 ) or Grimes, Bauch and Ioannidis (
http://rsos.royalsocietypublishing.org/content/5/1/171511 ) or even the works being discussed
by Prof Colquhoun as examples of researchers touching on these problems. Researchers are even
debunking the statistical manipulations of others (See this popular piece on SPRITE and James's
Heathers work - https://hackernoon.com/introducing-sprite-and-the-case-of-the-carthorse-
child-58683c2bfeb ). I do agree there is still a huge gap in understanding (especially in
8
applications of Bayes Theorem) but there is certainly an awareness that statistics are frequently
abused by researchers (see Fanelli 2009 -
http://journals.plos.org/plosone/article?id=10.1371/journal.pone.0005738 or introduction to
Grimes, Bauch Ioannidis for measures of awareness on this factor) . This is a minor point, but it's
far to say there's growing awareness of dubious manipulations across all of biomedical science.
(4) I have a number of issues with section 2 - the 'same pill' argument employed is rather
nebulous. In a thought experiment, I could conceive of a perfect manufacturing process where
deviations in the final product are negligible. in which case 'same pill' is perfectly acceptable.
Indeed, in physics we have particles that are indistinguishable, and in an experiment we would
be justified in saying we use the 'same' particles. The philosophical argument on unknown
unknowns does not render Prof Colquhoun wrong, nor his argument sloppy. I agree with the
author that if you had a PDF for manufacturing uncertainty you could factor this in to perpetuate
your errors and confidence levels - indeed this would be accounted for in an ideal experimental
design. But as deployed here it seems the author refers to completely hypothetical and un-
measurable errors. If this is the case, then the hypothesis is not testable - and by Popper's maxim
it is not a 'scientific' question in the first instance. I have no issues with the author robustly
defending a Bayesian approach, but I think the example chosen here doesn't rebut Prof
Colquhoun's argument above a semantic level.
(5) By contrast, I agree with many of the observations in section 3 - mechanistic understanding is
absolutely vital to avoid confounding influence, and this is an important aspect. I also agree that
'spike' is a vague term, though probably well-understood. I do have a query about something
here however - the author states that every possible outcome is consistent with an effect size of
zero. For example, that 1000 heads up is entirely consistent with a fair coin toss - but how so? The
probability of this (0.5)^1000, or ~ 9.33 x 10E-302, which is extraordinarily low, but non-zero. But
effect size (and there are to my knowledge numerous slightly different definitions and metrics)
quantifies the difference between two groups. In this case, you'd compare the 1000 heads coin to a
theoretically unbiased coin, and would get a non-zero effect size. For example, if I used the
definition of effect size as (mean(sample) - mean(control) )/ std(control) in this case, I'd get an
effect size of 0.9995, not zero. I really think this has to be better explained, as otherwise it jars with
the following points.
(6) In general, I'd like to see a slightly longer but more cautious comment - referring to other
statistical discussions is fine, but as Open Science is interdisciplinary, I think it would be
worthwhile to actually expand the discussion and outline what is meant, rather than delving into
technical arguments without any introduction. If this was done, I think this comment could be a
very valuable contribution to the literature. It might be worth framing the comment in a less
combative way too, and presume less knowledge on the reader's behalf - I feel this would yield a
much more powerful comment.
Author's Response to Decision Letter for (RSOS-180026.R0)
See Appendix B.
9
label_version_2
RSOS-181519.R0
label_author_4
Review form: Reviewer 1 (David Colquhoun)
Is the manuscript scientifically sound in its present form?
No
Are the interpretations and conclusions justified by the results?
No
Is the language acceptable?
No
Is it clear how to access all supporting data?
Not Applicable
Do you have any ethical concerns with this paper?
No
Have you any concerns about statistical analyses in this paper?
Yes
Recommendation?
label_recommendation_4
Reject
Comments to the Author(s)
label_comment_4
I think that this paper has been improved only slightly in revision.
The main argument appears to be the author's objection to the point null hypothesis Such
arguments are common among subjective Bayesians (though by no means universal: see, for
example, Richard Morey at
https://www.youtube.com/watch?time_continue=11&v=R7dQo9bW1DM ). But of course the
point null has been the mainstay of Fisherian statistics for a century.
The argument that two pills can't be identical seems to me to be absurdly pedantic: about as
helpful as arguing about the number of angels that can fit on the head of a pin.
Another problem is that the arguments are aimed entirely at my 2017 paper. Since then I have
written another in which many of my points have been amplified. In particular I have shown that
the results that I get are not dependent on the point null anyway. The approaches of Berger &
Sellke, and of Goodman, and also that of Valen Johnson, give results that are sufficiently similar
to mine that their interpretation would be similar in practice. -see Colquhoun (2018)
https://arxiv.org/abs/1802.04888 (in press, American Statistician). So even if you believe, like
the author, that point nulls are absurd, the fact that my results do not (to any substantial extent)
depend on that assumption should really have been mentioned.
I find that the revised version still comes across as nitpicking and unhelpful, insofar as the author
makes no suggestions about what he thinks should be done about p values. I suspect. that its
publication would not reflect well on the author, and certainly will not help experimenters or
journal editors whose aim is to improve reproducibility.
10
Of course I have a vested interest since it is I am the butt of the author's criticisms, So I leave it to
others to decide on whether or not it's worth publishing
label_author_5
Review form: Reviewer 2 (Stephen Senn)
Is the manuscript scientifically sound in its present form?
Yes
Are the interpretations and conclusions justified by the results?
No
Is the language acceptable?
No
Is it clear how to access all supporting data?
Not Applicable
Do you have any ethical concerns with this paper?
No
Have you any concerns about statistical analyses in this paper?
Yes
Recommendation?
label_recommendation_5
Accept as is
Comments to the Author(s)
label_comment_5
You state:
"Bayesian approaches allow for the effects of issues like those discussed to be identified and
understood better and with a lesser loss of information
(one of these being the separation of questions of statistical significance and the effect size as
proposed by the original author, and the combined treatment of the two using a Bayesian
approach)."
This may or may not be so but it is obviously not necessarily so, since Colquhoun's argument is
itself based on a Bayesian analysis and does not appear to address this point, which you,
apparently, consider important. Of course many Bayesian analyses are possible (and his
corresponds to one particular one) but very few if any, as far as I am aware, address Don Rubin's
assumption that 'there are no versions of the treatment'. by incorporating this formally into the
analysis. Perhaps you would like to cite a published example that does.Failing that, it seems to
me that you are left with a general assumption, which, like for example that the data have not
been fabricated, is made by all, even though occasionally proven to be false. Its role in your
criticism of Colquhoun is rather obscure. However, if you can provide a published example of
this being taken into account by a rival approach, this would at least give some substance to your
criticism. For an example of how assumptions of another common assumption 'competence'
might be taken into account in a Bayesian analysis see. Senn(1)
This is not just a quibble since the issue here is whether this a general point that all approaches
have to handle by assumption or an inherent weakness of one compared to another.
In fact, it is perfectly possible in a frequentist framework to address the issue of deciding that two
sets of treatments that are being compared are not identical without assuming that therefore that
all members of one of any given set being compared are identical, although one does have to be
11
careful as to one one then can assert if the nully hypotheses is "rejected": See the discussion of the
analysis of Rofecoxib by Juni et al (2) in Senn.(3).
References
1. Senn SJ. Inherent difficulties with active control equivalence studies. Statistics in Medicine.
1993;12(24):2367-2375.
2. Juni P, Nartey L, Reichenbach S, Sterchi R, Dieppe PA, Egger M. Risk of cardiovascular events
and rofecoxib: cumulative meta-analysis. Lancet. 2004;364(9450):2021-2029.
3. Senn SJ. Overstating the evidence: double counting in meta-analysis and related problems.
BMC Med Res Methodol. 2009;9:10.
label_author_6
Review form: Reviewer 4 (James Abdey)
Is the manuscript scientifically sound in its present form?
Yes
Are the interpretations and conclusions justified by the results?
Yes
Is the language acceptable?
Yes
Is it clear how to access all supporting data?
Not Applicable
Do you have any ethical concerns with this paper?
No
Have you any concerns about statistical analyses in this paper?
No
Recommendation?
label_recommendation_6
Accept with minor revision (please list in comments)
Comments to the Author(s)
label_comment_6
I note your detailed response to previous reviewers' comments which are comprehensive, and on
the whole, fair and reasonable.
My comments are my own form of "frivolous semantic pedantry"!
Regarding the use of less combative language, I am unaware of how the original version read, but
this version is generally tame. However, I suggest "nebulous" may come across as a little haughty
- instead I suggest "vague".
"An experiment resulting in 1000 heads up is entirely consistent with the hypothesis of fair coin
tosses." I fully agree, although another reviewer gave the exact probability of this and indicated,
though small, it was indeed > 0. I think to strengthen this point, you could quote the probability
result and emphasise the fact that although improbable, it is not impossible, and hence
"consistent with the hypothesis of fair coin tosses".
12
Typo - "suble" (line 9).
Overall, the very nature of the article means it rests on nuanced points, which seem well-argued.
label_end_comment
Decision letter (RSOS-181519.R0)
07-Feb-2019
Dear Professor Arandjelovic
On behalf of the Editor, I am pleased to inform you that your Manuscript RSOS-181519 entitled
"A More Principled Use of the p-Value? Not so Fast: A Critique of Colquhoun's Argument" has
been accepted for publication in Royal Society Open Science subject to minor revision in
accordance with the referee suggestions. Please find the referees' comments at the end of this
email.
The reviewers and Subject Editor have recommended publication, but also suggest some minor
revisions to your manuscript. Therefore, I invite you to respond to the comments and revise your
manuscript.
• Ethics statement
If your study uses humans or animals please include details of the ethical approval received,
including the name of the committee that granted approval. For human studies please also detail
whether informed consent was obtained. For field studies on animals please include details of all
permissions, licences and/or approvals granted to carry out the fieldwork.
• Data accessibility
It is a condition of publication that all supporting data are made available either as
supplementary information or preferably in a suitable permanent repository. The data
accessibility section should state where the article's supporting data can be accessed. This section
should also include details, where possible of where to access other relevant research materials
such as statistical tools, protocols, software etc can be accessed. If the data has been deposited in
an external repository this section should list the database, accession number and link to the DOI
for all data from the article that has been made publicly available. Data sets that have been
deposited in an external repository and have a DOI should also be appropriately cited in the
manuscript and included in the reference list.
If you wish to submit your supporting data or code to Dryad (http://datadryad.org/), or modify
your current submission to dryad, please use the following link:
http://datadryad.org/submit?journalID=RSOS&manu=RSOS-181519
• Competing interests
Please declare any financial or non-financial competing interests, or state that you have no
competing interests.
• Authors’ contributions
All submissions, other than those with a single author, must include an Authors’ Contributions
section which individually lists the specific contribution of each author. The list of Authors
should meet all of the following criteria; 1) substantial contributions to conception and design, or
13
acquisition of data, or analysis and interpretation of data; 2) drafting the article or revising it
critically for important intellectual content; and 3) final approval of the version to be published.
All contributors who do not meet all of these criteria should be included in the
acknowledgements.
We suggest the following format:
AB carried out the molecular lab work, participated in data analysis, carried out sequence
alignments, participated in the design of the study and drafted the manuscript; CD carried out
the statistical analyses; EF collected field data; GH conceived of the study, designed the study,
coordinated the study and helped draft the manuscript. All authors gave final approval for
publication.
• Acknowledgements
Please acknowledge anyone who contributed to the study but did not meet the authorship
criteria.
• Funding statement
Please list the source of funding for each author.
Please note that we cannot publish your manuscript without these end statements included. We
have included a screenshot example of the end statements for reference. If you feel that a given
heading is not relevant to your paper, please nevertheless include the heading and explicitly state
that it is not relevant to your work.
Because the schedule for publication is very tight, it is a condition of publication that you submit
the revised version of your manuscript before 16-Feb-2019. Please note that the revision deadline
will expire at 00.00am on this date. If you do not think you will be able to meet this date please let
me know immediately.
To revise your manuscript, log into https://mc.manuscriptcentral.com/rsos and enter your
Author Centre, where you will find your manuscript title listed under "Manuscripts with
Decisions". Under "Actions," click on "Create a Revision." You will be unable to make your
revisions on the originally submitted version of the manuscript. Instead, revise your manuscript
and upload a new version through your Author Centre.
When submitting your revised manuscript, you will be able to respond to the comments made by
the referees and upload a file "Response to Referees" in "Section 6 - File Upload". You can use this
to document any changes you make to the original manuscript. In order to expedite the
processing of the revised manuscript, please be as specific as possible in your response to the
referees.
When uploading your revised files please make sure that you have:
1) A text file of the manuscript (tex, txt, rtf, docx or doc), references, tables (including captions)
and figure captions. Do not upload a PDF as your "Main Document".
2) A separate electronic file of each figure (EPS or print-quality PDF preferred (either format
should be produced directly from original creation package), or original software format)
3) Included a 100 word media summary of your paper when requested at submission. Please
ensure you have entered correct contact details (email, institution and telephone) in your user
account
14
4) Included the raw data to support the claims made in your paper. You can either include your
data as electronic supplementary material or upload to a repository and include the relevant doi
within your manuscript
5) All supplementary materials accompanying an accepted article will be treated as in their final
form. Note that the Royal Society will neither edit nor typeset supplementary material and it will
be hosted as provided. Please ensure that the supplementary material includes the paper details
where possible (authors, article title, journal name).
Supplementary files will be published alongside the paper on the journal website and posted on
the online figshare repository (https://figshare.com). The heading and legend provided for each
supplementary file during the submission process will be used to create the figshare page, so
please ensure these are accurate and informative so that your files can be found in searches. Files
on figshare will be made available approximately one week before the accompanying article so
that the supplementary material can be attributed a unique DOI.
Once again, thank you for submitting your manuscript to Royal Society Open Science and I look
forward to receiving your revision. If you have any questions at all, please do not hesitate to get
in touch.
Kind regards,
Royal Society Open Science Editorial Office
Royal Society Open Science
openscience@royalsociety.org
on behalf of Professor Mark Chaplain (Subject Editor)
openscience@royalsociety.org
Associate Editor Comments to Author:
We would like you to revise the manuscript to take into account the changes requested by the
reviewers - we also want to ensure that when you submit the revision you include a full response
to the remaining comments of the reviewers to explain your changes and/or why you have not
made changes requested.
The reviewers are, to be blunt, ambivalent about your Comment -- it would perhaps have been
helpful if you had included more by way of suggestions for tackling the problems you discuss,
for instance -- but as Comment articles are a form of 'official' discussion in the community, and
the author of the original paper you are commenting upon will be invited to submit a Reply for
publication, the Editors have erred on the side of accepting the paper if you fully address and
respond to the concerns and critiques raised in this version.
Reviewer comments to Author:
Reviewer: 1
Comments to the Author(s)
I think that this paper has been improved only slightly in revision.
The main argument appears to be the author's objection to the point null hypothesis Such
arguments are common among subjective Bayesians (though by no means universal: see, for
example, Richard Morey at
https://www.youtube.com/watch?time_continue=11&v=R7dQo9bW1DM ). But of course the
point null has been the mainstay of Fisherian statistics for a century.
15
The argument that two pills can't be identical seems to me to be absurdly pedantic: about as
helpful as arguing about the number of angels that can fit on the head of a pin.
Another problem is that the arguments are aimed entirely at my 2017 paper. Since then I have
written another in which many of my points have been amplified. In particular I have shown that
the results that I get are not dependent on the point null anyway. The approaches of Berger &
Sellke, and of Goodman, and also that of Valen Johnson, give results that are sufficiently similar
to mine that their interpretation would be similar in practice. -see Colquhoun (2018)
https://arxiv.org/abs/1802.04888 (in press, American Statistician). So even if you believe, like
the author, that point nulls are absurd, the fact that my results do not (to any substantial extent)
depend on that assumption should really have been mentioned.
I find that the revised version still comes across as nitpicking and unhelpful, insofar as the author
makes no suggestions about what he thinks should be done about p values. I suspect. that its
publication would not reflect well on the author, and certainly will not help experimenters or
journal editors whose aim is to improve reproducibility.
Of course I have a vested interest since it is I am the butt of the author's criticisms, So I leave it to
others to decide on whether or not it's worth publishing
Reviewer: 2
Comments to the Author(s)
You state:
"Bayesian approaches allow for the effects of issues like those discussed to be identified and
understood better and with a lesser loss of information
(one of these being the separation of questions of statistical significance and the effect size as
proposed by the original author, and the combined treatment of the two using a Bayesian
approach)."
This may or may not be so but it is obviously not necessarily so, since Colquhoun's argument is
itself based on a Bayesian analysis and does not appear to address this point, which you,
apparently, consider important. Of course many Bayesian analyses are possible (and his
corresponds to one particular one) but very few if any, as far as I am aware, address Don Rubin's
assumption that 'there are no versions of the treatment'. by incorporating this formally into the
analysis. Perhaps you would like to cite a published example that does.Failing that, it seems to
me that you are left with a general assumption, which, like for example that the data have not
been fabricated, is made by all, even though occasionally proven to be false. Its role in your
criticism of Colquhoun is rather obscure. However, if you can provide a published example of
this being taken into account by a rival approach, this would at least give some substance to your
criticism. For an example of how assumptions of another common assumption 'competence'
might be taken into account in a Bayesian analysis see. Senn(1)
This is not just a quibble since the issue here is whether this a general point that all approaches
have to handle by assumption or an inherent weakness of one compared to another.
In fact, it is perfectly possible in a frequentist framework to address the issue of deciding that two
sets of treatments that are being compared are not identical without assuming that therefore that
all members of one of any given set being compared are identical, although one does have to be
careful as to one one then can assert if the nully hypotheses is "rejected": See the discussion of the
analysis of Rofecoxib by Juni et al (2) in Senn.(3).
16
References
1. Senn SJ. Inherent difficulties with active control equivalence studies. Statistics in Medicine.
1993;12(24):2367-2375.<Br>
2. Juni P, Nartey L, Reichenbach S, Sterchi R, Dieppe PA, Egger M. Risk of cardiovascular events
and rofecoxib: cumulative meta-analysis. Lancet. 2004;364(9450):2021-2029.<Br>
3. Senn SJ. Overstating the evidence: double counting in meta-analysis and related problems.
BMC Med Res Methodol. 2009;9:10.<Br>
Reviewer: 4
Comments to the Author(s)
I note your detailed response to previous reviewers' comments which are comprehensive, and on
the whole, fair and reasonable.
My comments are my own form of "frivolous semantic pedantry"!
Regarding the use of less combative language, I am unaware of how the original version read, but
this version is generally tame. However, I suggest "nebulous" may come across as a little haughty
- instead I suggest "vague".
"An experiment resulting in 1000 heads up is entirely consistent with the hypothesis of fair coin
tosses." I fully agree, although another reviewer gave the exact probability of this and indicated,
though small, it was indeed > 0. I think to strengthen this point, you could quote the probability
result and emphasise the fact that although improbable, it is not impossible, and hence
"consistent with the hypothesis of fair coin tosses".
Typo - "suble" (line 9).
Overall, the very nature of the article means it rests on nuanced points, which seem well-argued.
Author's Response to Decision Letter for (RSOS-181519.R0)
See Appendix C.
label_end_comment
Decision letter (RSOS-181519.R1)
16-Apr-2019
Dear Professor Arandjelovic,
I am pleased to inform you that your manuscript entitled "A More Principled Use of the p-Value?
Not so Fast: A Critique of Colquhoun's Argument" is now accepted for publication in Royal
Society Open Science.
17
You can expect to receive a proof of your article in the near future. Please contact the editorial
office (openscience_proofs@royalsociety.org and openscience@royalsociety.org) to let us know if
you are likely to be away from e-mail contact. Due to rapid publication and an extremely tight
schedule, if comments are not received, your paper may experience a delay in publication.
Royal Society Open Science operates under a continuous publication model
(http://bit.ly/cpFAQ). Your article will be published straight into the next open issue and this
will be the final version of the paper. As such, it can be cited immediately by other researchers.
As the issue version of your paper will be the only version to be published I would advise you to
check your proofs thoroughly as changes cannot be made once the paper is published.
On behalf of the Editors of Royal Society Open Science, we look forward to your continued
contributions to the Journal.
Kind regards,
Royal Society Open Science Editorial Office
Royal Society Open Science
openscience@royalsociety.org
on behalf of Professor Mark Chaplain (Subject Editor)
openscience@royalsociety.org
Follow Royal Society Publishing on Twitter: @RSocPublishing
Follow Royal Society Publishing on Facebook:
https://www.facebook.com/RoyalSocietyPublishing.FanPage/
Read Royal Society Publishing's blog: https://blogs.royalsociety.org/publishing/
Appendix A
Report on Ognjen Arandjelovic´ RSOS-180026
I’ll leave it to the editors to deal with the style of the paper. I notice that the author has
consistently spelt my name wrongly throughout. I’ll deal only with the technical points that
are made in the paper.
(1) I’m totally baffled by the author’s description of my paper as a “partial vindication of the p-
value” when it is, surely, clear that it is exactly the opposite of that. In the abstract it say
“Despite decades of warnings, many areas of science still insist on labelling a result of
p < 0.05 as ‘statistically significant’. This practice must contribute to the lack of reproducibility
in some areas of science”. How that can be interpreted as any sort of vindication of p-values
beats me.
(2) The author’s main objection seems to be that my argument is based on using a point null
hypothesis. This is a matter that’s much discussed in the statistical literature. There is no
unanimity, but plenty of other people agree with me that it’s reasonable to ask whether your
observations would have been likely to arise if (for example) the pills given to each group
were identical. The author is free to think that this is unreasonable but in doing so he’s
disagreeing not just with me but also with a large part of the statistical world. Not everyone
is a subjective Bayesian. The vast majority of studies (at least in biomedicine) that have
professional statisticians as authors test point null hypotheses.
(3) As for his objection that you can’t manufacture two exactly identical pills, I can only agree
with his surmise that this “criticism may come across as frivolous semantic pedantry of no
practical consequence”.
(4) The author says “Moreover, the assumption as expressed above, is nebulous and lacking
in clarity. What precisely does ‘a spike’ mean?”. I referred to the paper of Berger &
Delampaday (reference 4) which explains exactly that point.
(5) The author notes, as a criticism, that “No single hypothesis (including the null) can have a
finite probability associated with it.”. While this is obviously true, it certainly doesn’t
invalidate the idea of the likelihood ratio, which lies at the heart of my argument. We aren’t
saying that the effect size is exactly zero (that would obviously be impossible), We are
looking at the likelihood ratio, i.e. at the probability of the data if the true effect size were not
zero relative to the probability of the data if the effect size were zero. If the latter probability
is bigger than the former then clearly we can’t be sure that there is anything but chance at
work. This does not mean that we are saying that the effect size is exactly zero. From the
point of view of the experimenter, testing the point null makes total sense. I’m a bit baffled
by why the author should even think that this is a criticism of anything in my paper. There are
arguments against the use of point nulls, but this is not one of them.
(6) The author says “I demonstrate that the (often implicit) premises of this counterargument
are flawed”. Firstly, the assumptions that I make are stated explicitly and discussed at
length, especially in section 3 and appendix A1. I suggest that “implicit” is not the right
description. Second, the author doesn’t so much demonstrate that the assumptions are
flawed, but rather that he disagrees with them. It’s an unfortunate consequence of the
internecine war that rages among statisticians that each side says that any views other than
their own and flawed/wrong/silly. This tendency to personalise the squabbles, and lack of
willingness to seek common ground, is unfortunate, if only because it exasperates users of
statistics, and accounts to some extent for the failure of statisticians, over many decades, to
have much influence on experimental practice.
(7) The author does not seem to like the idea of a false positive result: he says “As should be
clear from my argumentation thus far, this claim fundamentally does not make sense”. I’m
certainly aware of the fact that some subjective Bayesians (especially Gelman and Morey)
deny the idea that a false positive can be defined consistently. To most of the rest of the
world this view seems simply bizarre. The idea that effects are claimed when none exists
(i.e. false positives) lies at the heart of every discussion of the reproducibility crisis.
Perhaps the author should have addressed his comments to Berger & Sellke (1987) and to
Johnson (2013, 2015), (refs 7, 8, 27, 29 in the paper). They come up with similar false
positive risks to mine, by different (and more complicated) arguments. Similar conclusions
to mine have been around for a long time. This was mentioned at the end of section 8 in my
paper, but it isn’t mentioned in the author’s critique.
(8) The author says “Much like before, Coloquin [sic] leaves the question of how the second
step is to be performed without much elaboration”. It’s true that I take for granted that
everyone would realise that, in the example I used, the best estimate of the effect size would
be the difference between the two observed sample means (the maximum likelihood
estimate of the true effect size). Surely this does not need to be spelled out explicitly?
(9) The author says (of the point null assumption) that “there is no attempt at justifying the
claimed reasonableness of the aforementioned assumption”. Perhaps he didn’t reach
appendix section A1 in my paper which attempts to justify the assumption at length,
(10) The author says
“the following claim by Coloquin [sic]
“Recently, it was asserted that if we observe a p-value just below 0.05, then
there is a chance of at least 26% that your result is a false positive [2].”
As should be clear from my argumentation thus far, this claim fundamentally does not
make sense”
I can agree that it might have been better if the word “asserted” had been replaced by
“suggested”, or by “demonstrated that under reasonable assumptions”. That apart, this
comment sounds like quibbling. It would be much more helpful to the reader if he pointed out
exactly which bit of the simulations in the 2014 paper, or the exact calculations in the 2017,
paper was wrong.
(9) I have read the paper by Ng, recommended by the author. In my view it is irrelevant to
the simple question that is asked in a two-sample t test, namely is there good evidence that
my results are not just chance? Or, put another way, will I make a fool of myself if I clam
that the intervention works.
(11) It seems strangely inconsistent that, having spent the whole article condemning my
assumptions, he then concludes
“I welcome Coloquin’s [sic] contribution as it provides a good basis for illustrating the
nuanced nature and the multiplicity of statistical, methodological, and epistemological
issues which must be considered in the analysis of empirical data. I believe that my
analysis has demonstrated comprehensively that the criticisms of the p-value are
well-founded and theoretically solid, strengthening the case against the continued
use of the statistic.”
It’s clear that the author wishes everyone to be a subjective Bayesian. My opinion is that
this will never happen, and probably should not because most experimentalists will never
accept the use of informative prior distributions that are based on expert opinion rather than
on data. The best chance of influencing statistical practice seems to me to be to provide an
alternative to the p-value that is simple enough to understand that it has a chance of being
adopted. That is what I tried to do. In practice the procedure does not have to be exact, For
example, I point out at the end of section 8
“… when p = 0.05 and prior probability is 0.5, Berger et al. [3,8] find a false positive
risk of at least 22% and Johnson [27] finds 17–25%. Both are close to the value of
26% found here by simpler arguments.”
It practice it doesn’t matter a damn whether the answer is 22% or 26%. Either number casts
sufficient doubt on the evidence provided by the observation of p = 0.05 that citing it should
go a long way to help to end the myth it’s sensible to declare that you have discovered
something on the basis p = 0.049 is “statistically significant”.
Appendix B
Ognjen Arandjelovic´, M.Eng. (Oxon), Ph.D. (Cantab)
University of St Andrews
Response to Feedback
Dear Author, Reviewers, and Editors,
I would like to start by sincerely thanking you for your thorough reading of my letter, and
thoughtful and detailed comments. Needless to say I have taken all of these as constructive
suggestions and criticisms which I sought to address in the present document and the revised
submission.
In this document I address your key comments point by point with, for the sake of readability,
the original review text shown in black ink and my replies in blue.
Author
the author’s description of my paper as a “partial vindication of the p-value” when it is, surely,
clear that it is exactly the opposite of that. In the abstract it say
“Despite decades of warnings, many areas of science still insist on labelling a
result of p < 0.05 as ‘statistically significant’. This practice must contribute to the
lack of reproducibility in some areas of science”.
-<U+2192> The author is confounding two separate issues and hence I beg to disagree with the
first claim above. Indeed the quoted excerpt from the original article supports my claim. The
1
quoted text objects to the manner in which the p-value is used (and the way in which the salient
threshold is chosen) rather than the use of p-value itself. In describing the author’s work I state
exactly this which is why I describe the original article as attempting a partial vindication of p-
value. However, given that I am not a native speaker, I recognize that there may be a difference
in how we understand the verb “to vindicate” (with the author’s interpretation being probably
the correct one, given that he is a native speaker). Hence, in the spirit of good faith and in line
with my argument that as intellectuals we need to accept the possibility of being mistaken, I
have removed this word and rephrased the corresponding sentences (including the title of the
submission itself).
agree with his surmise that this “criticism may come across as frivolous semantic pedantry
of no practical consequence”.
-<U+2192> The author has omitted my explanation of why this is not the case. Hence I am rather
disappointed by this comment which offers no attempt at an intellectual challenge.
The author notes, as a criticism, that “No single hypothesis (including the null) can have a
finite probability associated with it.”. While this is obviously true, it certainly doesn?t invali-
date the idea of the likelihood ratio, which lies at the heart of my argument. We aren?t saying
that the effect size is exactly zero (that would obviously be impossible), We are looking at the
likelihood ratio, i.e. at the probability of the data if the true effect size were not zero relative
to the probability of the data if the effect size were zero.
-<U+2192> The author has missed the essence of this criticism – the null hypothesis is a single point
whereas the cardinality of the set of alternative hypotheses is infinite.
The author says “I demonstrate that the (often implicit) premises of this counterargument
2
are flawed”. Firstly, the assumptions that I make are stated explicitly and discussed at length,
especially in section 3 and appendix A1.
-<U+2192> Some are but some of those that I discuss are not.
war that rages among statisticians that each side says that any views other than their own
and flawed/wrong/silly.
-<U+2192> Although the author does not explicitly say that I did, for the record I want to emphasise
that I neither said (that is, wrote) nor think that his view is “silly”. I am also not inclined to
use the word “wrong” either. As I emphasise throughout the text, the issue at hand is much
more nuanced than that and it deserves to be treated as such.
This tendency to personalise the squabbles
-<U+2192> Again, for the sake of the record, there is absolutely nothing personal in my objections
– they all deal with the substance at hand. Moreover, I neither refer to the author’s person nor
do I ever suggest any intention or malice, but merely what I consider to be methodological or
technical weaknesses in the author’s original article; that is all.
The idea that effects are claimed when none exists (i.e. false positives) lies at the heart of
every discussion of the reproducibility crisis.
-<U+2192> The author is confusing two things. If my condition improves, there certainly is an ef-
fect. The attribution of what caused this effect is what can be erroneous. An example of a false
positive would be an algorithm identifying an animal as a cat whereas it is a dog – in this case
the animal is not a cat.
3
Similar conclusions to mine have been around for a long time.
-<U+2192> They certainly have and this is precisely why I consider it important to expose the readers
to challenge to them. Needless to say, an argument being around for a long time does not imply
its correctness.
The author says “Much like before, Coloquin [sic] leaves the question of how the second step is to
be performed without much elaboration”. It’s true that I take for granted that everyone would
realise that, in the example I used, the best estimate of the effect size would be the difference
between the two observed sample means (the maximum likelihood estimate of the true effect
size). Surely this does not need to be spelled out explicitly?
-<U+2192> I do wish that the author did state this explicitly, yes, for I can indeed think of differ-
ent approaches which one could sensibly take. That being said, if the author did elaborate
along the lines suggested by the reply above, this too, I fear would open avenues for questioning;
however, having not done so in the original article the relevance of this counterfactual scenario
is moot.
The author says (of the point null assumption) that “there is no attempt at justifying the
claimed reasonableness of the aforementioned assumption”. Perhaps he didn?t reach appendix
section A1 in my paper which attempts to justify the assumption at length
-<U+2192> I have read the appendix and the relevant section thereof in detail, and I can find little
in terms of such an attempt. Rather it is what I consider a rather unconvincing appeal to and
claim of agreement by others researchers in the field. If the author disagrees with my view
that this indeed is unconvincing, I can appreciate that, but equally I hope that the author will
4
understand why I feel that a challenge is appropriate.
“As should be clear from my argumentation thus far, this claim fundamentally
does not make sense” (re: “Recently, it was asserted that if we observe a p-value just
below 0.05, then there is a chance of at least 26% that your result is a false positive
[2].”)
I can agree that it might have been better if the word “asserted” had been replaced by “sug-
gested”.
-<U+2192> The issue here is not that of word choice but that which I have already addressed in
my previous replies (re: cardinalities of hypothesis sets).
I have read the paper by Ng, recommended by the author. In my view it is irrelevant to
the simple question that is asked in a two-sample t test, namely is there good evidence that my
results are not just chance? Or, put another way, will I make a fool of myself if I clam that the
intervention works.
-<U+2192> I think that a thorough reading of Ng’s article makes is rather clear why its content is
highly relevant. As regards the author’s bottom line, as a scientist and a philosopher (and in
the present discussion we are fundamentally having to deal with the interface between scien-
tific and practical philosophical points), I do not consider the qualification of making “a fool of
oneself” sufficiently precise. Besides, surely our standards for decision-making should be higher
than that.
having spent the whole article condemning my assumptions
-<U+2192> I would not (and indeed have not) use such a strong word as “condemning” – “challenging”
5
or “questioning” would be more appropriate. Furthermore, given the nature of the submission,
it surely comes with the territory that most of the text is devoted to those aspects of the author’s
article that I disagree with. There are many aspects that I do agree with and I indeed praise
the author in several places for those.
It’s clear that the author wishes everyone to be a subjective Bayesian.
-<U+2192> Again, I would genuinely kindly ask the author not to put words into my mouth, espe-
cially given his expressions of regret on the attitude in the field and the lack of attempt to seek
common ground. My only intention (or “wish”, if I were to rewrite the author’s comment) is to
provide a challenge to the arguments in the author’s original article. I consider the topic to be
highly relevant and interesting, and I think that the readers would benefit from being exposed
to such a challenge, whomever they end up agreeing with. Besides, even if I did wish what
the author asserts (which I repeat, I do not), I do not see what would be wrong with that. Is
not one of the aims of putting forward an argument to try to convince others of its correctness
(another important aim being that of having one’s own beliefs scrutinized and challenged, and
hence possibly changed)?
Reviewer 2
In fact it has often been discussed by trialists also, that whereas one may usually assume (rea-
sonably) that pills are similar enough.
-<U+2192> I am afraid that this argument does not address the issue at hand. Quite on the con-
trary, it leads (or should lead) one to ask what that assumption is based on which in effect gets
us back to the original question. In other words, the reviewer’s argument is underlain by the
so-called “begging the question fallacy”.
6
In fact, in drug regulation the batch to batch variability to which the author refers is a practical
concern that is addressed by a number of guidelines.
-<U+2192> The fallacy at the heart of this comment is the same one as that which my previous
comment addressed, as I trust the reviewer will realize, so for the sake of succinctness I shall
not repeat it.
Furthermore, bioequivalence studies are required to show that generic forms are equivalent to
brand name ones.
-<U+2192> Firstly, I do not see how this comment is relevant in the present context, given that I
never raise the issue of generic vs. brand name drugs (indeed, my argument does not apply only
to drug based treatments only, as I state in the letter). Moreover, those studies too generally
adopt statistical analyses based on the same premises as the author of the present article, which
again does not lead us anywhere productive.
It is, however, a mistake to assume that all problems of equivalence are automatically solved by
Bayesian approaches
-<U+2192> Indeed, that would be a mistake and I make no such assumption or claim. My claim
is much more modest and it is, in brief, that Bayesian approaches allow for the effects of issues
like those discussed to be identified and understood better and with a lesser loss of information
(one of these being the separation of questions of statistical significance and the effect size as
proposed by the original author, and the combined treatment of the two using a Bayesian ap-
proach).
7
I note also that the author has no compunction about searching PubMed for P-values.
-<U+2192> I am rather confused by this comment. “Having no compunction” implies some moral
wrongness, intellectual disingenuity or underhandedness, of an action. Why would that be the
case with using PubMed for obtaining the quoted information?
Reviewer 3
Robust scientific debate is essential, and as such as I welcome both this comment and the paper
it refers to.
-<U+2192> I am genuinely grateful to the reviewer for this comment as it succinctly summarizes my
sentiments and the key reason behind my letter. As I hope I have made clear, I do not consider
the original author’s argument as unreasonable or unsound, merely as one that I disagree with
and which deserves a challenge due to the subtleties of the issues involved. Much like the re-
viewer I too welcome the original paper as I explicitly stated in my submission.
I do however have some concerns about the strength of language
-<U+2192> I do apologize if my writing came across that way – I can assure you that this was not
intended. Hence I really appreciate the reviewer bringing this to my attention, allowing me to
modify the statements which may be thus misunderstood. I have now made significant modifica-
tions to the original text which I trust the reviewer will agree come across milder while making
the key critical points, as surely should be permissible (and welcome) in a sound intellectual
debate.
8
I do not think anyone would . . . think it was an attempt to vindicate p-value as a metric
-<U+2192> As the reviewer said, and I fully agree with that sentiment, we need to be careful with
language here. I do not describe the original article as an attempt to vindicate the p-value but
partially vindicate. This qualification, I believe, alters the spirit of the claim substantially. How-
ever, as I noted in my response to the author, given that I am not a native speaker, I recognize
that there may be a difference in how we understand the verb “to vindicate” (with the author’s
interpretation being probably the correct one, given that he is a native speaker). Hence, in
the spirit of good faith and in line with my argument that as intellectuals we need to accept
the possibility of being mistaken, I have removed this word and rephrased the corresponding
sentences (including the title of the submission itself).
It is important to note that false positives and p-values are not synonymous
-<U+2192> Most certainly they are not, and I trust that nobody in the target audience would think
otherwise (especially after reading the original article, which presumably everybody reading my
contribution would, which explains this at some length).
I would dispute the claim that researcher en masse have ignored the warnings doled out by
the statistical community
-<U+2192> I am not sure why the reviewer feels the need to “dispute” such a claim as I certainly
never made it. Quite in fact, I state that
“. . .recently there has been an increase in the awareness of a variety of concerns
related to the p-value as a statistic itself [2, 5], as well as the manner in which it is
9
employed, interpreted, and understood [3, 6].”
This is a minor point, but it’s fa[i]r to say there’s growing awareness of dubious manipula-
tions across all of biomedical science.
-<U+2192> I do not disagree but issues pertaining to malicious manipulations are beside the central
point of my letter and thus I would rather leave it aside in the present discussion.
In a thought experiment, I could conceive of a perfect manufacturing process where devia-
tions in the final product are negligible. in which case ‘same pill’ is perfectly acceptable.
-<U+2192> I do not disagree in the least. However, such a thought experiment does not reflect the
reality in the crucial aspect under consideration. The key aim of thought experiments is to
extract the very essence of a problem whereas in what the reviewer describes the exact opposite
is done – the key practical difficulty is removed.
The philosophical argument on unknown unknowns does not render Prof Colquhoun wrong,
nor his argument sloppy.
-<U+2192> Again, I have to stress that nowhere in my letter do I make claims like these or even
close – I merely want to highlight some of the author’s often implicit assumptions which are
epistemologically dubious yet rather subtle, and could thus pass unnoticed and unchallenged.
Indeed, in the text I repeatedly emphasise the subtlety of the issues which my critique (intended
as constructive) relates to.
10
By contrast, I agree with many of the observations in section 3 – mechanistic understanding is
absolutely vital to avoid confounding influence, and this is an important aspect. I also agree that
’spike’ is a vague term, though probably well-understood. I do have a query about something
here however – the author states that every possible outcome is consistent with an effect size of
zero. For example, that 1000 heads up is entirely consistent with a fair coin toss – but how so?
The probability of this (0.5)1000 , or ~ 9.33×10E-302, which is extraordinarily low, but non-zero.
-<U+2192> Regarding the first part of the reviewer’s comment, while I can appreciate that a term
can be sensibly deemed vague yet well-understood in colloquial intercourse, the two qualifiers
are contradictory when it comes to writing of a statistical, technical nature. As for the lat-
ter question, I am a bit confused about it for the reviewer answered their own question – the
probability is non-zero, hence there is consistency. How we make further inferences about the
preferred hypothesis from the multitude of consistent hypotheses is a different question and one
of the reasons why I reference Ng’s work.
referring to other statistical discussions is fine, but as Open Science is interdisciplinary, I think
it would be worthwhile to actually expand the discussion and outline what is meant, rather than
delving into technical arguments without any introduction. . .and presume less knowledge on the
reader’s behalf. If this was done, I think this comment could be a very valuable contribution to
the literature.
-<U+2192> I fully agree with the overall sentiment of the reviewer. Not only for articles in Open
Science but generally too, I consider it desirable to make the content readable by as wide of an
audience as possible. At the same time, I hope that the reviewer will agree and appreciate that
the nature of my comments is fundamentally rather technical and hence that deviation from a
certain degree of rigour is not possible if the strength of the argument is to be retained (which,
surely, is crucial). Moreover, my submission directly builds upon the original article so the con-
text is already set. For background, I do reference relevant prior work and discussions; indeed
I praise the original author’s coverage already in Section 2 and advise the reader to refer to it
(both in general and as regards specific explanations). I would suggest that for the present type
of submission, detail pertaining to the context comprehensively discussed in related literature
11
is not desirable either by editors or readers, and that reasonable brevity and succinctness are
appreciated.
It might be worth framing the comment in a less combative way too
-<U+2192> I again wish to use this opportunity to apologize (to the author of the original article
most of all) if my comment came across that way and stress that this was certainly not in-
tended. I only wish that the reviewer had indicated the specific sentences or phrasings in my
original submission so that I can address them directly. Lacking this information, I have gone
through the entire text several times, identified what I could best discern as the potentially dis-
agreeable fragments, and redacted them suitably i.e. softened the wording as much as possible
while attempting to retain the substance of the criticism.
Sincerely yours,
Ognjen Arandjelovic´
School of Computer Science
University of St Andrews
St Andrews
United Kingdom
E-mail: ognjen.arandjelovic@gmail.com
12
Appendix C
Ognjen Arandjelovic´, M.Eng. (Oxon), Ph.D. (Cantab)
University of St Andrews
Response to Feedback
Dear Author, Reviewers, and Editors,
I would like to repeat my gratitude for your time and feedback. All of the changes
suggested have been implemented in the latest revision and the raised questions answered
in the present document. As before, for the sake of readability, the original review text
shown in black ink and my replies in blue.
Author
The argument that two pills can’t be identical seems to me to be absurdly pedantic
-<U+2192> I fear that the author is again making a straw man argument. Nowhere did I argue
the above nor implicitly suggested it. My claim was rather that one cannot ensure that
two pills are identical and this observation goes to the very heart of the epistemological
issue at hand. To dismiss it as “pedantic” is objectionable on many grounds. Firstly, the
nature of such argument itself is intellectually lazy for it could be applied with virtually
no effort or substantive thought to reject any objection one wishes to dismiss. Secondly,
though not unrelated to my previous point, I find it rather puzzling that the “overly
1
pedantic” objection would be used in the context of a discipline which is paved by poten-
tial traps if subtleties are overlooked. As I have stated numerous times, including in the
manuscript, and as correctly highlighted by Reviewer 3, the issue is inherently nuanced
and thus requires suitably careful consideration. Lastly, I feel compelled to say that the
author’s attitude here reflects well an issue which I am rather concerned by – the frequent
lack of recognition within the scientific community of very serious (both with regard to
theoretical foundations and practical application) epistemological issues which underlie
the scientific method.
Another problem is that the arguments are aimed entirely at my 2017 paper. Since
then I have written another in which many of my points have been amplified
-<U+2192> I am genuinely grateful to the author for pointing out his more recent paper which,
as all of his work, I found interesting to read and well argued, and would not hesitate in
recommending to my students or colleagues. However, that does not mean that I agree
with everything in it. Indeed, nothing in the paper addresses the issues which I raise here.
I find that the revised version still comes across as nitpicking and unhelpful, insofar
as the author makes no suggestions about what he thinks should be done about p values.
-<U+2192> But that is not correct – I believe that what I wrote makes it rather clear that
I, much like many others (n.b. not everybody, of course) would advise against their use.
Of course I have a vested interest since it is I am the butt of the author’s criticisms
2
-<U+2192> I regret that I must again take exception to the author’s choice of words. Noth-
ing that I wrote (in the manuscript or in my reply to comments) is aimed at the author
of the original article but the argument therein. To add to this, lest there be any doubt,
I explicitly emphasised this in my previous reply to reviewers and made all suggested
changes which sought to avoid wordings which could be misinterpreted.
Reviewer 2
. . .Colquhoun’s argument is itself based on a Bayesian analysis. . .
-<U+2192> It seems to me that the reviewer if conflating Bayesian inference, which I suggest,
and the use of Bayes’ theorem (or rule). There is a huge difference between the two and
the lack of recognition of this leads the reviewer’s argument astray.
very few if any, as far as I am aware, address Don Rubin’s assumption that ‘there are no
versions of the treatment’. by incorporating this formally into the analysis. Perhaps you
would like to cite a published example that does.
-<U+2192> I am not sure what relevance the reviewer feel this has. I am criticizing a specific
argument on the use, interpretation, etc. of the p-value and the assumptions underlying
it. I never made any claims as regards the contents of what the reviewer is talking about
or the existence or lack thereof of approaches which address them.
3
In fact, it is perfectly possible in a frequentist framework to address the issue of de-
ciding that two sets of treatments that are being compared are not identical without
assuming that therefore that all members of one of any given set being compared are
identical, although one does have to be careful as to one one then can assert if the nully
hypotheses is “rejected”: See the discussion of the analysis of Rofecoxib by Juni et al (2)
in Senn.(3).
-<U+2192> There first point which I would like to make here concerns the fundamental na-
ture of the reviewer’s opening statement, and which suggests that the reviewer does not
appear to appreciate the primary issue at hand. The ‘matter of fact’ claim expressed by
the reviewer makes no sense in the present context. The substance at the crux of the
argument lies in the epistemological basis of a particular approach to inference and the
interpretation of observations, which the reviewer overlooks entirely. If this is done, then
certainly, what is possible becomes a simple matter of fact in a manner similar in which
something can be considered to be true in a certain axiomatic system but fail to have any
relevance to everyday reality. However, the issue that I focus on and draw the readers’
attention to is precisely that which is overlooked by the reviewer.
In addition to the above, as regards the work referenced by the reviewer, these add ab-
solutely nothing at all to the discussion. I do not mean that in a critical way towards
the articles – they were not written with the intention of addressing the issues I raise and
thus quite understandably make no contribution to either side of the debate.
4
Reviewer 3
I note your detailed response to previous reviewers’ comments which are comprehensive,
and on the whole, fair and reasonable.
-<U+2192> I am genuinely very glad to read the reviewer’s kind words and wish to repeat my
gratitude for all the comments, both to the initial submission and the subsequent one,
which have helped me clarify my points and avoid misunderstanding.
My comments are my own form of “frivolous semantic pedantry”!
-<U+2192> I very much appreciate the reviewer’s sense of humour but, lest jest be confused
with seriously made points, let me say I certainly do not consider any of the comments
received to fit the description.
Regarding the use of less combative language, I am unaware of how the original ver-
sion read, but this version is generally tame. However, I suggest “nebulous” may come
across as a little haughty – instead I suggest “vague”.
-<U+2192> I fully understand where the reviewer is coming from and have hence made the
appropriate change. Thank you!
“An experiment resulting in 1000 heads up is entirely consistent with the hypothesis
of fair coin tosses.” I fully agree, although another reviewer gave the exact probability of
this and indicated, though small, it was indeed > 0. I think to strengthen this point, you
5
could quote the probability result and emphasise the fact that although improbable, it is
not impossible, and hence “consistent with the hypothesis of fair coin tosses”.
-<U+2192> Thank you for the suggestion; I have gladly incorporated it into the present revi-
sion.
Typo – “suble” (line 9).
-<U+2192> I am again most grateful to the reviewer for such thorough feedback; the typo has
been corrected.
Overall, the very nature of the article means it rests on nuanced points, which seem
well-argued.
-<U+2192> I would like to thank the reviewer again for the kind comments and the numer-
ous helpful suggestions regarding the manuscript, as well as express gratitude for the
reviewer’s respect for dispassionate debate over intellectual disagreements and the appre-
ciation of importance of the specific issues my manuscript raises.
Sincerely yours,
Ognjen Arandjelovic´
School of Computer Science
University of St Andrews
St Andrews
United Kingdom
E-mail: ognjen.arandjelovic@gmail.com
6
Society Open
