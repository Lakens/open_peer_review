A counterview of `An investigation of the false discovery
rate and the misinterpretation of p-values' by Colquhoun
(2014)
D. Loiselle and R. Ramchandra
Article citation details
R. Soc. open sci. 2: 150217.
http://dx.doi.org/10.1098/rsos.150217
Review timeline
Original submission: 19 May 2015 Note: Reports are unedited and appear as
1st revised submission: 29 June 2015 submitted by the referee. The review history
Final acceptance: 30 June 2015 appears in chronological order.
Review History
label_version_1
RSOS-150217.R0 (Original submission)
label_author_1
Review form: Reviewer 1 (Stephen Senn)
Is the manuscript scientifically sound in its present form?
Yes
Are the interpretations and conclusions justified by the results?
Yes
Is the language acceptable?
No
Is it clear how to access all supporting data?
The graph is based on a theoretical calculation. Supporting data as such are not needed. The
paper cites a number of example P-values. I have not accessed the papers cited.
Do you have any ethical concerns with this paper?
No
© 2015 The Authors. Published by the Royal Society under the terms of the Creative Commons
Attribution License http://creativecommons.org/licenses/by/4.0/, which permits unrestricted use,
provided the original author and source are credited
2
Have you any concerns about statistical analyses in this paper?
No
Recommendation?
label_recommendation_1
Accept with minor revision (please list in comments)
Comments to the Author(s)
label_comment_1
1) You should change your title' 'No need to adjust alpha' makes it seem that it is a contribution
to the literature on multiple testing.
2) I have checked your graph and it seems correct (see Appendix A)
3) I agree that Colquhoun's conclusions are highly dependent on prior assumptions
4) The problem with all such calculations is that the only thing that we know is alpha. We do not
not know beta since, even if (as you assume) it is usually the case that when scientists choose to
test a null hypothesis in favour of a more interesting alternative the null is false, it does not follow
that the parameter has the value they assume in their power calculation, even if, as will not
always be the case, they target a type II error of 0.2.
5) Note that the fact that a conventional type I error rate of 5% does not lead to a high false
positive proportion does not prove that 5% is well-chosen. For example it might be (as I think
your examples suggest) that many significant P-values are much less than 0.05 with those that are
just below 0.05 being rare but having a high false positive probability associated with them. From
a Bayesian pesrpective this might justify using a more stringent conventional level. What you
should emphasise is that your calculations show (given the assumptions) the consequenvces of
significant/not significant at the 5% level and not the consequences of using P-values per se.
6) I have my reservations about your handling of the empirical evidence you produce. There are
many problems with this approach and describing the results as unequivocal is hardlly justified.
You should use much more cautious language.
You may be interested in the following web discussions of Colquhoun's work
http://errorstatistics.com/2015/03/16/stephen-senn-the-pathetic-p-value-guest-post/
and
http://errorstatistics.com/2015/05/09/stephen-senn-double-jeopardy-judge-jeffreys-upholds-
the-law-guest-post/
Stephen Senn, Competence Center for Methodology and Statistics, Luxembourg Institute of
Health, Strassen, Luxembourg
label_author_2
Review form: Reviewer 2 (David Colquhoun)
Is the manuscript scientifically sound in its present form?
No
Are the interpretations and conclusions justified by the results?
No
Is the language acceptable?
Yes
Is it clear how to access all supporting data?
N/A
Do you have any ethical concerns with this paper?
No
3
Have you any concerns about statistical analyses in this paper?
Yes
Recommendation?
label_recommendation_2
Accept with minor revision (please list in comments)
Comments to the Author(s)
label_comment_2
I attach my report (it identifies me, but since I opted for open review, that's presumably not a
problem. (See Appendix A.)
label_author_3
Review form: Reviewer 3 (Marcus Munafo)
Is the manuscript scientifically sound in its present form?
Yes
Are the interpretations and conclusions justified by the results?
Yes
Is the language acceptable?
Yes
Is it clear how to access all supporting data?
Yes.
Do you have any ethical concerns with this paper?
No
Have you any concerns about statistical analyses in this paper?
No
Recommendation?
label_recommendation_3
Accept with minor revision (please list in comments)
Comments to the Author(s)
label_comment_3
I enjoyed reading this commentary. Although I would come to very different conclusions to the
authors, and my views align more closely with those of Prof Colquhoun, in a sense this is the
point of the commentary - models depend on their inputs and all of us would choose slightly
different inputs.
Therefore, I think this commentary should be published as long as Prof Colquhoun is offered the
opportunity to respond, because there is clearly a range of views on the issues raised by the
original article to which this commentary is responding, and these should be discussed openly. I
also imagine that the response will be similarly entertaining.
I can think of one analysis that would strengthen the commentary, which would be to take the p-
values from the articles surveyed (Appendix I) and apply a p-curve analysis to these, to
determine: 1) whether or not they contain evidential value (they almost certainly do), and 2)
whether there is any evidence of publication bias. The app for this analysis is available here:
http://www.p-curve.com
Although I would not suggest other substantial changes (because that would detract from the
engaging tone of the commentary) I would draw the authors' attention to:
4
Bones, A.K. (2012). We Knew the Future All Along : Scientific Hypothesizing is Much More
Accurate Than Other Forms of Precognition - A Satire in One Part. Perspectives on Psychological
Science, 7, 307.
Fanelli, D. (2010). "Positive" Results Increase Down the Hierarchy of the Sciences. PLoS ONE, 5,
e10068.
Kerr, N.L. (1998). HARKing: Hypothesizing After the Results are Known. Personality and Social
Psychology Review, 2, 196-217.
label_end_comment
Decision letter (RSOS-150217)
09-Jun-2015
Dear Dr Loiselle,
The Subject Editor assigned to your paper ("No Need to Adjust alpha: a comment on “An
Investigation of the False Discovery Rate and the Misinterpretation of p-Values” (D Colquhoun,
2014)") has now received comments from reviewers. We would like you to revise your paper in
accordance with the referee and Subject Editor suggestions which can be found below (not
including confidential reports to the Editor). Please note this decision does not guarantee
eventual acceptance.
Please submit a copy of your revised paper within three weeks (i.e. by the 02-Jul-2015). If we do
not hear from you within this time then it will be assumed that the paper has been withdrawn. In
exceptional circumstances, extensions may be possible if agreed with the Editorial Office in
advance.We do not allow multiple rounds of revision so we urge you to make every effort to
fully address all of the comments at this stage. If deemed necessary by the Editors, your
manuscript will be sent back to one or more of the original reviewers for assessment. If the
original reviewers are not available we may invite new reviewers.
To revise your manuscript, log into http://mc.manuscriptcentral.com/rsos and enter your
Author Centre, where you will find your manuscript title listed under "Manuscripts with
Decisions." Under "Actions," click on "Create a Revision." Your manuscript number has been
appended to denote a revision. Revise your manuscript and upload a new version through your
Author Centre.
When submitting your revised manuscript, you must respond to the comments made by the
referees and upload a file "Response to Referees" in "Section 6 - File Upload". Please use this to
document how you have responded to the comments, and the adjustments you have made. In
order to expedite the processing of the revised manuscript, please be as specific as possible in
your response.
In addition to addressing all of the reviewers' and editor's comments please also ensure that your
revised manuscript contains the following sections before the reference list:
• Ethics statement
If your study uses humans or animals please include details of the ethical approval received,
including the name of the committee that granted approval. For human studies please also detail
whether informed consent was obtained. For field studies on animals please include details of all
permissions, licences and/or approvals granted to carry out the fieldwork.
5
• Data accessibility
It is a condition of publication that all supporting data are made available either as
supplementary information or preferably in a suitable permanent repository. The data
accessibility section should state where the article's supporting data can be accessed. This section
should also include details, where possible of where to access other relevant research materials
such as statistical tools, protocols, software etc can be accessed. If the data has been deposited in
an external repository this section should list the database, accession number and link to the DOI
for all data from the article that has been made publicly available. Data sets that have been
deposited in an external repository and have a DOI should also be appropriately cited in the
manuscript and included in the reference list.
• Competing interests
Please declare any financial or non-financial competing interests, or state that you have no
competing interests.
• Authors’ contributions
All submissions, other than those with a single author, must include an Authors’ Contributions
section which individually lists the specific contribution of each author. The list of Authors
should meet all of the following criteria; 1) substantial contributions to conception and design, or
acquisition of data, or analysis and interpretation of data; 2) drafting the article or revising it
critically for important intellectual content; and 3) final approval of the version to be published.
All contributors who do not meet all of these criteria should be included in the
acknowledgements.
We suggest the following format:
AB carried out the molecular lab work, participated in data analysis, carried out sequence
alignments, participated in the design of the study and drafted the manuscript; CD carried out
the statistical analyses; EF collected field data; GH conceived of the study, designed the study,
coordinated the study and helped draft the manuscript. All authors gave final approval for
publication.
• Acknowledgements
Please acknowledge anyone who contributed to the study but did not meet the authorship
criteria.
• Funding statement
Please list the source of funding for each author.
Once again, thank you for submitting your manuscript to Royal Society Open Science and I look
forward to receiving your revision. If you have any questions at all, please do not hesitate to get
in touch.
Yours sincerely,
Emilie Aime
Senior Publishing Editor, Royal Society Open Science
openscience@royalsociety.org
Author's Response to Decision Letter for (RSOS-150217)
See Appendix B.
Appendix A
label_version_2
RSOS report Loiselle & Ramchandra
thank the authors for their interest.
see two main points of contention in their comments.
(1) The idea that you can estimate prevalence by looking at published work seems to
me not to be plausible. The “file drawer” effect is a well-known source of bias, and
who knows how many experiments have been done but never publish because they
“failed”. The people you spoke to may have been suffering from confirmation bias
(or just plain old hubris). And I wonder how many of the studies you mention were
adequately powered.
Put another way, how would one react when reviewing a paper that claimed to have
made a discovery on the basis of a statistical test that was based on the premise that
he finding was very likely to be true? I think that most people would think that to be
otally unacceptable –it would be laughed out of court. Of course it was not strictly
right to describe the FDR of 26% as a minimum: it’s the minimum given that no
reasonable referee would allow a test to be published if it assumed a prevalence
greater than 50%.
No doubt the appropriate value would be dependent on the field. As I pointed out,
0.1 would be very optimistic for testing of drug candidates. In other fields it could be
higher, but I see no compelling case for assuming anything above 0.5.
The authors have also not taken into account the amount of published work that is
under-powered, a problem that makes the FDR much higher. For example, powers
as low as 0.2 are not uncommon and that gives an FDR of 20% by the tree diagram
approach, not 6% for a prevalence of 0.5.
(2) More importantly, the authors seem to have ignored the second half of my paper
(section 10 onwards). I pointed out there that the FDR is only 6% for a prevalence of
0.5, if you look at all P = 0.05. But there is wide consensus among statisticians that
f you wish to interpret an observation of P = 0.047, then you should look only at tests
hat give P = 0.047 (or, in practice, close to 0.047; I used 0.045 – 0.05). If you
accept this, then you find that, even with a prevalence of 0.5, you get an FDR of 26%
(not 6%). That’s the smallest FDR you can get if you don’t allow any prevalence
greater than 0.5. So I would maintain that it’s not true to say “if a scientist makes
hypotheses that are correct at least 55% of the time, then he or she is, in fact,
already working at the commonly-assumed ‘significance’ level of 0.05”
Of course you may not agree that it’s appropriate to look only at P close to 0.047
when trying to interpret an observation of P = 0.047. But there is a consensus among
statisticians that this is the appropriate way to look at the problem, so I think that if
you don’t agree you’d have to lay out your reasons in detail.
You conclude that “we find it difficult to imagine how science could have achieved its
manifold successes if scientists have been wrong 90% of the time”. Perhaps you
should argue that out with John Ioannidis, who would not agree (“most published
results are wrong”). Apart from that, there are many reasons why it should not be as
high as 90%. For a start, many reported P values are well below 0.05, so (other
hings being equal) the FDR would be smaller than the case that I deal with. Also,
many of the “manifold successes” come from physics in which a 5 sigma rule is
common, far more rigorous than the 3 sigma rule that I suggest at the end.
Thus, though your comment is interesting, I think that that the two problems that I
have mentioned need attention before it’s published.
Appendix B
Response to Reviewers
n responding to the Reviewers’ comments and suggestions, we wish to make clear our aspiration to
retain the focus of our manuscript squarely on our divergent view of the appropriate magnitude of
the ‘prevalence’ parameter (0.1) adopted by Professor Colquhoun. This stance is guided, too, by the
generous comment of Reviewer 2.
Reviewers' Comments to Author:
Reviewer: 1
We wish to commence by thanking Professor Colquhoun for identifying himself and for his
generosity in making his R-code programme available.
We also wish to clarify that we were not so much ignoring the second part of your paper as choosing
to focus on our principal point of disagreement: namely, the magnitude of the ‘prevalence’
parameter, as noted, above.
Comments to the Author(s)
attach my report (it identifies me, but since I opted for open review, that's presumably not a
problem.
label_version_3
RSOS report Loiselle & Ramchandra
thank the authors for their interest.
see two main points of contention in their comments.
(1) The idea that you can estimate prevalence by looking at published work seems to
me not to be plausible. The “file drawer” effect is a well-known source of bias, and
who knows how many experiments have been done but never publish because they
“failed”. The people you spoke to may have been suffering from confirmation bias
(or just plain old hubris). And I wonder how many of the studies you mention were
adequately powered.
t is not impossible that our senior colleagues have file-drawers full of unpublished ‘failed’
experiments and perhaps many suffer from ‘confirmation bias’ (though not likely ‘hubris’ since a less
egotistical gaggle of academics is unlikely to be found).
More seriously, we are troubled by the implication that, in any given study in which n is of modest
magnitude, it is likely that a Type I Error has occurred. Perhaps our colleagues (as well as 22 of the 25
authors whose papers we examined) were consistently lucky in choosing interventions with large
effect sizes such that a small number of observations sufficed. For any particular study in which the
Null Hypothesis was rejected, despite small n, either a Type I Error was committed or the effect size
was large. Which interpretation is correct must await corroborative studies.
Put another way, how would one react when reviewing a paper that claimed to have
made a discovery on the basis of a statistical test that was based on the premise that
he finding was very likely to be true? I think that most people would think that to be
otally unacceptable –it would be laughed out of court. Of course it was not strictly
right to describe the FDR of 26% as a minimum: it’s the minimum given that no
reasonable referee would allow a test to be published if it assumed a prevalence
greater than 50%.
But should an untested premise (regardless of a Reviewer’s foresight or after-sight) remain
untested? Under that view, the Michelson-Morley experiment would likely not ever have been
performed. Does this point-of-view disallow (or, at least, downgrade the value of) studies of a
confirmatory nature? For example, there have been numerous ingenious experiments designed to
test the validity of the integer nature of Newton’s inverse square law of gravitational attraction
between two bodies. [For a review see Adelberger et al.(2003) Tests of the gravitational inverse-
square law. Annual Review of Nuclear and Particle Science 53: 77-121).] Should such studies be
aughed out of court on the reasonable grounds that Newton showed that the value was ‘2’
centuries ago?
We sense support for our position in statements by Ioannidis (2013) “Why most published research
findings are false”. After defining the variable R as the ratio of the number of true relationships to no
relationships among those tested in the field, he writes (page 46): “Before running an experiment,
nvestigators should consider what they believe the chances are that they are testing a true, rather
than a non-true, relationship. Speculated high R values then may be ascertained sometimes.
[Subsequently] … large studies with minimal bias should be performed on research findings that are
considered relatively established [our emphasis] to see how often they are indeed confirmed.”
No doubt the appropriate value would be dependent on the field. As I pointed out,
0.1 would be very optimistic for testing of drug candidates. In other fields it could be
higher, but I see no compelling case for assuming anything above 0.5.
We concur that the appropriate value of prevalence is dependent on the subject field. And we will
readily concede that a value of 0.1 may be appropriate (or even an over-estimate) for large
epidemiological studies, including ‘blunderbuss’ drug-testing, as well as for gene-association studies,
given that as long ago as 2001, in excess of 106 gene variant sites had been identified in the human
genome (The International SNP Map Working Group, Nature 409, 928-933, 2001). [Please note that
we neglected to include gene-association studies in our list of ‘rejected topics’. This inadvertent
omission has now been rectified in our revised manuscript.]
The authors have also not taken into account the amount of published work that is
under-powered, a problem that makes the FDR much higher. For example, powers
as low as 0.2 are not uncommon and that gives an FDR of 20% by the tree diagram
approach, not 6% for a prevalence of 0.5.
Here we return to our principal point-of-departure from Professor Colquhoun’s position. We
contend that we have (admittedly somewhat idiosyncratic) evidence that scientists (including those
of sufficient brilliance to produce research that spawns a Journal of Physiology Classic) correctly
hypothesise a high proportion of outcomes arising from sufficiently powered experiments - for the
reasons presented in the final sentence of our concluding paragraph. While it is impossible to
ascertain accurately the true prevalence of a real effect, all that we can state is that an increasing
evel of prevalence will decrease the FDR.
(2) More importantly, the authors seem to have ignored the second half of my paper
(section 10 onwards). I pointed out there that the FDR is only 6% for a prevalence of
0.5, if you look at all P = 0.05. But there is wide consensus among statisticians that
f you wish to interpret an observation of P = 0.047, then you should look only at tests
hat give P = 0.047 (or, in practice, close to 0.047; I used 0.045 – 0.05). If you
accept this, then you find that, even with a prevalence of 0.5, you get an FDR of 26%
(not 6%). That’s the smallest FDR you can get if you don’t allow any prevalence
greater than 0.5. So I would maintain that it’s not true to say “if a scientist makes
hypotheses that are correct at least 55% of the time, then he or she is, in fact,
already working at the commonly-assumed ‘significance’ level of 0.05”
Of course you may not agree that it’s appropriate to look only at P close to 0.047
when trying to interpret an observation of P = 0.047. But there is a consensus among
statisticians that this is the appropriate way to look at the problem, so I think that if
you don’t agree you’d have to lay out your reasons in detail.
Please refer to the results of our simulation now included as Appendix III.
You conclude that “we find it difficult to imagine how science could have achieved its
manifold successes if scientists have been wrong 90% of the time”. Perhaps you
should argue that out with John Ioannidis, who would not agree (“most published
results are wrong”).
Dr Ioannidis also focusses on low-prevalence areas of research: in particular, gene-association
studies. See, for example, Ioannidis (2003) Genetic associations: False or true? Trends in Molecular
Medicine 9: 135-138, the Abstract of which commences memorably:
“Genetic association studies for multigenetic diseases are like fishing for the truth in a sea of
trillions of candidate analyses. Red herrings are unavoidably common…”
Whereas the fields of epidemiology and gene-association are as legitimate as any other areas of
scientific endeavour, to us they seem to have a fundamentally different flavour from the tightly
controlled, rigorously-measured, laboratory-based investigations that characterise
experimentally-testable hypotheses. In this regard, we draw attention to the admonition of Leek
and Peng (2015), biostatisticians who emphasise the many critical steps in the data analysis
pipeline’ that precede the application of the p-value:
“Statisticians and the people they teach and collaborate with need to stop arguing about
P values, and prevent the rest of the iceberg from sinking science.
Apart from that, there are many reasons why it should not be as high as 90%. For a
start, many reported P values are well below 0.05, so (other things being equal) the FDR
would be smaller than the case that I deal with.
Also, many of the “manifold successes” come from physics in which a 5 sigma rule is
common, far more rigorous than the 3 sigma rule that I suggest at the end.
We cannot speak to the commonness of a 5 sigma rule in Physics, but we note that in the publication by
Adelberger et al. (referenced above), there are only two Figures in which measured values are presented and
both adopt 95% Confidence Limits.
Thus, though your comment is interesting, I think that that the two problems that I
have mentioned need attention before it’s published.
Finally (and with sincere hope that Professor Colquhoun will not take offence nor infer that we have inflated
egos), we note that his response, above, twice refers to ‘consensus among statisticians’. Our reading of the
iterature suggests that the sole consensus among statisticians is the lack thereof. And that reminds us of
Einstein’s comment on ‘consensus’: “One would suffice”, when informed that a book had just been published
entitled One Hundred Authors against Einstein: [Hundert Autoren Gegen Einstein (1931); Editors Israel,
Ruckhaber & Weinmann, R Voightländers Verlag, Leipzig.]
Reviewer: 2
Comments to the Author(s)
enjoyed reading this commentary. Although I would come to very different conclusions to the
authors, and my views align more closely with those of Prof Colquhoun, in a sense this is the point of
the commentary - models depend on their inputs and all of us would choose slightly different inputs.
We are heartened by the Reviewer’s support for open debate and open publication.
Therefore, I think this commentary should be published as long as Prof Colquhoun is offered the
opportunity to respond, because there is clearly a range of views on the issues raised by the original
article to which this commentary is responding, and these should be discussed openly. I also imagine
that the response will be similarly entertaining.
can think of one analysis that would strengthen the commentary, which would be to take the p-
values from the articles surveyed (Appendix I) and apply a p-curve analysis to these, to determine: 1)
whether or not they contain evidential value (they almost certainly do), and 2) whether there is any
evidence of publication bias. The app for this analysis is available here: http://www.p-curve.com
We are grateful to the Reviewer for drawing this work (with which we were previously unfamiliar) to
our attention and looked forward eagerly to the quantitative analysis proffered by its authors.
Regrettably, the p-curve is aimed at post-hoc investigation of a single hypothesis tested by multiple
authors, whereas our ‘literature survey’ examined a variety of unrelated hypotheses addressed by
multiple authors. Thus, whereas the appropriate subject matter for the analysis by Simonsohn et al.
2014) could be characterised as ‘meta-analyses’, ours is mere ‘mono-analysis’.
Although I would not suggest other substantial changes (because that would detract from the
engaging tone of the commentary) I would draw the authors' attention to:
Bones, A.K. (2012). We Knew the Future All Along : Scientific Hypothesizing is Much More Accurate
Than Other Forms of Precognition - A Satire in One Part. Perspectives on Psychological Science, 7,
307.
Fanelli, D. (2010). "Positive" Results Increase Down the Hierarchy of the Sciences. PLoS ONE, 5,
e10068.
Kerr, N.L. (1998). HARKing: Hypothesizing After the Results are Known. Personality and Social
Psychology Review, 2, 196-217.
We greatly enjoyed Bones’ “Satire in One Part” (2012) and much admire the clarity of the “Anti-
HARKING” article by Kerr (1998).
With respect to Fanelli (2010), since in our view “Physiology is the ‘Physics of Biology’”, we console
ourselves with the notion that we are not at the bottom of the science totem pole. Nevertheless, we
remain chastened by J Ross MacDonald’s editorial of nearly five decades ago: “Are the data worth
owning?”, Science 176 (4042): 1377 (1972), where he lamented the distressing fact that sequential
estimates (by astrophysicists) of the mean distance from sun to earth lay outside the standard errors
of values reported both earlier and later by others in the field.
Reviewer: 3
Comments to the Author(s)
1) You should change your title' 'No need to adjust alpha' makes it seem that it is a contribution to
the literature on multiple testing.
Title changed.
2) I have checked your graph and it seems correct (see attached)
Thank you for taking the time to perform an independent check of our graph using Mathcad
software. That is an unusually generous undertaking by a Reviewer and we are grateful.
3) I agree that Colquhoun's conclusions are highly dependent on prior assumptions
We hope that we have convinced Professor Colquhoun, as well.
4) The problem with all such calculations is that the only thing that we know is alpha. We do not not
know beta since, even if (as you assume) it is usually the case that when scientists choose to test a
null hypothesis in favour of a more interesting alternative the null is false, it does not follow that the
parameter has the value they assume in their power calculation, even if, as will not always be the
case, they target a type II error of 0.2.
We concur. We have tried to emphasize the point that such calculations are dependent on the input
parameters which, for the most part, we can only assume.
5) Note that the fact that a conventional type I error rate of 5% does not lead to a high false positive
proportion does not prove that 5% is well-chosen. For example it might be (as I think your examples
suggest) that many significant P-values are much less than 0.05 with those that are just below 0.05
being rare but having a high false positive probability associated with them. From a Bayesian
pesrpective this might justify using a more stringent conventional level. What you should emphasise
s that your calculations show (given the assumptions) the consequenvces of significant/not
significant at the 5% level and not the consequences of using P-values per se.
We agree that our points do not necessarily provide justification for choosing a value of 5% for the
Type I error rate. We have tried to emphasize that our calculations merely reflect the consequences
of differing values of prevalence of a real effect on the false discovery rate using a 5% significance
evel.
6) I have my reservations about your handling of the empirical evidence you produce. There are
many problems with this approach and describing the results as unequivocal is hardlly justified. You
should use much more cautious language.
We have both toned-down our language, as indicated in several places (trusting that it is now
sufficiently cautious). In addition, we have included a list of potential ‘sins’ in our work in the 3rd
sentence of the 4th paragraph of the text.
You may be interested in the following web discussions of Colquhoun's work
http://errorstatistics.com/2015/03/16/stephen-senn-the-pathetic-p-value-guest-post/
and
http://errorstatistics.com/2015/05/09/stephen-senn-double-jeopardy-judge-jeffreys-upholds-the-
aw-guest-post/
Stephen Senn, Competence Center for Methodology and Statistics, Luxembourg Institute of Health,
Strassen, Luxembourg
We have been reading the web discussions of Professor Colquhoun’s work and marvel at the
enviable depth of knowledge and equally enviable turn of phrase of many of its contributors.
Society Open
